{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives and Loss Functions | \u0627\u0644\u0645\u0634\u062a\u0642\u0627\u062a \u0648\u062f\u0648\u0627\u0644 \u0627\u0644\u062e\u0633\u0627\u0631\u0629\n",
    "\n",
    "## \ud83d\udcda Prerequisites (What You Need First) | \u0627\u0644\u0645\u062a\u0637\u0644\u0628\u0627\u062a \u0627\u0644\u0623\u0633\u0627\u0633\u064a\u0629\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- \u2705 **Module 01: Linear Algebra** - Understand vectors and matrices\n",
    "- \u2705 **Example 1: Vectors and Matrices** - Understand data representation\n",
    "- \u2705 **Basic calculus**: Understanding of slopes and rates of change\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why derivatives matter\n",
    "- Understanding gradient computation\n",
    "- Using derivatives for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udd17 Where This Notebook Fits | \u0645\u0643\u0627\u0646 \u0647\u0630\u0627 \u0627\u0644\u062f\u0641\u062a\u0631\n",
    "\n",
    "**This is the FIRST example of Module 02** - foundation for all optimization!\n",
    "\n",
    "**Why this example FIRST?**\n",
    "- **Before** you can compute gradients, you need to understand derivatives\n",
    "- **Before** you can optimize models, you need to understand loss functions\n",
    "- **Before** you can train neural networks, you need gradient descent\n",
    "\n",
    "**Builds on**: Module 01 (vectors and matrices)\n",
    "\n",
    "**Leads to**: \n",
    "- \ud83d\udcd3 Example 2: Gradients (needs derivative understanding)\n",
    "- \ud83d\udcd3 Example 3: Gradient Descent (needs gradient understanding)\n",
    "- \ud83d\udcd3 Module 03: Optimization (uses gradients from this module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Story: Understanding Before Using | \u0627\u0644\u0642\u0635\u0629: \u0627\u0644\u0641\u0647\u0645 \u0642\u0628\u0644 \u0627\u0644\u0627\u0633\u062a\u062e\u062f\u0627\u0645\n",
    "\n",
    "Imagine you're hiking and want to find the lowest point. **Before** you can find it, you need to know which direction is downhill. **After** understanding derivatives, you know the direction of steepest descent - exactly what gradient descent uses!\n",
    "\n",
    "Same with machine learning: **Before** training models, we need to know which direction to adjust parameters. **After** understanding derivatives, we can minimize loss functions and train models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why This Concept Matters | \u0644\u0645\u0627\u0630\u0627 \u064a\u0647\u0645 \u0647\u0630\u0627 \u0627\u0644\u0645\u0641\u0647\u0648\u0645\u061f\n",
    "\n",
    "### Why Derivatives Matter in ML\n",
    "\n",
    "**WHY** are derivatives essential for machine learning?\n",
    "\n",
    "1. **Finding Minimum Loss**:\n",
    "   - **WHY**: ML models are trained by minimizing loss functions\n",
    "   - **HOW**: Derivatives tell us which direction decreases loss\n",
    "   - **AFTER**: Enables model training\n",
    "\n",
    "2. **Optimization Direction**:\n",
    "   - **WHY**: Need to know which way to adjust parameters\n",
    "   - **HOW**: Derivative = slope = direction of change\n",
    "   - **AFTER**: Can optimize efficiently\n",
    "\n",
    "3. **Neural Network Training**:\n",
    "   - **WHY**: Backpropagation relies entirely on derivatives\n",
    "   - **HOW**: Chain rule computes derivatives through layers\n",
    "   - **AFTER**: Enables deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives | \u0623\u0647\u062f\u0627\u0641 \u0627\u0644\u062a\u0639\u0644\u0645\n",
    "1. Understand what derivatives represent\n",
    "2. Compute derivatives of loss functions\n",
    "3. See how derivatives guide optimization\n",
    "4. Visualize loss functions and derivatives\n",
    "5. Understand the connection to gradient descent\n",
    "6. Apply to real ML scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('\u2705 Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setting the Scene | \u0627\u0644\u062c\u0632\u0621 \u0627\u0644\u0623\u0648\u0644: \u0625\u0639\u062f\u0627\u062f \u0627\u0644\u0645\u0634\u0647\u062f\n",
    "\n",
    "**BEFORE**: We have loss functions but don't know how to minimize them.\n",
    "\n",
    "**AFTER**: We'll understand how derivatives guide us to minimize loss!\n",
    "\n",
    "**Why this matters**: Derivatives are the foundation of all ML optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Derivatives | \u0627\u0644\u062e\u0637\u0648\u0629 2: \u0641\u0647\u0645 \u0627\u0644\u0645\u0634\u062a\u0642\u0627\u062a\n",
    "\n",
    "**BEFORE**: We have a loss function but don't know how to minimize it.\n",
    "\n",
    "**AFTER**: We'll understand how derivatives tell us the direction to minimize loss!\n",
    "\n",
    "**Why derivatives?** They tell us the slope - positive slope means increasing (move left), negative slope means decreasing (move right)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 1: Understanding Derivatives\n",
    "# WHY: Derivatives tell us which direction to minimize loss\n",
    "# HOW: Derivative = rate of change = slope\n",
    "\n",
    "def loss_function(x):\n",
    "    \"\"\"A simple loss function: f(x) = x\u00b2 + 3x + 2\"\"\"\n",
    "    return x**2 + 3*x + 2\n",
    "\n",
    "def loss_derivative(x):\n",
    "    \"\"\"Derivative: f'(x) = 2x + 3\"\"\"\n",
    "    return 2*x + 3\n",
    "\n",
    "# Evaluate at different points\n",
    "x_values = [0, 1, 2, 3]\n",
    "print(\"Example 1: Understanding Derivatives\")\n",
    "print(\"=\" * 60)\n",
    "for x in x_values:\n",
    "    loss = loss_function(x)\n",
    "    deriv = loss_derivative(x)\n",
    "    direction = \"decreasing (move right)\" if deriv < 0 else \"increasing (move left)\"\n",
    "    print(f\"At x = {x}:\")\n",
    "    print(f\"  - Loss: {loss:.2f}\")\n",
    "    print(f\"  - Derivative: {deriv:.2f}\")\n",
    "    print(f\"  - Function is {direction}\")\n",
    "    print(f\"  - \ud83d\udca1 WHY: Derivative tells us direction to minimize!\")\n",
    "    print(f\"  - \ud83d\udca1 HOW: Negative derivative \u2192 move right, Positive \u2192 move left\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Derivatives and Loss Minimization | \u0627\u0644\u062e\u0637\u0648\u0629 3: \u0627\u0644\u0645\u0634\u062a\u0642\u0627\u062a \u0648\u062a\u0642\u0644\u064a\u0644 \u0627\u0644\u062e\u0633\u0627\u0631\u0629\n",
    "\n",
    "**BEFORE**: We understand derivatives but don't see how they minimize loss.\n",
    "\n",
    "**AFTER**: We'll see how derivatives guide us to the minimum of loss functions!\n",
    "\n",
    "**Why this matters?** This is exactly how ML models learn - by following derivatives to minimize loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 2: Using Derivatives to Find Minimum\n",
    "# WHY: We want to find x that minimizes loss\n",
    "# HOW: Set derivative to zero (slope = 0 at minimum)\n",
    "\n",
    "# Find minimum analytically\n",
    "# f'(x) = 2x + 3 = 0\n",
    "# x = -3/2 = -1.5\n",
    "\n",
    "optimal_x = -1.5\n",
    "optimal_loss = loss_function(optimal_x)\n",
    "\n",
    "print(\"Example 2: Finding the Minimum\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Optimal x (where derivative = 0): {optimal_x}\")\n",
    "print(f\"Minimum loss: {optimal_loss:.2f}\")\n",
    "print(f\"\\n\ud83d\udca1 WHY: At minimum, derivative = 0 (no slope)\")\n",
    "print(f\"\ud83d\udca1 HOW: Solve f'(x) = 0 to find optimal point\")\n",
    "print(f\"\ud83d\udca1 AFTER: This is the goal of optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udcca Visualization | \u0627\u0644\u062a\u0635\u0648\u0631\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize loss function and derivative\n",
    "x_range = np.linspace(-3, 2, 100)\n",
    "loss_values = [loss_function(x) for x in x_range]\n",
    "deriv_values = [loss_derivative(x) for x in x_range]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss function\n",
    "ax1.plot(x_range, loss_values, 'b-', linewidth=2, label='Loss Function')\n",
    "ax1.axvline(x=-1.5, color='r', linestyle='--', linewidth=2, label='Minimum (x=-1.5)')\n",
    "ax1.scatter([-1.5], [loss_function(-1.5)], color='red', s=100, zorder=5)\n",
    "ax1.set_xlabel('Parameter Value (x)', fontsize=12)\n",
    "ax1.set_ylabel('Loss Value', fontsize=12)\n",
    "ax1.set_title('Loss Function', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Derivative\n",
    "ax2.plot(x_range, deriv_values, 'g-', linewidth=2, label=\"Derivative f'(x)\")\n",
    "ax2.axhline(y=0, color='k', linestyle='-', linewidth=1, alpha=0.3)\n",
    "ax2.axvline(x=-1.5, color='r', linestyle='--', linewidth=2, label='Where derivative = 0')\n",
    "ax2.scatter([-1.5], [0], color='red', s=100, zorder=5)\n",
    "ax2.set_xlabel('Parameter Value (x)', fontsize=12)\n",
    "ax2.set_ylabel(\"Derivative Value\", fontsize=12)\n",
    "ax2.set_title('Derivative (Slope)', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Visualization:\")\n",
    "print(\"  - Left: Loss function with minimum marked\")\n",
    "print(\"  - Right: Derivative showing where it equals zero\")\n",
    "print(\"  - This is HOW derivatives help us find minima!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: AFTER - Results and Implications | \u0627\u0644\u062c\u0632\u0621 \u0627\u0644\u062b\u0627\u0644\u062b: \u0628\u0639\u062f - \u0627\u0644\u0646\u062a\u0627\u0626\u062c \u0648\u0627\u0644\u0622\u062b\u0627\u0631\n",
    "\n",
    "### What This Enables\n",
    "\n",
    "**AFTER** understanding derivatives:\n",
    "\n",
    "1. **Model Training**: Can minimize loss functions\n",
    "2. **Optimization**: Can find optimal parameters\n",
    "3. **Neural Networks**: Ready to understand backpropagation\n",
    "4. **Next Steps**: Learn gradients (Example 2) and gradient descent (Example 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udfaf Summary: What We Learned | \u0627\u0644\u0645\u0644\u062e\u0635: \u0645\u0627 \u062a\u0639\u0644\u0645\u0646\u0627\u0647\n",
    "\n",
    "**BEFORE this notebook**: We didn't understand how to minimize loss functions.\n",
    "\n",
    "**AFTER this notebook**: We can:\n",
    "- \u2705 Understand WHY derivatives matter for ML\n",
    "- \u2705 Compute HOW derivatives work\n",
    "- \u2705 See what happens AFTER (optimization direction)\n",
    "\n",
    "**Next Steps**: \n",
    "- \ud83d\udcd3 Example 2: Gradients (extend to multiple variables)\n",
    "- \ud83d\udcd3 Example 3: Gradient Descent (use derivatives for optimization)\n",
    "\n",
    "---\n",
    "\n",
    "## \u2705 Example Complete! | \u0627\u0643\u062a\u0645\u0644 \u0627\u0644\u0645\u062b\u0627\u0644!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}