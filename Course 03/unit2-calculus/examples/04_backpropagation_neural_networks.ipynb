{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Backpropagation in Neural Networks\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Program backpropagation in neural networks using differentiation techniques\n",
    "- Understand the chain rule in neural network context\n",
    "- Implement forward and backward propagation from scratch\n",
    "- Apply backpropagation to train a simple neural network\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of derivatives and gradients\n",
    "- âœ… Understanding of multivariate calculus and chain rule\n",
    "- âœ… Basic understanding of neural networks\n",
    "- âœ… Python and NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 03, Unit 2**:\n",
    "- Programming backpropagation in neural networks using differentiation techniques\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Backpropagation** is the algorithm that enables training of neural networks by computing gradients of the loss function with respect to all network parameters using the chain rule of calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Inputs & ðŸ“¤ Outputs | Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø®Ø±Ø¬Ø§Øª\n",
    "\n",
    "**Inputs:** What we use in this notebook\n",
    "\n",
    "- Libraries and concepts as introduced in this notebook; see prerequisites and code comments.\n",
    "\n",
    "**Outputs:** What you'll see when you run the cells\n",
    "\n",
    "- Printed results, figures, and summaries as shown when you run the cells.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nProgramming Backpropagation in Neural Networks\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple Neural Network with Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Simple Neural Network with Backpropagation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of ReLU\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Loss function\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"Mean Squared Error loss\"\"\"\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def mse_loss_derivative(y_pred, y_true):\n",
    "    \"\"\"Derivative of MSE loss\"\"\"\n",
    "    return 2 * (y_pred - y_true) / len(y_true)\n",
    "\n",
    "print(\"\\nâœ… Activation and loss functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2-layer neural network class with backpropagation\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        np.random.seed(42)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    def backward(self, X, y, y_pred):\n",
    "        m = X.shape[0]\n",
    "        dz2 = mse_loss_derivative(y_pred, y) * sigmoid_derivative(self.z2)\n",
    "        dW2 = (self.a1.T @ dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * sigmoid_derivative(self.z1)\n",
    "        dW1 = (X.T @ dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        return dW1, db1, dW2, db2\n",
    "    def update_weights(self, dW1, db1, dW2, db2):\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "    def train(self, X, y, epochs=1000):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = mse_loss(y_pred, y)\n",
    "            losses.append(loss)\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y, y_pred)\n",
    "            self.update_weights(dW1, db1, dW2, db2)\n",
    "        return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train network on XOR problem\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example: XOR Problem\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "print(\"\\nTraining data:\")\n",
    "print(\"X:\", X_xor)\n",
    "print(\"y:\", y_xor.flatten())\n",
    "\n",
    "# Create and train network\n",
    "nn = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)\n",
    "print(\"\\nTraining network...\")\n",
    "losses = nn.train(X_xor, y_xor, epochs=5000)\n",
    "\n",
    "# Test predictions\n",
    "predictions = nn.forward(X_xor)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nInput | Expected | Predicted | Correct\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(X_xor)):\n",
    "    pred = predictions[i, 0]\n",
    "    expected = y_xor[i, 0]\n",
    "    correct = \"âœ“\" if abs(pred - expected) < 0.5 else \"âœ—\"\n",
    "    print(f\"{X_xor[i]} | {expected:8.2f} | {pred:8.4f} | {correct}\")\n",
    "\n",
    "# Visualize training\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (Backpropagation)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Backpropagation training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding the Chain Rule in Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Understanding the Chain Rule in Backpropagation\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Backpropagation**: Algorithm for computing gradients using chain rule\n",
    "2. **Chain Rule**: dL/dW = (dL/da) * (da/dz) * (dz/dW)\n",
    "3. **Forward Pass**: Compute activations layer by layer\n",
    "4. **Backward Pass**: Compute gradients layer by layer (reverse direction)\n",
    "5. **Weight Update**: Update weights using computed gradients\n",
    "\n",
    "### Best Practices:\n",
    "- Use numerical gradients to verify analytical gradients\n",
    "- Initialize weights with small random values\n",
    "- Use appropriate learning rates\n",
    "- Monitor loss during training\n",
    "\n",
    "### Applications:\n",
    "- Training neural networks\n",
    "- Deep learning optimization\n",
    "- Automatic differentiation\n",
    "- Gradient-based optimization\n",
    "\n",
    "**Reference:** Course 03, Unit 2: \"Calculus for Machine Learning\" - Programming backpropagation practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
