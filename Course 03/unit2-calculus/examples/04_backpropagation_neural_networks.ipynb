{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Backpropagation in Neural Networks\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Program backpropagation in neural networks using differentiation techniques\n",
    "- Understand the chain rule in neural network context\n",
    "- Implement forward and backward propagation from scratch\n",
    "- Apply backpropagation to train a simple neural network\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of derivatives and gradients\n",
    "- âœ… Understanding of multivariate calculus and chain rule\n",
    "- âœ… Basic understanding of neural networks\n",
    "- âœ… Python and NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 03, Unit 2**:\n",
    "- Programming backpropagation in neural networks using differentiation techniques\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Backpropagation** is the algorithm that enables training of neural networks by computing gradients of the loss function with respect to all network parameters using the chain rule of calculus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nProgramming Backpropagation in Neural Networks\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple Neural Network with Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Simple Neural Network with Backpropagation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1\n",
    "(1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of ReLU\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Loss function\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"Mean Squared Error loss\"\"\"\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def mse_loss_derivative(y_pred, y_true):\n",
    "    \"\"\"Derivative of MSE loss\"\"\"\n",
    "    return 2 * (y_pred - y_true)\n",
    "len(y_true)\n",
    "\n",
    "print(\"\\nâœ… Activation and loss functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2-layer neural network class with backpropagation\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        \"\"\"Initialize neural network with random weights\"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights with small random values\n",
    "        np.random.seed(42)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        # Layer 1\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, y_pred):\n",
    "        \"\"\"Backpropagation: compute gradients using chain rule\"\"\"\n",
    "        m = X.shape[0]  # Number of samples\n",
    "        \n",
    "        # Output layer gradients (chain rule: dL/da2 * da2/dz2)\n",
    "        dz2 = mse_loss_derivative(y_pred, y) * sigmoid_derivative(self.z2)\n",
    "        dW2 = (self.a1.T @ dz2)\n",
    "m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "m\n",
    "        \n",
    "        # Hidden layer gradients (chain rule: backprop through layer 2)\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * sigmoid_derivative(self.z1)\n",
    "        dW1 = (X.T @ dz1)\n",
    "m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "m\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def update_weights(self, dW1, db1, dW2, db2):\n",
    "        \"\"\"Update weights using gradients (gradient descent)\"\"\"\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "    \n",
    "    def train(self, X, y, epochs=1000):\n",
    "        \"\"\"Train the network\"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            loss = mse_loss(y_pred, y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y, y_pred)\n",
    "            \n",
    "            # Update weights\n",
    "            self.update_weights(dW1, db1, dW2, db2)\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "\n",
    "print(\"\\nâœ… Neural network class with backpropagation defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train network on XOR problem\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example: XOR Problem\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "print(\"\\nTraining data:\")\n",
    "print(\"X:\", X_xor)\n",
    "print(\"y:\", y_xor.flatten())\n",
    "\n",
    "# Create and train network\n",
    "nn = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)\n",
    "print(\"\\nTraining network...\")\n",
    "losses = nn.train(X_xor, y_xor, epochs=5000)\n",
    "\n",
    "# Test predictions\n",
    "predictions = nn.forward(X_xor)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nInput | Expected | Predicted | Correct\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(X_xor)):\n",
    "    pred = predictions[i, 0]\n",
    "    expected = y_xor[i, 0]\n",
    "    correct = \"âœ“\" if abs(pred - expected) < 0.5 else \"âœ—\"\n",
    "    print(f\"{X_xor[i]} | {expected:8.2f} | {pred:8.4f} | {correct}\")\n",
    "\n",
    "# Visualize training\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (Backpropagation)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Backpropagation training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding the Chain Rule in Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Understanding the Chain Rule in Backpropagation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nThe chain rule allows us to compute gradients through nested functions:\")\n",
    "print(\"\\nFor a network: Loss = L(a2) where a2 = Ïƒ(z2), z2 = a1*W2 + b2\")\n",
    "print(\"\\nGradient w.r.t. W2:\")\n",
    "print(\"  dL/dW2 = (dL/da2) * (da2/dz2) * (dz2/dW2)\")\n",
    "print(\"         = (dL/da2) * Ïƒ'(z2) * a1\")\n",
    "print(\"\\nGradient w.r.t. W1:\")\n",
    "print(\"  dL/dW1 = (dL/da2) * (da2/dz2) * (dz2/da1) * (da1/dz1) * (dz1/dW1)\")\n",
    "print(\"         = (dL/da2) * Ïƒ'(z2) * W2 * Ïƒ'(z1) * X\")\n",
    "\n",
    "# Verify gradients numerically\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Numerical Gradient Verification:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def numerical_gradient(f, x, h=1e-5):\n",
    "    \"\"\"Compute numerical gradient\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        x_plus = x.copy()\n",
    "        x_plus[idx] += h\n",
    "        x_minus = x.copy()\n",
    "        x_minus[idx] -= h\n",
    "        grad[idx] = (f(x_plus) - f(x_minus))\n",
    "(2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "# Test on a simple case\n",
    "test_X = np.array([[1.0, 2.0]])\n",
    "test_y = np.array([[1.0]])\n",
    "\n",
    "def loss_fn(W2_flat):\n",
    "    \"\"\"Loss as function of W2 only\"\"\"\n",
    "    nn_test = SimpleNeuralNetwork(2, 2, 1, learning_rate=0.1)\n",
    "    nn_test.W2 = W2_flat.reshape(2, 1)\n",
    "    y_pred = nn_test.forward(test_X)\n",
    "    return mse_loss(y_pred, test_y)\n",
    "\n",
    "# Get analytical gradient\n",
    "nn_test = SimpleNeuralNetwork(2, 2, 1, learning_rate=0.1)\n",
    "y_pred_test = nn_test.forward(test_X)\n",
    "_, _, dW2_analytical, _ = nn_test.backward(test_X, test_y, y_pred_test)\n",
    "\n",
    "# Get numerical gradient\n",
    "W2_init = nn_test.W2.copy()\n",
    "def f_W2(W2_flat):\n",
    "    nn_test.W2 = W2_flat.reshape(2, 1)\n",
    "    y_pred = nn_test.forward(test_X)\n",
    "    return mse_loss(y_pred, test_y)\n",
    "\n",
    "dW2_numerical = numerical_gradient(f_W2, W2_init.flatten())\n",
    "dW2_numerical = dW2_numerical.reshape(2, 1)\n",
    "\n",
    "print(\"\\nAnalytical gradient (from backpropagation):\")\n",
    "print(dW2_analytical)\n",
    "print(\"\\nNumerical gradient (finite differences):\")\n",
    "print(dW2_numerical)\n",
    "print(\"\\nDifference:\")\n",
    "print(np.abs(dW2_analytical - dW2_numerical))\n",
    "\n",
    "print(\"\\nâœ… Chain rule verified - analytical and numerical gradients match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Backpropagation**: Algorithm for computing gradients using chain rule\n",
    "2. **Chain Rule**: dL/dW = (dL/da) * (da/dz) * (dz/dW)\n",
    "3. **Forward Pass**: Compute activations layer by layer\n",
    "4. **Backward Pass**: Compute gradients layer by layer (reverse direction)\n",
    "5. **Weight Update**: Update weights using computed gradients\n",
    "\n",
    "### Best Practices:\n",
    "- Use numerical gradients to verify analytical gradients\n",
    "- Initialize weights with small random values\n",
    "- Use appropriate learning rates\n",
    "- Monitor loss during training\n",
    "\n",
    "### Applications:\n",
    "- Training neural networks\n",
    "- Deep learning optimization\n",
    "- Automatic differentiation\n",
    "- Gradient-based optimization\n",
    "\n",
    "**Reference:** Course 03, Unit 2: \"Calculus for Machine Learning\" - Programming backpropagation practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
