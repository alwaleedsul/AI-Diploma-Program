{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Maximum Likelihood Estimation (MLE) for Different Distributions\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Implement Maximum Likelihood Estimation for different distributions\n",
    "- Understand the likelihood function and log-likelihood\n",
    "- Estimate parameters using MLE for Gaussian, Poisson, and Bernoulli distributions\n",
    "- Compare MLE estimates with true parameters\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of probability distributions\n",
    "- âœ… Understanding of optimization concepts\n",
    "- âœ… Python, NumPy, SciPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 03, Unit 5**:\n",
    "- Implementing Maximum Likelihood Estimation (MLE) for different distributions\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 5 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE)** is a method for estimating parameters of a probability distribution by maximizing the likelihood function, which measures how likely the observed data is given the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize_scalar, minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nImplementing Maximum Likelihood Estimation (MLE)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: MLE for Gaussian Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: MLE for Gaussian Distribution\")\n",
    "print(\"=\" * 60)\n",
    "# Generate data from Gaussian distribution\n",
    "np.random.seed(42)\n",
    "true_mu = 5.0\n",
    "true_sigma = 2.0\n",
    "data = np.random.normal(true_mu, true_sigma, 100)\n",
    "print(f\"\\nTrue parameters:\")\n",
    "print(f\" Î¼ (mean): {true_mu}\")\n",
    "print(f\" Ïƒ (std): {true_sigma}\")\n",
    "print(f\" Sample size: {len(data)}\")\n",
    "# MLE for Gaussian: sample mean and sample std\n",
    "mle_mu = np.mean(data)\n",
    "mle_sigma = np.std(data, ddof=0) # MLE uses N, not N-1\n",
    "print(f\"\\nMLE estimates:\")\n",
    "print(f\" Î¼_MLE (sample mean): {mle_mu:.4f}\")\n",
    "print(f\" Ïƒ_MLE (sample std): {mle_sigma:.4f}\")\n",
    "# Negative log-likelihood function (we minimize this)\n",
    "def neg_log_likelihood_gaussian(params, data):\n",
    "    \"\"\"Negative log-likelihood for Gaussian distribution\"\"\"\n",
    "    mu, sigma = params\n",
    "    if sigma <= 0:\n",
    "        return np.inf\n",
    "    n = len(data)\n",
    "    log_likelihood = -n * np.log(sigma * np.sqrt(2 * np.pi)) - np.sum((data - mu)**2) / (2 * sigma**2)\n",
    "    return -log_likelihood # Return negative for minimization\n",
    "    \"\"\"Negative log-likelihood for Gaussian distribution\"\"\"\n",
    "    mu, sigma = params\n",
    "    if sigma <= 0:\n",
    "        return np.inf\n",
    "    n = len(data)\n",
    "    log_likelihood = -n * np.log(sigma * np.sqrt(2 * np.pi)) - np.sum((data - mu)**2) / (2 * sigma**2)\n",
    "    return -log_likelihood # Return negative for minimization\n",
    "# Optimize using scipy\n",
    "result = minimize(lambda p: neg_log_likelihood_gaussian(p, data), \n",
    " x0=[mle_mu, mle_sigma], \n",
    " method='BFGS')\n",
    "mle_mu_opt = result.x[0]\n",
    "mle_sigma_opt = result.x[1]\n",
    "print(f\"\\nMLE from optimization:\")\n",
    "print(f\" Î¼_MLE: {mle_mu_opt:.4f}\")\n",
    "print(f\" Ïƒ_MLE: {mle_sigma_opt:.4f}\")\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data, bins=20, density=True, alpha=0.7, color='blue', label='Data')\n",
    "x = np.linspace(data.min(), data.max(), 100)\n",
    "plt.plot(x, stats.norm.pdf(x, true_mu, true_sigma), 'r-', linewidth=2, label=f'True (Î¼={true_mu}, Ïƒ={true_sigma})')\n",
    "plt.plot(x, stats.norm.pdf(x, mle_mu, mle_sigma), 'g--', linewidth=2, label=f'MLE (Î¼={mle_mu:.2f}, Ïƒ={mle_sigma:.2f})')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Gaussian Distribution MLE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.subplot(1, 2, 2)\n",
    "mu_range = np.linspace(3, 7, 100)\n",
    "log_likelihoods = [-neg_log_likelihood_gaussian([mu, mle_sigma], data) for mu in mu_range]\n",
    "plt.plot(mu_range, log_likelihoods, 'b-', linewidth=2)\n",
    "plt.axvline(true_mu, color='r', linestyle='--', label=f'True Î¼={true_mu}')\n",
    "plt.axvline(mle_mu, color='g', linestyle='--', label=f'MLE Î¼={mle_mu:.2f}')\n",
    "plt.xlabel('Î¼ (mean)')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.title('Log-Likelihood vs Mean')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nâœ… MLE for Gaussian distribution implemented!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: MLE for Poisson Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: MLE for Poisson Distribution\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate data from Poisson distribution\n",
    "np.random.seed(42)\n",
    "true_lambda = 3.5\n",
    "data_poisson = np.random.poisson(true_lambda, 100)\n",
    "\n",
    "print(f\"\\nTrue parameter:\")\n",
    "print(f\" Î» (rate): {true_lambda}\")\n",
    "print(f\" Sample size: {len(data_poisson)}\")\n",
    "\n",
    "# MLE for Poisson: sample mean\n",
    "mle_lambda = np.mean(data_poisson)\n",
    "\n",
    "print(f\"\\nMLE estimate:\")\n",
    "print(f\" Î»_MLE (sample mean): {mle_lambda:.4f}\")\n",
    "\n",
    "# Negative log-likelihood\n",
    "def neg_log_likelihood_poisson(lambda_param, data):\n",
    " \n",
    "    \n",
    "    \n",
    "    \"\"\"Negative log-likelihood for Poisson distribution\"\"\"\n",
    " if lambda_param <= 0:\n",
    " return np.inf\n",
    " n = len(data)\n",
    " log_likelihood = np.sum(data) * np.log(lambda_param) - n * lambda_param - np.sum([np.math.lgamma(x + 1) for x in data])\n",
    " return -log_likelihood\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "unique, counts = np.unique(data_poisson, return_counts=True)\n",
    "plt.bar(unique, counts/len(data_poisson), alpha=0.7, color='blue', label='Data (empirical)')\n",
    "x = np.arange(0, max(unique) + 3)\n",
    "plt.plot(x, stats.poisson.pmf(x, true_lambda), 'ro-', markersize=8, label=f'True (Î»={true_lambda})')\n",
    "plt.plot(x, stats.poisson.pmf(x, mle_lambda), 'go--', markersize=8, label=f'MLE (Î»={mle_lambda:.2f})')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Poisson Distribution MLE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "lambda_range = np.linspace(2, 5, 100)\n",
    "log_likelihoods = [-neg_log_likelihood_poisson(lam, data_poisson) for lam in lambda_range]\n",
    "plt.plot(lambda_range, log_likelihoods, 'b-', linewidth=2)\n",
    "plt.axvline(true_lambda, color='r', linestyle='--', label=f'True Î»={true_lambda}')\n",
    "plt.axvline(mle_lambda, color='g', linestyle='--', label=f'MLE Î»={mle_lambda:.2f}')\n",
    "plt.xlabel('Î» (rate)')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.title('Log-Likelihood vs Î»')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… MLE for Poisson distribution implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: MLE for Bernoulli Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: MLE for Bernoulli Distribution\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate data from Bernoulli distribution\n",
    "np.random.seed(42)\n",
    "true_p = 0.6\n",
    "data_bernoulli = np.random.binomial(1, true_p, 100)\n",
    "\n",
    "print(f\"\\nTrue parameter:\")\n",
    "print(f\" p (success probability): {true_p}\")\n",
    "print(f\" Sample size: {len(data_bernoulli)}\")\n",
    "print(f\" Number of successes: {data_bernoulli.sum()}\")\n",
    "\n",
    "# MLE for Bernoulli: sample proportion\n",
    "mle_p = np.mean(data_bernoulli)\n",
    "\n",
    "print(f\"\\nMLE estimate:\")\n",
    "print(f\" p_MLE (sample proportion): {mle_p:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "unique, counts = np.unique(data_bernoulli, return_counts=True)\n",
    "plt.bar(unique, counts/len(data_bernoulli), alpha=0.7, color='blue', label='Data (empirical)')\n",
    "x = np.array([0, 1])\n",
    "plt.plot(x, stats.bernoulli.pmf(x, true_p), 'ro-', markersize=15, label=f'True (p={true_p})')\n",
    "plt.plot(x, stats.bernoulli.pmf(x, mle_p), 'go--', markersize=15, label=f'MLE (p={mle_p:.2f})')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Bernoulli Distribution MLE')\n",
    "plt.xticks([0, 1])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "p_range = np.linspace(0.01, 0.99, 100)\n",
    "# Log-likelihood: k*log(p) + (n-k)*log(1-p) where k = sum(data)\n",
    "k = data_bernoulli.sum()\n",
    "n = len(data_bernoulli)\n",
    "log_likelihoods = k * np.log(p_range) + (n - k) * np.log(1 - p_range)\n",
    "plt.plot(p_range, log_likelihoods, 'b-', linewidth=2)\n",
    "plt.axvline(true_p, color='r', linestyle='--', label=f'True p={true_p}')\n",
    "plt.axvline(mle_p, color='g', linestyle='--', label=f'MLE p={mle_p:.2f}')\n",
    "plt.xlabel('p (probability)')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.title('Log-Likelihood vs p')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… MLE for Bernoulli distribution implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Likelihood Function**: Measures probability of observing data given parameters\n",
    "2. **Maximum Likelihood Estimation**: Finds parameters that maximize likelihood\n",
    "3. **Log-Likelihood**: Often easier to work with (converts products to sums)\n",
    "4. **MLE for Common Distributions**:\n",
    "   - Gaussian: sample mean and sample std\n",
    "   - Poisson: sample mean\n",
    "   - Bernoulli: sample proportion\n",
    "\n",
    "### Best Practices:\n",
    "- Use log-likelihood to avoid numerical underflow\n",
    "- Verify constraints (e.g., Ïƒ > 0, 0 < p < 1)\n",
    "- Compare MLE estimates with true parameters\n",
    "- Understand when MLE is appropriate\n",
    "\n",
    "### Applications:\n",
    "- Parameter estimation\n",
    "- Model fitting\n",
    "- Statistical inference\n",
    "- Machine learning (loss functions)\n",
    "\n",
    "**Reference:** Course 03, Unit 5: \"Probability and Statistical Inference\" - Maximum Likelihood Estimation practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
