{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 01\n",
    "\n",
    "**Module 02**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Exercise 01: Derivatives and Gradients\n\nThis exercise helps you understand how derivatives and gradients work,\nwhich are fundamental for training machine learning models.\n\nInstructions:\n1. Complete the functions below\n2. Understand how derivatives relate to optimization\n3. Test your solutions\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important: Implement Functions First!\n",
    "\n",
    "**Before running tests**, complete all function implementations:\n",
    "\n",
    "1. ‚úÖ Find all functions with `pass` statements\n",
    "2. ‚úÖ Replace `pass` with your implementation\n",
    "3. ‚úÖ Make sure functions return values (not None)\n",
    "\n",
    "**The test code will give helpful hints if you forget!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Your Implementation\n",
    "\n",
    "Complete the functions below. Replace `pass` with your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import approx_fprime\n",
    "# Note: scipy.misc.derivative moved, using approx_fprime instead\n",
    "\n",
    "\n",
    "def compute_derivative(func, x, h=1\n",
    "e-6):\n",
    " \n",
    " # TODO: Implement numerical derivative\n",
    " # Formula: (f(x+h) - f(x)) / \n",
    "h\n",
    " pass\n",
    "\n",
    "\n",
    "def compute_gradient(func, point):\n",
    " \n",
    " # TODO: Compute partial derivative\n",
    "s\n",
    " # Use compute_derivative for each variable\n",
    " h = 1\n",
    "e-6\n",
    " pass\n",
    "\n",
    "\n",
    "def gradient_descent_step(func, x, learning_rate=0.1):\n",
    " \n",
    " # TODO: \n",
    " # 1. Compute gradient at current poin\n",
    "t\n",
    " # 2. Move in opposite direction: x_new = x - lr * gradient\n",
    " pass\n",
    "\n",
    "\n",
    "# Test your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Test Your Solution\n",
    "\n",
    "**Run this after implementing all functions above:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    " print(\"Testing Exercise 01: Derivatives and Gradients\")\n",
    " print(\"=\" * 60)\n",
    " \n",
    " # Test 1: Derivative\n",
    " print(\"\\n1. Testing compute\\n_derivative:\")\n",
    " def f(x):\n",
    " return x**2 + 3*x + 2\n",
    " \n",
    " x0 = 2.0\n",
    " deriv = compute_derivative(f, x0)\n",
    " expected = 2*x0 + 3 # d/dx(x¬≤ + 3\n",
    "x + 2) = 2\n",
    "x + 3\n",
    " print(f\" Function: f(x) = x¬≤ + 3 x + 2\")\n",
    " print(f\" At x = {x0}:\")\n",
    " print(f\" Computed derivative: {deriv:.4 f}\")\n",
    " print(f\" Expected: {expected:.4 f}\")\n",
    " assert abs(deriv - expected) < 0.01, \"Derivative incorrect\"\n",
    " print(\" ‚úÖ Passed!\")\n",
    " \n",
    " # Test 2: Gradient\n",
    " print(\"\\n2. Testing compute\\n_gradient:\")\n",
    " def multivariable_func(point):\n",
    " x, y = pointreturn x**2 + y**2 + x*y\n",
    " \n",
    " point = np.array([1.0, 2.0])\n",
    " grad = compute_gradient(multivariable_func, point)\n",
    " expected = np.array([2*point[0] + point[1], 2*point[1] + point[0]])\n",
    " print(f\" Function: f(x, y) = x¬≤ + y¬≤ + xy\")\n",
    " print(f\" At point ({point[0]}, {point[1]}):\")\n",
    " print(f\" Computed gradient: {grad}\")\n",
    " print(f\" Expected: {expected}\")\n",
    " assert np.allclose(grad, expected, atol=0.1), \"Gradient incorrect\"\n",
    " print(\" ‚úÖ Passed!\")\n",
    " \n",
    " # Test 3: Gradient descen\n",
    "t\n",
    " print(\"\\n3. Testing gradient_descent_step:\")\n",
    " def loss_func(x):\n",
    " return (x - 3)**2 # Minimum atx = 3\n",
    " \n",
    " x = 5.0\n",
    " x_new = gradient_descent_step(loss_func, x, learning_rate=0.1)\n",
    " print(f\" Loss function: f(x) = (x - 3)¬≤\")\n",
    " print(f\" Starting at x = {x}\")\n",
    " print(f\" After one step: x = {x_new:.4\n",
    "f}\")\n",
    " print(f\" Expected: x should move closer to 3\")\n",
    " assert abs(x_new - 3) < abs(x - 3), \"Should move toward minimum\"\n",
    " print(\" ‚úÖ Passed!\")\n",
    " \n",
    " print(\"\\n\" + \"=\" * 60)\n",
    " print(\"üéâ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Check Solution\n",
    "\n",
    "Compare with: `exercises/solutions/solution_01.ipynb`\n",
    "\n",
    "**üí° Try solving it yourself first!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}