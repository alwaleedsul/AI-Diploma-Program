{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Advanced Tokenization\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Implement advanced tokenization (subword, BPE-style)\n",
    "- Handle morphologically rich languages\n",
    "- Choose tokenization strategies for different models\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 07, Unit 2** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Advanced Tokenization\n",
    "\n",
    "## ğŸ“š Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "\n",
    "This notebook demonstrates key concepts through hands-on examples.\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the core concepts\n",
    "- See practical implementations\n",
    "- Be ready for exercises\n",
    "\n",
    "## ğŸ”— Prerequisites | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "- âœ… Python 3.8+ installed\n",
    "- âœ… Required libraries (see `requirements.txt`)\n",
    "- âœ… Basic Python knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Code Example | Ù…Ø«Ø§Ù„ Ø§Ù„ÙƒÙˆØ¯\n",
    "\n",
    "Run the code below to see the demonstration:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Inputs & ğŸ“¤ Outputs | Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø®Ø±Ø¬Ø§Øª\n",
    "\n",
    "**Inputs:** What we use in this notebook\n",
    "\n",
    "- Libraries and concepts as introduced in this notebook; see prerequisites and code comments.\n",
    "\n",
    "**Outputs:** What you'll see when you run the cells\n",
    "\n",
    "- Printed results, figures, and summaries as shown when you run the cells.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualization: Word Frequency Chart\n",
    "# ØªØµÙˆØ±: Ù…Ø®Ø·Ø· ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Example word frequencies (from the code above)\n",
    "    words = ['natural', 'language', 'processing', 'nlp', 'artificial', 'intelligence']\n",
    "    frequencies = [2, 2, 1, 1, 1, 1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(words, frequencies, color='steelblue', alpha=0.7)\n",
    "    plt.xlabel('Words | Ø§Ù„ÙƒÙ„Ù…Ø§Øª', fontsize=12)\n",
    "    plt.ylabel('Frequency | Ø§Ù„ØªÙƒØ±Ø§Ø±', fontsize=12)\n",
    "    plt.title('Word Frequency Analysis | ØªØ­Ù„ÙŠÙ„ ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª', fontsize=14, pad=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"âœ… Word frequency chart displayed\")\n",
    "except ImportError:\n",
    "    print(\"Note: Install matplotlib for visualizations\")\n",
    "    print(\"Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª matplotlib Ù„Ù„ØªØµÙˆØ±Ø§Øª\")\n",
    "\n",
    "\"\"\"\n",
    "Unit 2 - Example 1: Advanced Tokenization\n",
    "Ø§Ù„ÙˆØ­Ø¯Ø© 2 - Ù…Ø«Ø§Ù„ 1: Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\n",
    "\n",
    "This example demonstrates:\n",
    "1. Word tokenization\n",
    "2. Sentence tokenization\n",
    "3. Subword tokenization\n",
    "4. Morphological analysis\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 1: Advanced Tokenization\")\n",
    "print(\"Ù…Ø«Ø§Ù„ 1: Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample text\n",
    "sample_text = \"\"\"\n",
    "Natural Language Processing is amazing! It helps computers understand human language.\n",
    "We can tokenize text into words, sentences, or even subwords. This is fundamental to NLP.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nOriginal Text:\")\n",
    "print(\"Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ:\")\n",
    "print(sample_text)\n",
    "\n",
    "# 1. Word Tokenization\n",
    "# ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. Word Tokenization\")\n",
    "print(\"ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def word_tokenize(text):\n",
    "    \"\"\"\n",
    "    Simple word tokenization.\n",
    "    ØªÙ‚Ø·ÙŠØ¹ Ø¨Ø³ÙŠØ· Ù„Ù„ÙƒÙ„Ù…Ø§Øª.\n",
    "    \"\"\"\n",
    "    # Remove punctuation and split\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = text.lower().split()\n",
    "    return words\n",
    "\n",
    "words = word_tokenize(sample_text)\n",
    "print(f\"\\nWords: {words}\")\n",
    "print(f\"Total words: {len(words)}\")\n",
    "\n",
    "# Word frequency\n",
    "word_freq = Counter(words)\n",
    "print(\"\\nMost common words:\")\n",
    "print(\"Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø´ÙŠÙˆØ¹Ø§Ù‹:\")\n",
    "for word, freq in word_freq.most_common(5):\n",
    "    print(f\"  {word}: {freq}\")\n",
    "\n",
    "# 2. Sentence Tokenization\n",
    "# ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ø¬Ù…Ù„\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Sentence Tokenization\")\n",
    "print(\"ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ø¬Ù…Ù„\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    \"\"\"\n",
    "    Simple sentence tokenization.\n",
    "    ØªÙ‚Ø·ÙŠØ¹ Ø¨Ø³ÙŠØ· Ù„Ù„Ø¬Ù…Ù„.\n",
    "    \"\"\"\n",
    "    # Split on sentence endings\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "sentences = sentence_tokenize(sample_text)\n",
    "print(f\"\\nSentences: {len(sentences)}\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"  {i}. {sentence}\")\n",
    "\n",
    "# 3. Subword Tokenization\n",
    "# ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ©\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. Subword Tokenization\")\n",
    "print(\"ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def subword_tokenize(word, n=3):\n",
    "    \"\"\"\n",
    "    Create n-gram subwords.\n",
    "    Ø¥Ù†Ø´Ø§Ø¡ ÙƒÙ„Ù…Ø§Øª ÙØ±Ø¹ÙŠØ© n-gram.\n",
    "    \"\"\"\n",
    "    subwords = []\n",
    "    for i in range(len(word) - n + 1):\n",
    "        subwords.append(word[i:i+n])\n",
    "    return subwords\n",
    "\n",
    "example_word = \"processing\"\n",
    "subwords = subword_tokenize(example_word, n=3)\n",
    "print(f\"\\nWord: {example_word}\")\n",
    "print(f\"3-gram subwords: {subwords}\")\n",
    "\n",
    "# 4. Morphological Analysis Example\n",
    "# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Morphological Analysis\")\n",
    "print(\"Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple stemming example\n",
    "# Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ· Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚\n",
    "def simple_stem(word):\n",
    "    \"\"\"\n",
    "    Simple stemming (remove common suffixes).\n",
    "    Ø§Ø´ØªÙ‚Ø§Ù‚ Ø¨Ø³ÙŠØ· (Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù„ÙˆØ§Ø­Ù‚ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©).\n",
    "    \"\"\"\n",
    "    suffixes = ['ing', 'ed', 's', 'es', 'ly', 'er', 'est']\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "test_words = ['running', 'jumped', 'cats', 'quickly', 'faster']\n",
    "print(\"\\nStemming examples:\")\n",
    "print(\"Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚:\")\n",
    "for word in test_words:\n",
    "    stem = simple_stem(word)\n",
    "    print(f\"  {word} -> {stem}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example completed successfully!\")\n",
    "print(\"ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNote: For production, use libraries like NLTK, spaCy, or transformers\")\n",
    "print(\"Ù…Ù„Ø§Ø­Ø¸Ø©: Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙØ¹Ù„ÙŠØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…ÙƒØªØ¨Ø§Øª Ù…Ø«Ù„ NLTK Ø£Ùˆ spaCy Ø£Ùˆ transformers\")\n",
    "\n",
    "\n",
    "\n",
    "# Visualize word frequencies\n",
    "# ØªØµÙˆØ± ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create bar chart of word frequencies\n",
    "    if 'word_freq' in locals() or 'word_freq' in globals():\n",
    "        words = list(word_freq.keys())[:10]  # Top 10 words\n",
    "        freqs = [word_freq[w] for w in words]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(words, freqs, color='skyblue')\n",
    "        plt.title(\"Top 10 Word Frequencies | Ø£Ø¹Ù„Ù‰ 10 ÙƒÙ„Ù…Ø§Øª Ù…Ù† Ø­ÙŠØ« Ø§Ù„ØªÙƒØ±Ø§Ø±\", fontsize=14)\n",
    "        plt.xlabel(\"Words | Ø§Ù„ÙƒÙ„Ù…Ø§Øª\")\n",
    "        plt.ylabel(\"Frequency | Ø§Ù„ØªÙƒØ±Ø§Ø±\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"âœ… Word frequency visualization displayed\")\n",
    "except (ImportError, NameError):\n",
    "    print(\"Note: Install matplotlib for visualization\")\n",
    "    print(\"Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª matplotlib Ù„Ù„ØªØµÙˆØ±\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Summary | Ø§Ù„Ù…Ù„Ø®Øµ\n",
    "\n",
    "Great job completing this example!\n",
    "\n",
    "**What you learned:**\n",
    "- Core concepts demonstrated in the code\n",
    "- Practical implementation details\n",
    "\n",
    "**Next steps:**\n",
    "- Complete the exercises in `exercises/` folder\n",
    "- Review the quiz materials\n",
    "- Proceed to the next example\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Tip:** If you see errors, make sure:\n",
    "- All libraries are installed: `pip install -r requirements.txt`\n",
    "- You're using Python 3.8+\n",
    "- Cells are executed in order\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}