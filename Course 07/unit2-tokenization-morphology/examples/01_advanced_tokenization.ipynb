{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 Advanced Tokenization\n",
        "\n",
        "## ğŸ“š Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
        "\n",
        "This notebook demonstrates key concepts through hands-on examples.\n",
        "\n",
        "By completing this notebook, you will:\n",
        "- Understand the core concepts\n",
        "- See practical implementations\n",
        "- Be ready for exercises\n",
        "\n",
        "## ğŸ”— Prerequisites | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
        "\n",
        "- âœ… Python 3.8+ installed\n",
        "- âœ… Required libraries (see `requirements.txt`)\n",
        "- âœ… Basic Python knowledge\n",
        "\n",
        "---\n",
        "\n",
        "## Code Example | Ù…Ø«Ø§Ù„ Ø§Ù„ÙƒÙˆØ¯\n",
        "\n",
        "Run the code below to see the demonstration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Unit 2 - Example 1: Advanced Tokenization\n",
        "Ø§Ù„ÙˆØ­Ø¯Ø© 2 - Ù…Ø«Ø§Ù„ 1: Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\n",
        "\n",
        "This example demonstrates:\n",
        "1. Word tokenization\n",
        "2. Sentence tokenization\n",
        "3. Subword tokenization\n",
        "4. Morphological analysis\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Example 1: Advanced Tokenization\")\n",
        "print(\"Ù…Ø«Ø§Ù„ 1: Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"\"\"\n",
        "Natural Language Processing is amazing! It helps computers understand human language.\n",
        "We can tokenize text into words, sentences, or even subwords. This is fundamental to NLP.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nOriginal Text:\")\n",
        "print(\"Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ:\")\n",
        "print(sample_text)\n",
        "\n",
        "# 1. Word Tokenization\n",
        "# ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"1. Word Tokenization\")\n",
        "print(\"ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def word_tokenize(text):\n",
        "    \"\"\"\n",
        "    Simple word tokenization.\n",
        "    ØªÙ‚Ø·ÙŠØ¹ Ø¨Ø³ÙŠØ· Ù„Ù„ÙƒÙ„Ù…Ø§Øª.\n",
        "    \"\"\"\n",
        "    # Remove punctuation and split\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = text.lower().split()\n",
        "    return words\n",
        "\n",
        "words = word_tokenize(sample_text)\n",
        "print(f\"\\nWords: {words}\")\n",
        "print(f\"Total words: {len(words)}\")\n",
        "\n",
        "# Word frequency\n",
        "word_freq = Counter(words)\n",
        "print(\"\\nMost common words:\")\n",
        "print(\"Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø´ÙŠÙˆØ¹Ø§Ù‹:\")\n",
        "for word, freq in word_freq.most_common(5):\n",
        "    print(f\"  {word}: {freq}\")\n",
        "\n",
        "# 2. Sentence Tokenization\n",
        "# ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ø¬Ù…Ù„\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"2. Sentence Tokenization\")\n",
        "print(\"ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ø¬Ù…Ù„\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def sentence_tokenize(text):\n",
        "    \"\"\"\n",
        "    Simple sentence tokenization.\n",
        "    ØªÙ‚Ø·ÙŠØ¹ Ø¨Ø³ÙŠØ· Ù„Ù„Ø¬Ù…Ù„.\n",
        "    \"\"\"\n",
        "    # Split on sentence endings\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return sentences\n",
        "\n",
        "sentences = sentence_tokenize(sample_text)\n",
        "print(f\"\\nSentences: {len(sentences)}\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"  {i}. {sentence}\")\n",
        "\n",
        "# 3. Subword Tokenization\n",
        "# ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ©\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"3. Subword Tokenization\")\n",
        "print(\"ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ©\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def subword_tokenize(word, n=3):\n",
        "    \"\"\"\n",
        "    Create n-gram subwords.\n",
        "    Ø¥Ù†Ø´Ø§Ø¡ ÙƒÙ„Ù…Ø§Øª ÙØ±Ø¹ÙŠØ© n-gram.\n",
        "    \"\"\"\n",
        "    subwords = []\n",
        "    for i in range(len(word) - n + 1):\n",
        "        subwords.append(word[i:i+n])\n",
        "    return subwords\n",
        "\n",
        "example_word = \"processing\"\n",
        "subwords = subword_tokenize(example_word, n=3)\n",
        "print(f\"\\nWord: {example_word}\")\n",
        "print(f\"3-gram subwords: {subwords}\")\n",
        "\n",
        "# 4. Morphological Analysis Example\n",
        "# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"4. Morphological Analysis\")\n",
        "print(\"Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Simple stemming example\n",
        "# Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ· Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚\n",
        "def simple_stem(word):\n",
        "    \"\"\"\n",
        "    Simple stemming (remove common suffixes).\n",
        "    Ø§Ø´ØªÙ‚Ø§Ù‚ Ø¨Ø³ÙŠØ· (Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù„ÙˆØ§Ø­Ù‚ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©).\n",
        "    \"\"\"\n",
        "    suffixes = ['ing', 'ed', 's', 'es', 'ly', 'er', 'est']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "test_words = ['running', 'jumped', 'cats', 'quickly', 'faster']\n",
        "print(\"\\nStemming examples:\")\n",
        "print(\"Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚:\")\n",
        "for word in test_words:\n",
        "    stem = simple_stem(word)\n",
        "    print(f\"  {word} -> {stem}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Example completed successfully!\")\n",
        "print(\"ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nNote: For production, use libraries like NLTK, spaCy, or transformers\")\n",
        "print(\"Ù…Ù„Ø§Ø­Ø¸Ø©: Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙØ¹Ù„ÙŠØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…ÙƒØªØ¨Ø§Øª Ù…Ø«Ù„ NLTK Ø£Ùˆ spaCy Ø£Ùˆ transformers\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœ… Summary | Ø§Ù„Ù…Ù„Ø®Øµ\n",
        "\n",
        "Great job completing this example!\n",
        "\n",
        "**What you learned:**\n",
        "- Core concepts demonstrated in the code\n",
        "- Practical implementation details\n",
        "\n",
        "**Next steps:**\n",
        "- Complete the exercises in `exercises/` folder\n",
        "- Review the quiz materials\n",
        "- Proceed to the next example\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ’¡ Tip:** If you see errors, make sure:\n",
        "- All libraries are installed: `pip install -r requirements.txt`\n",
        "- You're using Python 3.8+\n",
        "- Cells are executed in order\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}