{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Multi-Language Text Processing\n## AIAT 121 - Natural Language Processing\n\n## Learning Objectives\n\n- Implement tokenization for multiple languages\n- Handle different scripts and character sets\n- Compare tokenization methods\n- Apply to real-world multilingual data\n\n## Real-World Context\n\nYou're building a text analysis system for an international e-commerce platform that processes reviews in English, Arabic, and Chinese.\n\n**Task**: Preprocess and tokenize multilingual text data.\n\n---\n\n## Task 1: Multi-Language Tokenization (30 points)\n\nImplement tokenization functions for:\n1. English (word tokenization)\n2. Arabic (character/word tokenization)\n3. Chinese (character tokenization)\n\n**Instructions**:\n- Use appropriate tokenizers for each language\n- Handle special characters\n- Normalize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Inputs & ðŸ“¤ Outputs | Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø®Ø±Ø¬Ø§Øª\n",
    "\n",
    "**Inputs:** What we use in this notebook\n",
    "\n",
    "- Libraries and concepts as introduced in this notebook; see prerequisites and code comments.\n",
    "\n",
    "**Outputs:** What you'll see when you run the cells\n",
    "\n",
    "- Printed results, figures, and summaries as shown when you run the cells.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk -q\nimport nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt', quiet=True)\n\n# TODO: Implement multi-language tokenization\ndef tokenize_english(text):\n    # YOUR CODE HERE\n    pass\n\ndef tokenize_arabic(text):\n    # YOUR CODE HERE\n    pass\n\ndef tokenize_chinese(text):\n    # YOUR CODE HERE\n    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Compare Tokenization Methods (25 points)\n\nCompare different tokenization approaches and analyze results."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: Compare tokenization methods\n",
    "Test on sample texts\n",
    "Analyze token counts and quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Real-World Application (45 points)\n\nProcess a dataset of multilingual product reviews:\n1. Load and preprocess reviews\n2. Tokenize each review\n3. Extract key features\n4. Generate statistics\n\n**Submission**: Complete notebook with all implementations and analysis.\n\n**Grading**: 100 points total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}