{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(0) and n-Step TD Algorithms\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand Temporal Difference (TD) learning\n",
    "- Implement TD(0) algorithm\n",
    "- Implement n-step TD algorithms\n",
    "- Compare TD with Monte Carlo methods\n",
    "- Apply TD algorithms to RL environments\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Understanding of value functions\n",
    "- ‚úÖ Monte Carlo methods knowledge\n",
    "- ‚úÖ Python knowledge (functions, loops, NumPy)\n",
    "- ‚úÖ Understanding of bootstrapping\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 2**:\n",
    "- Running TD(0) and n-step TD algorithms in simple RL environments\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Temporal Difference (TD) learning** combines ideas from Monte Carlo (learning from experience) and Dynamic Programming (bootstrapping). TD methods update estimates based on other estimates, making them more sample-efficient than Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n",
    "print(\"\\nTD(0) and n-Step TD Algorithms\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding TD Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Understanding TD Learning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nTD Learning Key Concepts:\")\n",
    "print(\" 1. Bootstrap: Update estimate using other estimates\")\n",
    "print(\" 2. Incremental: Update after each step (no need to wait for episode end)\")\n",
    "print(\" 3. TD Error: Œ¥\"\n",
    "t = R_{t+1} + Œ≥V(S_{t+1}) - V(S_t)\")\n",
    "print(\" 4. Update: V(S_t) = V(S_t) + Œ±[Œ¥_t]\")\n",
    "\n",
    "print(\"\\nTD(0) Algorithm:\")\n",
    "print(\" V(S_t) ‚Üê V(S_t) + Œ±[R_{t+1} + Œ≥V(S_{t+1}) - V(S_t)]\")\n",
    "print(\" - Uses 1-step return: R_{t+1} + Œ≥V(S_{t+1})\")\n",
    "print(\" - Updates after each step\")\n",
    "\n",
    "print(\"\\nn-Step TD Algorithm:\")\n",
    "print(\" - Uses n-step return: R_{t+1} + Œ≥R_{t+2} + ... + Œ≥^n V(S_{t+n})\")\n",
    "print(\" - Balances bias and variance\")\n",
    "print(\" - n=1: TD(0), n=‚àû: Monte Carlo\")\n",
    "\n",
    "print(\"\\n‚úÖ TD learning concepts understood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: TD(0) Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: TD(0) Implementation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def td0(policy, env_simulator, n\n",
    "episodes =100, alpha=0.1, gamma=0.99):\n",
    " \n",
    "    \n",
    "    \"\"\"\n",
    " TD(0) for estimating state values.\n",
    " V(S_t) ‚Üê V(S_t) + Œ±[R_{t+1} + Œ≥V(S_{t+1}) - V(S_t)]\n",
    " \"\"\"\n",
    " V = defaultdict(float)\n",
    " \n",
    " for episode in range(n_episodes):\n",
    " state = env\n",
    "simulator.reset()\n",
    " done = False\n",
    " \n",
    " while not done:\n",
    " # Choose action (simplified)\n",
    " action = policy(state) if callable(policy) else policy.get(state, 0)\n",
    " \n",
    " # Take step (simplified - assuming env interface)\n",
    " next_state, reward, done = env\n",
    "simulator.step(action)\n",
    " \n",
    " # TD(0) update\n",
    " td\n",
    "target = reward + gamma * V[next_state]\n",
    " td\n",
    "error = td\n",
    "target - V[state]\n",
    " V[state] = V[state] + alpha * td_error\n",
    " \n",
    " state = next\n",
    "state\n",
    " \n",
    " return V\n",
    "\n",
    "# Simple example\n",
    "class SimpleEnv:\n",
    " \n",
    "def __init__(self):\n",
    " self.state = 1\n",
    " self.n\n",
    "states = 5\n",
    " \n",
    " def reset(self):\n",
    " self.state = 1\n",
    " return self.state\n",
    " \n",
    " def step(self, action):\n",
    " # Simple transition: move towards goal (state 4)\n",
    " if self.state < 4:\n",
    " self.state += 1\n",
    " reward = 1.0 if self.state == 4 else 0.0\n",
    " done = self.state == 4\n",
    " return self.state, reward, done\n",
    "\n",
    "env = SimpleEnv()\n",
    "def simple_policy(state):\n",
    " return 1 # Always move forward\n",
    "\n",
    "V\n",
    "td0 = td0(simple_policy, env, n\n",
    "episodes =100, alpha=0.1, gamma=1.0)\n",
    "\n",
    "print(\"\\nTD(0) Estimated State Values:\")\n",
    "for state in sorted(V_td0.keys()):\n",
    " print(f\" V({state}) = {V_td0[state]:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ TD(0) implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: n-Step TD Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: n-Step TD Implementation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def n_step_td(policy, env_simulator, n\n",
    "episodes =100, n=2, alpha=0.1, gamma=0.99):\n",
    " \n",
    "    \n",
    "    \"\"\"\n",
    " n-step TD for estimating state values.\n",
    " Uses n-step return: R_{t+1} + Œ≥R_{t+2} + ... + Œ≥^n V(S_{t+n})\n",
    " \"\"\"\n",
    " V = defaultdict(float)\n",
    " \n",
    " for episode in range(n_episodes):\n",
    " states = [env_simulator.reset()]\n",
    " rewards = []\n",
    " t = 0\n",
    " T = float('inf')\n",
    " \n",
    " while True:\n",
    " if t < T:\n",
    " # Take action\n",
    " action = policy(states[-1]) if callable(policy) else policy.get(states[-1], 0)\n",
    " next_state, reward, done = env\n",
    "simulator.step(action)\n",
    " \n",
    " states.append(next_state)\n",
    " rewards.append(reward)\n",
    " \n",
    " if done:\n",
    " T = t + 1\n",
    " \n",
    " # Update time\n",
    " tau = t - n + 1\n",
    " \n",
    " if tau >= 0:\n",
    " # Calculate n-step return\n",
    " G = sum(gamma ** i * rewards[tau + i] for i in range(min(n, T - tau)))\n",
    " if tau + n < T:\n",
    " G += gamma ** n * V[states[tau + n]]\n",
    " \n",
    " # Update value\n",
    " V[states[tau]] = V[states[tau]] + alpha * (G - V[states[tau]])\n",
    " \n",
    " t += 1\n",
    " if tau == T - 1:\n",
    " break\n",
    " \n",
    " # Keep only last n states/rewards\n",
    " if len(states) > n + 1:\n",
    " states.pop(0)\n",
    " rewards.pop(0)\n",
    " \n",
    " return V\n",
    "\n",
    "# Compare n-step TD for different n values\n",
    "env2 = SimpleEnv()\n",
    "V\n",
    "n1 = n\n",
    "step\n",
    "td(simple_policy, env2, n\n",
    "episodes =100, n=1, alpha=0.1, gamma=1.0)\n",
    "env3 = SimpleEnv()\n",
    "V\n",
    "n2 = n\n",
    "step\n",
    "td(simple_policy, env3, n\n",
    "episodes =100, n=2, alpha=0.1, gamma=1.0)\n",
    "env4 = SimpleEnv()\n",
    "V\n",
    "n4 = n\n",
    "step\n",
    "td(simple_policy, env4, n\n",
    "episodes =100, n=4, alpha=0.1, gamma=1.0)\n",
    "\n",
    "print(\"\\nn-Step TD Estimated State Values:\")\n",
    "print(\"n=1 (TD(0)):\")\n",
    "for state in sorted(V_n1.keys()):\n",
    " print(f\" V({state}) = {V_n1[state]:.4f}\")\n",
    "\n",
    "print(\"\\nn=2:\")\n",
    "for state in sorted(V_n2.keys()):\n",
    " print(f\" V({state}) = {V_n2[state]:.4f}\")\n",
    "\n",
    "print(\"\\nn=4:\")\n",
    "for state in sorted(V_n4.keys()):\n",
    " print(f\" V({state}) = {V_n4[state]:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ n-step TD implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **TD Learning**: Combines Monte Carlo (experience) and DP (bootstrapping)\n",
    "2. **TD(0)**: 1-step TD, updates: V(S_t) ‚Üê V(S_t) + Œ±[R + Œ≥V(S_{t+1}) - V(S_t)]\n",
    "3. **n-Step TD**: Uses n-step returns, balances bias and variance\n",
    "4. **Bootstrapping**: Update using other estimates (faster but biased)\n",
    "\n",
    "### Comparison:\n",
    "- **Monte Carlo**: High variance, no bias, requires episodes\n",
    "- **TD(0)**: Low variance, some bias, online (incremental)\n",
    "- **n-Step TD**: Trade-off between MC and TD(0)\n",
    "\n",
    "### Advantages:\n",
    "- Online learning (no need to wait for episode end)\n",
    "- Lower variance than Monte Carlo\n",
    "- More sample-efficient\n",
    "- Works for continuing tasks\n",
    "\n",
    "### Applications:\n",
    "- Value function estimation\n",
    "- Policy evaluation\n",
    "- Online learning scenarios\n",
    "\n",
    "### Next Steps:\n",
    "- SARSA and Q-learning (TD control)\n",
    "- Eligibility traces (TD(Œª))\n",
    "- Compare with Monte Carlo and DP\n",
    "\n",
    "**Reference:** Course 09, Unit 2: \"Prediction and Control without a Model\" - TD algorithms practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
