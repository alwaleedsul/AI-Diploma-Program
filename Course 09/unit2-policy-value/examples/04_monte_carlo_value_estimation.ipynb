{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Monte Carlo Methods for Estimating Value Functions\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand Monte Carlo methods for value estimation\n",
    "- Implement first-visit and every-visit Monte Carlo\n",
    "- Estimate state value functions using Monte Carlo\n",
    "- Compare Monte Carlo with other methods\n",
    "- Apply Monte Carlo to RL environments\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of value functions (V(s), Q(s,a))\n",
    "- âœ… Understanding of episodes and returns\n",
    "- âœ… Python knowledge (functions, dictionaries, loops)\n",
    "- âœ… NumPy, Matplotlib knowledge\n",
    "- âœ… Basic RL concepts (states, actions, rewards, policies)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 2**:\n",
    "- Implementing Monte Carlo methods for estimating value functions\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Monte Carlo methods** learn value functions from experience (sample episodes). They don't require a model of the environment and use actual returns (sum of rewards) observed from episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nImplementing Monte Carlo Methods for Value Estimation\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Understanding Monte Carlo Methods\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Understanding Monte Carlo Methods\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nMonte Carlo Key Concepts:\")\n",
    "print(\" 1. Learn from complete episodes (must wait until episode ends)\")\n",
    "print(\" 2. Use actual returns: G_t = R_{t+1} + Î³R_{t+2} + Î³Â²R_{t+3} + ...\")\n",
    "print(\" 3. Update value estimates using: V(s) = average of returns\")\n",
    "print(\" 4. No model required (model-free method)\")\n",
    "\n",
    "print(\"\\nTwo Approaches:\")\n",
    "print(\" - First-visit MC: Average returns only for first occurrence of state in episode\")\n",
    "print(\" - Every-visit MC: Average returns for every occurrence of state in episode\")\n",
    "\n",
    "print(\"\\nAlgorithm:\")\n",
    "print(\" 1. Generate episode following policy Ï€\")\n",
    "print(\" 2. For each state s in episode:\")\n",
    "print(\" - Calculate return G from that state\")\n",
    "print(\" - Append G to Returns(s)\")\n",
    "print(\" - V(s) = average(Returns(s))\")\n",
    "\n",
    "print(\"\\nâœ… Monte Carlo concepts understood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: First-Visit Monte Carlo Implementation\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: First-Visit Monte Carlo Implementation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def generate_episode(policy, env, max_steps=100):\n",
    " \n",
    "    \"\"\"Generate an episode following the policy.\"\"\"\n",
    " episode = []\n",
    " state = env.reset()[0] if hasattr(env.reset(), '__len__') else env.reset()\n",
    " \n",
    " for step in range(max_steps):\n",
    " # Choose action based on policy\n",
    " if isinstance(policy, dict):\n",
    " action = policy.get(state, random.choice(range(env.action_space.n)))\n",
    " else:\n",
    " action = policy(state)\n",
    " \n",
    " # Take action (simplified - assuming env.step returns tuple)\n",
    " if hasattr(env, 'step'):\n",
    " next_state, reward, done, truncated, info = env.step(action) if hasattr(env.step(action), '__len__') and len(env.step(action)) > 1 else (None, 0, True, False, {})\n",
    " if isinstance(env.step(action), tuple) and len(env.step(action)) >= 2:\n",
    " next_state, reward = env.step(action)[:2]\n",
    " done = env.step(action)[2] if len(env.step(action)) > 2 else Falseelse:\n",
    " next_state, reward, done = state, 0, Trueelse:\n",
    " next_state, reward, done = state, 0, True\n",
    " \n",
    " episode.append((state, action, reward))\n",
    " state = next_state\n",
    " \n",
    " if done:\n",
    " break\n",
    " \n",
    " return episode\n",
    "\n",
    "def first_visit_mc(policy, env, n_episodes=1000, gamma=0.99):\n",
    " \n",
    "    \"\"\"\n",
    " First-visit Monte Carlo for estimating state values.\n",
    " \"\"\"\n",
    " returns = defaultdict(list)\n",
    " V = defaultdict(float)\n",
    " \n",
    " for episode_num in range(n_episodes):\n",
    " episode = generate_episode(policy, env)\n",
    " \n",
    " # Calculate returns\n",
    " G = 0\n",
    " visited_states = set()\n",
    " \n",
    " # Process episode backwards\n",
    " for t in reversed(range(len(episode))):\n",
    " state, action, reward = episode[t]\n",
    " G = gamma * G + reward\n",
    " \n",
    " # First-visit: only update if state not visited yet in this episode\n",
    " if state not in visited_states:\n",
    " visited_states.add(state)\n",
    " returns[state].append(G)\n",
    " V[state] = np.mean(returns[state])\n",
    " \n",
    " return V, returns\n",
    "\n",
    "# Simple example: Random walk\n",
    "print(\"\\nExample: Simple Random Walk\")\n",
    "print(\" States: [0, 1, 2, 3, 4]\")\n",
    "print(\" Actions: Move left (-1) or right (+1)\")\n",
    "print(\" Goal: Estimate state values\")\n",
    "\n",
    "# Simplified environment simulation\n",
    "class SimpleRandomWalk:\n",
    " def __init__(self):\n",
    " self.state = 2\n",
    " self.n_states = 5\n",
    " \n",
    " def reset(self):\n",
    " self.state = 2\n",
    " return self.state\n",
    " \n",
    " def step(self, action):\n",
    " self.state = max(0, min(4, self.state + action))\n",
    " reward = 1.0 if self.state == 4 else 0.0\n",
    " done = self.state in [0, 4]\n",
    " return self.state, reward, done\n",
    " \n",
    " @property\n",
    " def action_space(self):\n",
    " class Space:\n",
    " n = 2\n",
    " return Space()\n",
    "\n",
    "# Random policy\n",
    "def random_policy(state):\n",
    " return random.choice([-1, 1])\n",
    "\n",
    "env_simple = SimpleRandomWalk()\n",
    "V_mc, returns_mc = first_visit_mc(random_policy, env_simple, n_episodes=100, gamma=1.0)\n",
    "\n",
    "print(f\"\\nEstimated State Values (First-Visit MC):\")\n",
    "for state in sorted(V_mc.keys()):\n",
    " print(f\" V({state}) = {V_mc[state]:.4f} (from {len(returns_mc[state])} visits)\")\n",
    "\n",
    "print(\"\\nâœ… First-visit Monte Carlo implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3: Every-Visit Monte Carlo Implementation\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: Every-Visit Monte Carlo Implementation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def every_visit_mc(policy, env, n_episodes=1000, gamma=0.99):\n",
    " \n",
    "    \"\"\"\n",
    " Every-visit Monte Carlo for estimating state values.\n",
    " \"\"\"\n",
    " returns = defaultdict(list)\n",
    " V = defaultdict(float)\n",
    " \n",
    " for episode_num in range(n_episodes):\n",
    " episode = generate_episode(policy, env)\n",
    " \n",
    " # Calculate returns\n",
    " G = 0\n",
    " \n",
    " # Process episode backwards\n",
    " for t in reversed(range(len(episode))):\n",
    " state, action, reward = episode[t]\n",
    " G = gamma * G + reward\n",
    " \n",
    " # Every-visit: update for every occurrence\n",
    " returns[state].append(G)\n",
    " V[state] = np.mean(returns[state])\n",
    " \n",
    " return V, returns\n",
    "\n",
    "# Compare first-visit vs every-visit\n",
    "env_simple2 = SimpleRandomWalk()\n",
    "V_every, returns_every = every_visit_mc(random_policy, env_simple2, n_episodes=100, gamma=1.0)\n",
    "\n",
    "print(f\"\\nEstimated State Values (Every-Visit MC):\")\n",
    "for state in sorted(V_every.keys()):\n",
    " print(f\" V({state}) = {V_every[state]:.4f} (from {len(returns_every[state])} visits)\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\" First-visit: Fewer samples per state, more focused\")\n",
    "print(f\" Every-visit: More samples per state, can be more efficient\")\n",
    "\n",
    "print(\"\\nâœ… Every-visit Monte Carlo implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Monte Carlo Methods**: Learn value functions from sample episodes\n",
    "2. **Returns**: G_t = R_{t+1} + Î³R_{t+2} + Î³Â²R_{t+3} + ...\n",
    "3. **First-Visit MC**: Average returns only for first occurrence in episode\n",
    "4. **Every-Visit MC**: Average returns for every occurrence in episode\n",
    "5. **Model-Free**: Don't require environment dynamics model\n",
    "\n",
    "### Advantages:\n",
    "- Simple and intuitive\n",
    "- No model required\n",
    "- Works well with function approximation\n",
    "- Can focus on specific states\n",
    "\n",
    "### Disadvantages:\n",
    "- Requires complete episodes (can't be incremental)\n",
    "- High variance in estimates\n",
    "- Slow convergence\n",
    "- Only works for episodic tasks\n",
    "\n",
    "### Applications:\n",
    "- Policy evaluation\n",
    "- Game playing (episodic)\n",
    "- Episodic control problems\n",
    "\n",
    "### Next Steps:\n",
    "- Monte Carlo control (policy improvement)\n",
    "- Compare with TD methods\n",
    "- Apply to more complex environments\n",
    "\n",
    "**Reference:** Course 09, Unit 2: \"Prediction and Control without a Model\" - Monte Carlo methods practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}