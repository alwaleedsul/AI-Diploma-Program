{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RL for Resource Optimization\n## AIAT 123 - Reinforcement Learning\n\n## Learning Objectives\n\n- Apply RL to resource allocation problems\n- Optimize data center resource usage\n- Implement energy management systems\n- Compare RL with traditional optimization\n\n## Real-World Context\n\nData center optimization, energy grid management, and resource allocation.\n\n**Industry Impact**: Google uses RL to reduce data center energy by 40%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install numpy matplotlib -q\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint('\u2705 Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Resource Allocation Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResourceAllocationEnv:\n    \"\"\"\n    Simplified data center resource allocation environment.\n    \n    Real-world: Optimizing server allocation for workloads\n    \"\"\"\n    def __init__(self, n_servers=5, n_workloads=10):\n        self.n_servers = n_servers\n        self.n_workloads = n_workloads\n        self.reset()\n    \n    def reset(self):\n        \"\"\"Reset environment\"\"\"\n        self.server_loads = np.zeros(self.n_servers)\n        self.workloads = np.random.randint(1, 10, self.n_workloads)\n        self.current_workload = 0\n        return self.get_state()\n    \n    def get_state(self):\n        \"\"\"Get current state\"\"\"\n        return np.concatenate([self.server_loads, [self.workloads[self.current_workload]]])\n    \n    def step(self, action):\n        \"\"\"\n        Allocate workload to server.\n        \n        Reward: Negative of load imbalance + efficiency bonus\n        \"\"\"\n        if self.current_workload >= len(self.workloads):\n            return self.get_state(), 0, True, {}\n        \n        # Allocate workload\n        self.server_loads[action] += self.workloads[self.current_workload]\n        self.current_workload += 1\n        \n        # Calculate reward (negative load imbalance)\n        load_std = np.std(self.server_loads)\n        reward = -load_std - 0.1 * np.max(self.server_loads)  # Penalize overload\n        \n        done = self.current_workload >= len(self.workloads)\n        return self.get_state(), reward, done, {}\n\nprint('\u2705 Resource allocation environment created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Q-Learning for Resource Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Q-learning for resource allocation\nclass ResourceOptimizer:\n    \"\"\"RL-based resource optimizer\"\"\"\n    def __init__(self, n_servers, learning_rate=0.1, discount=0.95, epsilon=0.1):\n        self.n_servers = n_servers\n        self.q_table = {}\n        self.lr = learning_rate\n        self.gamma = discount\n        self.epsilon = epsilon\n    \n    def get_state_key(self, state):\n        \"\"\"Discretize state for Q-table\"\"\"\n        # Simple discretization\n        return tuple((state / 10).astype(int))\n    \n    def select_action(self, state):\n        \"\"\"Select server using epsilon-greedy\"\"\"\n        if np.random.random() < self.epsilon:\n            return np.random.randint(self.n_servers)\n        \n        state_key = self.get_state_key(state)\n        q_values = [self.q_table.get((state_key, a), 0.0) for a in range(self.n_servers)]\n        return np.argmax(q_values)\n    \n    def update(self, state, action, reward, next_state, done):\n        \"\"\"Update Q-values\"\"\"\n        state_key = self.get_state_key(state)\n        current_q = self.q_table.get((state_key, action), 0.0)\n        \n        if done:\n            target_q = reward\n        else:\n            next_key = self.get_state_key(next_state)\n            max_next_q = max([self.q_table.get((next_key, a), 0.0) for a in range(self.n_servers)])\n            target_q = reward + self.gamma * max_next_q\n        \n        self.q_table[(state_key, action)] = current_q + self.lr * (target_q - current_q)\n\nprint('\u2705 Resource optimizer implemented')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Applications\n\n- **Data Centers**: Google DeepMind reduced energy by 40%\n- **Energy Grids**: Load balancing and demand response\n- **Cloud Computing**: Auto-scaling and resource allocation\n- **Manufacturing**: Production line optimization\n\n---\n\n**End of Notebook**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}