{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RL Agent for Game Playing\n",
        "\n",
        "## ðŸ“š Learning Objectives\n",
        "\n",
        "By completing this notebook, you will:\n",
        "- Understand the key concepts of this topic\n",
        "- Apply the topic using Python code examples\n",
        "- Practice with small, realistic datasets or scenarios\n",
        "\n",
        "## ðŸ”— Prerequisites\n",
        "\n",
        "- âœ… Basic Python\n",
        "- âœ… Basic NumPy/Pandas (when applicable)\n",
        "\n",
        "---\n",
        "\n",
        "## Official Structure Reference\n",
        "\n",
        "This notebook supports **Course 09, Unit 5** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RL Agent for Game Playing\n## AIAT 123 - Reinforcement Learning\n\n## Learning Objectives\n\n- Build RL agent for game playing\n- Implement Q-learning for games\n- Train agent to play Connect 4\n- Evaluate agent performance\n\n## Real-World Context\n\nGame AI development, strategy games, and competitive AI.\n\n**Industry Impact**: Powers game AI in chess, Go, video games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install numpy -q\nimport numpy as np\nprint('âœ… Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 1: Simple Game Environment (Tic-Tac-Toe)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TicTacToe:\n    \"\"\"Simple Tic-Tac-Toe game environment\"\"\"\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.board = np.zeros((3, 3), dtype=int)\n        self.current_player = 1\n        return self.board.copy()\n    \n    def step(self, action):\n        row, col = action // 3, action % 3\n        if self.board[row, col] != 0:\n            return self.board.copy(), -10, True, {}  # Invalid move\n        \n        self.board[row, col] = self.current_player\n        \n        # Check win\n        if self.check_win():\n            return self.board.copy(), 10, True, {}\n        \n        # Check draw\n        if np.all(self.board != 0):\n            return self.board.copy(), 0, True, {}\n        \n        self.current_player = -self.current_player\n        return self.board.copy(), 0, False, {}\n    \n    def check_win(self):\n        \"\"\"Check if current player won\"\"\"\n        player = self.current_player\n        # Check rows, columns, diagonals\n        for i in range(3):\n            if np.all(self.board[i] == player) or np.all(self.board[:, i] == player):\n                return True\n        if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n            return True\n        return False\n\nprint('âœ… Game environment created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 2: Q-Learning Agent\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningAgent:\n    \"\"\"Q-learning agent for game playing\"\"\"\n    def __init__(self, learning_rate=0.1, discount=0.95, epsilon=0.1):\n        self.q_table = {}\n        self.lr = learning_rate\n        self.gamma = discount\n        self.epsilon = epsilon\n    \n    def get_state_key(self, board):\n        \"\"\"Convert board to hashable key\"\"\"\n        return tuple(board.flatten())\n    \n    def get_q_value(self, state, action):\n        \"\"\"Get Q-value for state-action pair\"\"\"\n        key = (self.get_state_key(state), action)\n        return self.q_table.get(key, 0.0)\n    \n    def update_q_value(self, state, action, reward, next_state, done):\n        \"\"\"Update Q-value using Q-learning\"\"\"\n        key = (self.get_state_key(state), action)\n        current_q = self.q_table.get(key, 0.0)\n        \n        if done:\n            target_q = reward\n        else:\n            next_actions = [i for i in range(9) if next_state.flatten()[i] == 0]\n            if next_actions:\n                max_next_q = max([self.get_q_value(next_state, a) for a in next_actions])\n                target_q = reward + self.gamma * max_next_q\n            else:\n                target_q = reward\n        \n        self.q_table[key] = current_q + self.lr * (target_q - current_q)\n    \n    def select_action(self, state, available_actions):\n        \"\"\"Select action using epsilon-greedy\"\"\"\n        if np.random.random() < self.epsilon:\n            return np.random.choice(available_actions)\n        \n        q_values = [self.get_q_value(state, a) for a in available_actions]\n        return available_actions[np.argmax(q_values)]\n\nprint('âœ… Q-learning agent implemented')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Applications\n\n- **Chess/Go**: DeepMind AlphaZero\n- **Video Games**: Dota 2, StarCraft II\n- **Board Games**: Connect 4, Checkers\n- **Puzzle Games**: Rubik's Cube solving\n\n---\n\n**End of Notebook**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}