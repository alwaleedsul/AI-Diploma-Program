{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up RL Environment: OpenAI Gym\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Install and set up OpenAI Gym\n",
    "- Understand Gym environments (spaces, observations, actions)\n",
    "- Interact with Gym environments (step, reset, render)\n",
    "- Explore different Gym environments (CartPole, FrozenLake, etc.)\n",
    "- Set up Python-based frameworks for RL\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Python 3.8+ installed\n",
    "- âœ… pip package manager\n",
    "- âœ… Basic Python knowledge (functions, classes, loops)\n",
    "- âœ… Understanding of RL basics (agent, environment, rewards)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 1**:\n",
    "- Setting up RL environment: installing OpenAI Gym and using Python-based frameworks for RL\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 1 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**OpenAI Gym** is a toolkit for developing and comparing reinforcement learning algorithms. It provides a standardized interface for RL environments, making it easy to test algorithms across different problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenAI Gym (run this once)\n",
    "# !pip install gym matplotlib numpy\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(f\"OpenAI Gym version: {gym.__version__}\")\n",
    "print(\"\\nSetting up RL Environment: OpenAI Gym\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Installing and Setting Up OpenAI Gym\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Installing and Setting Up OpenAI Gym\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Installation:\")\n",
    "print(\" pip install gym\")\n",
    "print(\" (or: pip install gymnasium # newer version)\")\n",
    "\n",
    "print(\"\\n2. Import gym:\")\n",
    "print(\" import gym\")\n",
    "\n",
    "print(\"\\n3. List available environments:\")\n",
    "print(\" from gym import envs\")\n",
    "print(\" print(envs.registry.all())\")\n",
    "\n",
    "# List some popular environments\n",
    "print(\"\\n3. Popular Gym Environments:\")\n",
    "popular_envs = [\n",
    " \"CartPole-v1\", \"FrozenLake-v1\",\n",
    " \"MountainCar-v0\",\n",
    " \"Acrobot-v1\",\n",
    " \"LunarLander-v2\"\n",
    "]\n",
    "for env_name in popular_envs:\n",
    " print(f\" - {env_name}\")\n",
    "\n",
    "print(\"\\nâœ… Gym setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: Understanding Gym Environments\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Understanding Gym Environments\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple environment (CartPole)\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "print(\"\\nEnvironment: CartPole-v1\")\n",
    "print(f\" Observation space: {env.observation_space}\")\n",
    "print(f\" Action space: {env.action_space}\")\n",
    "\n",
    "# Get initial observation\n",
    "observation, info = env.reset()\n",
    "print(f\"\\n Initial observation: {observation}\")\n",
    "print(f\" Observation shape: {observation.shape}\")\n",
    "print(f\" Observation type: {type(observation)}\")\n",
    "\n",
    "print(\"\\nKey Gym Environment Attributes:\")\n",
    "print(\" - observation_space: Defines valid observations\")\n",
    "print(\" - action_space: Defines valid actions\")\n",
    "print(\" - reset(): Resets environment, returns initial observation\")\n",
    "print(\" - step(action): Takes action, returns (obs, reward, done, info)\")\n",
    "print(\" - render(): Visualizes environment state\")\n",
    "\n",
    "env.close()\n",
    "print(\"\\nâœ… Environment understanding complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3: Interacting with Gym Environments\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: Interacting with Gym Environments\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Reset environment\n",
    "observation, info = env.reset()\n",
    "print(f\"\\nInitial observation: {observation}\")\n",
    "\n",
    "# Take a few random actions\n",
    "total_reward = 0\n",
    "for step in range(5):\n",
    " # Sample random action\n",
    " action = env.action_space.sample()\n",
    " \n",
    " # Take step\n",
    " observation, reward, terminated, truncated, info = env.step(action)\n",
    " \n",
    " done = terminated or truncated\n",
    " total_reward += reward\n",
    " \n",
    " print(f\"\\nStep {step + 1}:\")\n",
    " print(f\" Action: {action}\")\n",
    " print(f\" Reward: {reward}\")\n",
    " print(f\" Done: {done}\")\n",
    " print(f\" Observation: {observation[:2]}...\") # Show first 2 values\n",
    " \n",
    " if done:\n",
    " print(\" Episode ended!\")\n",
    " observation, info = env.reset()\n",
    " break\n",
    "\n",
    "print(f\"\\nTotal reward: {total_reward}\")\n",
    "env.close()\n",
    "\n",
    "print(\"\\nâœ… Environment interaction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4: Exploring Different Gym Environments\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 4: Exploring Different Gym Environments\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List of environments to explore\n",
    "environments = [\n",
    " ('CartPole-v1', 'Classic control problem'),\n",
    " ('FrozenLake-v1', 'Grid world navigation'),\n",
    " ('MountainCar-v0', 'Continuous control challenge'),\n",
    "]\n",
    "\n",
    "for env_name, description in environments:\n",
    " try:\n",
    " env = gym.make(env_name)\n",
    " obs, info = env.reset()\n",
    " \n",
    " print(f\"\\n{env_name}:\")\n",
    " print(f\" Description: {description}\")\n",
    " print(f\" Observation space: {env.observation_space}\")\n",
    " print(f\" Action space: {env.action_space}\")\n",
    " print(f\" Observation shape: {obs.shape if hasattr(obs, 'shape') else 'N/A'}\")\n",
    " \n",
    " env.close()\n",
    " except Exception as e:\n",
    " print(f\"\\n{env_name}: Error - {e}\")\n",
    "\n",
    "print(\"\\nâœ… Environment exploration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **OpenAI Gym**: Standard toolkit for RL environments\n",
    "2. **Environment Interface**: reset(), step(action), render()\n",
    "3. **Spaces**: Observation space and action space define valid inputs/outputs\n",
    "4. **Environments**: CartPole, FrozenLake, MountainCar, etc.\n",
    "\n",
    "### Setup Steps:\n",
    "1. Install: `pip install gym`\n",
    "2. Import: `import gym`\n",
    "3. Create: `env = gym.make('EnvironmentName-v1')`\n",
    "4. Interact: reset, step, render, close\n",
    "\n",
    "### Common Environments:\n",
    "- **CartPole-v1**: Balance a pole on a cart\n",
    "- **FrozenLake-v1**: Navigate a frozen lake grid\n",
    "- **MountainCar-v0**: Drive a car up a hill\n",
    "- **LunarLander-v2**: Land a spaceship\n",
    "\n",
    "### Next Steps:\n",
    "- Implement RL algorithms (Q-learning, DQN)\n",
    "- Train agents in these environments\n",
    "- Compare algorithm performance\n",
    "\n",
    "**Reference:** Course 09, Unit 1: \"Introduction to Reinforcement Learning\" - Setting up RL environment practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}