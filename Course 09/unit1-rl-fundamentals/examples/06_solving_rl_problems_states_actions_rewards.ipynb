{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving RL Problems: Defining States, Actions, and Rewards\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Define state spaces for RL problems\n",
    "- Define action spaces for RL problems\n",
    "- Design reward functions\n",
    "- Run RL simulations\n",
    "- Apply these concepts to real environments\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… OpenAI Gym setup\n",
    "- âœ… Understanding of RL basics (agent, environment, episode)\n",
    "- âœ… Python knowledge (functions, classes, loops)\n",
    "- âœ… NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 1**:\n",
    "- Solving RL problems: defining states, actions, and rewards, running RL simulations\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 1 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Solving RL problems** requires carefully defining:\n",
    "1. **States**: What the agent observes from the environment\n",
    "2. **Actions**: What the agent can do\n",
    "3. **Rewards**: Feedback signals that guide learning\n",
    "\n",
    "Proper definition of these components is crucial for successful RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nSolving RL Problems: States, Actions, and Rewards\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Defining State Spaces\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Defining State Spaces\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "print(\"\\nCartPole-v1 Environment:\")\n",
    "observation, info = env.reset()\n",
    "print(f\" State/Observation: {observation}\")\n",
    "print(f\" State shape: {observation.shape}\")\n",
    "print(f\" State type: {type(observation)}\")\n",
    "print(f\" State space: {env.observation_space}\")\n",
    "\n",
    "print(\"\\nState Components (CartPole):\")\n",
    "print(\" [0] Cart position\")\n",
    "print(\" [1] Cart velocity\")\n",
    "print(\" [2] Pole angle\")\n",
    "print(\" [3] Pole angular velocity\")\n",
    "\n",
    "# Create FrozenLake environment\n",
    "env_fl = gym.make('FrozenLake-v1', render_mode='ansi')\n",
    "obs_fl, info_fl = env_fl.reset()\n",
    "print(f\"\\nFrozenLake-v1 Environment:\")\n",
    "print(f\" State/Observation: {obs_fl}\")\n",
    "print(f\" State space: {env_fl.observation_space}\")\n",
    "print(f\" State type: Discrete (single integer representing grid position)\")\n",
    "\n",
    "env.close()\n",
    "env_fl.close()\n",
    "\n",
    "print(\"\\nâœ… State spaces defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: Defining Action Spaces\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Defining Action Spaces\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CartPole actions\n",
    "env = gym.make('CartPole-v1')\n",
    "print(\"\\nCartPole-v1 Actions:\")\n",
    "print(f\" Action space: {env.action_space}\")\n",
    "print(f\" Number of actions: {env.action_space.n}\")\n",
    "print(\" Actions:\")\n",
    "print(\" 0: Push cart to the left\")\n",
    "print(\" 1: Push cart to the right\")\n",
    "\n",
    "# FrozenLake actions\n",
    "env_fl = gym.make('FrozenLake-v1')\n",
    "obs_fl, info_fl = env_fl.reset()\n",
    "print(f\"\\nFrozenLake-v1 Actions:\")\n",
    "print(f\" Action space: {env_fl.action_space}\")\n",
    "print(f\" Number of actions: {env_fl.action_space.n}\")\n",
    "print(\" Actions:\")\n",
    "print(\" 0: Move left\")\n",
    "print(\" 1: Move down\")\n",
    "print(\" 2: Move right\")\n",
    "print(\" 3: Move up\")\n",
    "\n",
    "env.close()\n",
    "env_fl.close()\n",
    "\n",
    "print(\"\\nâœ… Action spaces defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3: Designing Reward Functions\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: Designing Reward Functions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nReward Function Design Principles:\")\n",
    "print(\" 1. Provide clear feedback (positive for good, negative for bad)\")\n",
    "print(\" 2. Shape rewards to guide learning (sparse vs dense)\")\n",
    "print(\" 3. Balance immediate vs long-term rewards\")\n",
    "print(\" 4. Avoid reward hacking (unintended behaviors)\")\n",
    "\n",
    "# Test CartPole rewards\n",
    "env = gym.make('CartPole-v1')\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(\"\\nCartPole-v1 Reward Function:\")\n",
    "print(\" +1 for each step the pole remains balanced\")\n",
    "print(\" Episode ends when pole falls or cart goes out of bounds\")\n",
    "print(\" Maximum reward: 500 (episode length limit)\")\n",
    "\n",
    "total_reward = 0\n",
    "for step in range(10):\n",
    " action = env.action_space.sample()\n",
    " obs, reward, terminated, truncated, info = env.step(action)\n",
    " done = terminated or truncated\n",
    " total_reward += reward\n",
    " print(f\" Step {step+1}: Reward = {reward}, Total = {total_reward}\")\n",
    " if done:\n",
    " break\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nâœ… Reward functions understood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4: Running RL Simulations\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 4: Running RL Simulations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def run_random_episode(env, max_steps=100):\n",
    " \n",
    "    \"\"\"Run a single episode with random actions.\"\"\"\n",
    " obs, info = env.reset()\n",
    " total_reward = 0\n",
    " steps = 0\n",
    " \n",
    " for step in range(max_steps):\n",
    " action = env.action_space.sample()\n",
    " obs, reward, terminated, truncated, info = env.step(action)\n",
    " done = terminated or truncated\n",
    " total_reward += reward\n",
    " steps += 1\n",
    " \n",
    " if done:\n",
    " break\n",
    " \n",
    " return total_reward, steps\n",
    "\n",
    "# Run multiple episodes\n",
    "env = gym.make('CartPole-v1')\n",
    "n_episodes = 10\n",
    "\n",
    "print(f\"\\nRunning {n_episodes} random episodes in CartPole-v1:\")\n",
    "rewards = []\n",
    "steps_list = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    " reward, steps = run_random_episode(env)\n",
    " rewards.append(reward)\n",
    " steps_list.append(steps)\n",
    " print(f\" Episode {episode+1}: Reward = {reward:.1f}, Steps = {steps}\")\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\" Average reward: {np.mean(rewards):.2f}\")\n",
    "print(f\" Average steps: {np.mean(steps_list):.2f}\")\n",
    "print(f\" Best reward: {np.max(rewards):.2f}\")\n",
    "print(f\" Worst reward: {np.min(rewards):.2f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\nâœ… RL simulations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Components:\n",
    "1. **States**: What the agent observes (observation space)\n",
    "   - Continuous states (CartPole: position, velocity, angle)\n",
    "   - Discrete states (FrozenLake: grid position)\n",
    "2. **Actions**: What the agent can do (action space)\n",
    "   - Discrete actions (CartPole: left/right, FrozenLake: up/down/left/right)\n",
    "   - Continuous actions (MountainCar: acceleration)\n",
    "3. **Rewards**: Feedback signals\n",
    "   - Dense rewards (CartPole: +1 per step)\n",
    "   - Sparse rewards (FrozenLake: +1 only at goal)\n",
    "\n",
    "### Design Principles:\n",
    "- **States**: Include all relevant information for decision-making\n",
    "- **Actions**: Cover all possible behaviors the agent can take\n",
    "- **Rewards**: Provide clear feedback, shape learning, avoid hacking\n",
    "\n",
    "### Simulation Process:\n",
    "1. Reset environment (get initial state)\n",
    "2. Loop: Select action â†’ Step environment â†’ Receive reward â†’ Update state\n",
    "3. Repeat until episode ends\n",
    "4. Reset and repeat for multiple episodes\n",
    "\n",
    "### Next Steps:\n",
    "- Implement learning algorithms (Q-learning, policy gradients)\n",
    "- Train agents to maximize rewards\n",
    "- Evaluate performance across episodes\n",
    "\n",
    "**Reference:** Course 09, Unit 1: \"Introduction to Reinforcement Learning\" - Solving RL problems practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}