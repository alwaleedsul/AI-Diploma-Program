{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving RL Problems: Defining States, Actions, and Rewards\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Define state spaces for RL problems\n",
    "- Define action spaces for RL problems\n",
    "- Design reward functions\n",
    "- Run RL simulations\n",
    "- Apply these concepts to real environments\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… OpenAI Gym setup\n",
    "- âœ… Understanding of RL basics (agent, environment, episode)\n",
    "- âœ… Python knowledge (functions, classes, loops)\n",
    "- âœ… NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 1**:\n",
    "- Solving RL problems: defining states, actions, and rewards, running RL simulations\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 1 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Solving RL problems** requires carefully defining:\n",
    "1. **States**: What the agent observes from the environment\n",
    "2. **Actions**: What the agent can do\n",
    "3. **Rewards**: Feedback signals that guide learning\n",
    "\n",
    "Proper definition of these components is crucial for successful RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nSolving RL Problems: States, Actions, and Rewards\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining State Spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Defining State Spaces\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Defining Action Spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Defining Action Spaces\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CartPole actions\n",
    "env = gym.make('CartPole-v1')\n",
    "print(\"\\nCartPole-v1 Actions:\")\n",
    "print(f\" Action space: {env.action_space}\")\n",
    "print(f\" Number of actions: {env.action_space.n}\")\n",
    "print(\" Actions:\")\n",
    "print(\" 0: Push cart to the left\")\n",
    "print(\" 1: Push cart to the right\")\n",
    "\n",
    "# FrozenLake actions\n",
    "env\n",
    "fl = gym.make('FrozenLake-v1')\n",
    "obs_fl, info\n",
    "fl = env\n",
    "fl.reset()\n",
    "print(f\"\\nFrozenLake-v1 Actions:\")\n",
    "print(f\" Action space: {env_fl.action_space}\")\n",
    "print(f\" Number of actions: {env_fl.action_space.n}\")\n",
    "print(\" Actions:\")\n",
    "print(\" 0: Move left\")\n",
    "print(\" 1: Move down\")\n",
    "print(\" 2: Move right\")\n",
    "print(\" 3: Move up\")\n",
    "\n",
    "env.close()\n",
    "env_fl.close()\n",
    "\n",
    "print(\"\\nâœ… Action spaces defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Designing Reward Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: Designing Reward Functions\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Running RL Simulations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 4: Running RL Simulations\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Components:\n",
    "1. **States**: What the agent observes (observation space)\n",
    "   - Continuous states (CartPole: position, velocity, angle)\n",
    "   - Discrete states (FrozenLake: grid position)\n",
    "2. **Actions**: What the agent can do (action space)\n",
    "   - Discrete actions (CartPole: left/right, FrozenLake: up/down/left/right)\n",
    "   - Continuous actions (MountainCar: acceleration)\n",
    "3. **Rewards**: Feedback signals\n",
    "   - Dense rewards (CartPole: +1 per step)\n",
    "   - Sparse rewards (FrozenLake: +1 only at goal)\n",
    "\n",
    "### Design Principles:\n",
    "- **States**: Include all relevant information for decision-making\n",
    "- **Actions**: Cover all possible behaviors the agent can take\n",
    "- **Rewards**: Provide clear feedback, shape learning, avoid hacking\n",
    "\n",
    "### Simulation Process:\n",
    "1. Reset environment (get initial state)\n",
    "2. Loop: Select action â†’ Step environment â†’ Receive reward â†’ Update state\n",
    "3. Repeat until episode ends\n",
    "4. Reset and repeat for multiple episodes\n",
    "\n",
    "### Next Steps:\n",
    "- Implement learning algorithms (Q-learning, policy gradients)\n",
    "- Train agents to maximize rewards\n",
    "- Evaluate performance across episodes\n",
    "\n",
    "**Reference:** Course 09, Unit 1: \"Introduction to Reinforcement Learning\" - Solving RL problems practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}