{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Projects: CartPole, FrozenLake, Q-learning, and DQN\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Implement Q-learning algorithm\n",
    "- Apply Q-learning to FrozenLake environment\n",
    "- Apply Q-learning to CartPole environment (with state discretization)\n",
    "- Understand Deep Q-Network (DQN) concepts\n",
    "- Train and evaluate RL agents on classic environments\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… OpenAI Gym setup\n",
    "- âœ… Understanding of states, actions, rewards\n",
    "- âœ… Epsilon-Greedy exploration strategy\n",
    "- âœ… Python knowledge (functions, classes, loops, dictionaries)\n",
    "- âœ… NumPy knowledge\n",
    "- âœ… Basic understanding of neural networks (for DQN section)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 1**:\n",
    "- Mini projects: applying RL in games like CartPole and FrozenLake, implementing Q-learning and DQN\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 1 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook combines multiple mini projects:\n",
    "1. **Q-learning on FrozenLake**: Classic grid-world problem with discrete states\n",
    "2. **Q-learning on CartPole**: Continuous state space requiring discretization\n",
    "3. **DQN Introduction**: Deep reinforcement learning for high-dimensional states\n",
    "\n",
    "These projects demonstrate practical RL applications on classic benchmark environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nMini Projects: CartPole, FrozenLake, Q-learning, and DQN\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Q-Learning Algorithm Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Q-Learning Algorithm Implementation\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Q-Learning on FrozenLake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Q-Learning on FrozenLake\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Q-Learning on CartPole (with State Discretization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: Q-Learning on CartPole (with State Discretization)\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Introduction to Deep Q-Network (DQN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 4: Introduction to Deep Q-Network (DQN)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nDQN Overview:\")\n",
    "print(\" - Uses neural network to approximate Q-function (instead of Q-table)\")\n",
    "print(\" - Handles high-dimensional state spaces (e.g., images)\")\n",
    "print(\" - Key innovations:\")\n",
    "print(\" 1. Experience Replay: Store transitions, sample randomly for training\")\n",
    "print(\" 2. Target Network: Separate network for stable Q-targets\")\n",
    "print(\" 3. Neural Network: Approximate Q(s,a) for continuous/ high-dim states\")\n",
    "\n",
    "print(\"\\nDQN Architecture:\")\n",
    "print(\" Input: State (e.g., image, observation vector)\")\n",
    "print(\" Network: Fully connected or CNN layers\")\n",
    "print(\" Output: Q-values for each action\")\n",
    "\n",
    "print(\"\\nDQN vs Q-Learning:\")\n",
    "print(\" Q-Learning:\")\n",
    "print(\" - Uses Q-table (discrete states only)\")\n",
    "print(\" - Limited to small state spaces\")\n",
    "print(\" - Fast for tabular problems\")\n",
    "print(\" DQN:\")\n",
    "print(\" - Uses neural network (continuous/high-dim states)\")\n",
    "print(\" - Scales to complex problems (e.g., Atari games)\")\n",
    "print(\" - Requires more computation and tuning\")\n",
    "\n",
    "print(\"\\nDQN Algorithm (High-Level):\")\n",
    "print(\" 1. Initialize Q-network and target network\")\n",
    "print(\" 2. For each episode:\")\n",
    "print(\" a. Observe state\")\n",
    "print(\" b. Choose action using epsilon-greedy (using Q-network)\")\n",
    "print(\" c. Take action, store transition in replay buffer\")\n",
    "print(\" d. Sample batch from replay buffer\")\n",
    "print(\" e. Compute targets using target network\")\n",
    "print(\" f. Update Q-network using loss: (Q(s,a) - target)^2\")\n",
    "print(\" g. Periodically update target network\")\n",
    "\n",
    "print(\"\\nNote: Full DQN implementation will be covered in Unit 3 (Deep RL)\")\n",
    "print(\"This is an introduction to the concepts.\")\n",
    "\n",
    "print(\"\\nâœ… DQN introduction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Q-Learning**: Off-policy TD learning algorithm\n",
    "   - Updates: Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]\n",
    "   - Uses Q-table for discrete states\n",
    "   - Epsilon-greedy exploration\n",
    "\n",
    "2. **FrozenLake**: Discrete grid-world environment\n",
    "   - Perfect for tabular Q-learning\n",
    "   - 16 states, 4 actions\n",
    "   - Slippery/unslippery variants\n",
    "\n",
    "3. **CartPole**: Continuous state space\n",
    "   - Requires state discretization for Q-learning\n",
    "   - 4 continuous state variables â†’ discrete bins\n",
    "   - Alternative: Use DQN for continuous states\n",
    "\n",
    "4. **DQN**: Deep Q-Network\n",
    "   - Neural network approximates Q-function\n",
    "   - Handles high-dimensional/continuous states\n",
    "   - Experience replay and target networks for stability\n",
    "\n",
    "### Implementation Highlights:\n",
    "- **Q-Learning**: Tabular method, fast for discrete problems\n",
    "- **State Discretization**: Convert continuous to discrete for Q-learning\n",
    "- **Epsilon Decay**: Reduce exploration over time\n",
    "- **DQN**: Deep learning extension for complex problems\n",
    "\n",
    "### Best Practices:\n",
    "- Start with high epsilon (exploration), decay over time\n",
    "- Tune learning rate (alpha) and discount factor (gamma)\n",
    "- Monitor learning curves and success rates\n",
    "- Use experience replay and target networks for DQN stability\n",
    "\n",
    "### Next Steps:\n",
    "- Unit 2: Advanced Q-learning (SARSA, TD methods)\n",
    "- Unit 3: Deep RL (DQN, Actor-Critic, PPO)\n",
    "- Unit 4: Exploration strategies\n",
    "- Unit 5: Advanced applications\n",
    "\n",
    "**Reference:** Course 09, Unit 1: \"Introduction to Reinforcement Learning\" - Mini projects practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}