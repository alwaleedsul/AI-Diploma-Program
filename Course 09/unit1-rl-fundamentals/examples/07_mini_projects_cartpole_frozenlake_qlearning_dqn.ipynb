{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Projects: CartPole, FrozenLake, Q-learning, and DQN\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Implement Q-learning algorithm\n",
    "- Apply Q-learning to FrozenLake environment\n",
    "- Apply Q-learning to CartPole environment (with state discretization)\n",
    "- Understand Deep Q-Network (DQN) concepts\n",
    "- Train and evaluate RL agents on classic environments\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… OpenAI Gym setup\n",
    "- âœ… Understanding of states, actions, rewards\n",
    "- âœ… Epsilon-Greedy exploration strategy\n",
    "- âœ… Python knowledge (functions, classes, loops, dictionaries)\n",
    "- âœ… NumPy knowledge\n",
    "- âœ… Basic understanding of neural networks (for DQN section)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 1**:\n",
    "- Mini projects: applying RL in games like CartPole and FrozenLake, implementing Q-learning and DQN\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 1 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook combines multiple mini projects:\n",
    "1. **Q-learning on FrozenLake**: Classic grid-world problem with discrete states\n",
    "2. **Q-learning on CartPole**: Continuous state space requiring discretization\n",
    "3. **DQN Introduction**: Deep reinforcement learning for high-dimensional states\n",
    "\n",
    "These projects demonstrate practical RL applications on classic benchmark environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nMini Projects: CartPole, FrozenLake, Q-learning, and DQN\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Q-Learning Algorithm Implementation\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Q-Learning Algorithm Implementation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def epsilon_greedy_action(q_table, state, epsilon, n_actions):\n",
    " \n",
    "    \n",
    "    \"\"\"Choose action using epsilon-greedy strategy.\"\"\"\n",
    " if random.random() < epsilon:\n",
    " return random.randint(0, n_actions - 1)\n",
    " else:\n",
    " return np.argmax(q_table[state])\n",
    "\n",
    "def q_learning_update(q_table, state, action, reward, next_state, alpha, gamma):\n",
    " \n",
    "    \n",
    "    \"\"\"\n",
    " Q-learning update rule:\n",
    " Q(s,a) = Q(s,a) + Î±[r + Î³ * max(Q(s',a')) - Q(s,a)]\n",
    " \"\"\"\n",
    " current_q = q_table[state, action]\n",
    " max_next_q = np.max(q_table[next_state])\n",
    " new_q = current_q + alpha * (reward + gamma * max_next_q - current_q)\n",
    " q_table[state, action] = new_q\n",
    " return q_table\n",
    "\n",
    "print(\"\\nQ-Learning Algorithm:\")\n",
    "print(\" 1. Initialize Q-table (states Ã— actions)\")\n",
    "print(\" 2. For each episode:\")\n",
    "print(\" a. Initialize state\")\n",
    "print(\" b. While not done:\")\n",
    "print(\" - Choose action using epsilon-greedy\")\n",
    "print(\" - Take action, observe reward and next state\")\n",
    "print(\" - Update Q-table: Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]\")\n",
    "print(\" - Set state = next_state\")\n",
    "print(\" 3. Return Q-table\")\n",
    "\n",
    "print(\"\\nKey Parameters:\")\n",
    "print(\" - Î± (alpha): Learning rate (0.0 to 1.0)\")\n",
    "print(\" - Î³ (gamma): Discount factor (0.0 to 1.0)\")\n",
    "print(\" - Îµ (epsilon): Exploration rate\")\n",
    "print(\" - Q-table: State-action value function\")\n",
    "\n",
    "print(\"\\nâœ… Q-Learning algorithm understood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: Q-Learning on FrozenLake\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Q-Learning on FrozenLake\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create FrozenLake environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "\n",
    "# Q-learning parameters\n",
    "n_states = env.observation_space.nn_actions = env.action_space.n\n",
    "q_table = np.zeros((n_states, n_actions))\n",
    "# Training parameters\n",
    "n_episodes = 5000\n",
    "alpha = 0.1 # Learning rategamma = 0.99 # Discount factor\n",
    "epsilon = 1.0 # Start with full exploration\n",
    "epsilon_decay = 0.9995\n",
    "epsilon_min = 0.01\n",
    "\n",
    "rewards_history = []\n",
    "\n",
    "print(f\"\\nEnvironment: FrozenLake-v1\")\n",
    "print(f\" States: {n_states}\")\n",
    "print(f\" Actions: {n_actions}\")\n",
    "print(f\"\\nTraining for {n_episodes} episodes...\")\n",
    "\n",
    "# Training loop\n",
    "for episode in range(n_episodes):\n",
    " state, info = env.reset()\n",
    " total_reward = 0\n",
    " done = False\n",
    " \n",
    " while not done:\n",
    " # Choose action using epsilon-greedy\n",
    " action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
    " \n",
    " # Take action\n",
    " next_state, reward, terminated, truncated, info = env.step(action)\n",
    " done = terminated or truncated\n",
    " \n",
    " # Update Q-table\n",
    " q_learning_update(q_table, state, action, reward, next_state, alpha, gamma)\n",
    " \n",
    " state = next_state\n",
    " total_reward += reward\n",
    " \n",
    " rewards_history.append(total_reward)\n",
    " \n",
    " # Decay epsilon\n",
    " epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    " \n",
    " if (episode + 1) % 500 == 0:\n",
    " avg_reward = np.mean(rewards_history[-500:])\n",
    " success_rate = np.mean(rewards_history[-500:])\n",
    " print(f\" Episode {episode+1}: Avg reward = {avg_reward:.3f}, Success rate = {success_rate:.3f}, Îµ = {epsilon:.3f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Evaluate trained agent\n",
    "print(f\"\\nEvaluating trained agent...\")\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "test_episodes = 100\n",
    "successes = 0\n",
    "\n",
    "for episode in range(test_episodes):\n",
    " state, info = env.reset()\n",
    " done = False\n",
    " \n",
    " while not done:\n",
    " action = np.argmax(q_table[state]) # Greedy policy\n",
    " state, reward, terminated, truncated, info = env.step(action)\n",
    " done = terminated or truncated\n",
    " \n",
    " if reward > 0:\n",
    " successes += 1\n",
    " break\n",
    "\n",
    "success_rate = successes / test_episodes\n",
    "print(f\"Success rate: {success_rate:.2%} ({successes}/{test_episodes})\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Visualize learning\n",
    "plt.figure(figsize=(10, 6))\n",
    "window_size = 100\n",
    "if len(rewards_history) >= window_size:\n",
    " smoothed = np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid')\n",
    " plt.plot(smoothed, label=f'Smoothed (window={window_size})')\n",
    "plt.plot(rewards_history, alpha=0.3, label='Raw rewards')\n",
    "plt.xlabel('Episode', fontsize=12)\n",
    "plt.ylabel('Reward', fontsize=12)\n",
    "plt.title('Q-Learning on FrozenLake: Training Progress', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Q-Learning on FrozenLake complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3: Q-Learning on CartPole (with State Discretization)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: Q-Learning on CartPole (with State Discretization)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def discretize_state(observation, bins):\n",
    " \n",
    "    \n",
    "    \"\"\"Discretize continuous state into discrete bins.\"\"\"\n",
    " cart_pos, cart_vel, pole_angle, pole_vel = observationstate_idx = (\n",
    " np.digitize(cart_pos, bins[0]) * len(bins[1]) * len(bins[2]) * len(bins[3]) +\n",
    " np.digitize(cart_vel, bins[1]) * len(bins[2]) * len(bins[3]) +\n",
    " np.digitize(pole_angle, bins[2]) * len(bins[3]) +\n",
    " np.digitize(pole_vel, bins[3])\n",
    " )\n",
    " return min(state_idx, len(bins[0]) * len(bins[1]) * len(bins[2]) * len(bins[3]) - 1)\n",
    "\n",
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Discretization bins\n",
    "n_bins = 10\n",
    "cart_pos_bins = np.linspace(-2.4, 2.4, n_bins)\n",
    "cart_vel_bins = np.linspace(-3.0, 3.0, n_bins)\n",
    "pole_angle_bins = np.linspace(-0.2, 0.2, n_bins)\n",
    "pole_vel_bins = np.linspace(-3.0, 3.0, n_bins)\n",
    "bins = [cart_pos_bins, cart_vel_bins, pole_angle_bins, pole_vel_bins]\n",
    "\n",
    "n_states = n_bins ** 4\n",
    "n_actions = env.action_space.nq_table = np.zeros((n_states, n_actions))\n",
    "# Training parameters\n",
    "n_episodes = 5000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "\n",
    "rewards_history = []\n",
    "\n",
    "print(f\"\\nEnvironment: CartPole-v1\")\n",
    "print(f\" Discrete states: {n_states}\")\n",
    "print(f\" Actions: {n_actions}\")\n",
    "print(f\" Bins per dimension: {n_bins}\")\n",
    "print(f\"\\nTraining for {n_episodes} episodes...\")\n",
    "\n",
    "# Training loop\n",
    "for episode in range(n_episodes):\n",
    " obs, info = env.reset()\n",
    " state = discretize_state(obs, bins)\n",
    " total_reward = 0\n",
    " done = False\n",
    " \n",
    " while not done:\n",
    " # Choose action\n",
    " action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
    " \n",
    " # Take action\n",
    " next_obs, reward, terminated, truncated, info = env.step(action)\n",
    " done = terminated or truncated\n",
    " next_state = discretize_state(next_obs, bins)\n",
    " \n",
    " # Update Q-table\n",
    " q_learning_update(q_table, state, action, reward, next_state, alpha, gamma)\n",
    " \n",
    " state = next_state\n",
    " total_reward += reward\n",
    " \n",
    " rewards_history.append(total_reward)\n",
    " epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    " \n",
    " if (episode + 1) % 500 == 0:\n",
    " avg_reward = np.mean(rewards_history[-500:])\n",
    " print(f\" Episode {episode+1}: Avg reward = {avg_reward:.2f}, Îµ = {epsilon:.3f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Visualize learning\n",
    "plt.figure(figsize=(10, 6))\n",
    "window_size = 100\n",
    "if len(rewards_history) >= window_size:\n",
    " smoothed = np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid')\n",
    " plt.plot(smoothed, label=f'Smoothed (window={window_size})')\n",
    "plt.plot(rewards_history, alpha=0.3, label='Raw rewards')\n",
    "plt.xlabel('Episode', fontsize=12)\n",
    "plt.ylabel('Reward', fontsize=12)\n",
    "plt.title('Q-Learning on CartPole: Training Progress', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Q-Learning on CartPole complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4: Introduction to Deep Q-Network (DQN)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 4: Introduction to Deep Q-Network (DQN)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nDQN Overview:\")\n",
    "print(\" - Uses neural network to approximate Q-function (instead of Q-table)\")\n",
    "print(\" - Handles high-dimensional state spaces (e.g., images)\")\n",
    "print(\" - Key innovations:\")\n",
    "print(\" 1. Experience Replay: Store transitions, sample randomly for training\")\n",
    "print(\" 2. Target Network: Separate network for stable Q-targets\")\n",
    "print(\" 3. Neural Network: Approximate Q(s,a) for continuous/ high-dim states\")\n",
    "\n",
    "print(\"\\nDQN Architecture:\")\n",
    "print(\" Input: State (e.g., image, observation vector)\")\n",
    "print(\" Network: Fully connected or CNN layers\")\n",
    "print(\" Output: Q-values for each action\")\n",
    "\n",
    "print(\"\\nDQN vs Q-Learning:\")\n",
    "print(\" Q-Learning:\")\n",
    "print(\" - Uses Q-table (discrete states only)\")\n",
    "print(\" - Limited to small state spaces\")\n",
    "print(\" - Fast for tabular problems\")\n",
    "print(\" DQN:\")\n",
    "print(\" - Uses neural network (continuous/high-dim states)\")\n",
    "print(\" - Scales to complex problems (e.g., Atari games)\")\n",
    "print(\" - Requires more computation and tuning\")\n",
    "\n",
    "print(\"\\nDQN Algorithm (High-Level):\")\n",
    "print(\" 1. Initialize Q-network and target network\")\n",
    "print(\" 2. For each episode:\")\n",
    "print(\" a. Observe state\")\n",
    "print(\" b. Choose action using epsilon-greedy (using Q-network)\")\n",
    "print(\" c. Take action, store transition in replay buffer\")\n",
    "print(\" d. Sample batch from replay buffer\")\n",
    "print(\" e. Compute targets using target network\")\n",
    "print(\" f. Update Q-network using loss: (Q(s,a) - target)^2\")\n",
    "print(\" g. Periodically update target network\")\n",
    "\n",
    "print(\"\\nNote: Full DQN implementation will be covered in Unit 3 (Deep RL)\")\n",
    "print(\"This is an introduction to the concepts.\")\n",
    "\n",
    "print(\"\\nâœ… DQN introduction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Q-Learning**: Off-policy TD learning algorithm\n",
    "   - Updates: Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]\n",
    "   - Uses Q-table for discrete states\n",
    "   - Epsilon-greedy exploration\n",
    "\n",
    "2. **FrozenLake**: Discrete grid-world environment\n",
    "   - Perfect for tabular Q-learning\n",
    "   - 16 states, 4 actions\n",
    "   - Slippery/unslippery variants\n",
    "\n",
    "3. **CartPole**: Continuous state space\n",
    "   - Requires state discretization for Q-learning\n",
    "   - 4 continuous state variables â†’ discrete bins\n",
    "   - Alternative: Use DQN for continuous states\n",
    "\n",
    "4. **DQN**: Deep Q-Network\n",
    "   - Neural network approximates Q-function\n",
    "   - Handles high-dimensional/continuous states\n",
    "   - Experience replay and target networks for stability\n",
    "\n",
    "### Implementation Highlights:\n",
    "- **Q-Learning**: Tabular method, fast for discrete problems\n",
    "- **State Discretization**: Convert continuous to discrete for Q-learning\n",
    "- **Epsilon Decay**: Reduce exploration over time\n",
    "- **DQN**: Deep learning extension for complex problems\n",
    "\n",
    "### Best Practices:\n",
    "- Start with high epsilon (exploration), decay over time\n",
    "- Tune learning rate (alpha) and discount factor (gamma)\n",
    "- Monitor learning curves and success rates\n",
    "- Use experience replay and target networks for DQN stability\n",
    "\n",
    "### Next Steps:\n",
    "- Unit 2: Advanced Q-learning (SARSA, TD methods)\n",
    "- Unit 3: Deep RL (DQN, Actor-Critic, PPO)\n",
    "- Unit 4: Exploration strategies\n",
    "- Unit 5: Advanced applications\n",
    "\n",
    "**Reference:** Course 09, Unit 1: \"Introduction to Reinforcement Learning\" - Mini projects practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}