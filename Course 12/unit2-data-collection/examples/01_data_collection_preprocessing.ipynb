{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Preprocessing for Graduation Project\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Collect and acquire datasets for graduation project\n",
    "- Perform comprehensive data cleaning and preprocessing\n",
    "- Implement feature engineering techniques\n",
    "- Validate data quality and create train/validation/test splits\n",
    "- Document data collection and preprocessing procedures\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Unit 1: Project proposal completed\n",
    "- ‚úÖ Understanding of data science pipelines\n",
    "- ‚úÖ Python, Pandas, NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 12, Unit 2**:\n",
    "- Collecting and acquiring datasets\n",
    "- Performing data cleaning and preprocessing using Python libraries\n",
    "- Implementing feature engineering techniques\n",
    "- Validating data quality and preparing train/validation/test splits\n",
    "- Creating data exploration notebooks with visualizations\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Data Collection and Preparation** is critical for project success. Quality data leads to quality models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n",
    "print(\"Ready for data collection and preprocessing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection Strategies\n",
    "\n",
    "Explore different data sources and collection methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Data Collection Strategies\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_sources = {\n",
    "    \"Public Datasets\": {\n",
    "        \"sources\": [\n",
    "            \"Kaggle Datasets\", \"UCI Machine Learning Repository\",\n",
    "            \"Google Dataset Search\",\n",
    "            \"Papers with Code Datasets\",\n",
    "            \"Hugging Face Datasets\"\n",
    "        ],\n",
    "        \"advantages\": \"Ready to use, often cleaned, documented\",\n",
    "        \"considerations\": \"Check licensing, ensure relevance to your problem\"\n",
    "    },\n",
    "    \"APIs\": {\n",
    "        \"sources\": [\n",
    "            \"Twitter API (for social media analysis)\",\n",
    "            \"Reddit API\",\n",
    "            \"News APIs\",\n",
    "            \"Government data APIs\"\n",
    "        ],\n",
    "        \"advantages\": \"Real-time data, structured format\",\n",
    "        \"considerations\": \"API limits, authentication, rate limiting\"\n",
    "    },\n",
    "    \"Web Scraping\": {\n",
    "        \"sources\": [\n",
    "            \"BeautifulSoup + requests\",\n",
    "            \"Scrapy framework\",\n",
    "            \"Selenium for dynamic content\"\n",
    "        ],\n",
    "        \"advantages\": \"Access to large amounts of data\",\n",
    "        \"considerations\": \"Legal and ethical considerations, robots.txt, ToS\"\n",
    "    },\n",
    "    \"Custom Collection\": {\n",
    "        \"sources\": [\n",
    "            \"Surveys\",\n",
    "            \"Experiments\",\n",
    "            \"Sensors/IoT devices\"\n",
    "        ],\n",
    "        \"advantages\": \"Tailored to your specific needs\",\n",
    "        \"considerations\": \"Time-consuming, requires IRB approval for human subjects\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for source_type, details in data_sources.items():\n",
    "    print(f\"\\n{source_type}:\")\n",
    "    print(f\"  Sources: {', '.join(details['sources'][:3])}...\")\n",
    "    print(f\"  Advantages: {details['advantages']}\")\n",
    "    print(f\"  Considerations: {details['considerations']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Choose data sources aligned with your project goals and ethical guidelines!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Cleaning and Preprocessing Pipeline\n",
    "\n",
    "Comprehensive data cleaning workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data for demonstration\n",
    "np.random.seed(42)\n",
    "sample_data = pd.DataFrame({\n",
    "    'feature1': np.random.randn(1000), 'feature2': np.random.choice(['A', 'B', 'C'], 1000),\n",
    "    'feature3': np.random.randn(1000) + np.random.choice([0, np.nan], 1000, p=[0.95, 0.05]),\n",
    "    'target': np.random.choice([0, 1], 1000)\n",
    "})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Data Cleaning Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Initial Data Inspection:\")\n",
    "print(f\"   Shape: {sample_data.shape}\")\n",
    "print(f\"   Missing values:\\n{sample_data.isnull().sum()}\")\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\n2. Handling Missing Values:\")\n",
    "# Option 1: Drop rows with missing values\n",
    "data_dropped = sample_data.dropna()\n",
    "print(f\"   After dropping: {data_dropped.shape}\")\n",
    "\n",
    "# Option 2: Fill missing values (mean for numeric, mode for categorical)\n",
    "data_filled = sample_data.copy()\n",
    "data_filled['feature3'].fillna(data_filled['feature3'].mean(), inplace=True)\n",
    "print(f\"   After filling: Missing values = {data_filled.isnull().sum().sum()}\")\n",
    "\n",
    "# Handle categorical variables\n",
    "print(\"\\n3. Encoding Categorical Variables:\")\n",
    "le = LabelEncoder()\n",
    "data_encoded = data_filled.copy()\n",
    "data_encoded['feature2_encoded'] = le.fit_transform(data_encoded['feature2'])\n",
    "print(f\"   Original categories: {data_filled['feature2'].unique()}\")\n",
    "print(f\"   Encoded values: {data_encoded['feature2_encoded'].unique()}\")\n",
    "\n",
    "# Feature scaling\n",
    "print(\"\\n4. Feature Scaling:\")\n",
    "scaler = StandardScaler()\n",
    "numeric_features = ['feature1', 'feature3']\n",
    "data_scaled = data_encoded.copy()\n",
    "data_scaled[numeric_features] = scaler.fit_transform(data_scaled[numeric_features])\n",
    "print(f\"   Features scaled: {numeric_features}\")\n",
    "print(f\"   Mean after scaling: {data_scaled[numeric_features].mean().round(4).tolist()}\")\n",
    "print(f\"   Std after scaling: {data_scaled[numeric_features].std().round(4).tolist()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data cleaning pipeline complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train/Validation/Test Split\n",
    "\n",
    "Proper data splitting for model development and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = data_scaled[['feature1', 'feature3', 'feature2_encoded']]\n",
    "y = data_scaled['target']\n",
    "\n",
    "# Split: 60% train, 20% validation, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Data Splitting Strategy\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Training set: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Data split complete!\")\n",
    "print(\"üìù Use validation set for hyperparameter tuning\")\n",
    "print(\"üìù Use test set ONLY for final evaluation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Steps in Data Collection & Preparation:\n",
    "1. **Data Collection**: Identify and acquire relevant datasets\n",
    "2. **Data Inspection**: Understand data structure, types, distributions\n",
    "3. **Data Cleaning**: Handle missing values, outliers, inconsistencies\n",
    "4. **Feature Engineering**: Create meaningful features from raw data\n",
    "5. **Data Splitting**: Train/Validation/Test sets (60/20/20 or 70/15/15)\n",
    "6. **Documentation**: Document all preprocessing steps for reproducibility\n",
    "\n",
    "### Best Practices:\n",
    "- Always check data quality before modeling\n",
    "- Document all preprocessing steps\n",
    "- Save cleaned datasets for reproducibility\n",
    "- Use stratified splitting for imbalanced data\n",
    "- Validate data representativeness\n",
    "\n",
    "**Reference:** Course 12, Unit 2: \"Data Collection and Preparation\" - All practical activities covered\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}