{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDPs) and Value Iteration\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand Markov Decision Processes (MDPs) fundamentals\n",
    "- Implement simple MDPs and value iteration algorithms\n",
    "- Set up RL environments and agents\n",
    "- Apply value iteration to solve decision-making problems\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of probability and Markov chains\n",
    "- âœ… Python 3.8+ installed\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 02, Unit 3**:\n",
    "- Introduction to reinforcement learning: setting up environments and agents\n",
    "- Implementing simple MDPs and value iteration algorithms\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 3 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to MDPs\n",
    "\n",
    "**Markov Decision Processes (MDPs)** are mathematical frameworks for modeling decision-making in situations where outcomes are partly random and partly under control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Inputs & ðŸ“¤ Outputs | Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø®Ø±Ø¬Ø§Øª\n",
    "\n",
    "**Inputs:** What we use in this notebook\n",
    "\n",
    "- Libraries and concepts as introduced in this notebook; see prerequisites and code comments.\n",
    "\n",
    "**Outputs:** What you'll see when you run the cells\n",
    "\n",
    "- Printed results, figures, and summaries as shown when you run the cells.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"Ready to work with MDPs and Value Iteration!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple MDP Implementation\n",
    "\n",
    "Let's create a simple grid world MDP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMDP:\n",
    "    \"\"\"Simple Markov Decision Process implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, states, actions, transitions, rewards, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - states: List of states\n",
    "        - actions: List of actions\n",
    "        - transitions: Dict[state][action][next_state] = probability\n",
    "        - rewards: Dict[state][action] = reward\n",
    "        - gamma: Discount factor\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def get_reward(self, state, action):\n",
    "        \"\"\"Get reward for state-action pair\"\"\"\n",
    "        return self.rewards.get(state, {}).get(action, 0.0)\n",
    "    \n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\"Get transition probability P(next_state | state, action)\"\"\"\n",
    "        return self.transitions.get(state, {}).get(action, {}).get(next_state, 0.0)\n",
    "\n",
    "# Example: Simple 3-state MDP\n",
    "states = ['S0', 'S1', 'S2']\n",
    "actions = ['Left', 'Right']\n",
    "\n",
    "# Transition probabilities: P(next_state | current_state, action)\n",
    "transitions = {\n",
    "    'S0': {\n",
    "        'Left': {'S0': 0.8, 'S1': 0.2},\n",
    "        'Right': {'S1': 0.9, 'S2': 0.1}\n",
    "    },\n",
    "    'S1': {\n",
    "        'Left': {'S0': 0.7, 'S1': 0.3},\n",
    "        'Right': {'S1': 0.5, 'S2': 0.5}\n",
    "    },\n",
    "    'S2': {\n",
    "        'Left': {'S1': 1.0},\n",
    "        'Right': {'S2': 1.0}  # Terminal state\n",
    "    }\n",
    "}\n",
    "\n",
    "# Rewards: R(state, action)\n",
    "rewards = {\n",
    "    'S0': {'Left': -1, 'Right': 0},\n",
    "    'S1': {'Left': 0, 'Right': 5},\n",
    "    'S2': {'Left': 0, 'Right': 10}  # Terminal state reward\n",
    "}\n",
    "\n",
    "mdp = SimpleMDP(states, actions, transitions, rewards)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Simple MDP: Grid World\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"States: {states}\")\n",
    "print(f\"Actions: {actions}\")\n",
    "print(f\"Discount factor (gamma): {mdp.gamma}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Value Iteration Algorithm\n",
    "\n",
    "Value iteration computes the optimal value function V*(s) for all states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, theta=1e-6, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Value iteration algorithm to find optimal value function\n",
    "    \n",
    "    Parameters:\n",
    "    - mdp: MDP object\n",
    "    - theta: Convergence threshold\n",
    "    - max_iterations: Maximum iterations\n",
    "    \n",
    "    Returns:\n",
    "    - V: Optimal value function\n",
    "    - policy: Optimal policy\n",
    "    \"\"\"\n",
    "    # Initialize value function\n",
    "    V = {state: 0.0 for state in mdp.states}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        V_old = V.copy()\n",
    "        \n",
    "        # Update value for each state\n",
    "        for state in mdp.states:\n",
    "            # Compute Q-value for each action\n",
    "            Q_values = []\n",
    "            for action in mdp.actions:\n",
    "                # Q(s,a) = R(s,a) + gamma * sum(P(s'|s,a) * V(s'))\n",
    "                q_value = mdp.get_reward(state, action)\n",
    "                for next_state in mdp.states:\n",
    "                    prob = mdp.get_transition_prob(state, action, next_state)\n",
    "                    q_value += mdp.gamma * prob * V_old[next_state]\n",
    "                Q_values.append(q_value)\n",
    "            \n",
    "            # Value is maximum Q-value (optimal action)\n",
    "            V[state] = max(Q_values) if Q_values else 0.0\n",
    "        \n",
    "        # Check convergence\n",
    "        max_diff = max(abs(V[state] - V_old[state]) for state in mdp.states)\n",
    "        if max_diff < theta:\n",
    "            print(f\"Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    # Extract optimal policy\n",
    "    policy = {}\n",
    "    for state in mdp.states:\n",
    "        Q_values = []\n",
    "        for action in mdp.actions:\n",
    "            q_value = mdp.get_reward(state, action)\n",
    "            for next_state in mdp.states:\n",
    "                prob = mdp.get_transition_prob(state, action, next_state)\n",
    "                q_value += mdp.gamma * prob * V[next_state]\n",
    "            Q_values.append((q_value, action))\n",
    "        # Choose action with highest Q-value\n",
    "        policy[state] = max(Q_values, key=lambda x: x[0])[1]\n",
    "    \n",
    "    return V, policy\n",
    "\n",
    "# Run value iteration\n",
    "print(\"=\" * 60)\n",
    "print(\"Running Value Iteration:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "V_star, optimal_policy = value_iteration(mdp)\n",
    "\n",
    "print(\"\\nOptimal Value Function V*(s):\")\n",
    "for state, value in V_star.items():\n",
    "    print(f\"  V*({state}) = {value:.4f}\")\n",
    "\n",
    "print(\"\\nOptimal Policy Ï€*(s):\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"  Ï€*({state}) = {action}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **MDP Components**: States, actions, transition probabilities, rewards, discount factor\n",
    "2. **Value Function V(s)**: Expected cumulative reward from state s\n",
    "3. **Value Iteration**: Algorithm to compute optimal value function\n",
    "4. **Policy**: Mapping from states to actions\n",
    "\n",
    "### Applications:\n",
    "- Robotics (path planning)\n",
    "- Game AI\n",
    "- Resource allocation\n",
    "- Autonomous systems\n",
    "\n",
    "**Reference:** Course 02, Unit 3: \"Implementing simple MDPs and value iteration algorithms\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}