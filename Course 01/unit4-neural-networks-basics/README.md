# Unit 4: Neural Networks Fundamentals
# Ø§Ù„ÙˆØ­Ø¯Ø© 4: Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©

**Official Unit Name:** Neural Networks Fundamentals  
**Duration:** 14 hours (7 theory + 7 practical)

**ğŸ“– Official Structure:** See `../../../COMPLETE_COURSE_STRUCTURE_AND_CLOS.md` and `../../../DETAILED_UNIT_DESCRIPTIONS.md` for complete details.

---

## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…

By the end of this unit, students will be able to:
- Understand neurons and activation functions (Sigmoid, ReLU, Tanh, Softmax)
- Build and train multi-class classification models
- Implement multi-layer perceptrons (MLPs)
- Understand deep neural networks (DNNs) and their benefits
- Get introduced to CNNs, RNNs, LSTM, and GRU architectures
- Apply regularization techniques to prevent overfitting

---

## Theoretical Content | Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†Ø¸Ø±ÙŠ

### 1. The Neuron and Activation Functions
- Role of activation functions in neural networks
- Common activation functions (Sigmoid, ReLU, Tanh, Softmax)
- Mathematical representation and implementation

### 2. Multi-class Classification and Multi-layer Networks
- Introduction to multi-class classification problems
- Multi-layer networks for multi-class classification
- Implementing multi-class classification with Keras

### 3. Multi-Layer Perceptrons (MLP)
- Structure of multi-layer perceptrons (MLP)
- Forward and backward propagation
- Training deep neural networks

### 4. Deep Neural Networks (DNNs)
- Understanding deep learning and its benefits
- Challenges in training deep networks
- Techniques for improving deep learning performance

### 5. Introduction to Convolutional Neural Networks (CNNs)
- Structure and components of CNN (convolution, pooling, fully connected layers)
- Feature extraction in CNNs
- Real-world applications of CNNs

### 6. Advanced Deep Learning Architectures
- Introduction to Recurrent Neural Networks (RNNs)
- Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU)
- Comparison of CNNs and RNNs for different applications

### 7. Trends in Deep Learning Training
- Recent developments in neural network training
- Innovations in hardware and software for AI
- Transfer learning and pre-trained models

### 8. Overfitting and Underfitting in Deep Learning
- Causes and effects of overfitting and underfitting
- Techniques to prevent overfitting (dropout, regularization)
- Early stopping as a regularization technique

---

## Practical Content | Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ù…Ù„ÙŠ

### Hands-On Activities:
- âœ… Implementing a single neuron with different activation functions using Python
- âœ… Building a multi-class classification model with Keras
- âœ… Training a multi-layer perceptron (MLP) for classification tasks
- âœ… Implementing a CNN for image classification using TensorFlow/Keras
- âœ… Experimenting with RNN, LSTM, GRU for sequential data
- âœ… Applying early stopping and regularization to prevent overfitting

### Notebooks and Exercises:
- See `examples/` folder for code demonstrations
- Complete exercises in `exercises/` folder
- Check solutions in `solutions/` folder
- Test understanding with quizzes in `quizzes/` folder
