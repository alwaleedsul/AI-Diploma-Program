{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings: Word2Vec Training and Visualization\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Train word embeddings using Word2Vec from Gensim\n",
    "- Understand CBOW vs Skip-gram architectures\n",
    "- Visualize word embeddings using dimensionality reduction\n",
    "- Apply Word2Vec to real-world text datasets\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Unit 1: Text preprocessing completed\n",
    "- ‚úÖ Understanding of tokenization and text representation\n",
    "- ‚úÖ Python, NumPy, Matplotlib knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 07, Unit 2**:\n",
    "- Training and representing word embeddings using Word2Vec from Gensim\n",
    "- Applying dimensionality reduction on high-dimensional vectors and visualizing results\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Word2Vec** is a popular word embedding technique that learns dense vector representations of words by predicting words in context. It comes in two flavors: CBOW (Continuous Bag of Words) and Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:45:03.183329Z",
     "iopub.status.busy": "2026-01-20T11:45:03.183185Z",
     "iopub.status.idle": "2026-01-20T11:45:04.107210Z",
     "shell.execute_reply": "2026-01-20T11:45:04.107011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Gensim not available. Install with: pip install gensim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ scikit-learn available for visualization\n",
      "\n",
      "‚úÖ Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Try importing Gensim for Word2Vec\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    from gensim.models import KeyedVectors\n",
    "    HAS_GENSIM = True\n",
    "    print(\"‚úÖ Gensim available for Word2Vec\")\n",
    "except ImportError:\n",
    "    HAS_GENSIM = False\n",
    "    print(\"‚ö†Ô∏è  Gensim not available. Install with: pip install gensim\")\n",
    "\n",
    "# Try importing sklearn for dimensionality reduction\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    HAS_SKLEARN = True\n",
    "    print(\"‚úÖ scikit-learn available for visualization\")\n",
    "except ImportError:\n",
    "    HAS_SKLEARN = False\n",
    "    print(\"‚ö†Ô∏è  scikit-learn not available. Install with: pip install scikit-learn\")\n",
    "\n",
    "print(\"\\n‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Training Word2Vec Model with Gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:45:04.108175Z",
     "iopub.status.busy": "2026-01-20T11:45:04.108069Z",
     "iopub.status.idle": "2026-01-20T11:45:04.111061Z",
     "shell.execute_reply": "2026-01-20T11:45:04.110733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Word2Vec Training (Installation Required)\n",
      "============================================================\n",
      "\n",
      "    To train Word2Vec models:\n",
      "    \n",
      "    1. Install Gensim:\n",
      "       pip install gensim\n",
      "    \n",
      "    2. Prepare tokenized sentences:\n",
      "       sentences = [['word1', 'word2'], ['word3', 'word4'], ...]\n",
      "    \n",
      "    3. Train model:\n",
      "       from gensim.models import Word2Vec\n",
      "       model = Word2Vec(sentences, vector_size=100, window=5, sg=1)\n",
      "    \n",
      "    4. Use embeddings:\n",
      "       vector = model.wv['word']\n",
      "       similar = model.wv.most_similar('word')\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if HAS_GENSIM:\n",
    "    # Sample text corpus (tokenized sentences)\n",
    "    # In practice, you would use preprocessed text from a dataset\n",
    "    sentences = [\n",
    "        ['natural', 'language', 'processing', 'is', 'important'],\n",
    "        ['machine', 'learning', 'helps', 'with', 'nlp'],\n",
    "        ['word', 'embeddings', 'represent', 'words', 'as', 'vectors'],\n",
    "        ['deep', 'learning', 'models', 'use', 'embeddings'],\n",
    "        ['neural', 'networks', 'process', 'language', 'data'],\n",
    "        ['nlp', 'applications', 'include', 'translation', 'and', 'summarization'],\n",
    "        ['text', 'classification', 'uses', 'word', 'vectors'],\n",
    "        ['language', 'models', 'understand', 'context', 'and', 'meaning']\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training Word2Vec Model\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Number of sentences: {len(sentences)}\")\n",
    "    print(f\"Sample sentence: {sentences[0]}\")\n",
    "    \n",
    "    # Train Word2Vec model (Skip-gram)\n",
    "    print(\"\\nTraining Word2Vec with Skip-gram architecture...\")\n",
    "    model_skipgram = Word2Vec(\n",
    "        sentences=sentences, vector_size=100,      # Dimension of word vectors\n",
    "        window=5,             # Context window size\n",
    "        min_count=1,          # Minimum word count\n",
    "        sg=1,                 # Skip-gram (1) vs CBOW (0)\n",
    "        epochs=100,\n",
    "        workers=1\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model trained! Vocabulary size: {len(model_skipgram.wv.key_to_index)}\")\n",
    "    print(f\"Word vector dimensions: {model_skipgram.wv.vector_size}\")\n",
    "    \n",
    "    # Train Word2Vec model (CBOW)\n",
    "    print(\"\\nTraining Word2Vec with CBOW architecture...\")\n",
    "    model_cbow = Word2Vec(\n",
    "        sentences=sentences, vector_size=100,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        sg=0,                 # CBOW (0)\n",
    "        epochs=100,\n",
    "        workers=1\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ CBOW model trained! Vocabulary size: {len(model_cbow.wv.key_to_index)}\")\n",
    "    \n",
    "    # Get word vectors\n",
    "    print(\"\\nSample word vectors:\")\n",
    "    sample_word = 'language'\n",
    "    if sample_word in model_skipgram.wv.key_to_index:\n",
    "        vector = model_skipgram.wv[sample_word]\n",
    "        print(f\"Word: '{sample_word}'\")\n",
    "        print(f\"Vector shape: {vector.shape}\")\n",
    "        print(f\"First 5 values: {vector[:5]}\")\n",
    "        \n",
    "        # Find similar words\n",
    "        similar_words = model_skipgram.wv.most_similar(sample_word, topn=5)\n",
    "        print(f\"\\nWords similar to '{sample_word}':\")\n",
    "        for word, similarity in similar_words:\n",
    "            print(f\"  {word}: {similarity:.4f}\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Word2Vec Training (Installation Required)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "    To train Word2Vec models:\n",
    "    \n",
    "    1. Install Gensim:\n",
    "       pip install gensim\n",
    "    \n",
    "    2. Prepare tokenized sentences:\n",
    "       sentences = [['word1', 'word2'], ['word3', 'word4'], ...]\n",
    "    \n",
    "    3. Train model:\n",
    "       from gensim.models import Word2Vec\n",
    "       model = Word2Vec(sentences, vector_size=100, window=5, sg=1)\n",
    "    \n",
    "    4. Use embeddings:\n",
    "       vector = model.wv['word']\n",
    "       similar = model.wv.most_similar('word')\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing Word Embeddings with Dimensionality Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:45:04.111929Z",
     "iopub.status.busy": "2026-01-20T11:45:04.111864Z",
     "iopub.status.idle": "2026-01-20T11:45:04.114770Z",
     "shell.execute_reply": "2026-01-20T11:45:04.114606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Install gensim and scikit-learn to visualize word embeddings\n"
     ]
    }
   ],
   "source": [
    "if HAS_GENSIM and HAS_SKLEARN:\n",
    "    # Select words to visualize\n",
    "    words_to_visualize = ['natural', 'language', 'processing', 'machine', 'learning', \n",
    "                         'word', 'embeddings', 'neural', 'networks', 'text', 'models']\n",
    "    \n",
    "    # Get word vectors\n",
    "    word_vectors = []\n",
    "    words_found = []\n",
    "    for word in words_to_visualize:\n",
    "        if word in model_skipgram.wv.key_to_index:\n",
    "            word_vectors.append(model_skipgram.wv[word])\n",
    "            words_found.append(word)\n",
    "    \n",
    "    word_vectors = np.array(word_vectors)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Visualizing Word Embeddings\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Original dimensions: {word_vectors.shape}\")\n",
    "    print(f\"Words to visualize: {len(words_found)}\")\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction (2D)\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    vectors_2d_pca = pca.fit_transform(word_vectors)\n",
    "    \n",
    "    print(f\"\\nPCA explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "    \n",
    "    # Apply t-SNE for dimensionality reduction (2D)\n",
    "    print(\"\\nApplying t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(words_found)-1))\n",
    "    vectors_2d_tsne = tsne.fit_transform(word_vectors)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # PCA visualization\n",
    "    axes[0].scatter(vectors_2d_pca[:, 0], vectors_2d_pca[:, 1], s=100, alpha=0.7)\n",
    "    for i, word in enumerate(words_found):\n",
    "        axes[0].annotate(word, (vectors_2d_pca[i, 0], vectors_2d_pca[i, 1]), \n",
    "                        fontsize=10, alpha=0.8)\n",
    "    axes[0].set_title('Word Embeddings Visualization (PCA)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # t-SNE visualization\n",
    "    axes[1].scatter(vectors_2d_tsne[:, 0], vectors_2d_tsne[:, 1], s=100, alpha=0.7, c='green')\n",
    "    for i, word in enumerate(words_found):\n",
    "        axes[1].annotate(word, (vectors_2d_tsne[i, 0], vectors_2d_tsne[i, 1]), \n",
    "                        fontsize=10, alpha=0.8)\n",
    "    axes[1].set_title('Word Embeddings Visualization (t-SNE)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('t-SNE Component 1')\n",
    "    axes[1].set_ylabel('t-SNE Component 2')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Word embeddings visualized!\")\n",
    "    print(\"  - PCA: Linear dimensionality reduction\")\n",
    "    print(\"  - t-SNE: Non-linear, preserves local neighborhoods\")\n",
    "    print(\"  - Similar words should be close together\")\n",
    "else:\n",
    "    print(\"Note: Install gensim and scikit-learn to visualize word embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: CBOW vs Skip-gram Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:45:04.115567Z",
     "iopub.status.busy": "2026-01-20T11:45:04.115512Z",
     "iopub.status.idle": "2026-01-20T11:45:04.117279Z",
     "shell.execute_reply": "2026-01-20T11:45:04.117118Z"
    }
   },
   "outputs": [],
   "source": [
    "if HAS_GENSIM:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CBOW vs Skip-gram Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_word = 'language'\n",
    "    if test_word in model_skipgram.wv.key_to_index and test_word in model_cbow.wv.key_to_index:\n",
    "        print(f\"\\nSimilar words to '{test_word}':\")\n",
    "        \n",
    "        print(\"\\nSkip-gram results:\")\n",
    "        skipgram_similar = model_skipgram.wv.most_similar(test_word, topn=3)\n",
    "        for word, sim in skipgram_similar:\n",
    "            print(f\"  {word}: {sim:.4f}\")\n",
    "        \n",
    "        print(\"\\nCBOW results:\")\n",
    "        cbow_similar = model_cbow.wv.most_similar(test_word, topn=3)\n",
    "        for word, sim in cbow_similar:\n",
    "            print(f\"  {word}: {sim:.4f}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Key Differences:\")\n",
    "        print(\"  - Skip-gram: Better for rare words, slower training\")\n",
    "        print(\"  - CBOW: Faster training, better for frequent words\")\n",
    "        print(\"  - Skip-gram: Generally better for small datasets\")\n",
    "        print(\"  - CBOW: More efficient for large datasets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Word2Vec**: Word embedding technique that learns dense vector representations\n",
    "   - **CBOW**: Predicts target word from context (faster, better for frequent words)\n",
    "   - **Skip-gram**: Predicts context from target word (better for rare words)\n",
    "\n",
    "2. **Training**: Use Gensim Word2Vec to train on tokenized sentences\n",
    "3. **Visualization**: Use PCA or t-SNE to reduce dimensions for visualization\n",
    "4. **Applications**: Word similarity, semantic relationships, feature representation\n",
    "\n",
    "### Best Practices:\n",
    "- Preprocess text properly (tokenization, lowercasing)\n",
    "- Use appropriate window size (typically 5-10)\n",
    "- Train on large corpora for better embeddings\n",
    "- Use pretrained models (Google News, GloVe) for better results\n",
    "- Visualize embeddings to understand relationships\n",
    "\n",
    "**Reference:** Course 07, Unit 2: \"Text Representation and Feature Engineering\" - Word2Vec practical content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
