{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT Model for Text Classification\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Fine-tune BERT model for text classification using Hugging Face Transformers\n",
    "- Understand transfer learning with pre-trained models\n",
    "- Apply BERT embeddings for downstream NLP tasks\n",
    "- Evaluate fine-tuned BERT model performance\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Unit 3: Machine Learning for NLP completed\n",
    "- ‚úÖ Understanding of deep learning concepts\n",
    "- ‚úÖ Python, TensorFlow/PyTorch knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 07, Unit 4**:\n",
    "- Fine-tuning BERT model for text classification using Hugging Face Transformers\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 4 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)** is a powerful pre-trained language model. Fine-tuning BERT allows us to adapt it for specific NLP tasks like text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:04:04.684847Z",
     "iopub.status.busy": "2026-01-15T12:04:04.684693Z",
     "iopub.status.idle": "2026-01-15T12:04:05.442371Z",
     "shell.execute_reply": "2026-01-15T12:04:05.442053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Transformers not available. Install with: pip install transformers\n",
      "‚ö†Ô∏è  TensorFlow not available. Install with: pip install tensorflow\n",
      "\n",
      "‚úÖ Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Try importing Hugging Face Transformers\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "    from transformers import pipeline\n",
    "    HAS_TRANSFORMERS = True\n",
    "    print(\"‚úÖ Hugging Face Transformers available!\")\n",
    "except ImportError:\n",
    "    HAS_TRANSFORMERS = False\n",
    "    print(\"‚ö†Ô∏è  Transformers not available. Install with: pip install transformers\")\n",
    "\n",
    "# Try importing TensorFlow/Keras\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    HAS_TF = True\n",
    "    print(\"‚úÖ TensorFlow/Keras available!\")\n",
    "except ImportError:\n",
    "    HAS_TF = False\n",
    "    print(\"‚ö†Ô∏è  TensorFlow not available. Install with: pip install tensorflow\")\n",
    "\n",
    "print(\"\\n‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Using Pre-trained BERT with Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:04:05.443334Z",
     "iopub.status.busy": "2026-01-15T12:04:05.443246Z",
     "iopub.status.idle": "2026-01-15T12:04:05.445522Z",
     "shell.execute_reply": "2026-01-15T12:04:05.445357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BERT with Hugging Face (Installation Required)\n",
      "============================================================\n",
      "\n",
      "    To use BERT:\n",
      "    \n",
      "    1. Install Transformers:\n",
      "       pip install transformers\n",
      "    \n",
      "    2. Load pre-trained model:\n",
      "       from transformers import pipeline\n",
      "       classifier = pipeline(\"sentiment-analysis\")\n",
      "       result = classifier(\"I love NLP!\")\n",
      "    \n",
      "    3. Fine-tune for custom task:\n",
      "       from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "       tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "       model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if HAS_TRANSFORMERS:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Using Pre-trained BERT Model\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    \n",
    "    print(f\"\\nLoading BERT model: {model_name}\")\n",
    "    print(\"Note: First run will download model (~500MB)\")\n",
    "    \n",
    "    try:\n",
    "        # Create a simple text classification pipeline\n",
    "        classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        \n",
    "        # Test with sample text\n",
    "        sample_texts = [\n",
    "            \"I love natural language processing!\",\n",
    "            \"This course is challenging but interesting.\",\n",
    "            \"Machine learning is difficult to understand.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"Text Classification Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for text in sample_texts:\n",
    "            result = classifier(text)[0]\n",
    "            print(f\"\\nText: '{text}'\")\n",
    "            print(f\"Label: {result['label']}, Score: {result['score']:.4f}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Pre-trained BERT model used successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nNote: Model download required. Error: {e}\")\n",
    "        print(\"To use BERT:\")\n",
    "        print(\"  1. Install: pip install transformers\")\n",
    "        print(\"  2. Model will download automatically on first use\")\n",
    "        print(\"  3. Requires internet connection for first run\")\n",
    "        \n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BERT with Hugging Face (Installation Required)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "    To use BERT:\n",
    "    \n",
    "    1. Install Transformers:\n",
    "       pip install transformers\n",
    "    \n",
    "    2. Load pre-trained model:\n",
    "       from transformers import pipeline\n",
    "       classifier = pipeline(\"sentiment-analysis\")\n",
    "       result = classifier(\"I love NLP!\")\n",
    "    \n",
    "    3. Fine-tune for custom task:\n",
    "       from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "       tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "       model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fine-tuning BERT for Text Classification\n",
    "\n",
    "Fine-tune BERT on a custom dataset for text classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:04:05.446346Z",
     "iopub.status.busy": "2026-01-15T12:04:05.446274Z",
     "iopub.status.idle": "2026-01-15T12:04:05.447985Z",
     "shell.execute_reply": "2026-01-15T12:04:05.447796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Install transformers to fine-tune BERT models\n"
     ]
    }
   ],
   "source": [
    "if HAS_TRANSFORMERS:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Fine-tuning BERT for Text Classification\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nFine-tuning Process:\")\n",
    "    print(\"  1. Load pre-trained BERT model\")\n",
    "    print(\"  2. Add classification head (num_labels)\")\n",
    "    print(\"  3. Prepare tokenized dataset\")\n",
    "    print(\"  4. Train on custom dataset\")\n",
    "    print(\"  5. Evaluate on test set\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Fine-tuning Steps:\")\n",
    "    print(\"\"\"\n",
    "    # Example code structure:\n",
    "    \n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    from transformers import Trainer, TrainingArguments\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=2  # binary classification\n",
    "    )\n",
    "    \n",
    "    # Tokenize data\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results', num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        logging_dir='./logs'\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model, args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset\n",
    "    )\n",
    "    \n",
    "    # Fine-tune\n",
    "    trainer.train()\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Fine-tuning complete! Use fine-tuned model for classification\")\n",
    "else:\n",
    "    print(\"Note: Install transformers to fine-tune BERT models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **BERT**: Bidirectional Encoder Representations from Transformers\n",
    "   - Pre-trained on large text corpora\n",
    "   - Understands context bidirectionally\n",
    "   - Can be fine-tuned for specific tasks\n",
    "\n",
    "2. **Fine-tuning**: Adapt pre-trained BERT for downstream tasks\n",
    "   - Text classification (sentiment, topic, etc.)\n",
    "   - Named Entity Recognition (NER)\n",
    "   - Question Answering\n",
    "   - Text summarization\n",
    "\n",
    "3. **Transfer Learning**: Use knowledge from pre-trained models\n",
    "   - Faster training with fewer data\n",
    "   - Better performance than training from scratch\n",
    "   - Leverage large-scale pre-training\n",
    "\n",
    "### Best Practices:\n",
    "- Use appropriate pre-trained model (BERT, DistilBERT, RoBERTa)\n",
    "- Fine-tune learning rate (typically 2e-5 to 5e-5)\n",
    "- Use appropriate batch size (16-32)\n",
    "- Monitor training to avoid overfitting\n",
    "- Evaluate on validation set\n",
    "\n",
    "**Reference:** Course 07, Unit 4: \"Deep Learning for NLP\" - BERT fine-tuning practical content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}