{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Models with Attention for Machine Translation\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand sequence-to-sequence (seq2seq) architectures\n",
    "- Implement attention mechanisms for seq2seq models\n",
    "- Build a machine translation model using seq2seq with attention\n",
    "- Apply encoder-decoder architectures for sequence tasks\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Unit 4: RNNs and LSTMs completed\n",
    "- ‚úÖ Understanding of encoder-decoder architectures\n",
    "- ‚úÖ Python, TensorFlow/Keras knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 07, Unit 4**:\n",
    "- Implementing machine translation model using seq2seq with attention mechanisms\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 4 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Sequence-to-Sequence (seq2seq)** models use encoder-decoder architectures to map sequences to sequences. **Attention mechanisms** help models focus on relevant parts of the input when generating output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T15:34:28.904885Z",
     "iopub.status.busy": "2026-01-22T15:34:28.904802Z",
     "iopub.status.idle": "2026-01-22T15:34:29.860840Z",
     "shell.execute_reply": "2026-01-22T15:34:29.860488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  TensorFlow not available. Install with: pip install tensorflow\n",
      "‚úÖ Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Try importing TensorFlow/Keras\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Embedding, Input, Attention\n",
    "    HAS_TF = True\n",
    "    print(\"‚úÖ TensorFlow/Keras available!\")\n",
    "except ImportError:\n",
    "    HAS_TF = False\n",
    "    print(\"‚ö†Ô∏è  TensorFlow not available. Install with: pip install tensorflow\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Seq2Seq Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T15:34:29.876226Z",
     "iopub.status.busy": "2026-01-22T15:34:29.876007Z",
     "iopub.status.idle": "2026-01-22T15:34:29.878236Z",
     "shell.execute_reply": "2026-01-22T15:34:29.877986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Sequence-to-Sequence (Seq2Seq) Architecture\n",
      "============================================================\n",
      "\n",
      "Key Components:\n",
      "  1. Encoder: Processes input sequence\n",
      "  2. Decoder: Generates output sequence\n",
      "  3. Attention: Connects encoder and decoder\n",
      "  4. Context Vector: Summary of input sequence\n",
      "\n",
      "------------------------------------------------------------\n",
      "Seq2Seq with Attention:\n",
      "------------------------------------------------------------\n",
      "  Encoder: Input sequence ‚Üí Hidden states\n",
      "  Attention: Hidden states ‚Üí Attention weights\n",
      "  Decoder: Attention weights + Previous output ‚Üí Next token\n",
      "  Output: Generated sequence\n",
      "\n",
      "‚úÖ Attention allows model to focus on relevant input parts!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Sequence-to-Sequence (Seq2Seq) Architecture\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nKey Components:\")\n",
    "print(\"  1. Encoder: Processes input sequence\")\n",
    "print(\"  2. Decoder: Generates output sequence\")\n",
    "print(\"  3. Attention: Connects encoder and decoder\")\n",
    "print(\"  4. Context Vector: Summary of input sequence\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Seq2Seq with Attention:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"  Encoder: Input sequence ‚Üí Hidden states\")\n",
    "print(\"  Attention: Hidden states ‚Üí Attention weights\")\n",
    "print(\"  Decoder: Attention weights + Previous output ‚Üí Next token\")\n",
    "print(\"  Output: Generated sequence\")\n",
    "\n",
    "print(\"\\n‚úÖ Attention allows model to focus on relevant input parts!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing Seq2Seq with Attention for Translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T15:34:29.879559Z",
     "iopub.status.busy": "2026-01-22T15:34:29.879329Z",
     "iopub.status.idle": "2026-01-22T15:34:29.881698Z",
     "shell.execute_reply": "2026-01-22T15:34:29.881443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Seq2Seq with Attention (Installation Required)\n",
      "============================================================\n",
      "\n",
      "    To implement seq2seq with attention:\n",
      "    \n",
      "    1. Install TensorFlow:\n",
      "       pip install tensorflow\n",
      "    \n",
      "    2. Build encoder-decoder architecture:\n",
      "       - Encoder: LSTM/GRU to encode input\n",
      "       - Decoder: LSTM/GRU to decode output\n",
      "       - Attention: Connect encoder and decoder\n",
      "    \n",
      "    3. Train on parallel corpora:\n",
      "       - Input sequences (source language)\n",
      "       - Target sequences (target language)\n",
      "    \n",
      "    4. Use attention to focus on relevant input parts\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if HAS_TF:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Seq2Seq with Attention Implementation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nImplementation Structure:\")\n",
    "    print(\"\"\"\n",
    "    # Simplified Seq2Seq with Attention\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=(max_len,))\n",
    "    encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
    "    encoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_inputs = Input(shape=(max_len,))\n",
    "    decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
    "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    \n",
    "    # Attention\n",
    "    attention = Attention()\n",
    "    context_vector = attention([decoder_outputs, encoder_outputs])\n",
    "    \n",
    "    # Output\n",
    "    decoder_concat = tf.concat([decoder_outputs, context_vector], axis=-1)\n",
    "    decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "    output = decoder_dense(decoder_concat)\n",
    "    \n",
    "    # Model\n",
    "    model = Model([encoder_inputs, decoder_inputs], output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Model Architecture:\")\n",
    "    print(\"  - Encoder: LSTM processes source sequence\")\n",
    "    print(\"  - Decoder: LSTM generates target sequence\")\n",
    "    print(\"  - Attention: Focuses on relevant encoder states\")\n",
    "    print(\"  - Output: Probability distribution over target vocabulary\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Applications:\")\n",
    "    print(\"  - Machine Translation (EN‚ÜíFR, EN‚ÜíAR, etc.)\")\n",
    "    print(\"  - Text Summarization\")\n",
    "    print(\"  - Question Answering\")\n",
    "    print(\"  - Chatbot dialogue generation\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Seq2Seq with Attention (Installation Required)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "    To implement seq2seq with attention:\n",
    "    \n",
    "    1. Install TensorFlow:\n",
    "       pip install tensorflow\n",
    "    \n",
    "    2. Build encoder-decoder architecture:\n",
    "       - Encoder: LSTM/GRU to encode input\n",
    "       - Decoder: LSTM/GRU to decode output\n",
    "       - Attention: Connect encoder and decoder\n",
    "    \n",
    "    3. Train on parallel corpora:\n",
    "       - Input sequences (source language)\n",
    "       - Target sequences (target language)\n",
    "    \n",
    "    4. Use attention to focus on relevant input parts\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Seq2Seq Architecture**: Encoder-decoder for sequence tasks\n",
    "   - **Encoder**: Processes input sequence into hidden states\n",
    "   - **Decoder**: Generates output sequence from hidden states\n",
    "   - **Attention**: Allows decoder to focus on relevant encoder states\n",
    "\n",
    "2. **Attention Mechanism**: \n",
    "   - Computes attention weights for each encoder state\n",
    "   - Creates context vector from weighted encoder states\n",
    "   - Improves translation quality for long sequences\n",
    "\n",
    "3. **Machine Translation**:\n",
    "   - Input: Source language sequence\n",
    "   - Output: Target language sequence\n",
    "   - Train on parallel corpora (aligned sentence pairs)\n",
    "\n",
    "### Applications:\n",
    "- Machine Translation (Google Translate, DeepL)\n",
    "- Text Summarization\n",
    "- Question Answering\n",
    "- Dialogue Systems\n",
    "\n",
    "**Reference:** Course 07, Unit 4: \"Deep Learning for NLP\" - Seq2seq with attention practical content\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
