{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Advanced Data Loading | ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 05, Unit 2** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Advanced Data Loading | ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\n",
    "\n",
    "\n",
    "**All concepts are explained in the code comments below - you can learn everything from this notebook alone!**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Unit 1: All examples** - You need pandas, NumPy, cuDF basics\n",
    "- âœ… **Understanding of DataFrames and basic file operations**\n",
    "- âœ… **Knowledge of different file formats** (CSV, Excel, JSON)\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding DataFrame operations\n",
    "- Knowing which loading method to use\n",
    "- Handling file paths and errors\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the FIRST example in Unit 2** - Data Cleaning and Preparation!\n",
    "\n",
    "**Why this example FIRST in Unit 2?**\n",
    "- **Before** you can clean data, you need to load it from files\n",
    "- **Before** you can analyze data, you need to load it correctly\n",
    "- **Before** you can handle large datasets, you need chunking strategies\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Unit 1: pandas DataFrame knowledge\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 5: Missing Values & Duplicates (needs data loading skills)\n",
    "- ğŸ““ Example 6: Outliers & Transformation (needs loaded data)\n",
    "- ğŸ““ All cleaning operations (all need data loaded first!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Data loading is the first step (can't clean what you don't have)\n",
    "2. Advanced loading teaches chunking (essential for large data)\n",
    "3. Error handling in loading (foundation for robust code)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Getting Your Ingredients | Ø§Ù„Ù‚ØµØ©: Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª\n",
    "\n",
    "Imagine you're cooking. **Before** you can clean and prepare ingredients, you need to get \n",
    "them from the store - know what's available, handle different packages, check if items \n",
    "are missing. **After** getting everything loaded, you can start preparing!\n",
    "\n",
    "Same with data science: **Before** cleaning and analyzing, we load data from files - know \n",
    "file formats, handle different sources, check for errors. **After** loading successfully, \n",
    "we can start cleaning!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Advanced Data Loading Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\n",
    "\n",
    "Advanced loading techniques are essential because:\n",
    "- **Multiple Formats**: Real data comes in CSV, Excel, JSON, databases\n",
    "- **Large Files**: Need chunking to load huge datasets that don't fit in memory\n",
    "- **Error Handling**: Files may be missing, corrupted, or have encoding issues\n",
    "- **Performance**: Correct loading options make your code faster\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Load data from CSV files with advanced options\n",
    "2. Load data from Excel files and multiple sheets\n",
    "3. Load data from JSON files (including nested structures)\n",
    "4. Handle large files using chunking techniques\n",
    "5. Implement error handling for robust data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T15:11:12.238868Z",
     "iopub.status.busy": "2026-01-22T15:11:12.238738Z",
     "iopub.status.idle": "2026-01-22T15:11:12.494154Z",
     "shell.execute_reply": "2026-01-22T15:11:12.493781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "\n",
      "ğŸ“š What each library does:\n",
      "   - pandas: Load data from CSV, Excel, JSON files\n",
      "   - numpy: Generate sample data for examples\n",
      "   - json: Handle JSON file formats\n",
      "   - os/pathlib: Handle file paths and directories\n",
      "\n",
      "======================================================================\n",
      "Example 4: Advanced Data Loading | ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\n",
      "======================================================================\n",
      "\n",
      "ğŸ“š Prerequisites: Unit 1 completed, pandas DataFrame knowledge\n",
      "ğŸ”— This is the FIRST example in Unit 2 - foundation for data cleaning\n",
      "ğŸ¯ Goal: Master loading data from multiple sources and formats\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each library does:\")\n",
    "print(\"   - pandas: Load data from CSV, Excel, JSON files\")\n",
    "print(\"   - numpy: Generate sample data for examples\")\n",
    "print(\"   - json: Handle JSON file formats\")\n",
    "print(\"   - os/pathlib: Handle file paths and directories\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Example 4: Advanced Data Loading | ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nğŸ“š Prerequisites: Unit 1 completed, pandas DataFrame knowledge\")\n",
    "print(\"ğŸ”— This is the FIRST example in Unit 2 - foundation for data cleaning\")\n",
    "print(\"ğŸ¯ Goal: Master loading data from multiple sources and formats\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading from CSV Files | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ù…Ù„ÙØ§Øª CSV\n",
    "\n",
    "**BEFORE**: You know basic CSV loading but not advanced options.\n",
    "\n",
    "**AFTER**: You'll load CSV files with encoding, missing value handling, and date parsing!\n",
    "\n",
    "**Why this matters**: Real CSV files have encoding issues, missing values, and date formats!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading from CSV | Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ Ù…Ù† CSV\n",
    "\n",
    "**BEFORE**: We need sample data, but we don't have any CSV files yet.\n",
    "\n",
    "**AFTER**: We'll create sample CSV data and load it with different options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T15:11:12.524882Z",
     "iopub.status.busy": "2026-01-22T15:11:12.523068Z",
     "iopub.status.idle": "2026-01-22T15:11:12.546175Z",
     "shell.execute_reply": "2026-01-22T15:11:12.545834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Loading from CSV\n",
      "----------------------------------------------------------------------\n",
      "âœ“ Created sample CSV file: unit2-cleaning/examples/sample_data.csv\n",
      "\n",
      "Loading CSV with default options\n",
      "Shape: (100, 6)\n",
      "\n",
      "Loading CSV with specific options\n",
      "Shape: (100, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. Loading from CSV\")\n",
    "print(\"-\" * 70)\n",
    "# Create sample CSV data\n",
    "sample_data = {\n",
    "'id': range(1, 101),\n",
    "'name': [f'Person_{i}' for i in range(1, 101)],\n",
    "'age': np.random.randint(18, 80, 100),\n",
    "'salary': np.random.normal(50000, 15000, 100),\n",
    "'department': np.random.choice(['IT', 'HR', 'Finance', 'Sales'], 100),\n",
    "'join_date': pd.date_range('2020-01-01', periods=100, freq='D')\n",
    "}\n",
    "df_sample = pd.DataFrame(sample_data)\n",
    "csv_file = 'unit2-cleaning/examples/sample_data.csv'\n",
    "os.makedirs(os.path.dirname(csv_file), exist_ok=True)\n",
    "df_sample.to_csv(csv_file, index=False)\n",
    "print(f\"âœ“ Created sample CSV file: {csv_file}\")\n",
    "# Load CSV with different options\n",
    "print(\"\\nLoading CSV with default options\")\n",
    "df1 = pd.read_csv(csv_file)\n",
    "print(f\"Shape: {df1.shape}\")\n",
    "print(\"\\nLoading CSV with specific options\")\n",
    "df2 = pd.read_csv(csv_file, encoding='utf-8',\n",
    "na_values=['', 'NULL', 'null'],\n",
    "parse_dates=['join_date'])\n",
    "print(f\"Shape: {df2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading from Excel Files | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ù…Ù„ÙØ§Øª Excel\n",
    "\n",
    "**BEFORE**: You can load CSV but not Excel files.\n",
    "\n",
    "**AFTER**: You'll load Excel files and handle multiple sheets!\n",
    "\n",
    "**Why this matters**: Many organizations use Excel - you need to handle .xlsx files!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Loading from Excel | Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªØ­Ù…ÙŠÙ„ Ù…Ù† Excel\n",
    "\n",
    "**BEFORE**: We have CSV data but need Excel format.\n",
    "\n",
    "**AFTER**: We'll create Excel files and load from specific sheets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T15:11:12.547842Z",
     "iopub.status.busy": "2026-01-22T15:11:12.547708Z",
     "iopub.status.idle": "2026-01-22T15:11:12.698760Z",
     "shell.execute_reply": "2026-01-22T15:11:12.698346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2. Loading from Excel\n",
      "----------------------------------------------------------------------\n",
      "âœ“ Created sample Excel file: unit2-cleaning/examples/sample_data.xlsx\n",
      "\n",
      "Loading Excel file\n",
      "Shape: (100, 6)\n",
      "Columns: ['id', 'name', 'age', 'salary', 'department', 'join_date']\n",
      "\n",
      "Loading specific sheet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (100, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n2. Loading from Excel\")\n",
    "print(\"-\" * 70)\n",
    "excel_file = 'unit2-cleaning/examples/sample_data.xlsx'\n",
    "df_sample.to_excel(excel_file, index=False, sheet_name='Employees')\n",
    "print(f\"âœ“ Created sample Excel file: {excel_file}\")\n",
    "# Load Excel file\n",
    "print(\"\\nLoading Excel file\")\n",
    "df_excel = pd.read_excel(excel_file, sheet_name='Employees')\n",
    "print(f\"Shape: {df_excel.shape}\")\n",
    "print(f\"Columns: {list(df_excel.columns)}\")\n",
    "# Load specific sheet\n",
    "print(\"\\nLoading specific sheet\")\n",
    "df_excel2 = pd.read_excel(excel_file, sheet_name=0)  # First sheet\n",
    "print(f\"Shape: {df_excel2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading from JSON Files | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù„Ø«: ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ù…Ù„ÙØ§Øª JSON\n",
    "\n",
    "**BEFORE**: You can load CSV/Excel but not JSON (API data).\n",
    "\n",
    "**AFTER**: You'll load JSON files and handle nested structures!\n",
    "\n",
    "**Why this matters**: APIs return JSON - web data is often in JSON format!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Loading from JSON | Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªØ­Ù…ÙŠÙ„ Ù…Ù† JSON\n",
    "\n",
    "**BEFORE**: We have structured data but need JSON format (like APIs).\n",
    "\n",
    "**AFTER**: We'll create JSON files and load nested data structures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T15:11:12.700765Z",
     "iopub.status.busy": "2026-01-22T15:11:12.700562Z",
     "iopub.status.idle": "2026-01-22T15:11:12.707341Z",
     "shell.execute_reply": "2026-01-22T15:11:12.706879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3. Loading from JSON\n",
      "----------------------------------------------------------------------\n",
      "âœ“ Created sample JSON file: unit2-cleaning/examples/sample_data.json\n",
      "\n",
      "Loading JSON file\n",
      "Shape: (20, 1)\n",
      "\n",
      "Normalized JSON shape: (20, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n3. Loading from JSON\")\n",
    "print(\"-\" * 70)\n",
    "# Create sample JSON data\n",
    "json_data = {\n",
    "'employees': [\n",
    "{'id': i, 'name': f'Employee_{i}', 'age': np.random.randint(25, 55)}\n",
    "for i in range(1, 21)\n",
    "]\n",
    "}\n",
    "json_file = 'unit2-cleaning/examples/sample_data.json'\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "print(f\"âœ“ Created sample JSON file: {json_file}\")\n",
    "# Load JSON\n",
    "print(\"\\nLoading JSON file\")\n",
    "df_json = pd.read_json(json_file)\n",
    "print(f\"Shape: {df_json.shape}\")\n",
    "# Load JSON with nested structure\n",
    "df_json2 = pd.json_normalize(json_data, record_path='employees')\n",
    "print(f\"\\nNormalized JSON shape: {df_json2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Handling Large Files - Chunking | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø¹: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©\n",
    "\n",
    "**BEFORE**: You can load small files but not large ones (memory issues).\n",
    "\n",
    "**AFTER**: You'll load huge files in chunks without running out of memory!\n",
    "\n",
    "**Why this matters**: Large datasets don't fit in memory - chunking is essential!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Chunking Large Files | Ø§Ù„Ø®Ø·ÙˆØ© 4: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©\n",
    "\n",
    "**BEFORE**: Large files crash our program (out of memory).\n",
    "\n",
    "**AFTER**: We'll process large files in manageable chunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T15:11:12.709218Z",
     "iopub.status.busy": "2026-01-22T15:11:12.709082Z",
     "iopub.status.idle": "2026-01-22T15:11:12.834384Z",
     "shell.execute_reply": "2026-01-22T15:11:12.833972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "4. Handling Large Files (Chunking)\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created large CSV file: unit2-cleaning/examples/large_data.csv (100,000 rows)\n",
      "\n",
      "Loading in chunks\n",
      "  Loaded chunk with 10000 rows\n",
      "  Loaded chunk with 10000 rows\n",
      "  Loaded chunk with 10000 rows\n",
      "  Loaded chunk with 10000 rows\n",
      "  Loaded chunk with 10000 rows\n",
      "  Loaded chunk with 10000 rows\n",
      "  Loaded chunk with 10000 rows\n",
      "  Loaded chunk with 10000 rows\n",
      "  Loaded chunk with 10000 rows\n",
      "  Loaded chunk with 10000 rows\n",
      "\n",
      "âœ“ Total rows loaded: 100,000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n4. Handling Large Files (Chunking)\")\n",
    "print(\"-\" * 70)\n",
    "# Create larger dataset\n",
    "large_data = {\n",
    "'id': range(1, 100001),\n",
    "'value': np.random.randn(100000), 'category': np.random.choice(['A', 'B', 'C'], 100000)\n",
    "}\n",
    "df_large = pd.DataFrame(large_data)\n",
    "large_csv = 'unit2-cleaning/examples/large_data.csv'\n",
    "df_large.to_csv(large_csv, index=False)\n",
    "print(f\"âœ“ Created large CSV file: {large_csv} ({len(df_large):,} rows)\")\n",
    "# Load in chunks\n",
    "print(\"\\nLoading in chunks\")\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(large_csv, chunksize=chunk_size):\n",
    "    chunks.append(chunk)\n",
    "    print(f\"  Loaded chunk with {len(chunk)} rows\")\n",
    "df_chunked = pd.concat(chunks, ignore_index=True)\n",
    "print(f\"\\nâœ“ Total rows loaded: {len(df_chunked):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Error Handling | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§Ù…Ø³: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\n",
    "\n",
    "**BEFORE**: Your code crashes when files are missing or corrupted.\n",
    "\n",
    "**AFTER**: You'll handle errors gracefully and keep your code running!\n",
    "\n",
    "**Why this matters**: Real-world files are missing, corrupted, or have wrong formats!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Robust Error Handling | Ø§Ù„Ø®Ø·ÙˆØ© 5: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø¨Ø´ÙƒÙ„ Ù‚ÙˆÙŠ\n",
    "\n",
    "**BEFORE**: Missing files cause crashes.\n",
    "\n",
    "**AFTER**: We'll create functions that handle errors gracefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T15:11:12.836517Z",
     "iopub.status.busy": "2026-01-22T15:11:12.836196Z",
     "iopub.status.idle": "2026-01-22T15:11:12.846010Z",
     "shell.execute_reply": "2026-01-22T15:11:12.845696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5. Error Handling During Loading\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Testing error handling\n",
      "âœ— Error: File nonexistent_file.csv not found\n",
      "âœ“ Successfully loaded 100 rows from unit2-cleaning/examples/sample_data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "      <th>department</th>\n",
       "      <th>join_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Person_1</td>\n",
       "      <td>58</td>\n",
       "      <td>38858.430673</td>\n",
       "      <td>Finance</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Person_2</td>\n",
       "      <td>56</td>\n",
       "      <td>51451.028872</td>\n",
       "      <td>Sales</td>\n",
       "      <td>2020-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Person_3</td>\n",
       "      <td>54</td>\n",
       "      <td>38403.425158</td>\n",
       "      <td>Sales</td>\n",
       "      <td>2020-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Person_4</td>\n",
       "      <td>18</td>\n",
       "      <td>50936.685076</td>\n",
       "      <td>Sales</td>\n",
       "      <td>2020-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Person_5</td>\n",
       "      <td>56</td>\n",
       "      <td>55063.625244</td>\n",
       "      <td>IT</td>\n",
       "      <td>2020-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Person_96</td>\n",
       "      <td>63</td>\n",
       "      <td>28570.772391</td>\n",
       "      <td>Sales</td>\n",
       "      <td>2020-04-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Person_97</td>\n",
       "      <td>66</td>\n",
       "      <td>32150.011437</td>\n",
       "      <td>IT</td>\n",
       "      <td>2020-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Person_98</td>\n",
       "      <td>37</td>\n",
       "      <td>51213.189839</td>\n",
       "      <td>Sales</td>\n",
       "      <td>2020-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Person_99</td>\n",
       "      <td>22</td>\n",
       "      <td>46205.810024</td>\n",
       "      <td>IT</td>\n",
       "      <td>2020-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Person_100</td>\n",
       "      <td>70</td>\n",
       "      <td>43211.264774</td>\n",
       "      <td>HR</td>\n",
       "      <td>2020-04-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id        name  age        salary department   join_date\n",
       "0     1    Person_1   58  38858.430673    Finance  2020-01-01\n",
       "1     2    Person_2   56  51451.028872      Sales  2020-01-02\n",
       "2     3    Person_3   54  38403.425158      Sales  2020-01-03\n",
       "3     4    Person_4   18  50936.685076      Sales  2020-01-04\n",
       "4     5    Person_5   56  55063.625244         IT  2020-01-05\n",
       "..  ...         ...  ...           ...        ...         ...\n",
       "95   96   Person_96   63  28570.772391      Sales  2020-04-05\n",
       "96   97   Person_97   66  32150.011437         IT  2020-04-06\n",
       "97   98   Person_98   37  51213.189839      Sales  2020-04-07\n",
       "98   99   Person_99   22  46205.810024         IT  2020-04-08\n",
       "99  100  Person_100   70  43211.264774         HR  2020-04-09\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\\n5. Error Handling During Loading\")\n",
    "print(\"-\" * 70)\n",
    "def safe_load_csv(filepath, **kwargs):\n",
    "    \"\"\"Safely load CSV with error handling\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, **kwargs)\n",
    "        print(f\"âœ“ Successfully loaded {len(df)} rows from {filepath}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âœ— Error: File {filepath} not found\")\n",
    "        return None\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"âœ— Error: File {filepath} is empty\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error loading {filepath}: {str(e)}\")\n",
    "        return None\n",
    "# Test error handling\n",
    "print(\"\\nTesting error handling\")\n",
    "safe_load_csv('nonexistent_file.csv')\n",
    "safe_load_csv(csv_file)  # Should succeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Summary: What We Learned | Ù…Ù„Ø®Øµ: Ù…Ø§ ØªØ¹Ù„Ù…Ù†Ø§Ù‡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary | Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T15:11:12.847616Z",
     "iopub.status.busy": "2026-01-22T15:11:12.847487Z",
     "iopub.status.idle": "2026-01-22T15:11:12.851361Z",
     "shell.execute_reply": "2026-01-22T15:11:12.851054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ¯ SUMMARY: What We Learned\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‹ BEFORE this notebook:\n",
      "   - You could load basic CSV files but not advanced formats\n",
      "   - You didn't know how to handle large files (memory issues)\n",
      "   - Your code crashed when files were missing or corrupted\n",
      "\n",
      "âœ… AFTER this notebook:\n",
      "   - You can load CSV, Excel, and JSON files with advanced options\n",
      "   - You can handle large files using chunking (no memory issues)\n",
      "   - You can handle errors gracefully (robust code)\n",
      "   - You know when to use each loading method\n",
      "\n",
      "ğŸ“š Key Concepts Covered:\n",
      "   1. CSV Loading (encoding, missing values, date parsing)\n",
      "   2. Excel Loading (multiple sheets, specific sheet selection)\n",
      "   3. JSON Loading (nested structures, normalization)\n",
      "   4. Chunking Large Files (process in parts, avoid memory issues)\n",
      "   5. Error Handling (robust loading, handle missing/corrupted files)\n",
      "\n",
      "ğŸ”— Where Data Loading Fits:\n",
      "   - FIRST step in any data science project (need data before analysis)\n",
      "   - Foundation for all cleaning operations (must load before cleaning)\n",
      "   - Essential for production pipelines (robust error handling)\n",
      "\n",
      "â¡ï¸  Next Steps:\n",
      "   - Continue to Example 5: Missing Values & Duplicates\n",
      "   - You'll learn how to clean data AFTER loading it\n",
      "   - Loading skills are essential for the cleaning techniques!\n",
      "\n",
      "âœ“ Sample files created in 'unit2-cleaning/examples' directory\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ¯ SUMMARY: What We Learned\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nğŸ“‹ BEFORE this notebook:\")\n",
    "print(\"   - You could load basic CSV files but not advanced formats\")\n",
    "print(\"   - You didn't know how to handle large files (memory issues)\")\n",
    "print(\"   - Your code crashed when files were missing or corrupted\")\n",
    "\n",
    "print(\"\\nâœ… AFTER this notebook:\")\n",
    "print(\"   - You can load CSV, Excel, and JSON files with advanced options\")\n",
    "print(\"   - You can handle large files using chunking (no memory issues)\")\n",
    "print(\"   - You can handle errors gracefully (robust code)\")\n",
    "print(\"   - You know when to use each loading method\")\n",
    "\n",
    "print(\"\\nğŸ“š Key Concepts Covered:\")\n",
    "print(\"   1. CSV Loading (encoding, missing values, date parsing)\")\n",
    "print(\"   2. Excel Loading (multiple sheets, specific sheet selection)\")\n",
    "print(\"   3. JSON Loading (nested structures, normalization)\")\n",
    "print(\"   4. Chunking Large Files (process in parts, avoid memory issues)\")\n",
    "print(\"   5. Error Handling (robust loading, handle missing/corrupted files)\")\n",
    "\n",
    "print(\"\\nğŸ”— Where Data Loading Fits:\")\n",
    "print(\"   - FIRST step in any data science project (need data before analysis)\")\n",
    "print(\"   - Foundation for all cleaning operations (must load before cleaning)\")\n",
    "print(\"   - Essential for production pipelines (robust error handling)\")\n",
    "\n",
    "print(\"\\nâ¡ï¸  Next Steps:\")\n",
    "print(\"   - Continue to Example 5: Missing Values & Duplicates\")\n",
    "print(\"   - You'll learn how to clean data AFTER loading it\")\n",
    "print(\"   - Loading skills are essential for the cleaning techniques!\")\n",
    "\n",
    "print(\"\\nâœ“ Sample files created in 'unit2-cleaning/examples' directory\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš« When Data Loading Hits a Dead End | Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙˆØ§Ø¬Ù‡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø·Ø±ÙŠÙ‚ Ù…Ø³Ø¯ÙˆØ¯\n",
    "\n",
    "**BEFORE**: We've successfully loaded data from multiple sources.\n",
    "\n",
    "**AFTER**: We discover the loaded data has problems - missing values, duplicates, and quality issues!\n",
    "\n",
    "**Why this matters**: Loading data is just the first step - real-world data is messy and needs cleaning!\n",
    "\n",
    "---\n",
    "\n",
    "### The Problem We've Discovered\n",
    "\n",
    "We've learned:\n",
    "- âœ… How to load data from CSV, Excel, and JSON\n",
    "- âœ… How to handle large files with chunking\n",
    "- âœ… How to handle errors during loading\n",
    "\n",
    "**But we have a problem:**\n",
    "- â“ **What if the loaded data has missing values?**\n",
    "- â“ **What if there are duplicate rows?**\n",
    "- â“ **What if the data quality is poor?**\n",
    "\n",
    "**The Dead End:**\n",
    "- We can load data successfully\n",
    "- But the data itself has quality issues\n",
    "- We can't proceed with analysis until we clean the data!\n",
    "\n",
    "---\n",
    "\n",
    "### Demonstrating the Problem\n",
    "\n",
    "Let's check the data we just loaded - does it have quality issues?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T15:11:12.853179Z",
     "iopub.status.busy": "2026-01-22T15:11:12.852997Z",
     "iopub.status.idle": "2026-01-22T15:11:12.900677Z",
     "shell.execute_reply": "2026-01-22T15:11:12.900275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸš« DEMONSTRATING THE DEAD END: Data Quality Issues\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Checking data quality in loaded dataset...\n",
      "   Dataset shape: 100 rows Ã— 6 columns\n",
      "\n",
      "âš ï¸  Missing Values Found:\n",
      "   - No missing values in current dataset\n",
      "   - But real-world data often has missing values!\n",
      "\n",
      "   ğŸ“‹ Simulating real-world scenario with missing values:\n",
      "   - Total missing values: 25\n",
      "   - Missing values per column:\n",
      "     â€¢ age: 15 missing (15.0%)\n",
      "     â€¢ salary: 10 missing (10.0%)\n",
      "\n",
      "âš ï¸  Duplicates Found:\n",
      "   - No duplicates in current dataset\n",
      "   - But real-world data often has duplicate records!\n",
      "\n",
      "ğŸ’¡ The Problem:\n",
      "   - We've loaded the data successfully\n",
      "   - But the data has quality issues (missing values, potential duplicates)\n",
      "   - We can't proceed with analysis until we clean the data!\n",
      "   - Statistical operations will fail or give wrong results with missing values\n",
      "   - Duplicates can skew our analysis and lead to incorrect conclusions\n",
      "\n",
      "â¡ï¸  Solution Needed:\n",
      "   - We need techniques to handle missing values\n",
      "   - We need methods to detect and remove duplicates\n",
      "   - This leads us to Example 5: Missing Values & Duplicates\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸš« DEMONSTRATING THE DEAD END: Data Quality Issues\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load the data we created earlier\n",
    "df_check = pd.read_csv(csv_file)\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = df_check.isnull().sum().sum()\n",
    "missing_per_col = df_check.isnull().sum()\n",
    "\n",
    "print(f\"\\nğŸ“Š Checking data quality in loaded dataset...\")\n",
    "print(f\"   Dataset shape: {df_check.shape[0]} rows Ã— {df_check.shape[1]} columns\")\n",
    "\n",
    "print(f\"\\nâš ï¸  Missing Values Found:\")\n",
    "if missing_count > 0:\n",
    "    print(f\"   - Total missing values: {missing_count}\")\n",
    "    print(f\"   - Missing values per column:\")\n",
    "    for col, count in missing_per_col.items():\n",
    "        if count > 0:\n",
    "            print(f\"     â€¢ {col}: {count} missing ({count/len(df_check)*100:.1f}%)\")\n",
    "else:\n",
    "    # Create a version with missing values to demonstrate the problem\n",
    "    print(f\"   - No missing values in current dataset\")\n",
    "    print(f\"   - But real-world data often has missing values!\")\n",
    "    # Add some missing values to demonstrate\n",
    "    df_with_missing = df_check.copy()\n",
    "    # Randomly set some values to NaN\n",
    "    np.random.seed(42)\n",
    "    missing_indices = np.random.choice(df_with_missing.index, size=15, replace=False)\n",
    "    df_with_missing.loc[missing_indices, 'age'] = np.nan\n",
    "    missing_indices = np.random.choice(df_with_missing.index, size=10, replace=False)\n",
    "    df_with_missing.loc[missing_indices, 'salary'] = np.nan\n",
    "    \n",
    "    missing_count = df_with_missing.isnull().sum().sum()\n",
    "    missing_per_col = df_with_missing.isnull().sum()\n",
    "    print(f\"\\n   ğŸ“‹ Simulating real-world scenario with missing values:\")\n",
    "    print(f\"   - Total missing values: {missing_count}\")\n",
    "    print(f\"   - Missing values per column:\")\n",
    "    for col, count in missing_per_col.items():\n",
    "        if count > 0:\n",
    "            print(f\"     â€¢ {col}: {count} missing ({count/len(df_with_missing)*100:.1f}%)\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_count = df_check.duplicated().sum()\n",
    "print(f\"\\nâš ï¸  Duplicates Found:\")\n",
    "if duplicate_count > 0:\n",
    "    print(f\"   - Total duplicate rows: {duplicate_count}\")\n",
    "else:\n",
    "    print(f\"   - No duplicates in current dataset\")\n",
    "    print(f\"   - But real-world data often has duplicate records!\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ The Problem:\")\n",
    "print(f\"   - We've loaded the data successfully\")\n",
    "print(f\"   - But the data has quality issues (missing values, potential duplicates)\")\n",
    "print(f\"   - We can't proceed with analysis until we clean the data!\")\n",
    "print(f\"   - Statistical operations will fail or give wrong results with missing values\")\n",
    "print(f\"   - Duplicates can skew our analysis and lead to incorrect conclusions\")\n",
    "\n",
    "print(f\"\\nâ¡ï¸  Solution Needed:\")\n",
    "print(f\"   - We need techniques to handle missing values\")\n",
    "print(f\"   - We need methods to detect and remove duplicates\")\n",
    "print(f\"   - This leads us to Example 5: Missing Values & Duplicates\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Need Next\n",
    "\n",
    "**The Solution**: We need data cleaning techniques:\n",
    "- **Missing value handling**: Fill, drop, or impute missing values\n",
    "- **Duplicate detection**: Find and remove duplicate rows\n",
    "- **Data quality checks**: Validate data before analysis\n",
    "\n",
    "**This dead end leads us to Example 5: Missing Values & Duplicates**\n",
    "- Example 5 will teach us how to handle missing values\n",
    "- We'll learn strategies for dealing with duplicates\n",
    "- This solves the data quality problem so we can proceed with analysis!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}