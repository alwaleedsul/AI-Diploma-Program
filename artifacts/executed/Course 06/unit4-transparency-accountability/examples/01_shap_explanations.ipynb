{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. SHAP Explanations | ØªÙØ³ÙŠØ±Ø§Øª SHAP\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 06, Unit 4** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. SHAP Explanations | ØªÙØ³ÙŠØ±Ø§Øª SHAP\n",
    "\n",
    "## ðŸš¨ THE PROBLEM: AI Systems Are \"Black Boxes\" | Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ \"ØµÙ†Ø§Ø¯ÙŠÙ‚ Ø³ÙˆØ¯Ø§Ø¡\"\n",
    "\n",
    "**Remember the transition from Unit 3?**\n",
    "\n",
    "We completed Unit 3: Privacy and Security, where we learned:\n",
    "- How to protect data and ensure privacy\n",
    "- How to comply with regulations\n",
    "- How to build secure AI systems\n",
    "\n",
    "**But there's a new challenge:**\n",
    "\n",
    "**AI systems also raise transparency and accountability concerns!**\n",
    "\n",
    "**The Problem**: As we build AI systems, we need to consider:\n",
    "- âŒ **Transparency**: How do we explain AI decisions?\n",
    "- âŒ **Accountability**: Who is responsible for AI outcomes?\n",
    "- âŒ **Explainability**: How do we make AI understandable?\n",
    "- âŒ **Auditability**: How do we track and verify AI behavior?\n",
    "\n",
    "**We've learned:**\n",
    "- âœ… How to build private and secure AI systems (Unit 3)\n",
    "- âœ… How to ensure GDPR compliance\n",
    "- âœ… How to protect data\n",
    "\n",
    "**But we haven't learned:**\n",
    "- âŒ How to **explain** AI model decisions\n",
    "- âŒ How to **understand** why models make certain predictions\n",
    "- âŒ How to **interpret** model behavior\n",
    "- âŒ How to **provide explanations** to users\n",
    "\n",
    "**We need explainability techniques** to:\n",
    "1. Explain individual predictions (local explanations)\n",
    "2. Understand overall model behavior (global explanations)\n",
    "3. Identify important features\n",
    "4. Make AI decisions transparent\n",
    "\n",
    "**This notebook solves that problem** by teaching you SHAP (SHapley Additive exPlanations) for model interpretability!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Unit 1: Ethics Foundations** - Understanding ethical principles\n",
    "- âœ… **Unit 2: Bias and Justice** - Understanding fairness\n",
    "- âœ… **Unit 3: Privacy and Security** - Understanding privacy and security\n",
    "- âœ… **Basic Python knowledge**: Functions, data manipulation, ML concepts\n",
    "- âœ… **Basic ML knowledge**: Model training, predictions, feature importance\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why explainability matters\n",
    "- Knowing how to interpret model predictions\n",
    "- Understanding feature importance concepts\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the FIRST example in Unit 4** - it teaches you how to explain AI decisions!\n",
    "\n",
    "**Why this example FIRST?**\n",
    "- **Before** you can use other explanation methods, you need to understand SHAP\n",
    "- **Before** you can ensure accountability, you need explainability\n",
    "- **Before** you can build transparent systems, you need explanation techniques\n",
    "\n",
    "**Builds on**: \n",
    "- ðŸ““ Unit 1: Ethics Foundations (ethical principles)\n",
    "- ðŸ““ Unit 2: Bias and Justice (fairness concepts)\n",
    "- ðŸ““ Unit 3: Privacy and Security (privacy and security)\n",
    "\n",
    "**Leads to**: \n",
    "- ðŸ““ Example 2: LIME Explanations (alternative explanation method)\n",
    "- ðŸ““ Example 3: Counterfactual Analysis (what-if explanations)\n",
    "- ðŸ““ Example 4: Accountability Frameworks (accountability structures)\n",
    "- ðŸ““ Example 5: Human-in-the-Loop (HITL approaches)\n",
    "- ðŸ““ Example 6: Transparency Tools (transparency frameworks)\n",
    "\n",
    "**Why this order?**\n",
    "1. SHAP provides **foundation** for explainability (needed before other methods)\n",
    "2. SHAP teaches **feature importance** (critical for understanding)\n",
    "3. SHAP shows **local and global explanations** (essential for transparency)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Opening the Black Box | Ø§Ù„Ù‚ØµØ©: ÙØªØ­ Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø£Ø³ÙˆØ¯\n",
    "\n",
    "Imagine you're a doctor using AI to diagnose patients. **Before** explainability, you'd get a diagnosis but wouldn't know why (black box!). **After** using SHAP, you can see which symptoms most influenced the diagnosis - transparent and understandable!\n",
    "\n",
    "Same with AI: **Before** we have models that make predictions but we can't explain why, now we learn SHAP - calculate feature contributions to understand each prediction! **After** SHAP, we can explain why models make specific decisions!\n",
    "\n",
    "---\n",
    "\n",
    "## Why SHAP Explanations Matter | Ù„Ù…Ø§Ø°Ø§ ØªÙ‡Ù… ØªÙØ³ÙŠØ±Ø§Øª SHAPØŸ\n",
    "\n",
    "SHAP explanations are essential for ethical AI:\n",
    "- **Transparency**: Understand why models make decisions\n",
    "- **Trust**: Build user confidence through explanations\n",
    "- **Debugging**: Identify model errors and biases\n",
    "- **Compliance**: Meet explainability requirements\n",
    "- **Accountability**: Enable responsibility for AI outcomes\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Understand SHAP values and their meaning\n",
    "2. Learn how to calculate SHAP values\n",
    "3. Generate local explanations for individual predictions\n",
    "4. Generate global explanations for overall model behavior\n",
    "5. Visualize SHAP explanations\n",
    "6. Interpret SHAP results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:02:52.038190Z",
     "iopub.status.busy": "2026-01-15T12:02:52.038120Z",
     "iopub.status.idle": "2026-01-15T12:02:54.360114Z",
     "shell.execute_reply": "2026-01-15T12:02:54.359872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: SHAP library not available. Using simplified SHAP implementation.\n",
      "================================================================================\n",
      "Unit 4 - Example 1: SHAP Explanations\n",
      "================================================================================\n",
      "\n",
      "Generating dataset...\n",
      "Dataset shape: (1000, 5)\n",
      "\n",
      "Training Random Forest model...\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy: 0.9367\n",
      "\n",
      "Calculating SHAP values...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP values shape: (100, 4)\n",
      "\n",
      "================================================================================\n",
      "Creating Visualizations...\n",
      "================================================================================\n",
      "âœ… Saved: shap_summary.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: shap_waterfall.png\n",
      "âœ… Saved: shap_dependence.png\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. SHAP values explain individual predictions\n",
      "2. SHAP summary shows global feature importance\n",
      "3. Waterfall plots show how features contribute to a specific prediction\n",
      "4. Dependence plots show how SHAP values vary with feature values\n",
      "5. SHAP provides model-agnostic explanations\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unit 4: Interpretability, Transparency, and Accountability\n",
    "Example 1: SHAP Explanations\n",
    "This example demonstrates SHAP (SHapley Additive exPlanations) for model interpretability:\n",
    "- SHAP values calculation\n",
    "- Global and local explanations\n",
    "- Feature importance visualization\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Try to import SHAP, use simplified version if not available\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"Note: SHAP library not available. Using simplified SHAP implementation.\")\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "# ============================================================================\n",
    "# SIMPLIFIED SHAP IMPLEMENTATION (if SHAP not available)\n",
    "# ============================================================================\n",
    "def calculate_shap_values_simple(model, X, feature_names):\n",
    "    \"\"\"\n",
    "    Simplified SHAP value calculation using permutation importance\n",
    "    \"\"\"\n",
    "    baseline_pred = model.predict_proba(X)[:, 1].mean()\n",
    "    shap_values = []\n",
    "    for i in range(len(X)):\n",
    "        sample = X[i:i+1]\n",
    "        base_pred = model.predict_proba(sample)[0, 1]\n",
    "        sample_shap = []\n",
    "        for j in range(X.shape[1]):\n",
    "            # Permute feature j\n",
    "            X_permuted = X.copy()\n",
    "            X_permuted[:, j] = sample[0, j]\n",
    "            perm_pred = model.predict_proba(X_permuted)[:, 1].mean()\n",
    "            # SHAP value approximation\n",
    "            shap_val = base_pred - perm_pred\n",
    "            sample_shap.append(shap_val)\n",
    "        shap_values.append(sample_shap)\n",
    "    return np.array(shap_values)\n",
    "# ============================================================================\n",
    "# GENERATE DATASET\n",
    "# ============================================================================\n",
    "def generate_dataset(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate synthetic dataset for SHAP demonstration\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    # Features\n",
    "    age = np.random.randint(18, 80, n_samples)\n",
    "    income = np.random.normal(50000, 20000, n_samples)\n",
    "    credit_score = np.random.normal(650, 100, n_samples)\n",
    "    debt_ratio = np.random.uniform(0.1, 0.6, n_samples)\n",
    "    # Target (loan approval)\n",
    "    approval_prob = (credit_score / 850 * 0.4 +\n",
    "                     (income / 100000) * 0.3 +\n",
    "                     (1 - debt_ratio) * 0.2 +\n",
    "                     (age / 80) * 0.1 +\n",
    "                     np.random.normal(0, 0.05, n_samples))\n",
    "    approval = (approval_prob > 0.5).astype(int)\n",
    "    df = pd.DataFrame({\n",
    "        'age': age, 'income': income,\n",
    "        'credit_score': credit_score,\n",
    "        'debt_ratio': debt_ratio,\n",
    "        'approved': approval\n",
    "    })\n",
    "    return df\n",
    "# ============================================================================\n",
    "# SHAP EXPLANATIONS\n",
    "# ============================================================================\n",
    "def explain_with_shap(model, X, feature_names, use_library=True):\n",
    "    \"\"\"\n",
    "    Generate SHAP explanations for the model\n",
    "    \"\"\"\n",
    "    if SHAP_AVAILABLE and use_library:\n",
    "        # Use actual SHAP library\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]  # For binary classification, use positive class\n",
    "        return shap_values, explainer\n",
    "    else:\n",
    "        # Use simplified implementation\n",
    "        shap_values = calculate_shap_values_simple(model, X, feature_names)\n",
    "        return shap_values, None\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "def plot_shap_summary(shap_values, X, feature_names):\n",
    "    \"\"\"\n",
    "    Plot SHAP summary plot\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    # Calculate mean absolute SHAP values for feature importance\n",
    "    mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "    # Sort by importance\n",
    "    indices = np.argsort(mean_shap)\n",
    "    y_pos = np.arange(len(feature_names))\n",
    "    colors = plt.cm.RdYlGn(mean_shap / mean_shap.max())\n",
    "    ax.barh(y_pos, mean_shap[indices], color=colors[indices])\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([feature_names[i] for i in indices])\n",
    "    ax.set_xlabel('Mean |SHAP Value|', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('SHAP Feature Importance Summary', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unit4-transparency-accountability', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: shap_summary.png\")\n",
    "    plt.close()\n",
    "def plot_shap_waterfall(shap_values, X, feature_names, sample_idx=0):\n",
    "    \"\"\"\n",
    "    Plot SHAP waterfall plot for a single prediction\n",
    "    \"\"\"\n",
    "    sample_shap = shap_values[sample_idx]\n",
    "    sample_values = X[sample_idx]\n",
    "    # Sort by absolute SHAP value\n",
    "    indices = np.argsort(np.abs(sample_shap))[::-1]\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    # Calculate cumulative SHAP values\n",
    "    cumulative = np.cumsum([0] + list(sample_shap[indices]))\n",
    "    # Plot waterfall\n",
    "    for i in range(len(indices)):\n",
    "        idx = indices[i]\n",
    "        color = 'red' if sample_shap[idx] < 0 else 'green'\n",
    "        ax.barh(i, sample_shap[idx], left=cumulative[i], color=color, alpha=0.7)\n",
    "        ax.text(cumulative[i] + sample_shap[idx]/2, i, \n",
    "               f'{feature_names[idx]}\\n={sample_values[idx]:.2f}',\n",
    "               ha='center', va='center', fontsize=9)\n",
    "    ax.set_yticks(range(len(indices)))\n",
    "    ax.set_yticklabels([feature_names[i] for i in indices])\n",
    "    ax.set_xlabel('SHAP Value', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'SHAP Waterfall Plot (Sample {sample_idx})', fontsize=12, fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unit4-transparency-accountability', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: shap_waterfall.png\")\n",
    "    plt.close()\n",
    "def plot_shap_dependence(shap_values, X, feature_names, feature_idx=0):\n",
    "    \"\"\"\n",
    "    Plot SHAP dependence plot for a specific feature\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    feature_values = X[:, feature_idx]\n",
    "    feature_shap = shap_values[:, feature_idx]\n",
    "    scatter = ax.scatter(feature_values, feature_shap, alpha=0.5, c=feature_shap, \n",
    "                        cmap='RdBu_r', s=30)\n",
    "    ax.set_xlabel(feature_names[feature_idx], fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('SHAP Value', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'SHAP Dependence Plot: {feature_names[feature_idx]}', fontsize=12, fontweight='bold')\n",
    "    ax.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax, label='SHAP Value')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unit4-transparency-accountability', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: shap_dependence.png\")\n",
    "    plt.close()\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"Unit 4 - Example 1: SHAP Explanations\")\n",
    "    print(\"=\"*80)\n",
    "    # Generate dataset\n",
    "    print(\"\\nGenerating dataset...\")\n",
    "    df = generate_dataset(n_samples=1000)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    # Prepare data\n",
    "    feature_names = ['age', 'income', 'credit_score', 'debt_ratio']\n",
    "    X = df[feature_names].values\n",
    "    y = df['approved'].values\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # Train model\n",
    "    print(\"\\nTraining Random Forest model...\")\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train_scaled))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    # Calculate SHAP values\n",
    "    print(\"\\nCalculating SHAP values...\")\n",
    "    shap_values, explainer = explain_with_shap(\n",
    "        model, X_test_scaled[:100], feature_names, use_library=SHAP_AVAILABLE\n",
    "    )\n",
    "    print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "    # Create visualizations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Creating Visualizations...\")\n",
    "    print(\"=\"*80)\n",
    "    plot_shap_summary(shap_values, X_test_scaled[:100], feature_names)\n",
    "    plot_shap_waterfall(shap_values, X_test_scaled[:100], feature_names, sample_idx=0)\n",
    "    plot_shap_dependence(shap_values, X_test_scaled[:100], feature_names, feature_idx=2)\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nKey Takeaways:\")\n",
    "    print(\"1. SHAP values explain individual predictions\")\n",
    "    print(\"2. SHAP summary shows global feature importance\")\n",
    "    print(\"3. Waterfall plots show how features contribute to a specific prediction\")\n",
    "    print(\"4. Dependence plots show how SHAP values vary with feature values\")\n",
    "    print(\"5. SHAP provides model-agnostic explanations\")\n",
    "    print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸš« When SHAP Explanations Hit a Limitation | Ø¹Ù†Ø¯Ù…Ø§ ØªØµÙ„ ØªÙØ³ÙŠØ±Ø§Øª SHAP Ø¥Ù„Ù‰ Ø­Ø¯\n",
    "\n",
    "### The Limitation We Discovered\n",
    "\n",
    "We've learned SHAP for explaining model predictions. **But there's still a challenge:**\n",
    "\n",
    "**What if we need a simpler, faster explanation method that works with any model?**\n",
    "\n",
    "SHAP works well when:\n",
    "- âœ… We need mathematically rigorous explanations\n",
    "- âœ… We can afford computational cost\n",
    "- âœ… We want feature importance based on game theory\n",
    "\n",
    "**But sometimes we need:**\n",
    "- âŒ **Faster explanations** (SHAP can be slow for large datasets)\n",
    "- âŒ **Simpler methods** (easier to understand and implement)\n",
    "- âŒ **Model-agnostic approaches** (work with any black-box model)\n",
    "- âŒ **Local explanations** (explain individual predictions quickly)\n",
    "\n",
    "### Why This Is a Problem\n",
    "\n",
    "When SHAP is too slow or complex:\n",
    "- We may need faster explanations for real-time systems\n",
    "- We may want simpler methods for non-technical users\n",
    "- We may need alternatives for different use cases\n",
    "- We may want to compare different explanation methods\n",
    "\n",
    "### The Solution: LIME Explanations\n",
    "\n",
    "We need **LIME (Local Interpretable Model-agnostic Explanations)** to:\n",
    "1. Provide fast local explanations\n",
    "2. Work with any black-box model\n",
    "3. Offer simpler, more intuitive explanations\n",
    "4. Enable real-time explainability\n",
    "\n",
    "**This is exactly what we'll learn in the next notebook: LIME Explanations!**\n",
    "\n",
    "---\n",
    "\n",
    "## âž¡ï¸ Next Steps | Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©\n",
    "\n",
    "**You've completed this notebook!** Now you understand:\n",
    "- âœ… How to use SHAP for model explanations\n",
    "- âœ… How to generate local and global explanations\n",
    "- âœ… **The limitation**: We need faster, simpler alternatives!\n",
    "\n",
    "**Next notebook**: `02_lime_explanations.ipynb`\n",
    "- Learn about LIME for fast local explanations\n",
    "- Understand model-agnostic explainability\n",
    "- Compare SHAP vs LIME approaches\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}