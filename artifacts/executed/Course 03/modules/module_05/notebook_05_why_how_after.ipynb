<<<<<<< Current (Your changes)
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: Probabilities, Samples, and Statistical Inference\n",
    "\n",
    "ÿßŸÑÿßÿ≠ÿ™ŸÖÿßŸÑÿßÿ™ÿå ŸàÿßŸÑÿπŸäŸÜÿßÿ™ÿå ŸàÿßŸÑÿßÿ≥ÿ™ÿØŸÑÿßŸÑ ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿä\n",
    "\n",
    "> **Note**: Content should be populated from `course-content/05.pptx`\n",
    "\n",
    "---\n",
    "\n",
    "## üéì The Final Piece of the Puzzle\n",
    "\n",
    "**Congratulations on reaching the final module!** This module integrates everything you've learned:\n",
    "\n",
    "- **From Module 01**: Data representation as vectors/matrices for probabilistic models\n",
    "- **From Module 03**: Statistical foundations ‚Üí Extended to **statistical inference**\n",
    "- **From Module 04**: Evaluating reduced-dimensional representations statistically\n",
    "\n",
    "> üí° **Complete Picture**: Now you can:\n",
    "> - Represent data (Module 01)\n",
    "> - Train models with optimization (Module 03, using gradients from Module 02)\n",
    "> - Reduce dimensions efficiently (Module 04)\n",
    "> - Evaluate and make predictions with confidence (Module 05)\n",
    "\n",
    "**You're now ready to apply all these concepts to real ML projects!**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this module, you will learn:\n",
    "- Probability theory and distributions for ML\n",
    "- Random variables and sampling techniques\n",
    "- Statistical inference methods\n",
    "- Hypothesis testing for model evaluation\n",
    "- Bayesian inference concepts\n",
    "- Applying probabilistic models to ML problems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: WHY\n",
    "\n",
    "## Understanding the Motivation\n",
    "\n",
    "### Why do Probabilities and Statistical Inference matter for Machine Learning?\n",
    "\n",
    "Probability and statistics are fundamental to understanding ML:\n",
    "\n",
    "- **Uncertainty**: ML models make predictions with uncertainty that we need to quantify\n",
    "- **Data Sampling**: Understanding sampling helps with train/test splits and cross-validation\n",
    "- **Model Evaluation**: Statistical tests help determine if model improvements are significant\n",
    "- **Bayesian Methods**: Many ML algorithms use probabilistic frameworks\n",
    "- **Decision Making**: Probabilities help make informed decisions from model outputs\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "- **Confidence Intervals**: Understanding prediction uncertainty\n",
    "- **A/B Testing**: Statistical tests for comparing models\n",
    "- **Bayesian Networks**: Probabilistic graphical models\n",
    "- **Naive Bayes**: Classification using probability theory\n",
    "- **Monte Carlo Methods**: Sampling-based inference\n",
    "\n",
    "\n",
    "> üí° **Instructor Note**: You can add more domain-specific examples from your slides here, such as:\n",
    "> - A/B testing: Statistical tests for comparing model versions in production\n",
    "> - Medical AI: Confidence intervals for diagnostic predictions\n",
    "> - Financial ML: Uncertainty quantification for risk assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:46.561198Z",
     "iopub.status.busy": "2026-01-20T05:44:46.561123Z",
     "iopub.status.idle": "2026-01-20T05:44:46.564582Z",
     "shell.execute_reply": "2026-01-20T05:44:46.564409Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "    if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "        #     if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "                # NOTE: Auto-suppressed invalid cell\n",
    "                # # Example: Why Probabilities and Inference matter in ML\n",
    "                # import numpy as np\n",
    "                # from scipy import stats\n",
    "\n",
    "                # # Example: Understanding prediction uncertaint\n",
    "                # y\n",
    "                # = np.array([0.85, 0.92, 0.78, 0.95, 0.88])\n",
    "                # mean_pred = np.mean(predictions)\n",
    "                # std_pred = np.std(predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"Model Predictions with Uncertainty:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(f\"Mean prediction: {mean_pred:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(f\"Standard deviation: {std_pred:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(f\"95% confidence interval: [{mean_pred - 1.96*std_pred:.2f}, {mean_pred + 1.96*std_pred:.2f}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"\\nStatistical inference helps us:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"- Understand prediction reliability\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"- Compare different models\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"- Make informed decisions\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: HOW\n",
    "\n",
    "## Implementation and Practical Application\n",
    "\n",
    "### How Probabilities and Statistical Inference Work in Machine Learning\n",
    "\n",
    "Let's understand **HOW** probability and statistical inference operations work and **WHY** each is essential:\n",
    "\n",
    "#### 1. **WHY Probability Distributions?**\n",
    "- **WHY**: ML models deal with uncertainty - we need to model and quantify it\n",
    "- **HOW**: Probability distributions describe the likelihood of different outcomes\n",
    "  - Normal distribution: For continuous data (e.g., prediction errors)\n",
    "  - Binomial distribution: For binary outcomes (e.g., classification)\n",
    "  - Poisson distribution: For count data (e.g., number of events)\n",
    "\n",
    "#### 2. **WHY Sampling?**\n",
    "- **WHY**: We can't use all data - need representative samples for training/testing\n",
    "- **HOW**: Random sampling techniques ensure:\n",
    "  - Train/test splits are representative\n",
    "  - Cross-validation gives reliable estimates\n",
    "  - Bootstrap methods provide confidence intervals\n",
    "\n",
    "#### 3. **WHY Statistical Tests?**\n",
    "- **WHY**: Need to determine if model improvements are real or due to chance\n",
    "- **HOW**: Statistical tests (t-test, chi-square, etc.) compute:\n",
    "  - Test statistic: Measures the difference\n",
    "  - p-value: Probability of observing this difference by chance\n",
    "  - Decision: Reject null hypothesis if p < significance level (e.g., 0.05)\n",
    "\n",
    "#### 4. **WHY Confidence Intervals?**\n",
    "- **WHY**: Point estimates don't show uncertainty - need range of likely values\n",
    "- **HOW**: Confidence intervals provide:\n",
    "  - Range of values that likely contains the true parameter\n",
    "  - Level of confidence (e.g., 95% CI means 95% chance true value is in range)\n",
    "  - Uncertainty quantification for predictions\n",
    "\n",
    "#### 5. **WHY Bayesian Inference?**\n",
    "- **WHY**: Incorporate prior knowledge and update beliefs with data\n",
    "- **HOW**: Bayesian methods:\n",
    "  - Start with prior probability (what we believe before seeing data)\n",
    "  - Update with likelihood (what data tells us)\n",
    "  - Get posterior probability (updated belief after seeing data)\n",
    "\n",
    "### Step-by-Step: Working with Probabilities and Statistical Inference in Python\n",
    "\n",
    "**Step-by-Step Guide to Working with Probabilities and Statistical Inference in Python:**\n",
    "\n",
    "Here's how to apply probability and statistical inference concepts in practice:\n",
    "\n",
    "1. **Probability Distributions**: Creating and working with probability distributions\n",
    "2. **Random Sampling**: Generating samples from distributions\n",
    "3. **Statistical Tests**: Performing hypothesis tests (t-test, chi-square, etc.)\n",
    "4. **Confidence Intervals**: Computing confidence intervals for estimates\n",
    "5. **Bayesian Inference**: Basic Bayesian updating concepts\n",
    "6. **Applications**: Using these concepts in ML contexts\n",
    "\n",
    "### Key Concepts to Cover:\n",
    "\n",
    "- Probability mass functions (PMF) and probability density functions (PDF)\n",
    "- Common distributions (normal, binomial, Poisson, etc.)\n",
    "- Sampling methods (random, stratified, bootstrap)\n",
    "- Hypothesis testing framework\n",
    "- Type I and Type II errors\n",
    "- Confidence intervals and interpretation\n",
    "- Bayesian vs. frequentist approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:46.565379Z",
     "iopub.status.busy": "2026-01-20T05:44:46.565326Z",
     "iopub.status.idle": "2026-01-20T05:44:46.567811Z",
     "shell.execute_reply": "2026-01-20T05:44:46.567639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # Example: Basic Probability and Statistical Inference Operations in Python\n",
    "#         # This shows HOW each concept works and WHY it's used in ML\n",
    "#         # import numpy as np\n",
    "#         from scipy import stats   import matplotlib.pyplot as plt\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"HOW Probabilities and Statistical Inference Work in ML\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # 1. Working with probability distributions\n",
    "#         # WHY: ML models need to understand data distributions\n",
    "#         # HOW: Use probability distributions to model uncertainty\n",
    "#         # print(\"\\n1. Probability Distributions (Modeling Uncertainty):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" WHY: Need to understand and model data distributions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" HOW: Use probability distributions to describe data patterns\")\n",
    "\n",
    "#         # = np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Normal distribution: mean={np.mean(samples):.4f}, std={np.std(samples):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" HOW: This distribution models continuous data (e.g., prediction errors)\")\n",
    "\n",
    "#         # = np.random.binomial(n, p, 1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Binomial distribution: mean={np.mean(binomial_samples):.4f}, std={np.std(binomial_samples):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" HOW: This distribution models binary outcomes (e.g., classification)\")\n",
    "\n",
    "#         # 2. Statistical hypothesis testing\n",
    "#         # WHY: Determine \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         if differences between models are statistically significant\n",
    "#         # HOW: Compute test statistic and p-value\n",
    "#         # print(\"\\n2. Statistical Hypothesis Testing (Model Comparison):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" WHY: Need to know\")\n",
    "\n",
    "\n",
    "\n",
    "#         if model improvements are real or by chance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" HOW: Use statistical tests to compare models\")\n",
    "\n",
    "#         # = np.random.normal(0.85, 0.02, 100)\n",
    "#         # model_b_scores = np.random.normal(0.87, 0.02, 100)\n",
    "\n",
    "#         # t_stat, p_value = stats.ttest_ind(model_a_scores, model_b_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Model A mean: {np.mean(model_a_scores):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Model B mean: {np.mean(model_b_scores):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" t-statistic: {t_stat:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" p-value: {p_value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if p_value < 0.05:\n",
    "#         #  print(\" ‚úÖ Significant difference! (p < 0.05)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(\" HOW: p-value < 0.05 means < 5% chance this difference is by chance\")\n",
    "#         # else:\n",
    "#         #  print(\" ‚ùå No significant difference (p >= 0.05)\")\n",
    "\n",
    "#         # 3. Confidence intervals\n",
    "#         # WHY: Point estimates don't show uncertainty\n",
    "#         # HOW: Compute range of likely values\n",
    "#         # print(\"\\n3. Confidence Intervals (Uncertainty Quantification):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" WHY: Need to know the range of likely values, not just a point estimate\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" HOW: Compute confidence interval using standard error\")\n",
    "\n",
    "#         # Example: Confidence interval \n",
    "#         # for meanmean_score =\n",
    "#         np.mean(model_a_scores)\n",
    "#         # std_error = stats.sem(model_a_scores)\n",
    "#         # ci_95 = stats.t.interval(0.95, len(model_a_scores)-1, loc=mean_score, scale=std_error)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Mean score: {mean_score:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" 95% Confidence Interval: [{ci_95[0]:.4f}, {ci_95[1]:.4f}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" HOW: 95% CI means we're 95% confident the true mean is in this range\")\n",
    "\n",
    "#         # 4. Sampling methods\n",
    "#         # WHY: Need representative samples \n",
    "#         # for training/testing\n",
    "#         # HOW: Use proper sampling techniques\n",
    "#         # print(\"\\n4. Sampling Methods (Data Splitting):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" WHY: Can't use all data - need representative train/test splits\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" HOW: Use random sampling to ensure representativeness\")\n",
    "\n",
    "#         # = np.random.normal(0, 1, 1000)\n",
    "#         # train_size = int(0.8 * len(data))\n",
    "#         # train_data = data[:train_size]\n",
    "#         # test_data = data[train_size:]\n",
    "#         # print(f\" Total data: {len(data)} samples\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Training set: {len(train_data)} samples ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Test set: {len(test_data)} samples ({len(test_data)/len(data)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" HOW: Random split ensures both sets are representative\")\n",
    "\n",
    "#         # 5. Bayesian inference basics\n",
    "#         # WHY: Incorporate prior knowledge into models\n",
    "#         # HOW: Update prior beliefs with observed data\n",
    "#         # print(\"\\n5. Bayesian Inference (Prior Knowledge Integration):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" WHY: Want to incorporate what we know before seeing data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" HOW: Start with prior, update with data to get posterior\")\n",
    "\n",
    "#         # = 0.8 \n",
    "#         # Data suggests 80% chanc\n",
    "#         # e\n",
    "#         # = (prior * likelihood) / normalization\n",
    "#         # In practice, use Bayes' theorem properly\n",
    "#         # print(f\" Prior belief: {prior_belief:.1%}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Likelihood from data: {likelihood:.1%}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" HOW: Bayes' theorem combines prior knowledge with observed data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Result: Updated belief (posterior) that balances both\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"üí° Key Takeaway:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Probabilities quantify uncertainty\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Statistical inference helps make data-driven decisions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Both are essential for evaluating and improving ML models!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:46.568540Z",
     "iopub.status.busy": "2026-01-20T05:44:46.568489Z",
     "iopub.status.idle": "2026-01-20T05:44:46.570177Z",
     "shell.execute_reply": "2026-01-20T05:44:46.570000Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Example: Probabilities and Statistical Inference in Python\n",
    "        # This builds on statistics \n",
    "        # from Module 03 and uses data representation \n",
    "        # from Module 01 import numpy as np\n",
    "        from scipy import stats\n",
    "        # print(\"üîó Module Connections in Statistical Inference:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 50)\n",
    "\n",
    "        # 1. Working with probability distributions\n",
    "        # Normal distribution - foundation \n",
    "        # for many ML modelsmu, sigma = 0, 1samples =\n",
    "        # np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n1. Probability Distribution (Foundation):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Sample mean: {np.mean(samples):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Sample std: {np.std(samples):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" (This extends statistical concepts from Module 03)\")\n",
    "\n",
    "        # 2. Statistical test example - comparing model\n",
    "        # s\n",
    "        # = np.random.normal(0.85, 0.02, 100)\n",
    "        # model_b_scores = np.random.normal(0.87, 0.02, 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n2. Model Comparison (Using Module 03 Statistics):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Model A mean: {np.mean(model_a_scores):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Model B mean: {np.mean(model_b_scores):.4f}\")\n",
    "\n",
    "        # = stats.ttest_ind(model_a_scores, model_b_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n t-statistic: {t_stat:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" p-value: {p_value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if p_value < 0.05:\n",
    "        #  print(f\" ‚úÖ Models are significantly different! (p < 0.05)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" üí° This statistical test (Module 03 ‚Üí Module 05) helps us choose the best model!\")\n",
    "        # else:\n",
    "        #  print(f\" ‚ö†Ô∏è No significant difference found\")\n",
    "\n",
    "        # 3. Confidence intervals - quantifying uncertainty\n",
    "        # print(f\"\\n3. Confidence Intervals (Uncertainty Quantification):\")\n",
    "        # ci_lower = np.mean(model_a_scores) - 1.96 * np.std(model_a_scores) / np.sqrt(len(model_a_scores))\n",
    "        # ci_upper = np.mean(model_a_scores) + 1.96 * np.std(model_a_scores) / np.sqrt(len(model_a_scores))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" 95% CI for Model A: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" This tells us the range of likely performance!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nüí° All modules working together:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Module 01: Data as vectors/arrays\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Module 03: Statistical measures\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Module 05: Statistical inference for decision-making\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:46.570891Z",
     "iopub.status.busy": "2026-01-20T05:44:46.570847Z",
     "iopub.status.idle": "2026-01-20T05:44:46.572832Z",
     "shell.execute_reply": "2026-01-20T05:44:46.572661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # Interactive Example: Complete ML Pipeline Using All Modules\n",
    "#         # VISUAL: See how all modules work together with statistical analysis!\n",
    "\n",
    "#         # import numpy as np\n",
    "#         from scipy import stats\n",
    "#         # stats\n",
    "#         from sklearn.decomposition \n",
    "#         # PCA\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         # print(\"üéì Complete ML Workflow - All Modules Together:\")\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # Step 1: Data representation (Module 01)\n",
    "#         # np.random.seed(42)\n",
    "#         # data = np.random.randn(100, 20) \n",
    "#         # High-dimensional data\n",
    "#         # print(\"\\n1. Module 01 (Linear Algebra):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Data represented as matrix: {data.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Each row is a vector (data point)\")\n",
    "\n",
    "#         # Step 2: Dimensionality reduction (Module 01 + 03 + 04)\n",
    "#         # pca = PCA(n_components=5)\n",
    "#         # data_reduced = pca.fit_transform(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n2. Module 04 (Dimensionality Reduction):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Reduced to: {data_reduced.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Uses eigenvalues (Module 01) + optimization (Module 03)\")\n",
    "\n",
    "#         # Step 3: Model training simulation (Module 02 + 03)\n",
    "#         # train_scores = np.random.normal(0.88, 0.03, 50)\n",
    "#         # val_scores = np.random.normal(0.85, 0.03, 50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n3. Modules 02-03 (Calculus + Optimization):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Model trained using optimization (Module 03, which uses gradients from Module 02)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Optimized using Adam (Module 03)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Training: {np.mean(train_scores):.2%} ¬± {np.std(train_scores):.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Validation: {np.mean(val_scores):.2%} ¬± {np.std(val_scores):.2%}\")\n",
    "\n",
    "#         # Step 4: Statistical evaluation (Module 03 + 05)\n",
    "#         # t_stat, p_value = stats.ttest_ind(train_scores, val_scores)\n",
    "\n",
    "#         # 10))\n",
    "#         # Plot 1: Data reductionaxes[0, 0].scatter(data_reduced[:, 0], data_reduced[:, 1], alph\n",
    "#         # a=0.6, \n",
    "#         # c='blue', s=30)\n",
    "#         # axes[0, 0].set_xlabel('First Principal Component', fontsize=11)\n",
    "#         # axes[0, 0].set_ylabel('Second Principal Component', fontsize=11)\n",
    "#         # axes[0, 0].set_title('Step 1-2: Data Reduction (Modules 01, 04)', fontsize=12, fontweight='bold')\n",
    "#         # axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "#         # Plot 2: Model performanceaxes[0, 1].bar(['Training', 'Validation'], \n",
    "#         #  [np.mean(train_scores), np.mean(val_scores)], yerr=[np.std(train_scores), np.std(val_scores)], color=['green', 'orange'], alpha=0.7, capsize=10)\n",
    "#         # axes[0, 1].set_ylabel('Accuracy', fontsize=11)\n",
    "#         # axes[0, 1].set_title('Step 3: Model Performance (Modules 02-03)', fontsize=12, fontweight='bold')\n",
    "#         # axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "#         # axes[0, 1].set_ylim(0.7, 1.0)\n",
    "\n",
    "\n",
    "#         # axes[1, 0].hist(val_scores, bins=15, alpha=0.6, color='orange', label='Validation', density=True)\n",
    "#         # axes[1, 0].set_xlabel('Accuracy Score', fontsize=11)\n",
    "#         # axes[1, 0].set_ylabel('Density', fontsize=11)\n",
    "#         # axes[1, 0].set_title('Step 4: Score Distributions (Module 05)', fontsize=12, fontweight='bold')\n",
    "#         # axes[1, 0].legend()\n",
    "#         # axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "#         # Plot 4: Confidence intervals - FIX: use separate errorbar calls \n",
    "#         # for each colormeans = [\n",
    "#         np.mean(train_scores), np.mean(val_scores)]\n",
    "#         # stds = [np.std(train_scores), np.std(val_scores)]\n",
    "#         # n = len(train_scores)\n",
    "#         # ci_lower = [m - 1.96*s/np.sqrt(n) for m, s in zip(means, stds)]\n",
    "#         # ci_upper = [m + 1.96*s/np.sqrt(n) for m, s in zip(means, stds)]\n",
    "\n",
    "#         # Plot each point separately with its own coloraxes[1, 1].errorbar(['Training'], [means[0]],\n",
    "#         #  yerr=[[means[0] - ci_lower[0]], [ci_upper[0] - means[0]]],\n",
    "#         #  fmt='o', capsize=10, capthick=2, markersize=10,\n",
    "#         #  color='green', alpha=0.7, label='Training')\n",
    "#         # axes[1, 1].errorbar(['Validation'], [means[1]],\n",
    "#         #  yerr=[[means[1] - ci_lower[1]], [ci_upper[1] - means[1]]],\n",
    "#         #  color='orange', alpha=0.7, label='Validation')\n",
    "#         # axes[1, 1].fill_between(['Training', 'Validation'], ci_lower, ci_upper, alpha=0.2, color=['green', 'orange'])\n",
    "#         # axes[1, 1].set_ylabel('Accuracy with 95% CI', fontsize=11)\n",
    "#         # axes[1, 1].set_title('Statistical Inference (Module 05)', fontsize=12, fontweight='bold')\n",
    "#         # axes[1, 1].legend()\n",
    "#         # axes[1, 1].grid(True, alpha=0.3)\n",
    "#         # axes[1, 1].set_ylim(0.7, 1.0)\n",
    "\n",
    "#         # plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n4. Module 05 (Statistical Inference):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" t-test: t = {t_stat:.4f}, p = {p_value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if p_value < 0.05:\n",
    "#         #  print(\" ‚ö†Ô∏è Significant difference - possible overfitting!\")\n",
    "#         # else:\n",
    "#         #  print(\" ‚úÖ Good generalization!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" 95% CI Training: [{ci_lower[0]:.2%}, {ci_upper[0]:.2%}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" 95% CI Validation: [{ci_lower[1]:.2%}, {ci_upper[1]:.2%}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nüìä Visual Summary:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Top-left: Data reduction (Modules 01, 04)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Top-right: Model performance (Modules 02-03)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Bottom-left: Score distributions (Module 05)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Bottom-right: Confidence intervals (Module 05)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nüéâ All 5 modules working together!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:46.573538Z",
     "iopub.status.busy": "2026-01-20T05:44:46.573479Z",
     "iopub.status.idle": "2026-01-20T05:44:46.575422Z",
     "shell.execute_reply": "2026-01-20T05:44:46.575259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # Example: Visualizing Probability Distributions and Statistical Inference\n",
    "#         # This demonstrates HOW to visualize and interpret statistical concepts\n",
    "#         # import numpy as np\n",
    "#         from scipy import stats   import matplotlib.pyplot as plt\n",
    "\n",
    "#         # 10))\n",
    "#         # = np.linspace(-4, 4, 1000)\n",
    "#         normal_p\n",
    "#         # df = stats.norm.pdf(x, 0, 1)\n",
    "#         # binomial_pmf = stats.binom.pmf(range(11), 10, 0.5)\n",
    "\n",
    "#         # axes[0, 0].plot(x, normal_pdf, 'b-', linewidth=2, label='Normal Distribution')\n",
    "#         # axes[0, 0].fill_between(x, 0, normal_pdf, alpha=0.3, color='blue')\n",
    "#         # axes[0, 0].set_title('Probability Density Function (PDF)', fontsize=12, fontweight='bold')\n",
    "#         # axes[0, 0].set_xlabel('Value')\n",
    "#         # axes[0, 0].set_ylabel('Probability Density')\n",
    "#         # axes[0, 0].grid(True, alpha=0.3)\n",
    "#         # axes[0, 0].legend()\n",
    "\n",
    "#         # axes[0, 1].bar(range(11), binomial_pmf, color='green', alpha=0.7, edgecolor='black')\n",
    "#         # axes[0, 1].set_title('Probability Mass Function (PMF)', fontsize=12, fontweight='bold')\n",
    "#         # axes[0, 1].set_xlabel('Number of Successes')\n",
    "#         # axes[0, 1].set_ylabel('Probability')\n",
    "#         # axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "#         # 2. Confidence intervals visualizationnp.random.seed(42)\n",
    "#         # means = []\n",
    "#         # cis = []\n",
    "#         # for i in range(5):\n",
    "#         #  sample = np.random.normal(0.85, 0.05, 50)\n",
    "#         #  mean_val = np.mean(sample)\n",
    "#         #  ci = stats.t.interval(0.95, len(sample)-1, loc=mean_val, scale=stats.sem(sample))\n",
    "#          means.append(mean_val)\n",
    "#          cis.append(ci)\n",
    "\n",
    "#         # axes[1, 0].errorbar(range(5), means, \n",
    "#         #  yerr=[[m - c[0] for m, c in zip(means, cis)],\n",
    "#         #  [c[1] - m for m, c in zip(means, cis)]],\n",
    "#         #  fmt='o', capsize=5, capthick=2, markersize=8, color='red')\n",
    "#         # axes[1, 0].axhline(y=0.85, color='blue', linestyle='--', linewidth=2, label='True Mean')\n",
    "#         # axes[1, 0].set_title('Confidence Intervals (95% CI)', fontsize=12, fontweight='bold')\n",
    "#         # axes[1, 0].set_xlabel('Sample Number')\n",
    "#         # axes[1, 0].set_ylabel('Mean with 95% CI')\n",
    "#         # axes[1, 0].legend()\n",
    "#         # axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "#         # = np.random.normal(0.85, 0.02, 100)\n",
    "#         # model_b = np.random.normal(0.87, 0.02, 100)\n",
    "\n",
    "#         # axes[1, 1].hist(model_a, bins=20, alpha=0.6, color='blue', label='Model A', edgecolor='black')\n",
    "#         # axes[1, 1].hist(model_b, bins=20, alpha=0.6, color='orange', label='Model B', edgecolor='black')\n",
    "#         # axes[1, 1].axvline(np.mean(model_a), color='blue', linestyle='--', linewidth=2)\n",
    "#         # axes[1, 1].axvline(np.mean(model_b), color='orange', linestyle='--', linewidth=2)\n",
    "#         # axes[1, 1].set_title('Model Comparison (Hypothesis Testing)', fontsize=12, fontweight='bold')\n",
    "#         # axes[1, 1].set_xlabel('Model Score')\n",
    "#         # axes[1, 1].set_ylabel('Frequency')\n",
    "#         # axes[1, 1].legend()\n",
    "#         # axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "#         # t_stat, p_val = stats.ttest_ind(model_a, model_b)\n",
    "#         # axes[1, 1].text(0.05, 0.95, f't-statistic: {t_stat:.3f}\\np-value: {p_val:.4f}',\n",
    "#         #  transform=axes[1, 1].transAxes, fontsize=10,\n",
    "#         #  verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "#         # plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nüìä Visual Summary:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Top-left: Normal distribution (PDF) - for continuous data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Top-right: Binomial distribution (PMF) - for discrete outcomes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Bottom-left: Confidence intervals showing uncertainty\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Bottom-right: Hypothesis testing comparing two models\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nüí° These visualizations help understand HOW statistical inference works!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "\n",
    "**Best Practices for Statistical Inference in ML:**\n",
    "\n",
    "- **Choose Appropriate Tests**: Use t-test for comparing means, chi-square for categorical data, ANOVA for multiple groups\n",
    "- **Set Significance Level**: Use Œ±=0.05 (95% confidence) as standard, but adjust based on your domain (e.g., medical research may use Œ±=0.01)\n",
    "- **Report Confidence Intervals**: Always report confidence intervals along with point estimates to show uncertainty\n",
    "- **Check Assumptions**: Verify test assumptions (normality, independence, equal variances) before running statistical tests\n",
    "- **Multiple Comparisons**: Use Bonferroni correction or FDR when testing multiple hypotheses to avoid false discoveries\n",
    "- **Sample Size Matters**: Ensure adequate sample size for reliable statistical inference (use power analysis if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: AFTER\n",
    "\n",
    "## Analyzing Results and Understanding Implications\n",
    "\n",
    "### What Happens Next?\n",
    "\n",
    "After understanding probabilities and statistical inference:\n",
    "\n",
    "- **Confident Predictions**: You can quantify uncertainty in model predictions\n",
    "- **Model Evaluation**: You can statistically compare different models\n",
    "- **Decision Making**: You can make informed decisions based on statistical evidence\n",
    "- **Advanced ML**: You're ready for probabilistic ML models (Bayesian methods, etc.)\n",
    "\n",
    "### How This Connects to Machine Learning\n",
    "\n",
    "- **Uncertainty Quantification**: Understanding prediction confidence\n",
    "- **Model Selection**: Statistical tests help choose the best model\n",
    "- **A/B Testing**: Statistical inference for production model comparisons\n",
    "- **Bayesian ML**: Foundation for probabilistic machine learning methods\n",
    "\n",
    "\n",
    "> üí° **Key Insight**: Statistical inference transforms ML from \"the model says X\" to \"we're 95% confident the true value is between Y and Z.\" This confidence is crucial for making real-world decisions based on ML predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:46.576203Z",
     "iopub.status.busy": "2026-01-20T05:44:46.576156Z",
     "iopub.status.idle": "2026-01-20T05:44:46.578255Z",
     "shell.execute_reply": "2026-01-20T05:44:46.578109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # Example: Analyzing Results and Understanding Implications\n",
    "#         # This demonstrates what happens AFTER applying statistical inference\n",
    "#         # import numpy as np\n",
    "#         from scipy import stats   import matplotlib.pyplot as plt\n",
    "#         # print('=\" * 60)\")\n",
    "\n",
    "\n",
    "#         # print(\"AFTER: Understanding the Impact of Statistical Inference\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # Simulate model evaluation resultsnp.random.seed(42)\n",
    "#         # model_predictions = np.random.normal(0.88, 0.05, 100)\n",
    "#         # true_values = np.random.normal(0.87, 0.03, 100)\n",
    "\n",
    "#         # = np.mean(model_predictions)\n",
    "#         # std_pred = np.std(model_predictions)\n",
    "#         # ci_95 = stats.t.interval(0.95, len(model_predictions)-1, loc=mean_pred, scale=stats.sem(model_predictions))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\n1. Model Prediction Analysis:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Mean prediction: {mean_pred:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Standard deviation: {std_pred:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" 95% Confidence Interval: [{ci_95[0]:.4f}, {ci_95[1]:.4f}]\")\n",
    "\n",
    "#         # = 0.85t_stat, p_value = stats.ttest_1samp(model_predictions, baseline)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\n2. Statistical Significance Test:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Testing\")\n",
    "\n",
    "\n",
    "\n",
    "#         # if model (mean={mean_pred:.4f}) is better than baseline ({baseline})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" t-statistic: {t_stat:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" p-value: {p_value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if p_value < 0.05:\n",
    "#         #  print(f\" ‚úÖ Model is significantly better than baseline! (p < 0.05)\")\n",
    "#         # else:\n",
    "#         #  print(f\" ‚ùå No significant improvement over baseline (p >= 0.05)\")\n",
    "\n",
    "#         # 5))\n",
    "\n",
    "#         # axes[0].axvline(mean_pred, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_pred:.3f}')\n",
    "#         # axes[0].axvline(ci_95[0], color='green', linestyle=':', linewidth=2, label=f'95% CI: [{ci_95[0]:.3f}, {ci_95[1]:.3f}]')\n",
    "#         # axes[0].axvline(ci_95[1], color='green', linestyle=':', linewidth=2)\n",
    "#         # axes[0].axvline(baseline, color='orange', linestyle='-', linewidth=2, label=f'Baseline: {baseline}')\n",
    "#         # axes[0].set_title('Prediction Distribution with Confidence Interval', fontsize=12, fontweight='bold')\n",
    "#         # axes[0].set_xlabel('Prediction Value')\n",
    "#         # axes[0].set_ylabel('Frequency')\n",
    "#         # axes[0].legend()\n",
    "#         # axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "#         # Comparison with baselineaxes[1].bar(['Baseline', 'Model'], [baseline, mean_pred], \n",
    "#         #  yerr=[[0, 0], [mean_pred - ci_95[0], ci_95[1] - mean_pred]],\n",
    "#         #  color=['orange', 'blue'], alpha=0.7, capsize=10, capthick=2, edgecolor='black')\n",
    "#         # axes[1].set_title('Model vs Baseline Comparison', fontsize=12, fontweight='bold')\n",
    "#         # axes[1].set_ylabel('Score')\n",
    "#         # axes[1].set_ylim(0.8, 0.95)\n",
    "#         # axes[1].grid(True, alpha=0.3, axis='y')\n",
    "#         # axes[1].text(1, mean_pred + 0.01, f'p={p_value:.4f}', ha='center', fontsize=10,\n",
    "#         #  bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "#         # plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n3. Key Insights:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" ‚úÖ Quantified uncertainty in predictions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" ‚úÖ Statistically compared model to baseline\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" ‚úÖ Made data-driven decision about model performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" ‚úÖ Understand confidence in model predictions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n4. Implications for Machine Learning:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" üìä Uncertainty Quantification: Know how confident we are in predictions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" üéØ Model Selection: Use statistical tests to choose best model\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" üî¨ A/B Testing: Compare models in production using statistical inference\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" üìà Decision Making: Make informed decisions based on statistical evidence\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" üß† Bayesian ML: Foundation for probabilistic machine learning methods\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n5. When to Use Statistical Inference:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" ‚úÖ Comparing different models\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" ‚úÖ Evaluating model improvements\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" ‚úÖ Quantifying prediction uncertainty\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" ‚úÖ Making production deployment decisions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" ‚úÖ Understanding model reliability\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"üí° Remember: Statistical inference helps make data-driven decisions!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Always quantify uncertainty and test significance before deploying models.\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Pitfalls and How to Avoid Them\n",
    "\n",
    "**Common Pitfalls and How to Avoid Them:**\n",
    "\n",
    "1. **P-Value Misinterpretation**: Treating p-value as probability that null hypothesis is true\n",
    "   - **Solution**: Remember p-value is probability of observing data given null hypothesis is true, not probability null is true\n",
    "\n",
    "2. **Multiple Testing Without Correction**: Running many tests increases chance of false positives\n",
    "   - **Solution**: Use Bonferroni correction, FDR, or other multiple comparison corrections when testing multiple hypotheses\n",
    "\n",
    "3. **Ignoring Effect Size**: Focusing only on statistical significance without considering practical importance\n",
    "   - **Solution**: Always report effect size (Cohen's d, correlation coefficient) along with p-values\n",
    "\n",
    "4. **Data Dredging**: Testing many hypotheses and only reporting significant ones (p-hacking)\n",
    "   - **Solution**: Pre-specify hypotheses before data collection, use cross-validation, and report all tests performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Topics\n",
    "\n",
    "**Advanced Topics for Further Learning:**\n",
    "\n",
    "- **Bayesian Inference**: Updating beliefs with data using Bayes' theorem for probabilistic reasoning\n",
    "- **Bootstrap Methods**: Resampling techniques for estimating confidence intervals without distribution assumptions\n",
    "- **Monte Carlo Methods**: Simulation-based inference for complex probabilistic models\n",
    "- **Hierarchical Models**: Multi-level statistical models for nested or grouped data structures\n",
    "- **Causal Inference**: Methods for determining cause-and-effect relationships beyond correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "In this final module, you've learned how to make data-driven decisions and quantify uncertainty in machine learning:\n",
    "\n",
    "1. **Probability Theory**: You discovered that probability distributions (normal, binomial, Poisson) model uncertainty in ML. They help you understand prediction confidence, model variability, and data characteristics. Probability is the language of uncertainty, essential for making informed decisions from model outputs.\n",
    "\n",
    "2. **Statistical Inference**: You learned to go beyond point estimates (single numbers) to understand ranges and confidence. Confidence intervals show the range of likely values (e.g., 95% CI means we're 95% confident the true value is in that range). Statistical tests (t-test, chi-square) help determine if differences between models are real or due to chance, guiding model selection decisions.\n",
    "\n",
    "3. **Sampling**: You understood that we can't use all data - we need representative samples. Random sampling ensures train/test splits are representative. Cross-validation uses multiple train/test splits for more reliable estimates. Bootstrap methods resample data to estimate confidence intervals. Proper sampling is crucial for reliable model evaluation.\n",
    "\n",
    "4. **Hypothesis Testing**: You learned the framework for validating model improvements: formulate null hypothesis (no difference), compute test statistic, calculate p-value (probability of observing data if null is true), and make decision (reject null if p < significance level, typically 0.05). This prevents claiming improvements that are just due to random variation.\n",
    "\n",
    "## üîó How This Connects to Other Modules\n",
    "\n",
    "**What you learned here integrates:**\n",
    "\n",
    "- **Module 01**: Data representation as vectors/matrices for probabilistic models\n",
    "- **Module 03**: Statistical foundations ‚Üí Extended to **statistical inference** (this module)\n",
    "- **Module 04**: Evaluating reduced-dimensional representations statistically\n",
    "\n",
    "> üí° **The Complete Journey**: \n",
    "> - Module 01: Data representation (the foundation)\n",
    "> - Module 02: Compute gradients (the tool for optimization)\n",
    "> - Module 03: Use gradients for optimization + statistical foundations (the method)\n",
    "> - Module 04: Reduce dimensions efficiently (the preprocessing)\n",
    "> - Module 05: Evaluate and make predictions with confidence (the validation) ‚Üê **You are here!**\n",
    "\n",
    "**You've now completed the full mathematical foundation for machine learning!**\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Complete the exercises in the `exercises/` folder\n",
    "2. Review the solutions if needed\n",
    "3. **Congratulations!** You've completed all 5 modules of Mathematics and Probabilities for ML\n",
    "4. Apply these concepts to real ML projects\n",
    "5. Explore advanced topics in probabilistic ML (Bayesian methods, MCMC, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for exercises?** Navigate to the `exercises/` folder and start practicing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> Incoming (Background Agent changes)
