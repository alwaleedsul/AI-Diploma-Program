<<<<<<< Current (Your changes)
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Function Approximation on Real-World ML Models\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Apply function approximation on real-world ML models\n",
    "- Understand power series and function approximations\n",
    "- Use Taylor series for function approximation\n",
    "- Apply approximations to optimize ML models\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of derivatives and calculus\n",
    "- âœ… Understanding of power series\n",
    "- âœ… Basic understanding of ML models\n",
    "- âœ… Python and NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 03, Unit 2**:\n",
    "- Applying function approximation on real-world ML models\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Function approximation** allows us to approximate complex functions using simpler forms (like power series), which is essential for optimization and understanding model behavior in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:48.070406Z",
     "iopub.status.busy": "2026-01-20T05:44:48.070356Z",
     "iopub.status.idle": "2026-01-20T05:44:48.072400Z",
     "shell.execute_reply": "2026-01-20T05:44:48.072236Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        from scipy.optimize import minimize\n",
    "        # from sklearn.linear_model import LinearRegression\n",
    "        # from sklearn.metrics import mean_squared_error\n",
    "        # print(\"âœ… Libraries imported!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nApplying Function Approximation on Real-World ML Models\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Taylor Series Approximation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:48.073173Z",
     "iopub.status.busy": "2026-01-20T05:44:48.073125Z",
     "iopub.status.idle": "2026-01-20T05:44:48.076245Z",
     "shell.execute_reply": "2026-01-20T05:44:48.076049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # print('=\" * 60)\")\n",
    "\n",
    "\n",
    "# # print(\"Part 1: Taylor Series Approximation\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "# # Taylor series: \n",
    "# # + f'(a)(x-a) + f''(a)(x-a)Â²/2! + ...\n",
    "\n",
    "# # def exp_taylor(x, a=0, n_terms=5):\n",
    "# #  \"\"\"Taylor series approximation of exp(x) around point a\"\"\"\n",
    "# #  result = 0x_minus_a = x - afor n in range(n_terms):\n",
    "#  # f^(n)(a) = exp(a) \n",
    "# # for all nresult += (\n",
    "# # np.exp(a) * (x_minus_a)**n) / np.math.factorial(n)\n",
    "# #  return resultdef sin_taylor(x, a=0, n_terms=5):\n",
    "# #  \"\"\"Taylor series approximation of sin(x) around point a\"\"\"\n",
    " \n",
    "# # = [np.sin(a), np.cos(a), -np.sin(a), -np.cos(a)]\n",
    "# #  result += (derivatives[n % 4] * (x_minus_a)**n) / np.math.factorial(n)\n",
    "# #  return result\n",
    "\n",
    "# # = np.linspace(-2, 2, 100)\n",
    "\n",
    "# # plt.figure(figsize=(14, 5))\n",
    "# # Exponential approximationplt.subplot(1, 2, 1)\n",
    "# # plt.plot(x, np.exp(x), 'b-', label='exp(x)', linewidth=2)\n",
    "\n",
    "# # for n in [1, 2, 3, 5, 10]:\n",
    "# #  y_approx = [exp_taylor(xi, a=0, n_terms=n) for xi in x]\n",
    "# #  plt.plot(x, y_approx, '--', label=f'Taylor (n={n})', alpha=0.7)\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "# # plt.title('Taylor Series Approximation of exp(x)')\n",
    "# plt.legend()\n",
    "# # plt.grid(True, alpha=0.3)\n",
    "# # plt.xlim(-2, 2)\n",
    "# # plt.ylim(-1, 8)\n",
    "\n",
    "# # Sine approximationplt.subplot(1, 2, 2)\n",
    "# # plt.plot(x, np.sin(x), 'r-', label='sin(x)', linewidth=2)\n",
    "\n",
    "# # for n in [1, 3, 5, 7, 9]:\n",
    "# #  y_approx = [sin_taylor(xi, a=0, n_terms=n) for xi in x]\n",
    "# # plt.title('Taylor Series Approximation of sin(x)')\n",
    "# # plt.ylim(-1.5, 1.5)\n",
    "\n",
    "# # plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nâœ… Taylor series approximations visualized!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Function Approximation in ML Loss Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:48.076994Z",
     "iopub.status.busy": "2026-01-20T05:44:48.076943Z",
     "iopub.status.idle": "2026-01-20T05:44:48.078764Z",
     "shell.execute_reply": "2026-01-20T05:44:48.078577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "# # print(\"Part 2: Function Approximation in ML Loss Functions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "# # Example: Approximate loss function around optimal point\n",
    "# # This helps understand optimization behavior\n",
    "\n",
    "# # Generate sample datanp.random.seed(42)\n",
    "# # X = np.random.randn(100, 1)\n",
    "# # y = 2 * X.flatten() + 1 + 0.5 * np.random.randn(100)\n",
    "\n",
    "# # = LinearRegression()\n",
    "# # model.fit(X, y)\n",
    "# # w_opt = model.coef_[0]\n",
    "# # b_opt = model.intercept_\n",
    "# # print(f\"\\nOptimal parameters: w={w_opt:.4f}, b={b_opt:.4f}\")\n",
    "\n",
    "# # Define loss functiondef loss_function(w, b):\n",
    "# #  y_pred = w * X.flatten() + breturn mean_squared_error(y, y_pred)\n",
    "\n",
    "# # = np.linspace(w_opt - 1, w_opt + 1, 50)\n",
    "# # b_range = np.linspace(b_opt - 1, b_opt + 1, 50)\n",
    "# # W, B = np.meshgrid(w_range, b_range)\n",
    "\n",
    "# # = np.zeros_like(W)\n",
    "\n",
    "# # for i in range(W.shape[0]):\n",
    "# #  for j in range(W.shape[1]):\n",
    "# #  loss_surface[i, j] = loss_function(W[i, j], B[i, j])\n",
    "\n",
    "# # Quadratic approximation (Taylor series up to 2nd order)\n",
    "# # L(w, b) â‰ˆ L(w_opt, b_opt) + gradient^T * [w-w_opt, b-b_opt] + 0.5 * [w-w_opt, b-b_opt]^T * Hessian * [w-w_opt, b-b_opt]\n",
    "\n",
    "# # Compute gradient numericallydef compute_gradient(w, b, \n",
    "# # h=1e-5):\n",
    "# #  grad_w = (loss_function(w + h, b) - loss_function(w - h, b)) / (2 * h)\n",
    "# #  grad_b = (loss_function(w, b + h) - loss_function(w, b - h)) / (2 * h)\n",
    "# #  return grad_w, grad_b\n",
    "\n",
    "# # Compute Hessian numericallydef compute_hessian(w, b, \n",
    "# # h=1e-3):\n",
    "# #  grad_w_plus, grad_b_plus = compute_gradient(w + h, b, h)\n",
    "# #  grad_w_minus, grad_b_minus = compute_gradient(w - h, b, h)\n",
    "# #  grad_w_w, grad_w_b = (grad_w_plus - grad_w_minus) / (2 * h), (grad_b_plus - grad_b_minus) / (2 * h)\n",
    " \n",
    "# #  _, grad_b_plus2 = compute_gradient(w, b + h, h)\n",
    "# #  _, grad_b_minus2 = compute_gradient(w, b - h, h)\n",
    "# #  grad_b_w = (grad_w_plus - grad_w_minus) / (2 * h)\n",
    "# #  grad_b_b = (grad_b_plus2 - grad_b_minus2) / (2 * h)\n",
    " \n",
    "# #  return np.array([[grad_w_w, grad_w_b], [grad_b_w, grad_b_b]])\n",
    "\n",
    "# # grad_w, grad_b = compute_gradient(w_opt, b_opt)\n",
    "# # hessian = compute_hessian(w_opt, b_opt)\n",
    "\n",
    "# # loss_opt = loss_function(w_opt, b_opt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nOptimal loss: {loss_opt:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"Gradient at optimum: [{grad_w:.6f}, {grad_b:.6f}]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"Hessian at optimum:\\n{hessian}\")\n",
    "\n",
    "# # Quadratic approximationdef quadratic_approximation(w, b):\n",
    "# #  dw = w - w_optdb = b - b_optdelta = np.array([dw, db])\n",
    "# #  return loss_opt + np.array([grad_w, grad_b]) @ delta + 0.5 * delta @ hessian @ delta\n",
    "\n",
    "\n",
    "# #  loss_approx[i, j] = quadratic_approximation(W[i, j], B[i, j])\n",
    "\n",
    "# # fig = plt.figure(figsize=(14, 5))\n",
    "# # ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "# # surf1 = ax1.plot_surface(W, B, loss_surface, cmap='viridis', alpha=0.8)\n",
    "# # ax1.scatter([w_opt], [b_opt], [loss_opt], c='r', s=100, marker='*')\n",
    "# # ax1.set_xlabel('w')\n",
    "# # ax1.set_ylabel('b')\n",
    "# # ax1.set_zlabel('Loss')\n",
    "# # ax1.set_title('True Loss Surface')\n",
    "\n",
    "# # ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "# # surf2 = ax2.plot_surface(W, B, loss_approx, cmap='plasma', alpha=0.8)\n",
    "# # ax2.scatter([w_opt], [b_opt], [loss_opt], c='r', s=100, marker='*')\n",
    "# # ax2.set_xlabel('w')\n",
    "# # ax2.set_ylabel('b')\n",
    "# # ax2.set_zlabel('Loss')\n",
    "# # ax2.set_title('Quadratic Approximation (Taylor)')\n",
    "\n",
    "# # plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nâœ… Function approximation applied to ML loss function!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Linear Approximation for Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:48.079622Z",
     "iopub.status.busy": "2026-01-20T05:44:48.079482Z",
     "iopub.status.idle": "2026-01-20T05:44:48.081186Z",
     "shell.execute_reply": "2026-01-20T05:44:48.081030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "# # print(\"Part 3: Linear Approximation for Optimization\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "# # Linear approximation is used in gradient descent\n",
    "# # + f'(x0)(x - x0)\n",
    "# # = x_old - learning_rate * f'(x_old)\n",
    "\n",
    "# # def gradient_descent_with_approximation(f, df, x0, learning_rate=0.1, n_iter=50):\n",
    "# #  \"\"\"Gradient descent using linear approximation\"\"\"\n",
    "# #  x = x0history = [x]\n",
    "# #  values = [f(x)]\n",
    " \n",
    "# #  for i in range(n_iter):\n",
    " \n",
    "# # Linear approximation: \n",
    "# + f'(x) * (x_new - x)\n",
    "\n",
    "# # = x - learning_rate * df(x)\n",
    "#  history.append(x)\n",
    "# #  values.append(f(x))\n",
    "# #  return x, history, values\n",
    "\n",
    "# # = (x - 3)^2def f_example(x):\n",
    "# #  return (x - 3)**2def df_example(x):\n",
    "# #  return 2 * (x - 3)\n",
    "\n",
    "# # x0 = 0.0x_opt, history, values = gradient_descent_with_approximation(f_example, df_example, x0, learning_rate=0.1, n_iter=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nStarting point: x0 = {x0}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"Optimal point: x* = {x_opt:.4f} (true optimum: 3.0)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"Final function value: f(x*) = {f_example(x_opt):.6f}\")\n",
    "\n",
    "# # = np.linspace(-1, 5, 100)\n",
    "# # y_range = [f_example(x) for x in x_range]\n",
    "\n",
    "# # plt.figure(figsize=(12, 5))\n",
    "# # plt.subplot(1, 2, 1)\n",
    "# # plt.plot(x_range, y_range, 'b-', label='f(x) = (x-3)Â²', linewidth=2)\n",
    "# # plt.plot(history, values, 'ro-', label='Gradient Descent', markersize=8)\n",
    "# # plt.axvline(x=3, color='g', linestyle='--', label='True Optimum', alpha=0.7)\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('f(x)')\n",
    "# # plt.title('Gradient Descent Using Linear Approximation')\n",
    "# plt.legend()\n",
    "# # plt.grid(True, alpha=0.3)\n",
    "\n",
    "# # Show linear approximations at each stepplt.subplot(1, 2, 2)\n",
    "# # plt.plot(x_range, y_range, 'b-', label='f(x)', linewidth=2, alpha=0.5)\n",
    "\n",
    "# # for i, x_i in enumerate(history[:5]):\n",
    " \n",
    "# # Linear approximation: \n",
    "# + f'(x_i)(x - x_i)\n",
    "# #  y_i = f_example(x_i)\n",
    "# #  df_i = df_example(x_i)\n",
    "# #  approx = [y_i + df_i * (x - x_i) for x in x_range]\n",
    "# #  plt.plot(x_range, approx, '--', alpha=0.7, label=f'Approx at x={x_i:.2f}')\n",
    "# # plt.title('Linear Approximations at Each Step')\n",
    "# # plt.xlim(-1, 5)\n",
    "\n",
    "# # plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nâœ… Linear approximation used for gradient descent optimization!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Taylor Series**: Approximate functions using power series\n",
    "2. **Linear Approximation**: f(x) â‰ˆ f(a) + f'(a)(x-a)\n",
    "3. **Quadratic Approximation**: Includes second-order terms (Hessian)\n",
    "4. **Optimization**: Use approximations to find optimal points\n",
    "5. **Gradient Descent**: Uses linear approximation to minimize functions\n",
    "\n",
    "### Best Practices:\n",
    "- Use Taylor series for local approximations\n",
    "- Verify approximations near the expansion point\n",
    "- Higher-order terms improve accuracy\n",
    "- Linear approximation sufficient for gradient descent\n",
    "\n",
    "### Applications:\n",
    "- Function optimization\n",
    "- Gradient descent algorithms\n",
    "- Understanding loss surfaces\n",
    "- Model behavior analysis\n",
    "\n",
    "**Reference:** Course 03, Unit 2: \"Calculus for Machine Learning\" - Function approximation practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
>>>>>>> Incoming (Background Agent changes)
