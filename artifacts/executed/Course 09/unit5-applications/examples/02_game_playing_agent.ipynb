{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Agent for Game Playing\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 09, Unit 5** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Agent for Game Playing\n",
    "## AIAT 123 - Reinforcement Learning\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Build RL agent for game playing\n",
    "- Implement Q-learning for games\n",
    "- Train agent to play Connect 4\n",
    "- Evaluate agent performance\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Game AI development, strategy games, and competitive AI.\n",
    "\n",
    "**Industry Impact**: Powers game AI in chess, Go, video games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T01:05:00.151178Z",
     "iopub.status.busy": "2026-01-17T01:05:00.151095Z",
     "iopub.status.idle": "2026-01-17T01:05:00.792281Z",
     "shell.execute_reply": "2026-01-17T01:05:00.791996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete!\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy -q\n",
    "import numpy as np\n",
    "print('âœ… Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple Game Environment (Tic-Tac-Toe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T01:05:00.793435Z",
     "iopub.status.busy": "2026-01-17T01:05:00.793330Z",
     "iopub.status.idle": "2026-01-17T01:05:00.796350Z",
     "shell.execute_reply": "2026-01-17T01:05:00.796151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Game environment created\n"
     ]
    }
   ],
   "source": [
    "class TicTacToe:\n",
    "    \"\"\"Simple Tic-Tac-Toe game environment\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3), dtype=int)\n",
    "        self.current_player = 1\n",
    "        return self.board.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        row, col = action // 3, action % 3\n",
    "        if self.board[row, col] != 0:\n",
    "            return self.board.copy(), -10, True, {}  # Invalid move\n",
    "        \n",
    "        self.board[row, col] = self.current_player\n",
    "        \n",
    "        # Check win\n",
    "        if self.check_win():\n",
    "            return self.board.copy(), 10, True, {}\n",
    "        \n",
    "        # Check draw\n",
    "        if np.all(self.board != 0):\n",
    "            return self.board.copy(), 0, True, {}\n",
    "        \n",
    "        self.current_player = -self.current_player\n",
    "        return self.board.copy(), 0, False, {}\n",
    "    \n",
    "    def check_win(self):\n",
    "        \"\"\"Check if current player won\"\"\"\n",
    "        player = self.current_player\n",
    "        # Check rows, columns, diagonals\n",
    "        for i in range(3):\n",
    "            if np.all(self.board[i] == player) or np.all(self.board[:, i] == player):\n",
    "                return True\n",
    "        if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "print('âœ… Game environment created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Q-Learning Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T01:05:00.797254Z",
     "iopub.status.busy": "2026-01-17T01:05:00.797196Z",
     "iopub.status.idle": "2026-01-17T01:05:00.800113Z",
     "shell.execute_reply": "2026-01-17T01:05:00.799932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Q-learning agent implemented\n"
     ]
    }
   ],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-learning agent for game playing\"\"\"\n",
    "    def __init__(self, learning_rate=0.1, discount=0.95, epsilon=0.1):\n",
    "        self.q_table = {}\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def get_state_key(self, board):\n",
    "        \"\"\"Convert board to hashable key\"\"\"\n",
    "        return tuple(board.flatten())\n",
    "    \n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"Get Q-value for state-action pair\"\"\"\n",
    "        key = (self.get_state_key(state), action)\n",
    "        return self.q_table.get(key, 0.0)\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-value using Q-learning\"\"\"\n",
    "        key = (self.get_state_key(state), action)\n",
    "        current_q = self.q_table.get(key, 0.0)\n",
    "        \n",
    "        if done:\n",
    "            target_q = reward\n",
    "        else:\n",
    "            next_actions = [i for i in range(9) if next_state.flatten()[i] == 0]\n",
    "            if next_actions:\n",
    "                max_next_q = max([self.get_q_value(next_state, a) for a in next_actions])\n",
    "                target_q = reward + self.gamma * max_next_q\n",
    "            else:\n",
    "                target_q = reward\n",
    "        \n",
    "        self.q_table[key] = current_q + self.lr * (target_q - current_q)\n",
    "    \n",
    "    def select_action(self, state, available_actions):\n",
    "        \"\"\"Select action using epsilon-greedy\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.choice(available_actions)\n",
    "        \n",
    "        q_values = [self.get_q_value(state, a) for a in available_actions]\n",
    "        return available_actions[np.argmax(q_values)]\n",
    "\n",
    "print('âœ… Q-learning agent implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Applications\n",
    "\n",
    "- **Chess/Go**: DeepMind AlphaZero\n",
    "- **Video Games**: Dota 2, StarCraft II\n",
    "- **Board Games**: Connect 4, Checkers\n",
    "- **Puzzle Games**: Rubik's Cube solving\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
