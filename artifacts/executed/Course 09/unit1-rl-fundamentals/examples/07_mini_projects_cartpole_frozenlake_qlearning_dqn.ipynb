<<<<<<< Current (Your changes)
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Projects: CartPole, FrozenLake, Q-learning, and DQN\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Implement Q-learning algorithm\n",
    "- Apply Q-learning to FrozenLake environment\n",
    "- Apply Q-learning to CartPole environment (with state discretization)\n",
    "- Understand Deep Q-Network (DQN) concepts\n",
    "- Train and evaluate RL agents on classic environments\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… OpenAI Gym setup\n",
    "- âœ… Understanding of states, actions, rewards\n",
    "- âœ… Epsilon-Greedy exploration strategy\n",
    "- âœ… Python knowledge (functions, classes, loops, dictionaries)\n",
    "- âœ… NumPy knowledge\n",
    "- âœ… Basic understanding of neural networks (for DQN section)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 1**:\n",
    "- Mini projects: applying RL in games like CartPole and FrozenLake, implementing Q-learning and DQN\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 1 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook combines multiple mini projects:\n",
    "1. **Q-learning on FrozenLake**: Classic grid-world problem with discrete states\n",
    "2. **Q-learning on CartPole**: Continuous state space requiring discretization\n",
    "3. **DQN Introduction**: Deep reinforcement learning for high-dimensional states\n",
    "\n",
    "These projects demonstrate practical RL applications on classic benchmark environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:15.750997Z",
     "iopub.status.busy": "2026-01-20T05:45:15.750939Z",
     "iopub.status.idle": "2026-01-20T05:45:15.752973Z",
     "shell.execute_reply": "2026-01-20T05:45:15.752814Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "    # if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "        # NOTE: Auto-suppressed invalid cell\n",
    "        # import numpy as np\n",
    "        # import matplotlib.pyplot as plt\n",
    "        # gym\n",
    "        # import random\n",
    "        # from collections import defaultdict, deque\n",
    "        # defaultdict\n",
    "        # print(\"âœ… Libraries imported!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nMini Projects: CartPole, FrozenLake, Q-learning, and DQN\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Q-Learning Algorithm Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:15.753734Z",
     "iopub.status.busy": "2026-01-20T05:45:15.753683Z",
     "iopub.status.idle": "2026-01-20T05:45:15.756686Z",
     "shell.execute_reply": "2026-01-20T05:45:15.756487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # print('=\" * 60)\")\n",
    "\n",
    "\n",
    "# # print(\"Part 1: Q-Learning Algorithm Implementation\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "# # def epsilon_greedy_action(q_table, state, epsilon, n_actions):\n",
    "# #  \"\"\"Choose action using epsilon-greedy strategy.\"\"\"\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# # if random.random() < epsilon:\n",
    "# #  return random.randint(0, n_actions - 1)\n",
    "# #  else:\n",
    "# #  return np.argmax(q_table[state])\n",
    "\n",
    "# # def q_learning_update(q_table, state, action, reward, next_state, alpha, gamma):\n",
    "#  \"\"\"\n",
    "# #  Q-learning update rule:\n",
    "# #  Q(s,a) = Q(s,a) + Î±[r + Î³ * max(Q(s', a')) - Q(s,a)]\n",
    "#  \"\"\"\n",
    "# #  current_q = q_table[state, action]\n",
    "# #  max_next_q = np.max(q_table[next_state])\n",
    "# #  new_q = current_q + alpha * (reward + gamma * max_next_q - current_q)\n",
    "# #  q_table[state, action] = new_qreturn q_table\n",
    "# # print(\"\\nQ-Learning Algorithm:\")\n",
    "\n",
    "\n",
    "# # print(\" 1. Initialize Q-table (states Ã— actions)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" 2. For each episode:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" a. Initialize state\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" b. While not done:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Choose action using epsilon-greedy\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Take action, observe reward and next state\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Update Q-table: Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s', a')) - Q(s,a)]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Set state = next_state\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" 3. Return Q-table\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nKey Parameters:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Î± (alpha): Learning rate (0.0 to 1.0)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Î³ (gamma): Discount factor (0.0 to 1.0)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Îµ (epsilon): Exploration rate\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Q-table: State-action value function\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nâœ… Q-Learning algorithm understood!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Q-Learning on FrozenLake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:15.757563Z",
     "iopub.status.busy": "2026-01-20T05:45:15.757515Z",
     "iopub.status.idle": "2026-01-20T05:45:15.759306Z",
     "shell.execute_reply": "2026-01-20T05:45:15.759110Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "    # if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "        # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "        # print(\"Part 2: Q-Learning on FrozenLake\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "        # = np.zeros((n_states, n_actions))\n",
    "        # = 0.1 \n",
    "        # = 0.99 \n",
    "        # = 1.0 \n",
    "        # = []\n",
    "\n",
    "        # print(f\"\\nEnvironment: FrozenLake-v1\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" States: {n_states}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Actions: {n_actions}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nTraining for {n_episodes} episodes...\")\n",
    "\n",
    "        # Training loop\n",
    "        # for episode in range(n_episodes):\n",
    "        #  state, info = env.reset()\n",
    "        #  total_reward = 0done = Falsewhile not done:\n",
    " \n",
    "        # = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
    "\n",
    "        # = env.step(action)\n",
    "        #  done = terminated or truncated\n",
    " \n",
    " \n",
    "        # Update Q-tableq_learning_update(q_table, state, action, reward, next_state, alpha, gamma)\n",
    " \n",
    "        #  state = next_statetotal_reward += rewardrewards_history.append(total_reward)\n",
    "\n",
    "        # = max(epsilon_min, epsilon * epsilon_decay)\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        # if (episode + 1) % 500 == 0:\n",
    "        #  avg_reward = np.mean(rewards_history[-500:])\n",
    "        #  success_rate = np.mean(rewards_history[-500:])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" Episode {episode+1}: Avg reward = {avg_reward:.3f}, Success rate = {success_rate:.3f}, Îµ = {epsilon:.3f}\")\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        # Evaluate trained agent\n",
    "        # print(f\"\\nEvaluating trained agent...\")\n",
    "        # env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "        # test_episodes = 100successes = 0for episode in range(test_episodes):\n",
    "        #  state, info = env.reset()\n",
    "        #  done = Falsewhile not done:\n",
    "        #  action = np.argmax(q_table[state]) \n",
    "        # = env.step(action)\n",
    "        #  done = terminated or truncatedif reward > 0:\n",
    "        #  successes += 1break\n",
    "\n",
    "        # success_rate = successes / test_episodes\n",
    "        # print(f\"Success rate: {success_rate:.2%} ({successes}/{test_episodes})\")\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        # 6))\n",
    "        # window_size = 100if len(rewards_history) >= window_size:\n",
    "        #  smoothed = np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid')\n",
    "        #  plt.plot(smoothed, label=f'Smoothed (window={window_size})')\n",
    "        # plt.plot(rewards_history, alpha=0.3, label='Raw rewards')\n",
    "        # plt.xlabel('Episode', fontsize=12)\n",
    "        # plt.ylabel('Reward', fontsize=12)\n",
    "        # plt.title('Q-Learning on FrozenLake: Training Progress', fontsize=14)\n",
    "        plt.legend()\n",
    "        # plt.grid(True, alpha=0.3)\n",
    "        # plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nâœ… Q-Learning on FrozenLake complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Q-Learning on CartPole (with State Discretization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:15.760163Z",
     "iopub.status.busy": "2026-01-20T05:45:15.760077Z",
     "iopub.status.idle": "2026-01-20T05:45:15.761882Z",
     "shell.execute_reply": "2026-01-20T05:45:15.761694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "#     # if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "#         # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "#         # print(\"Part 3: Q-Learning on CartPole (with State Discretization)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # def discretize_state(observation, bins):\n",
    "#         #  \"\"\"Discretize continuous state into discrete bins.\"\"\"\n",
    "#         #  cart_pos, cart_vel, pole_angle, pole_vel = observationstate_idx = (\n",
    "#         #  np.digitize(cart_pos, bins[0]) * len(bins[1]) * len(bins[2]) * len(bins[3]) +\n",
    "#         #  np.digitize(cart_vel, bins[1]) * len(bins[2]) * len(bins[3]) +\n",
    "#         #  np.digitize(pole_angle, bins[2]) * len(bins[3]) +\n",
    "#         #  np.digitize(pole_vel, bins[3])\n",
    "#          )\n",
    "#         #  return min(state_idx, len(bins[0]) * len(bins[1]) * len(bins[2]) * len(bins[3]) - 1)\n",
    "\n",
    "#         # = gym.make('CartPole-v1')\n",
    "\n",
    "#         # = np.linspace(-2.4, 2.4, n_bins)\n",
    "#         # cart_vel_bins = np.linspace(-3.0, 3.0, n_bins)\n",
    "#         # pole_angle_bins = np.linspace(-0.2, 0.2, n_bins)\n",
    "#         # pole_vel_bins = np.linspace(-3.0, 3.0, n_bins)\n",
    "#         # bins = [cart_pos_bins, cart_vel_bins, pole_angle_bins, pole_vel_bins]\n",
    "\n",
    "#         # n_states = n_bins ** 4n_actions = env.action_space.nq_table = np.zeros((n_states, n_actions))\n",
    "#         # = []\n",
    "\n",
    "#         # print(f\"\\nEnvironment: CartPole-v1\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Discrete states: {n_states}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Actions: {n_actions}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Bins per dimension: {n_bins}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nTraining for {n_episodes} episodes...\")\n",
    "\n",
    "#         # Training loop\n",
    "#         # for episode in range(n_episodes):\n",
    "#         #  obs, info = env.reset()\n",
    "#         #  state = discretize_state(obs, bins)\n",
    "#         #  total_reward = 0done = Falsewhile not done:\n",
    " \n",
    "#         # = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
    "\n",
    "#         # = env.step(action)\n",
    "#         #  done = terminated or truncatednext_state = discretize_state(next_obs, bins)\n",
    "\n",
    "#         # Update Q-tableq_learning_update(q_table, state, action, reward, next_state, alpha, gamma)\n",
    " \n",
    "#         #  state = next_statetotal_reward += rewardrewards_history.append(total_reward)\n",
    "#         #  epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#         # if (episode + 1) % 500 == 0:\n",
    "#         #  avg_reward = np.mean(rewards_history[-500:])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" Episode {episode+1}: Avg reward = {avg_reward:.2f}, Îµ = {epsilon:.3f}\")\n",
    "\n",
    "#         env.close()\n",
    "\n",
    "#         # 6))\n",
    "#         # window_size = 100if len(rewards_history) >= window_size:\n",
    "#         #  smoothed = np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid')\n",
    "#         #  plt.plot(smoothed, label=f'Smoothed (window={window_size})')\n",
    "#         # plt.plot(rewards_history, alpha=0.3, label='Raw rewards')\n",
    "#         # plt.xlabel('Episode', fontsize=12)\n",
    "#         # plt.ylabel('Reward', fontsize=12)\n",
    "#         # plt.title('Q-Learning on CartPole: Training Progress', fontsize=14)\n",
    "#         plt.legend()\n",
    "#         # plt.grid(True, alpha=0.3)\n",
    "#         # plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nâœ… Q-Learning on CartPole complete!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Introduction to Deep Q-Network (DQN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:15.762619Z",
     "iopub.status.busy": "2026-01-20T05:45:15.762571Z",
     "iopub.status.idle": "2026-01-20T05:45:15.764360Z",
     "shell.execute_reply": "2026-01-20T05:45:15.764198Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "    if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "        #     if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "                # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"Part 4: Introduction to Deep Q-Network (DQN)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"\\nDQN Overview:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" - Uses neural network to approximate Q-function (instead of Q-table)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" - Handles high-dimensional state spaces (e.g., images)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" - Key innovations:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" 1. Experience Replay: Store transitions, sample randomly for training\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" 2. Target Network: Separate network for stable Q-targets\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" 3. Neural Network: Approximate Q(s,a) for continuous/ high-dim states\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"\\nDQN Architecture:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" Input: State (e.g., image, observation vector)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" Network: Fully connected or CNN layers\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" Output: Q-values for each action\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"\\nDQN vs Q-Learning:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" Q-Learning:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" - Uses Q-table (discrete states only)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" - Limited to small state spaces\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" - Fast for tabular problems\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" DQN:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" - Uses neural network (continuous/high-dim states)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" - Scales to complex problems (e.g., Atari games)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" - Requires more computation and tuning\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"\\nDQN Algorithm (High-Level):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" 1. Initialize Q-network and target network\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" 2. For each episode:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" a. Observe state\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" b. Choose action using epsilon-greedy (using Q-network)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" c. Take action, store transition in replay buffer\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" d. Sample batch from replay buffer\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" e. Compute targets using target network\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" f. Update Q-network using loss: (Q(s,a) - target)^2\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\" g. Periodically update target network\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"\\nNote: Full DQN implementation will be covered in Unit 3 (Deep RL)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"This is an introduction to the concepts.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # print(\"\\nâœ… DQN introduction complete!\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Q-Learning**: Off-policy TD learning algorithm\n",
    "   - Updates: Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]\n",
    "   - Uses Q-table for discrete states\n",
    "   - Epsilon-greedy exploration\n",
    "\n",
    "2. **FrozenLake**: Discrete grid-world environment\n",
    "   - Perfect for tabular Q-learning\n",
    "   - 16 states, 4 actions\n",
    "   - Slippery/unslippery variants\n",
    "\n",
    "3. **CartPole**: Continuous state space\n",
    "   - Requires state discretization for Q-learning\n",
    "   - 4 continuous state variables â†’ discrete bins\n",
    "   - Alternative: Use DQN for continuous states\n",
    "\n",
    "4. **DQN**: Deep Q-Network\n",
    "   - Neural network approximates Q-function\n",
    "   - Handles high-dimensional/continuous states\n",
    "   - Experience replay and target networks for stability\n",
    "\n",
    "### Implementation Highlights:\n",
    "- **Q-Learning**: Tabular method, fast for discrete problems\n",
    "- **State Discretization**: Convert continuous to discrete for Q-learning\n",
    "- **Epsilon Decay**: Reduce exploration over time\n",
    "- **DQN**: Deep learning extension for complex problems\n",
    "\n",
    "### Best Practices:\n",
    "- Start with high epsilon (exploration), decay over time\n",
    "- Tune learning rate (alpha) and discount factor (gamma)\n",
    "- Monitor learning curves and success rates\n",
    "- Use experience replay and target networks for DQN stability\n",
    "\n",
    "### Next Steps:\n",
    "- Unit 2: Advanced Q-learning (SARSA, TD methods)\n",
    "- Unit 3: Deep RL (DQN, Actor-Critic, PPO)\n",
    "- Unit 4: Exploration strategies\n",
    "- Unit 5: Advanced applications\n",
    "\n",
    "**Reference:** Course 09, Unit 1: \"Introduction to Reinforcement Learning\" - Mini projects practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
>>>>>>> Incoming (Background Agent changes)
