<<<<<<< Current (Your changes)
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving RL Problems: Defining States, Actions, and Rewards\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Define state spaces for RL problems\n",
    "- Define action spaces for RL problems\n",
    "- Design reward functions\n",
    "- Run RL simulations\n",
    "- Apply these concepts to real environments\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… OpenAI Gym setup\n",
    "- âœ… Understanding of RL basics (agent, environment, episode)\n",
    "- âœ… Python knowledge (functions, classes, loops)\n",
    "- âœ… NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 1**:\n",
    "- Solving RL problems: defining states, actions, and rewards, running RL simulations\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 1 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Solving RL problems** requires carefully defining:\n",
    "1. **States**: What the agent observes from the environment\n",
    "2. **Actions**: What the agent can do\n",
    "3. **Rewards**: Feedback signals that guide learning\n",
    "\n",
    "Proper definition of these components is crucial for successful RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:14.806258Z",
     "iopub.status.busy": "2026-01-20T05:45:14.804571Z",
     "iopub.status.idle": "2026-01-20T05:45:15.126620Z",
     "shell.execute_reply": "2026-01-20T05:45:15.126254Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# gym\n",
    "# print(\"âœ… Libraries imported!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nSolving RL Problems: States, Actions, and Rewards\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining State Spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:15.128619Z",
     "iopub.status.busy": "2026-01-20T05:45:15.128360Z",
     "iopub.status.idle": "2026-01-20T05:45:15.131435Z",
     "shell.execute_reply": "2026-01-20T05:45:15.131119Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "    # if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "        # print('=\" * 60)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Part 1: Defining State Spaces\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "        # = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nCartPole-v1 Environment:\")\n",
    "        # observation, info = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" State/Observation: {observation}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" State shape: {observation.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" State type: {type(observation)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" State space: {env.observation_space}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nState Components (CartPole):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" [0] Cart position\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" [1] Cart velocity\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" [2] Pole angle\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" [3] Pole angular velocity\")\n",
    "\n",
    "\n",
    "        # obs_fl, info_fl = env_fl.reset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nFrozenLake-v1 Environment:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" State/Observation: {obs_fl}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" State space: {env_fl.observation_space}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" State type: Discrete (single integer representing grid position)\")\n",
    "\n",
    "        env.close()\n",
    "        env_fl.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nâœ… State spaces defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Defining Action Spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:15.133165Z",
     "iopub.status.busy": "2026-01-20T05:45:15.133032Z",
     "iopub.status.idle": "2026-01-20T05:45:15.135578Z",
     "shell.execute_reply": "2026-01-20T05:45:15.135241Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "    # if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Part 2: Defining Action Spaces\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "        # = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nCartPole-v1 Actions:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Action space: {env.action_space}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Number of actions: {env.action_space.n}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" Actions:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" 0: Push cart to the left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" 1: Push cart to the right\")\n",
    "\n",
    "        # = gym.make('FrozenLake-v1')\n",
    "        # obs_fl, info_fl = env_fl.reset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nFrozenLake-v1 Actions:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Action space: {env_fl.action_space}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Number of actions: {env_fl.action_space.n}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" Actions:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" 0: Move left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" 1: Move down\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" 2: Move right\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" 3: Move up\")\n",
    "\n",
    "        env.close()\n",
    "        env_fl.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nâœ… Action spaces defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Designing Reward Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:15.137124Z",
     "iopub.status.busy": "2026-01-20T05:45:15.136988Z",
     "iopub.status.idle": "2026-01-20T05:45:15.139695Z",
     "shell.execute_reply": "2026-01-20T05:45:15.139401Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "    # if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Part 3: Designing Reward Functions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nReward Function Design Principles:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" 1. Provide clear feedback (positive for good, negative for bad)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" 2. Shape rewards to guide learning (sparse vs dense)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" 3. Balance immediate vs long-term rewards\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" 4. Avoid reward hacking (unintended behaviors)\")\n",
    "\n",
    "        # = gym.make('CartPole-v1')\n",
    "        # obs, info = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nCartPole-v1 Reward Function:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" +1 for each step the pole remains balanced\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" Episode ends when pole falls or cart goes out of bounds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" Maximum reward: 500 (episode length limit)\")\n",
    "\n",
    "        # total_reward = 0for step in range(10):\n",
    "        #  action = env.action_space.sample()\n",
    "        #  obs, reward, terminated, truncated, info = env.step(action)\n",
    "        #  done = terminated or truncatedtotal_reward += reward\n",
    "        # print(f\" Step {step+1}: Reward = {reward}, Total = {total_reward}\")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        # if done:\n",
    "         breakenv.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nâœ… Reward functions understood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Running RL Simulations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:15.141144Z",
     "iopub.status.busy": "2026-01-20T05:45:15.141010Z",
     "iopub.status.idle": "2026-01-20T05:45:15.144207Z",
     "shell.execute_reply": "2026-01-20T05:45:15.143804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "#     # if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"Part 4: Running RL Simulations\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # def run_random_episode(env, max_steps=100):\n",
    "#         #  \"\"\"Run a single episode with random actions.\"\"\"\n",
    "#         #  obs, info = env.reset()\n",
    "#         #  total_reward = 0steps = 0for step in range(max_steps):\n",
    "#         #  action = env.action_space.sample()\n",
    "#         #  obs, reward, terminated, truncated, info = env.step(action)\n",
    "#         #  done = terminated or truncatedtotal_reward += rewardsteps += 1if done:\n",
    "#         #  breakreturn total_reward, steps\n",
    "\n",
    "#         # = gym.make('CartPole-v1')\n",
    "#         # n_episodes = 10print(f\"\\nRunning {n_episodes} random episodes in CartPole-v1:\")\n",
    "#         # rewards = []\n",
    "#         # steps_list = []\n",
    "\n",
    "#         # for episode in range(n_episodes):\n",
    "#         #  reward, steps = run_random_episode(env)\n",
    "#          rewards.append(reward)\n",
    "#          steps_list.append(steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" Episode {episode+1}: Reward = {reward:.1f}, Steps = {steps}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nStatistics:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Average reward: {np.mean(rewards):.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Average steps: {np.mean(steps_list):.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Best reward: {np.max(rewards):.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Worst reward: {np.min(rewards):.2f}\")\n",
    "\n",
    "#         env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nâœ… RL simulations complete!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Components:\n",
    "1. **States**: What the agent observes (observation space)\n",
    "   - Continuous states (CartPole: position, velocity, angle)\n",
    "   - Discrete states (FrozenLake: grid position)\n",
    "2. **Actions**: What the agent can do (action space)\n",
    "   - Discrete actions (CartPole: left/right, FrozenLake: up/down/left/right)\n",
    "   - Continuous actions (MountainCar: acceleration)\n",
    "3. **Rewards**: Feedback signals\n",
    "   - Dense rewards (CartPole: +1 per step)\n",
    "   - Sparse rewards (FrozenLake: +1 only at goal)\n",
    "\n",
    "### Design Principles:\n",
    "- **States**: Include all relevant information for decision-making\n",
    "- **Actions**: Cover all possible behaviors the agent can take\n",
    "- **Rewards**: Provide clear feedback, shape learning, avoid hacking\n",
    "\n",
    "### Simulation Process:\n",
    "1. Reset environment (get initial state)\n",
    "2. Loop: Select action â†’ Step environment â†’ Receive reward â†’ Update state\n",
    "3. Repeat until episode ends\n",
    "4. Reset and repeat for multiple episodes\n",
    "\n",
    "### Next Steps:\n",
    "- Implement learning algorithms (Q-learning, policy gradients)\n",
    "- Train agents to maximize rewards\n",
    "- Evaluate performance across episodes\n",
    "\n",
    "**Reference:** Course 09, Unit 1: \"Introduction to Reinforcement Learning\" - Solving RL problems practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
>>>>>>> Incoming (Background Agent changes)
