<<<<<<< Current (Your changes)
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(0) and n-Step TD Algorithms\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand Temporal Difference (TD) learning\n",
    "- Implement TD(0) algorithm\n",
    "- Implement n-step TD algorithms\n",
    "- Compare TD with Monte Carlo methods\n",
    "- Apply TD algorithms to RL environments\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of value functions\n",
    "- âœ… Monte Carlo methods knowledge\n",
    "- âœ… Python knowledge (functions, loops, NumPy)\n",
    "- âœ… Understanding of bootstrapping\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 2**:\n",
    "- Running TD(0) and n-step TD algorithms in simple RL environments\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Temporal Difference (TD) learning** combines ideas from Monte Carlo (learning from experience) and Dynamic Programming (bootstrapping). TD methods update estimates based on other estimates, making them more sample-efficient than Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:18.744940Z",
     "iopub.status.busy": "2026-01-20T05:45:18.744893Z",
     "iopub.status.idle": "2026-01-20T05:45:18.876259Z",
     "shell.execute_reply": "2026-01-20T05:45:18.876050Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from collections import defaultdict\n",
    "# print(\"âœ… Libraries imported!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nTD(0) and n-Step TD Algorithms\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding TD Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:18.877198Z",
     "iopub.status.busy": "2026-01-20T05:45:18.877123Z",
     "iopub.status.idle": "2026-01-20T05:45:18.878701Z",
     "shell.execute_reply": "2026-01-20T05:45:18.878513Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Part 1: Understanding TD Learning\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nTD Learning Key Concepts:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" 1. Bootstrap: Update estimate using other estimates\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" 2. Incremental: Update after each step (no need to wait for episode end)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" 3. TD Error: Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" 4. Update: V(S_t) = V(S_t) + Î±[Î´_t]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nTD(0) Algorithm:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" V(S_t) â† V(S_t) + Î±[R_{t+1} + Î³V(S_{t+1}) - V(S_t)]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Uses 1-step return: R_{t+1} + Î³V(S_{t+1})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Updates after each step\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nn-Step TD Algorithm:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Uses n-step return: R_{t+1} + Î³R_{t+2} + ... + Î³^n V(S_{t+n})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Balances bias and variance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - n=1: TD(0), n=âˆž: Monte Carlo\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nâœ… TD learning concepts understood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: TD(0) Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:18.879396Z",
     "iopub.status.busy": "2026-01-20T05:45:18.879333Z",
     "iopub.status.idle": "2026-01-20T05:45:18.881028Z",
     "shell.execute_reply": "2026-01-20T05:45:18.880849Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "    # if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Part 2: TD(0) Implementation\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "        # def td0(policy, env_simulator, n_episodes=100, alpha=0.1, gamma=0.99):\n",
    "         \"\"\"\n",
    "        #  TD(0) for estimating state values.\n",
    "        #  V(S_t) â† V(S_t) + Î±[R_{t+1} + Î³V(S_{t+1}) - V(S_t)]\n",
    "         \"\"\"\n",
    "        #  V = defaultdict(float)\n",
    " \n",
    "        #  for episode in range(n_episodes):\n",
    "        #  state = env_simulator.reset()\n",
    "        #  done = Falsewhile not done:\n",
    " \n",
    "        # Choose action (simplified)\n",
    "        #  action = policy(state) \n",
    "\n",
    "\n",
    "\n",
    "        # if callable(policy) else policy.get(state, 0)\n",
    "\n",
    "        # Take step (simplified - assuming env interface)\n",
    "        #  next_state, reward, done = env_simulator.step(action)\n",
    "\n",
    "        # = reward + gamma * V[next_state]\n",
    "        #  td_error = td_target - V[state]\n",
    "        #  V[state] = V[state] + alpha * td_errorstate = next_statereturn V\n",
    "\n",
    "        # Simple exampleclass SimpleEnv:\n",
    "        #  def __init__(self):\n",
    "        #  self.state = 1self.n_states = 5def reset(self):\n",
    "        #  self.state = 1return self.statedef step(self, action):\n",
    " \n",
    "        # Simple transition: move towards goal (state 4)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        # if self.state < 4:\n",
    "        #  self.state += 1reward = 1.0 \n",
    "\n",
    "\n",
    "\n",
    "        # if self.state == 4 else 0.0done = self.state == 4return self.state, reward, doneenv = SimpleEnv()\n",
    "\n",
    "        # def simple_policy(state):\n",
    "        #  return 1 \n",
    "        # Always move \n",
    "        # forwardV_td0 = td0(simple_policy, env, n_episodes=100, alpha=0.1, gamma=1.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nTD(0) Estimated State Values:\")\n",
    "\n",
    "        # for state in sorted(V_td0.keys()):\n",
    "        #  print(f\" V({state}) = {V_td0[state]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nâœ… TD(0) implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: n-Step TD Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:45:18.881870Z",
     "iopub.status.busy": "2026-01-20T05:45:18.881805Z",
     "iopub.status.idle": "2026-01-20T05:45:18.883543Z",
     "shell.execute_reply": "2026-01-20T05:45:18.883378Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "    # if 'GYM_AVAILABLE' in globals() and GYM_AVAILABLE:\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Part 3: n-Step TD Implementation\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "        # def n_step_td(policy, env_simulator, n_episodes=100, n=2, alpha=0.1, gamma=0.99):\n",
    "         \"\"\"\n",
    "        #  n-step TD for estimating state values.\n",
    "        #  Uses n-step return: R_{t+1} + Î³R_{t+2} + ... + Î³^n V(S_{t+n})\n",
    "         \"\"\"\n",
    "        #  V = defaultdict(float)\n",
    " \n",
    "        #  for episode in range(n_episodes):\n",
    "        #  states = [env_simulator.reset()]\n",
    "        #  rewards = []\n",
    "        #  t = 0T = float('inf')\n",
    " \n",
    "        #  while True:\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        # if t < T:\n",
    " \n",
    "        # = policy(states[-1]) \n",
    "\n",
    "\n",
    "\n",
    "        # if callable(policy) else policy.get(states[-1], 0)\n",
    "        #  next_state, reward, done = env_simulator.step(action)\n",
    " \n",
    "         states.append(next_state)\n",
    "         rewards.append(reward)\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        # if done:\n",
    "        #  T = t + 1\n",
    " \n",
    " \n",
    "        # = t - n + 1\n",
    "\n",
    "\n",
    "        # if tau >= 0:\n",
    " \n",
    "        # Calculate n-step returnG = sum(gamma ** i * rewards[tau + i] \n",
    "        # for i in range(min(n, T - tau)))\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        # if tau + n < T:\n",
    "        #  G += gamma ** n * V[states[tau + n]]\n",
    " \n",
    " \n",
    "        # Update valueV[states[tau]] = V[states[tau]] + alpha * (G - V[states[tau]])\n",
    " \n",
    "        #  t += 1if tau == T - 1:\n",
    "        #  break\n",
    " \n",
    " \n",
    "        # Keep only last n states/rewards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if len(states) > n + 1:\n",
    "        #  states.pop(0)\n",
    "        #  rewards.pop(0)\n",
    " \n",
    "        #  return V\n",
    "\n",
    "        # Compare n-step TD \n",
    "        # for different n valuesenv2 = SimpleEnv()\n",
    "        # V_n1 = n_step_td(simple_policy, env2, n_episodes=100, n=1, alpha=0.1, gamma=1.0)\n",
    "        # env3 = SimpleEnv()\n",
    "        # V_n2 = n_step_td(simple_policy, env3, n_episodes=100, n=2, alpha=0.1, gamma=1.0)\n",
    "        # env4 = SimpleEnv()\n",
    "        # V_n4 = n_step_td(simple_policy, env4, n_episodes=100, n=4, alpha=0.1, gamma=1.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nn-Step TD Estimated State Values:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"n=1 (TD(0):\")\n",
    "\n",
    "        # for state in sorted(V_n1.keys()):\n",
    "        #  print(f\" V({state}) = {V_n1[state]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nn=2:\")\n",
    "\n",
    "        # for state in sorted(V_n2.keys()):\n",
    "        #  print(f\" V({state}) = {V_n2[state]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nn=4:\")\n",
    "\n",
    "        # for state in sorted(V_n4.keys()):\n",
    "        #  print(f\" V({state}) = {V_n4[state]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nâœ… n-step TD implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **TD Learning**: Combines Monte Carlo (experience) and DP (bootstrapping)\n",
    "2. **TD(0)**: 1-step TD, updates: V(S_t) â† V(S_t) + Î±[R + Î³V(S_{t+1}) - V(S_t)]\n",
    "3. **n-Step TD**: Uses n-step returns, balances bias and variance\n",
    "4. **Bootstrapping**: Update using other estimates (faster but biased)\n",
    "\n",
    "### Comparison:\n",
    "- **Monte Carlo**: High variance, no bias, requires episodes\n",
    "- **TD(0)**: Low variance, some bias, online (incremental)\n",
    "- **n-Step TD**: Trade-off between MC and TD(0)\n",
    "\n",
    "### Advantages:\n",
    "- Online learning (no need to wait for episode end)\n",
    "- Lower variance than Monte Carlo\n",
    "- More sample-efficient\n",
    "- Works for continuing tasks\n",
    "\n",
    "### Applications:\n",
    "- Value function estimation\n",
    "- Policy evaluation\n",
    "- Online learning scenarios\n",
    "\n",
    "### Next Steps:\n",
    "- SARSA and Q-learning (TD control)\n",
    "- Eligibility traces (TD(Î»))\n",
    "- Compare with Monte Carlo and DP\n",
    "\n",
    "**Reference:** Course 09, Unit 2: \"Prediction and Control without a Model\" - TD algorithms practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
>>>>>>> Incoming (Background Agent changes)
