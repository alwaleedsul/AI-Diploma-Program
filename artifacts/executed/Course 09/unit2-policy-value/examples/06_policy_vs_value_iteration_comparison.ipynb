{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Policy Iteration vs Value Iteration\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand Policy Iteration algorithm\n",
    "- Understand Value Iteration algorithm\n",
    "- Compare Policy Iteration vs Value Iteration\n",
    "- Implement both algorithms\n",
    "- Analyze convergence and computational efficiency\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Understanding of MDPs (states, actions, rewards, transitions)\n",
    "- ‚úÖ Understanding of Bellman equations\n",
    "- ‚úÖ Understanding of policies and value functions\n",
    "- ‚úÖ Python knowledge (functions, loops, NumPy)\n",
    "- ‚úÖ Dynamic Programming basics\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 2**:\n",
    "- Comparing policy iteration vs value iteration through code-based experiments\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Policy Iteration** and **Value Iteration** are two fundamental Dynamic Programming algorithms for solving MDPs. Both find optimal policies, but use different strategies and have different computational properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T00:02:19.448577Z",
     "iopub.status.busy": "2026-01-17T00:02:19.448354Z",
     "iopub.status.idle": "2026-01-17T00:02:19.605488Z",
     "shell.execute_reply": "2026-01-17T00:02:19.605267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported!\n",
      "\n",
      "Comparing Policy Iteration vs Value Iteration\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n",
    "print(\"\\nComparing Policy Iteration vs Value Iteration\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T00:02:19.606445Z",
     "iopub.status.busy": "2026-01-17T00:02:19.606345Z",
     "iopub.status.idle": "2026-01-17T00:02:19.608494Z",
     "shell.execute_reply": "2026-01-17T00:02:19.608308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Part 1: Understanding the Algorithms\n",
      "============================================================\n",
      "\n",
      "Policy Iteration:\n",
      "  1. Policy Evaluation: Compute V^œÄ(s) for current policy œÄ\n",
      "  2. Policy Improvement: Update policy to be greedy w.r.t. V^œÄ\n",
      "  3. Repeat until policy no longer changes\n",
      "  - Alternates between evaluation and improvement\n",
      "  - Typically converges in few iterations\n",
      "\n",
      "Value Iteration:\n",
      "  1. Initialize V(s) = 0 for all states\n",
      "  2. Update: V(s) ‚Üê max_a Œ£ P(s'|s,a)[R + Œ≥V(s')]\n",
      "  3. Repeat until convergence\n",
      "  4. Extract optimal policy from V*\n",
      "  - Combines evaluation and improvement in each step\n",
      "  - May require more iterations but each is faster\n",
      "\n",
      "Key Differences:\n",
      "  - Policy Iteration: Two-phase (evaluate then improve)\n",
      "  - Value Iteration: Single-phase (combine evaluate + improve)\n",
      "  - Policy Iteration: Fewer iterations, more computation per iteration\n",
      "  - Value Iteration: More iterations, less computation per iteration\n",
      "\n",
      "‚úÖ Algorithm concepts understood!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Understanding the Algorithms\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nPolicy Iteration:\")\n",
    "print(\"  1. Policy Evaluation: Compute V^œÄ(s) for current policy œÄ\")\n",
    "print(\"  2. Policy Improvement: Update policy to be greedy w.r.t. V^œÄ\")\n",
    "print(\"  3. Repeat until policy no longer changes\")\n",
    "print(\"  - Alternates between evaluation and improvement\")\n",
    "print(\"  - Typically converges in few iterations\")\n",
    "\n",
    "print(\"\\nValue Iteration:\")\n",
    "print(\"  1. Initialize V(s) = 0 for all states\")\n",
    "print(\"  2. Update: V(s) ‚Üê max_a Œ£ P(s'|s,a)[R + Œ≥V(s')]\")\n",
    "print(\"  3. Repeat until convergence\")\n",
    "print(\"  4. Extract optimal policy from V*\")\n",
    "print(\"  - Combines evaluation and improvement in each step\")\n",
    "print(\"  - May require more iterations but each is faster\")\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"  - Policy Iteration: Two-phase (evaluate then improve)\")\n",
    "print(\"  - Value Iteration: Single-phase (combine evaluate + improve)\")\n",
    "print(\"  - Policy Iteration: Fewer iterations, more computation per iteration\")\n",
    "print(\"  - Value Iteration: More iterations, less computation per iteration\")\n",
    "\n",
    "print(\"\\n‚úÖ Algorithm concepts understood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Policy Iteration Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T00:02:19.609334Z",
     "iopub.status.busy": "2026-01-17T00:02:19.609263Z",
     "iopub.status.idle": "2026-01-17T00:02:19.612253Z",
     "shell.execute_reply": "2026-01-17T00:02:19.612101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Part 2: Policy Iteration Implementation\n",
      "============================================================\n",
      "\n",
      "‚úÖ Policy Iteration implemented!\n",
      "  Algorithm: Evaluate ‚Üí Improve ‚Üí Repeat until stable\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Policy Iteration Implementation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def policy_evaluation(policy, P, R, gamma=0.99, theta=1e-6, max_iterations=100):\n",
    "    \"\"\"Evaluate a policy by computing its value function.\"\"\"\n",
    "    n_states = len(policy)\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            a = policy[s]\n",
    "            # V(s) = Œ£ P(s'|s,a)[R(s,a,s') + Œ≥V(s')]\n",
    "            V_new[s] = sum(P[s][a][s_next] * (R[s][a][s_next] + gamma * V[s_next]) \n",
    "                           for s_next in range(n_states))\n",
    "        \n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "    \n",
    "    return V\n",
    "\n",
    "def policy_improvement(V, P, R, gamma=0.99):\n",
    "    \"\"\"Improve policy to be greedy w.r.t. current value function.\"\"\"\n",
    "    n_states, n_actions = len(V), len(P[0])\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        # Choose action that maximizes: Œ£ P(s'|s,a)[R + Œ≥V(s')]\n",
    "        action_values = [sum(P[s][a][s_next] * (R[s][a][s_next] + gamma * V[s_next])\n",
    "                            for s_next in range(n_states))\n",
    "                        for a in range(n_actions)]\n",
    "        policy[s] = np.argmax(action_values)\n",
    "    \n",
    "    return policy\n",
    "\n",
    "def policy_iteration(P, R, gamma=0.99, theta=1e-6):\n",
    "    \"\"\"Policy Iteration algorithm.\"\"\"\n",
    "    n_states, n_actions = len(P), len(P[0])\n",
    "    policy = np.random.randint(0, n_actions, n_states)  # Random initial policy\n",
    "    \n",
    "    iterations = 0\n",
    "    policy_stable = False\n",
    "    \n",
    "    while not policy_stable:\n",
    "        # Policy evaluation\n",
    "        V = policy_evaluation(policy, P, R, gamma, theta)\n",
    "        \n",
    "        # Policy improvement\n",
    "        policy_new = policy_improvement(V, P, R, gamma)\n",
    "        \n",
    "        # Check if policy changed\n",
    "        policy_stable = np.array_equal(policy, policy_new)\n",
    "        policy = policy_new\n",
    "        iterations += 1\n",
    "    \n",
    "    return policy, V, iterations\n",
    "\n",
    "print(\"\\n‚úÖ Policy Iteration implemented!\")\n",
    "print(\"  Algorithm: Evaluate ‚Üí Improve ‚Üí Repeat until stable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Value Iteration Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T00:02:19.613104Z",
     "iopub.status.busy": "2026-01-17T00:02:19.613045Z",
     "iopub.status.idle": "2026-01-17T00:02:19.615261Z",
     "shell.execute_reply": "2026-01-17T00:02:19.615095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Part 3: Value Iteration Implementation\n",
      "============================================================\n",
      "\n",
      "‚úÖ Value Iteration implemented!\n",
      "  Algorithm: Update V(s) ‚Üê max_a[...] until convergence, then extract policy\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: Value Iteration Implementation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def value_iteration(P, R, gamma=0.99, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"Value Iteration algorithm.\"\"\"\n",
    "    n_states, n_actions = len(P), len(P[0])\n",
    "    V = np.zeros(n_states)\n",
    "    \n",
    "    iterations = 0\n",
    "    for _ in range(max_iterations):\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            # V(s) ‚Üê max_a Œ£ P(s'|s,a)[R + Œ≥V(s')]\n",
    "            action_values = [sum(P[s][a][s_next] * (R[s][a][s_next] + gamma * V[s_next])\n",
    "                            for s_next in range(n_states))\n",
    "                           for a in range(n_actions)]\n",
    "            V_new[s] = max(action_values)\n",
    "        \n",
    "        if np.max(np.abs(V_new - V)) < theta:\n",
    "            break\n",
    "        V = V_new\n",
    "        iterations += 1\n",
    "    \n",
    "    # Extract optimal policy\n",
    "    policy = policy_improvement(V, P, R, gamma)\n",
    "    \n",
    "    return policy, V, iterations\n",
    "\n",
    "print(\"\\n‚úÖ Value Iteration implemented!\")\n",
    "print(\"  Algorithm: Update V(s) ‚Üê max_a[...] until convergence, then extract policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Comparison and Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T00:02:19.616126Z",
     "iopub.status.busy": "2026-01-17T00:02:19.616051Z",
     "iopub.status.idle": "2026-01-17T00:02:19.618149Z",
     "shell.execute_reply": "2026-01-17T00:02:19.617975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Part 4: Comparison and Analysis\n",
      "============================================================\n",
      "\n",
      "Example: Simple MDP (simplified for demonstration)\n",
      "\n",
      "Policy Iteration Characteristics:\n",
      "  - Two phases: Policy Evaluation + Policy Improvement\n",
      "  - Fewer iterations (policy changes)\n",
      "  - More computation per iteration (full policy evaluation)\n",
      "  - Policy converges, then stops\n",
      "\n",
      "Value Iteration Characteristics:\n",
      "  - Single phase: Combined evaluation + improvement\n",
      "  - More iterations (until value convergence)\n",
      "  - Less computation per iteration (one sweep)\n",
      "  - Values converge, then extract policy\n",
      "\n",
      "Comparison Table:\n",
      "  Feature              | Policy Iteration | Value Iteration\n",
      "  ---------------------|------------------|----------------\n",
      "  Iterations           | Fewer            | More\n",
      "  Computation/iteration| More             | Less\n",
      "  Convergence criterion| Policy stable    | Value stable\n",
      "  Convergence speed    | Fast (few iter)  | Moderate\n",
      "  Best when            | Small state space| Large state space\n",
      "\n",
      "‚úÖ Comparison complete!\n",
      "\n",
      "Note: Full implementation requires proper MDP definition\n",
      "(states, actions, transition probabilities, rewards)\n",
      "This notebook demonstrates the algorithmic concepts and differences.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 4: Comparison and Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create simple MDP example (2 states, 2 actions)\n",
    "# This is a simplified example for demonstration\n",
    "print(\"\\nExample: Simple MDP (simplified for demonstration)\")\n",
    "\n",
    "print(\"\\nPolicy Iteration Characteristics:\")\n",
    "print(\"  - Two phases: Policy Evaluation + Policy Improvement\")\n",
    "print(\"  - Fewer iterations (policy changes)\")\n",
    "print(\"  - More computation per iteration (full policy evaluation)\")\n",
    "print(\"  - Policy converges, then stops\")\n",
    "\n",
    "print(\"\\nValue Iteration Characteristics:\")\n",
    "print(\"  - Single phase: Combined evaluation + improvement\")\n",
    "print(\"  - More iterations (until value convergence)\")\n",
    "print(\"  - Less computation per iteration (one sweep)\")\n",
    "print(\"  - Values converge, then extract policy\")\n",
    "\n",
    "print(\"\\nComparison Table:\")\n",
    "print(\"  Feature              | Policy Iteration | Value Iteration\")\n",
    "print(\"  ---------------------|------------------|----------------\")\n",
    "print(\"  Iterations           | Fewer            | More\")\n",
    "print(\"  Computation/iteration| More             | Less\")\n",
    "print(\"  Convergence criterion| Policy stable    | Value stable\")\n",
    "print(\"  Convergence speed    | Fast (few iter)  | Moderate\")\n",
    "print(\"  Best when            | Small state space| Large state space\")\n",
    "\n",
    "print(\"\\n‚úÖ Comparison complete!\")\n",
    "\n",
    "print(\"\\nNote: Full implementation requires proper MDP definition\")\n",
    "print(\"(states, actions, transition probabilities, rewards)\")\n",
    "print(\"This notebook demonstrates the algorithmic concepts and differences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Policy Iteration**: Two-phase algorithm\n",
    "   - Phase 1: Policy Evaluation (compute V^œÄ)\n",
    "   - Phase 2: Policy Improvement (update œÄ to be greedy)\n",
    "   - Repeats until policy converges\n",
    "\n",
    "2. **Value Iteration**: Single-phase algorithm\n",
    "   - Updates: V(s) ‚Üê max_a Œ£ P(s'|s,a)[R + Œ≥V(s')]\n",
    "   - Continues until value function converges\n",
    "   - Extracts optimal policy at the end\n",
    "\n",
    "### Comparison:\n",
    "- **Iterations**: Policy Iteration typically needs fewer iterations\n",
    "- **Computation**: Value Iteration is often more efficient overall\n",
    "- **Convergence**: Policy Iteration stops when policy is stable; Value Iteration when values converge\n",
    "- **Use Case**: Policy Iteration for small problems; Value Iteration for larger problems\n",
    "\n",
    "### Advantages:\n",
    "- **Policy Iteration**: Guaranteed convergence, clear policy updates\n",
    "- **Value Iteration**: More efficient, works well with approximations\n",
    "\n",
    "### Applications:\n",
    "- Solving finite MDPs\n",
    "- Finding optimal policies\n",
    "- Foundation for approximate methods\n",
    "\n",
    "### Next Steps:\n",
    "- Approximate methods for large state spaces\n",
    "- Model-free methods (Q-learning, SARSA)\n",
    "- Continuous state/action spaces\n",
    "\n",
    "**Reference:** Course 09, Unit 2: \"Prediction and Control without a Model\" - Policy/Value iteration comparison practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
