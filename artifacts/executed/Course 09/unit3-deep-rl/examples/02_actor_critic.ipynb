{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Methods in Reinforcement Learning\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 09, Unit 3** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Methods in Reinforcement Learning\n",
    "## AIAT 123 - Reinforcement Learning\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand Actor-Critic architecture\n",
    "- Implement Actor-Critic algorithm\n",
    "- Apply to continuous control tasks\n",
    "- Compare with value-based methods\n",
    "\n",
    "## Real-World Context\n",
    "\n",
    "Robotics control, autonomous systems, and continuous action spaces.\n",
    "\n",
    "**Industry Impact**: Used in robotics, autonomous vehicles, and game AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:13:11.235226Z",
     "iopub.status.busy": "2026-01-15T12:13:11.235136Z",
     "iopub.status.idle": "2026-01-15T12:13:16.607066Z",
     "shell.execute_reply": "2026-01-15T12:13:16.606852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym torch numpy matplotlib -q\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print('âœ… Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Actor-Critic Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:13:16.608048Z",
     "iopub.status.busy": "2026-01-15T12:13:16.607967Z",
     "iopub.status.idle": "2026-01-15T12:13:16.609980Z",
     "shell.execute_reply": "2026-01-15T12:13:16.609748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Actor-Critic architecture defined\n"
     ]
    }
   ],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic network.\n",
    "    Actor: Policy network (outputs action probabilities)\n",
    "    Critic: Value network (estimates state values)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(), nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Critic (value)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        features = self.shared(state)\n",
    "        action_probs = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        return action_probs, value\n",
    "\n",
    "print('âœ… Actor-Critic architecture defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:13:16.610874Z",
     "iopub.status.busy": "2026-01-15T12:13:16.610814Z",
     "iopub.status.idle": "2026-01-15T12:13:16.613158Z",
     "shell.execute_reply": "2026-01-15T12:13:16.612975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training function ready!\n",
      "\n",
      "Note: Full training takes time. This demonstrates the concept.\n"
     ]
    }
   ],
   "source": [
    "def train_actor_critic(env_name='CartPole-v1', episodes=500):\n",
    "    \"\"\"\n",
    "    Train Actor-Critic agent.\n",
    "    \n",
    "    Real-world: Training robots or game agents\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    model = ActorCritic(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            # Get action probabilities and value\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs, value = model(state_tensor)\n",
    "            \n",
    "            # Sample action\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done or truncated:\n",
    "                # Calculate advantage\n",
    "                advantage = reward - value.item()\n",
    "                \n",
    "                # Update actor (policy)\n",
    "                actor_loss = -torch.log(action_probs[0][action]) * advantage\n",
    "                \n",
    "                # Update critic (value)\n",
    "                critic_loss = advantage ** 2\n",
    "                \n",
    "                # Total loss\n",
    "                loss = actor_loss + critic_loss\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        rewards_history.append(episode_reward)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-50:])\n",
    "            print(f'Episode {episode+1}, Avg Reward: {avg_reward:.2f}')\n",
    "    \n",
    "    env.close()\n",
    "    return rewards_history\n",
    "\n",
    "print('âœ… Training function ready!')\n",
    "print('\\nNote: Full training takes time. This demonstrates the concept.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Applications\n",
    "\n",
    "- **Robotics**: Continuous control (robot arm, walking)\n",
    "- **Autonomous Vehicles**: Steering, acceleration control\n",
    "- **Game AI**: Real-time strategy games\n",
    "- **Finance**: Portfolio optimization\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}