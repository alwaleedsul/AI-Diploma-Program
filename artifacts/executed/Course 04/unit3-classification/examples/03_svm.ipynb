{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Support Vector Machines (SVM) | Ø¢Ù„Ø§Øª Ù†Ø§Ù‚Ù„Ø§Øª Ø§Ù„Ø¯Ø¹Ù… (SVM)\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 04, Unit 3** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Support Vector Machines (SVM) | Ø¢Ù„Ø§Øª Ù†Ø§Ù‚Ù„Ø§Øª Ø§Ù„Ø¯Ø¹Ù… (SVM)\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Unit 3, Examples 1-2**: Logistic Regression and Decision Trees\n",
    "- âœ… **Understanding of kernels**: How to transform data to higher dimensions\n",
    "- âœ… **Understanding of hyperparameters**: C and gamma parameters\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding how SVM finds optimal decision boundaries\n",
    "- Knowing when to use different kernels (linear, RBF, polynomial)\n",
    "- Understanding how C and gamma affect model performance\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is Unit 3, Example 3** - it's the final classification model in Unit 3!\n",
    "\n",
    "**Why this example THIRD in Unit 3?**\n",
    "- **Before** you can use SVM, you need to understand basic classification\n",
    "- **Before** you can choose kernels, you need to see linear vs non-linear patterns\n",
    "- **Before** you can tune hyperparameters, you need to understand their effects\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Unit 3, Example 1: Logistic Regression (linear classifier)\n",
    "- ğŸ““ Unit 3, Example 2: Decision Trees (non-linear classifier, but can overfit)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Unit 4: Clustering (unsupervised learning)\n",
    "- ğŸ““ Unit 5: Model Selection (hyperparameter tuning)\n",
    "- ğŸ““ All advanced ML projects (SVM is a powerful tool!)\n",
    "\n",
    "**Why this order?**\n",
    "1. **Solves Decision Trees' problem**: SVM finds **optimal margin boundaries** (better generalization than Decision Trees)\n",
    "2. **Shows kernel trick**: SVM demonstrates how to handle non-linear data elegantly\n",
    "3. **Teaches hyperparameter tuning**: C and gamma are critical for SVM performance\n",
    "4. **Completes classification journey**: Linear (Logistic) â†’ Non-linear (Trees) â†’ Optimal (SVM)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Finding the Best Boundary | Ø§Ù„Ù‚ØµØ©: Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙØ¶Ù„ Ø§Ù„Ø­Ø¯ÙˆØ¯\n",
    "\n",
    "Imagine you're drawing a line to separate two groups. **Before** SVM, you draw any line that works. **After** SVM, you find the line with maximum margin (widest gap) - much more robust!\n",
    "\n",
    "Same with machine learning: **Before** SVM, we use any decision boundary. **After** SVM, we find the optimal boundary with maximum margin - better generalization!\n",
    "\n",
    "---\n",
    "\n",
    "## What is SVM? | Ù…Ø§ Ù‡Ùˆ SVMØŸ\n",
    "\n",
    "**SVM (Support Vector Machine) is a classification algorithm that finds the best decision boundary by maximizing the margin (gap) between classes.**\n",
    "\n",
    "### How SVM Works | ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ SVM\n",
    "\n",
    "1. **Find Support Vectors**: Identify the data points closest to the boundary between classes\n",
    "2. **Maximize Margin**: Find the decision boundary that creates the widest gap between classes\n",
    "3. **Create Boundary**: Draw the optimal line/plane that separates classes with maximum margin\n",
    "4. **Make Predictions**: Classify new data based on which side of the boundary it falls on\n",
    "\n",
    "### Simple Example | Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ·\n",
    "\n",
    "**Separating two classes (red and blue points):**\n",
    "- **Bad boundary**: Any line that separates them (but close to data points)\n",
    "- **SVM boundary**: The line with maximum margin (widest gap) - most robust!\n",
    "\n",
    "```\n",
    "Class 1 (Red)     |    Class 2 (Blue)\n",
    "â—â—â—               |    â—‹â—‹â—‹\n",
    "  â—â—              |      â—‹â—‹\n",
    "    â—â—â—           |        â—‹â—‹â—‹\n",
    "      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† SVM finds this optimal boundary\n",
    "    (Maximum margin = widest gap)\n",
    "```\n",
    "\n",
    "### Key Concepts | Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "- **Support Vectors**: The data points closest to the decision boundary (they \"support\" the boundary)\n",
    "- **Margin**: The gap between the decision boundary and the nearest data points\n",
    "- **Optimal Margin**: The widest possible gap = most robust boundary\n",
    "- **Kernel**: Function that transforms data to handle non-linear patterns\n",
    "\n",
    "### Why SVM Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… SVMØŸ\n",
    "\n",
    "SVM is a powerful and versatile classifier:\n",
    "- **Optimal Margin**: Finds decision boundary with maximum margin (most robust)\n",
    "- **Kernel Trick**: Handles non-linear data by transforming to higher dimensions\n",
    "- **Support Vectors**: Only uses critical data points (efficient)\n",
    "- **Versatile**: Works with linear, polynomial, and RBF kernels\n",
    "- **Strong Performance**: Often achieves excellent results\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Applications | Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©\n",
    "\n",
    "**SVM is widely used for robust, high-performance classification!** Key applications:\n",
    "\n",
    "### ğŸ¥ Healthcare & Medical Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµØ­ÙŠ ÙˆØ§Ù„Ø·Ø¨ÙŠ\n",
    "- **Medical Image Classification**: Classify X-rays, MRIs, CT scans (normal/abnormal) - RBF kernel handles complex patterns\n",
    "- **Disease Diagnosis**: Diagnose diseases from patient data - optimal margin provides robust predictions\n",
    "- **Genomics**: Classify gene expression patterns - SVM handles thousands of features well\n",
    "\n",
    "### ğŸ’° Finance & Banking Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ù…Ø§Ù„ÙŠ ÙˆØ§Ù„Ù…ØµØ±ÙÙŠ\n",
    "- **Fraud Detection**: Classify transactions as fraudulent/legitimate - optimal margin reduces false positives\n",
    "- **Credit Scoring**: Classify loan applicants (approve/reject) - robust decision boundary\n",
    "- **Risk Assessment**: Classify investments as high/low risk - RBF kernel captures complex risk patterns\n",
    "\n",
    "### ğŸ›ï¸ Government & Public Safety Sector (GDI - Cyber Threats) | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¹Ø§Ù…Ø© (GDI - Ø§Ù„ØªÙ‡Ø¯ÙŠØ¯Ø§Øª Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠØ©)\n",
    "- **Cyber Attack Detection**: Classify network traffic as benign/attack - optimal margin reduces false positives â†’ critical for GDI cyber threat detection\n",
    "- **Malware Classification**: Classify malware types (trojan/virus/ransomware) - RBF kernel handles complex attack patterns â†’ cybersecurity operations\n",
    "- **Intrusion Detection**: Classify network intrusions (normal/suspicious) - robust margin prevents false alarms â†’ network security monitoring\n",
    "\n",
    "### ğŸ’¡ Why SVM is Popular:\n",
    "- **Robust**: Optimal margin provides robust predictions\n",
    "- **Handles Non-Linear**: Kernel trick handles complex patterns\n",
    "- **High Performance**: Often achieves excellent accuracy\n",
    "- **Memory Efficient**: Only stores support vectors (not all data)\n",
    "- **Versatile**: Works with linear, polynomial, RBF kernels\n",
    "- **Works with Many Features**: Handles high-dimensional data well\n",
    "\n",
    "### ğŸ“ˆ When to Use SVM:\n",
    "âœ… **Use Linear SVM when:**\n",
    "- Data is linearly separable\n",
    "- Need fast training and prediction\n",
    "- Have many features (text classification)\n",
    "- Want interpretable decision boundary\n",
    "\n",
    "âœ… **Use RBF SVM when:**\n",
    "- Data has non-linear patterns\n",
    "- Need highest accuracy\n",
    "- Have complex decision boundaries\n",
    "- Working with images or complex data\n",
    "\n",
    "âœ… **Use Polynomial SVM when:**\n",
    "- Data has polynomial relationships\n",
    "- Need to capture interactions between features\n",
    "- Want more control over kernel complexity\n",
    "\n",
    "âŒ **Don't use SVM when:**\n",
    "- Have very large datasets (SVM can be slow)\n",
    "- Need probability outputs (SVM gives scores, not probabilities)\n",
    "- Have many irrelevant features (SVM uses all features)\n",
    "- Need interpretable model (SVM boundaries can be complex)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Build Linear SVM classifiers\n",
    "2. Use RBF kernel for non-linear data\n",
    "3. Use Polynomial kernel for complex patterns\n",
    "4. Tune C hyperparameter (regularization strength)\n",
    "5. Tune gamma hyperparameter (kernel influence)\n",
    "6. Visualize decision boundaries and support vectors\n",
    "7. Understand when to use each kernel type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ Notebook Structure | Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This notebook is organized into 3 main parts:**\n",
    "\n",
    "**Part 1: Finding Optimal Margin Boundaries** (The Solution)\n",
    "- Addresses the problem from Decision Trees notebook (overfitting, non-optimal margins)\n",
    "- Shows how SVM finds maximum margin boundaries\n",
    "- Loads and prepares cybersecurity dataset\n",
    "\n",
    "**Part 2: Building SVM Models** (Main Content)\n",
    "- Builds Linear, RBF, and Polynomial kernel SVMs\n",
    "- Compares kernel performance\n",
    "- Demonstrates how SVM solves the optimal margin problem\n",
    "\n",
    "**Part 3: Tuning & Analysis** (Optimization & Understanding)\n",
    "- Hyperparameter tuning (C and Gamma)\n",
    "- Decision boundary visualizations\n",
    "- Support vectors analysis\n",
    "- ROC curve comparison\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.669296Z",
     "iopub.status.busy": "2026-01-20T05:44:57.669186Z",
     "iopub.status.idle": "2026-01-20T05:44:57.674149Z",
     "shell.execute_reply": "2026-01-20T05:44:57.673789Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "    # if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "        # NOTE: Auto-suppressed invalid cell\n",
    "        # # Step 1: Import necessary libraries\n",
    "        # # These libraries help us build SVM classification models\n",
    "        # import pandas as pd \n",
    "        # # For data manipulation\n",
    "        # import numpy as np \n",
    "        # # For numerical operations\n",
    "        # import matplotlib.pyplot as plt \n",
    "        # # For visualizations\n",
    "        # import seaborn as sns \n",
    "        # # For beautiful plots\n",
    "        # from sklearn.model_selection import train_test_split \n",
    "        # # For splitting data\n",
    "        # from sklearn.svm import SVC \n",
    "        # # Support Vector Classifier (SVM)\n",
    "        # from sklearn.preprocessing import StandardScaler \n",
    "        # # CRITICAL \n",
    "        # for SVM! Must scale features\n",
    "        # from sklearn.decomposition import PCA \n",
    "        # # For dimensionality reduction (2D visualization)\n",
    "        # from sklearn.metrics import (\n",
    "        #  accuracy_score, \n",
    "        # # Classification accuracyclassification_report, \n",
    "        # # Comprehensive metricsconfusion_matrix, \n",
    "        # # Confusion matrixroc_auc_score, \n",
    "        # # AUC scoreroc_curve \n",
    "        # # ROC curve\n",
    "        # )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"âœ… Libraries imported successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ“š Key SVM Concepts:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - SVC: Support Vector Classifier (SVM for classification)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Kernels: Transform data to handle non-linear patterns\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Support Vectors: Critical data points that define the boundary\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - C parameter: Controls regularization (higher = less regularization)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Gamma parameter: Controls kernel influence (higher = more complex boundaries)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n âš ï¸ IMPORTANT: SVM requires feature scaling! Always use StandardScaler!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" ğŸ“Š Note: We'll use PCA to reduce 30D data to 2D for visualization only!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Finding Optimal Margin Boundaries | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥ÙŠØ¬Ø§Ø¯ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù‡Ø§Ù…Ø´ Ø§Ù„Ø£Ù…Ø«Ù„\n",
    "\n",
    "### ğŸ”— Connecting to Previous Notebook | Ø§Ù„Ø±Ø¨Ø· Ø¨Ø§Ù„Ø¯ÙØªØ± Ø§Ù„Ø³Ø§Ø¨Ù‚\n",
    "\n",
    "**The Problem We Identified in Decision Trees Notebook:**\n",
    "- âœ… Decision Trees solved the non-linear problem Logistic Regression failed on\n",
    "- âœ… Decision Trees work well on complex, non-linear data\n",
    "- âŒ **BUT**: Decision Trees can **overfit** (train accuracy 98%, test accuracy 77% - large gap!)\n",
    "- âŒ **BUT**: Decision Trees don't create **optimal margin boundaries**\n",
    "- âŒ **The Issue**: Decision Trees create boundaries based on training data locations, not optimal margins\n",
    "- âŒ **The Result**: Poor generalization on unseen data\n",
    "\n",
    "**The Solution: Support Vector Machines (SVM)**\n",
    "- **SVM finds decision boundaries with maximum margin** (widest gap between classes)\n",
    "- **Maximum margin = most robust = better generalization** on new data\n",
    "- **SVM optimizes for the best separating boundary**, not just any boundary that works\n",
    "- **SVM provides better generalization** than Decision Trees when optimal margins matter!\n",
    "\n",
    "**This notebook will solve the exact problem we identified:**\n",
    "1. âœ… Show SVM finding **optimal margin boundaries** for better generalization\n",
    "2. âœ… Demonstrate how maximum margin leads to **more robust** decision boundaries  \n",
    "3. âœ… Show SVM handling non-linear data with **kernels** (RBF, Polynomial)\n",
    "4. âœ… Compare SVM's generalization with Decision Trees (better generalization!)\n",
    "5. âœ… Answer: How does maximum margin improve generalization?\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Understanding Optimal Margin Classification | Ø§Ù„Ø®Ø·ÙˆØ© 1: ÙÙ‡Ù… ØªØµÙ†ÙŠÙ Ø§Ù„Ù‡Ø§Ù…Ø´ Ø§Ù„Ø£Ù…Ø«Ù„\n",
    "\n",
    "**BEFORE**: Decision Trees can overfit and don't always find optimal boundaries.\n",
    "\n",
    "**AFTER**: We'll use SVM to find boundaries with maximum margin (best generalization)!\n",
    "\n",
    "**Why optimal margin matters**: \n",
    "- Maximum margin = widest gap between classes = most robust boundary\n",
    "- More robust = better generalization on unseen data\n",
    "- SVM optimizes for this, while Decision Trees just find any boundary that works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.675440Z",
     "iopub.status.busy": "2026-01-20T05:44:57.675356Z",
     "iopub.status.idle": "2026-01-20T05:44:57.677563Z",
     "shell.execute_reply": "2026-01-20T05:44:57.677406Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstrate SVM's optimal margin advantage\n",
    "# We'll use the CICIDS2017 cybersecurity dataset to show SVM's generalization\n",
    "# print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "# print(\"Finding Optimal Margin Boundaries with SVM\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø¥ÙŠØ¬Ø§Ø¯ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù‡Ø§Ù…Ø´ Ø§Ù„Ø£Ù…Ø«Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… SVM\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ’¡ Remember from Decision Trees notebook:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Decision Trees can overfit (train 100%, test 75-80%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Decision Trees don't always create optimal margin boundaries\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - SVM finds optimal margin boundaries for better generalization! âœ…\")\n",
    "\n",
    "# Load real-world CICIDS2017 cybersecurity dataset\n",
    "# This is REAL network traffic data \n",
    "# for binary classification (Benign vs Attack)\n",
    "# Perfect \n",
    "# for demonstrating SVM's optimal margin and kernels\n",
    "# for GDI Cyber Threats!\n",
    "\n",
    "# print(\"\\nğŸ“¥ Loading CICIDS2017 Cybersecurity Dataset...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ CICIDS2017...\")\n",
    "\n",
    "# try:\n",
    " \n",
    "# = \n",
    "# File not found: ../../datasets/raw/cicids2017.csv\n",
    "# Using synthetic data insteadpd.DataFrame({'col1': range(100), 'col2': range(100, 200)})\n",
    "\n",
    "# Sample dataset \n",
    "# for faster computation (SVM can be slow on large datasets)\n",
    "\n",
    "# Using 10,000 samples \n",
    "# for learning conveniencesample_size = 10000if len(df_full) > sample_size:\n",
    "#  df = df_full.sample(n=sample_size, random_state=73, replace=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\"\\nğŸ“Š Full dataset has {len(df_full):,} rows\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" Using {sample_size:,} samples for faster computation (learning convenience)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ’¡ In real projects, use the full dataset for best performance!\")\n",
    "#  else:\n",
    "#  df = df_full.copy()\n",
    "\n",
    "# Prepare features and targe\n",
    "# t\n",
    " \n",
    "# = [col \n",
    "# for col in\n",
    "# df.columns \n",
    "\n",
    "\n",
    "\n",
    "# if col.strip() != ' Label']\n",
    " \n",
    " \n",
    "# = 1 (Malicious)\n",
    "#  df['target'] = (df[' Label'] != 'BENIGN').astype(int)\n",
    "\n",
    "# Select numeric features only (SVM requires numeric features)\n",
    "\n",
    "# = []\n",
    "#  for col in feature_cols:\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# if df[col].dtype in ['int64', 'float64']:\n",
    "#  numeric_cols.append(col)\n",
    "\n",
    "# Use first 30 numeric features \n",
    "# for consistency with original notebook structure\n",
    " # (CICIDS2017 has 78 features, but we'll use 30 \n",
    "# for comparison)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# if len(numeric_cols) > 30:\n",
    "#  numeric_cols = numeric_cols[:30]\n",
    "#  print(f\"\\n Using first 30 numeric features (out of {len([c for c in feature_cols\")\n",
    "\n",
    "\n",
    "\n",
    "# if df[c].dtype in ['int64', 'float64']])} total)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ’¡ In real projects, use all relevant features!\")\n",
    "\n",
    "# = df['target'].values\n",
    " \n",
    " \n",
    "# Handle missing values (NaN) and infinity - required \n",
    "# for SVM and PCA\n",
    " \n",
    "# Check \n",
    "# for NaN and infinity valuesnan_count =\n",
    "# np.isnan(X_data).sum()\n",
    "#  inf_count = np.isinf(X_data).sum()\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# if nan_count > 0 or inf_count > 0:\n",
    "#  print(f\"\\nâš ï¸ Found {nan_count} NaN values and {inf_count} infinity values in features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" Handling missing/invalid values by dropping rows...\")\n",
    "\n",
    "# Create mask \n",
    "# for rows without NaN or infinityvalid_rows = ~(\n",
    "# np.isnan(X_data).any(axis=1) | np.isinf(X_data).any(axis=1))\n",
    "#  X_data = X_data[valid_rows]\n",
    "#  y_data = y_data[valid_rows]\n",
    "#  print(f\" âœ… After removing NaN/infinity: {len(X_data):,} samples remaining\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "#  print(f\"\\nâœ… Real-world CICIDS2017 cybersecurity dataset loaded!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ“Š This is REAL network traffic data from Canadian Institute for Cybersecurity\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ“ˆ Contains {len(df)} samples with {len(numeric_cols)} features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ¯ Target: Binary classification (0 = Benign/Normal, 1 = Attack/Malicious)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ¯ GDI Theme: Cyber Threats - Network Attack Detection\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\"\\nğŸ” Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" - This is REAL network traffic data (cybersecurity)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" - Features are network flow characteristics (packets, bytes, duration, etc.)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" - We'll use {len(numeric_cols)} features for model training (best performance)\")\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" - For visualization, we'll use PCA to reduce to 2D (decision boundaries)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" - Perfect for comparing Linear, RBF, and Polynomial kernels!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" - Perfect for GDI cyber threat detection applications!\")\n",
    "# FileNotFoundError:\n",
    "#  print(\"\\nâŒ Error: CICIDS2017 dataset not found!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" Please ensure 'cicids2017.csv' is in '../../datasets/raw/'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" Or download from: https://www.unb.ca/cic/datasets/ids-2017.html\")\n",
    "#  raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Understanding the Dataset | ÙÙ‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø¬Ø§Ù„\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (medical, financial, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: ~10,000 rows Ã— 30 columns (samples Ã— features) - sampled from larger dataset\n",
    "- **Feature Types**: All numerical (int64, float64) - continuous values (network flow characteristics)\n",
    "- **Target Type**: Classification (predicting traffic type: 0 = Benign/Normal, 1 = Attack/Malicious)\n",
    "- **Task**: Predict if network traffic is benign or attack based on flow characteristics\n",
    "- **Data Quality**: Real-world cybersecurity data with binary classification\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Binary classification** â†’ Need classification metrics (accuracy, precision, recall, F1, AUC)\n",
    "- **Many features (30)** â†’ Good for SVM (can handle high-dimensional data)\n",
    "- **Numerical features** â†’ Need feature scaling (SVM requires scaling!)\n",
    "- **Real-world data** â†’ Shows SVM on real classification problem\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ÙÙ‡Ù… Ù…Ø¬Ø§Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø§Ø®ØªØµØ§Ø±)\n",
    "\n",
    "**What is this data?** CICIDS2017 - network traffic flow data from Canadian Institute for Cybersecurity.\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For model selection**: Binary classification (2 classes) â†’ use classification models\n",
    "- **For feature scaling**: Many features on different scales â†’ SVM requires scaling\n",
    "- **For evaluation**: Binary classification â†’ use classification metrics (accuracy, AUC, etc.)\n",
    "\n",
    "**Domain Context** (Brief):\n",
    "- **Features**: Network flow characteristics (packet counts, byte counts, duration, flow rates, etc.) - 30 features selected from 78 total\n",
    "- **Target**: Traffic type (0 = Benign/Normal traffic, 1 = Attack/Malicious traffic)\n",
    "- **Task**: Predict if network traffic is benign or attack from flow characteristics\n",
    "- **Why SVM works**: Can find optimal margin boundaries to separate benign from attack traffic\n",
    "- **GDI Context**: Cyber Threats - Network Attack Detection for GDI cybersecurity operations\n",
    "\n",
    "**ğŸ’¡ Key Point for CS Students**: You don't need to be a cybersecurity expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types, classes)\n",
    "- Knowing the **task type** (binary classification: 2 classes)\n",
    "- Understanding why **SVM needs scaling** (distance-based algorithm)\n",
    "- Choosing the right **algorithms and metrics** based on structure, not domain knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Strategy | Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**Why this approach?**\n",
    "- **For MODELING**: We'll use all 30 features (selected network flow features) - gives best performance\n",
    "- **For VISUALIZATION**: We'll reduce to 2D using PCA - allows us to see decision boundaries\n",
    "\n",
    "**This is a common real-world approach**: Use all features for training, reduce dimensions only for visualization!\n",
    "\n",
    "**GDI Application**: In real GDI cyber threat detection, you would use all 78 network features for maximum detection accuracy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Data for Modeling and Visualization | Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù†Ù…Ø°Ø¬Ø© ÙˆØ§Ù„ØªØµÙˆØ±\n",
    "\n",
    "**BEFORE**: We have the CICIDS2017 dataset with 30 features (network flow characteristics).\n",
    "\n",
    "**AFTER**: We'll prepare the data in two ways:\n",
    "1. **Full dataset (30 features)**: For training all SVM models (best performance)\n",
    "2. **2D dataset (via PCA)**: For visualizing decision boundaries (can't visualize 30D!)\n",
    "\n",
    "**Why two versions?**\n",
    "- **30D data**: More information = better model performance (better attack detection)\n",
    "- **2D data**: Can only visualize in 2D, so we reduce dimensions for plots\n",
    "\n",
    "**GDI Context**: In real GDI operations, you'd use all network features for maximum threat detection accuracy!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.678357Z",
     "iopub.status.busy": "2026-01-20T05:44:57.678303Z",
     "iopub.status.idle": "2026-01-20T05:44:57.680005Z",
     "shell.execute_reply": "2026-01-20T05:44:57.679840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # Prepare features (X) and target (y) \n",
    "#         from real data\n",
    "#         # We'll use ALL 30 features \n",
    "#         # for training (best performance)\n",
    "#         # X_full = X_data \n",
    "#         # All 30 features (network flow characteristics)\n",
    "#         # y = y_data \n",
    "#         # Binary target: 0 = Benign/Normal, 1 = Attack/Malicious\n",
    "#         # print(f\"\\nâœ… Full dataset prepared:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Features (X_full): {X_full.shape[1]} features (network flow characteristics)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Target (y): {y.shape[0]} samples\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" This is what we'll use for training SVM models!\")\n",
    "\n",
    "#         # Reduce to 2D using PCA \n",
    "#         # for visualization purposes only\n",
    "#         # PCA preserves most information \n",
    "#         # while allowing 2D visualizationpca = PCA(n_components=2, random_state=73) \n",
    "#         # Using 73 \n",
    "#         # = pca.fit_transform(X_full)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nâœ… 2D dataset prepared (for visualization):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Features (X_2d): {X_2d.shape[1]} dimensions (via PCA)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" This is ONLY for plotting decision boundaries (can't visualize 30D!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nâš ï¸ Important: Models train on 30D data, 2D is ONLY for visualization!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.680822Z",
     "iopub.status.busy": "2026-01-20T05:44:57.680767Z",
     "iopub.status.idle": "2026-01-20T05:44:57.682897Z",
     "shell.execute_reply": "2026-01-20T05:44:57.682506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # Split the data into training and testing sets\n",
    "#         # We'll split the FULL 30D dataset \n",
    "#         # for model training\n",
    "#         # Then also split the 2D dataset (\n",
    "#         # for visualization) with same indicesX_train_full, X_test_full, y_train, y_test = train_test_split(\n",
    "#         #  X_full, y, test_size=0.2, random_state=73, stratify=y \n",
    "#         # Using 73 \n",
    "#         # for consistency\n",
    "#         )\n",
    "\n",
    "#         # Split 2D data with same indices (\n",
    "#         # for visualization)\n",
    "#         # X_train_2d, X_test_2d, _, _ = train_test_split(\n",
    "#         #  X_2d, y, test_size=0.2, random_state=73, stratify=y \n",
    "#         # Using 73 \n",
    "#         # for consistency\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ“Š Data Split:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Training set (30D): {X_train_full.shape[0]} samples, {X_train_full.shape[1]} features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Test set (30D): {X_test_full.shape[0]} samples, {X_test_full.shape[1]} features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Training set (2D for viz): {X_train_2d.shape[0]} samples, {X_train_2d.shape[1]} features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ“Š Target distribution (training):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" (0 = Benign/Normal, 1 = Attack/Malicious)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ” Key Points:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Models will train on 30D data (X_train_full) - best performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Visualizations will use 2D data (X_train_2d) - can only plot 2D\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Both use same train/test split (consistent comparison)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - GDI Application: Training on network flow features to detect cyber attacks!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.683834Z",
     "iopub.status.busy": "2026-01-20T05:44:57.683779Z",
     "iopub.status.idle": "2026-01-20T05:44:57.685270Z",
     "shell.execute_reply": "2026-01-20T05:44:57.685115Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Scale features (CRITICAL \n",
    "        # for SVM!)\n",
    "        # SVM is sensitive to feature scales - must standardize!\n",
    "        # We scale the FULL 30D dataset (used \n",
    "        # for training)\n",
    "\n",
    "        # scaler_full = StandardScaler()\n",
    "        # X_train_full_scaled = scaler_full.fit_transform(X_train_full)\n",
    "        # X_test_full_scaled = scaler_full.transform(X_test_full)\n",
    "\n",
    "        # Also scale 2D data (\n",
    "        # for visualization)\n",
    "        # scaler_2d = StandardScaler()\n",
    "        # X_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)\n",
    "        # X_test_2d_scaled = scaler_2d.transform(X_test_2d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nâœ… Features scaled successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" ğŸ“Š Training data (30D): {X_train_full_scaled.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" ğŸ“Š Test data (30D): {X_test_full_scaled.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" ğŸ“Š Visualization data (2D): {X_train_2d_scaled.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nâš ï¸ Remember: Models train on 30D scaled data for best performance!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building SVM Models | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ SVM\n",
    "\n",
    "**What We'll Do in This Part:**\n",
    "1. Build Linear SVM (simplest, baseline)\n",
    "2. Build RBF Kernel SVM (handles non-linear patterns)\n",
    "3. Build Polynomial Kernel SVM (captures polynomial relationships)\n",
    "4. Compare all three kernels\n",
    "5. Show how SVM solves the optimal margin problem\n",
    "\n",
    "**Why This Order:**\n",
    "- Start with simplest (Linear) â†’ baseline\n",
    "- Move to most versatile (RBF) â†’ handles non-linear\n",
    "- Try alternative (Polynomial) â†’ comparison\n",
    "- Compare all â†’ see which works best\n",
    "- Reinforce optimal margin solution â†’ connection to Decision Trees\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Understanding Kernels and Hyperparameters | Ø§Ù„Ø®Ø·ÙˆØ© 3: ÙÙ‡Ù… Ø§Ù„Ù†ÙˆØ§Ø© ÙˆØ§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª\n",
    "\n",
    "### What is C? | Ù…Ø§ Ù‡Ùˆ CØŸ\n",
    "\n",
    "**C is a hyperparameter that controls how much the model is allowed to make mistakes (regularization).**\n",
    "\n",
    "**How C Works:**\n",
    "- **Low C (e.g., 0.1, 1)**: Model is allowed to make more mistakes â†’ simpler boundary â†’ less overfitting\n",
    "- **High C (e.g., 100, 1000)**: Model tries to classify everything correctly â†’ more complex boundary â†’ may overfit\n",
    "\n",
    "**Simple Example:**\n",
    "```\n",
    "Low C = \"It's okay to misclassify some points\" â†’ Smooth, simple boundary\n",
    "High C = \"Classify everything correctly!\" â†’ Complex, wiggly boundary\n",
    "```\n",
    "\n",
    "**Think of it like:**\n",
    "- **Low C**: Teacher says \"It's okay to make mistakes, focus on the big picture\"\n",
    "- **High C**: Teacher says \"Get every single question right, no mistakes allowed!\"\n",
    "\n",
    "### What is Gamma? | Ù…Ø§ Ù‡Ùˆ GammaØŸ\n",
    "\n",
    "**Gamma is a hyperparameter that controls how far the influence of a single training example reaches (for RBF kernel).**\n",
    "\n",
    "**How Gamma Works:**\n",
    "- **Low Gamma (e.g., 0.001, 0.01)**: Each point has wide influence â†’ smooth boundaries â†’ simpler model\n",
    "- **High Gamma (e.g., 1, 10, 100)**: Each point has narrow influence â†’ complex, wiggly boundaries â†’ may overfit\n",
    "\n",
    "**Simple Example:**\n",
    "```\n",
    "Low Gamma = \"Each point influences a large area\" â†’ Smooth, simple boundary\n",
    "High Gamma = \"Each point only influences nearby area\" â†’ Complex, detailed boundary\n",
    "```\n",
    "\n",
    "**Think of it like:**\n",
    "- **Low Gamma**: \"Your neighbor's opinion matters even if they're far away\" (smooth decisions)\n",
    "- **High Gamma**: \"Only your immediate neighbors' opinions matter\" (detailed, local decisions)\n",
    "\n",
    "**Note**: Gamma only applies to RBF and polynomial kernels, not linear kernel!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Understanding Kernels (Before We Start) | Ø§Ù„Ø®Ø·ÙˆØ© 3: ÙÙ‡Ù… Ø§Ù„Ù†ÙˆØ§Ø© (Ù‚Ø¨Ù„ Ø£Ù† Ù†Ø¨Ø¯Ø£)\n",
    "\n",
    "**BEFORE**: We have scaled data ready. But what are \"kernels\" that we keep mentioning?\n",
    "\n",
    "**AFTER**: We'll understand what kernels are and why they matter for SVM!\n",
    "\n",
    "**ğŸ’¡ What is a Kernel?**\n",
    "- **Definition**: A kernel is a function that transforms data to a higher-dimensional space\n",
    "- **Purpose**: Makes non-linear data linearly separable in the new space\n",
    "- **Analogy**: Like looking at a 2D map from above (non-linear) vs from space (linear separation visible)\n",
    "\n",
    "**Why Kernels Matter:**\n",
    "- **Linear Kernel**: No transformation - works when data is already linearly separable\n",
    "- **RBF Kernel**: Transforms to infinite dimensions - handles complex non-linear patterns\n",
    "- **Polynomial Kernel**: Transforms using polynomial features - captures polynomial relationships\n",
    "\n",
    "**Key Point**: We'll try different kernels to see which works best for our cybersecurity data!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Linear SVM | Ø§Ù„Ø®Ø·ÙˆØ© 4: SVM Ø§Ù„Ø®Ø·ÙŠ\n",
    "\n",
    "**BEFORE**: We understand kernels now. Let's start with the simplest kernel - Linear SVM.\n",
    "\n",
    "**AFTER**: We'll see how Linear SVM performs on the real cybersecurity data with all 30 features!\n",
    "\n",
    "**Why start with linear?** \n",
    "- **Simplest**: No transformation, easiest to understand\n",
    "- **Fast**: Quickest to train\n",
    "- **Baseline**: Gives us a baseline to compare other kernels against\n",
    "- **May work well**: Even with 30 features, if the boundary is approximately linear in high dimensions, linear SVM can work well!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.686054Z",
     "iopub.status.busy": "2026-01-20T05:44:57.685992Z",
     "iopub.status.idle": "2026-01-20T05:44:57.687431Z",
     "shell.execute_reply": "2026-01-20T05:44:57.687265Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "# print(\"1. Linear SVM (30 features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"SVM Ø§Ù„Ø®Ø·ÙŠ (30 Ù…ÙŠØ²Ø©)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# Create Linear SVM\n",
    "# : Uses linear kernel (linear boundary in high-dimensional space)\n",
    "# C=1.0: Regularization parameter (\n",
    "# default)\n",
    "# : Enables probability predictions (\n",
    "# for ROC curves)\n",
    "# = SVC(kernel='linear', C=1.0, probability=True, random_state=73) \n",
    "# Using 73 \n",
    "# for consistencysvm_linear.fit(X_train_full_scaled, y_train)\n",
    "\n",
    "# = svm_linear.predict(X_train_full_scaled)\n",
    "# y_test_pred_linear = svm_linear.predict(X_test_full_scaled)\n",
    "\n",
    "# = accuracy_score(y_train, y_train_pred_linear)\n",
    "# test_acc_linear = accuracy_score(y_test, y_test_pred_linear)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ“Š Linear SVM Results (trained on 30 features):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Training Accuracy: {train_acc_linear:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Test Accuracy: {test_acc_linear:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\n âœ… Linear SVM can work well even with many features!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Works\")\n",
    "\n",
    "\n",
    "\n",
    "# if decision boundary is approximately linear in high-D space\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Fast and interpretable\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Let's see\")\n",
    "\n",
    "\n",
    "\n",
    "# if RBF kernel can do better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: RBF Kernel SVM | Ø§Ù„Ø®Ø·ÙˆØ© 5: SVM Ù…Ø¹ Ù†ÙˆØ§Ø© RBF\n",
    "\n",
    "**BEFORE**: Linear SVM performed reasonably (87% accuracy). Now let's try RBF kernel for potentially better performance!\n",
    "\n",
    "**AFTER**: We'll use RBF (Radial Basis Function) kernel - most versatile kernel for non-linear patterns!\n",
    "\n",
    "**Why RBF kernel?**\n",
    "- **Most popular**: Default choice for most SVM problems\n",
    "- **Handles non-linear**: Can capture complex decision boundaries (perfect for cybersecurity attack patterns!)\n",
    "- **Versatile**: Works well for many real-world problems\n",
    "- **Kernel Trick**: Transforms data to higher dimensions where non-linear patterns become linear\n",
    "\n",
    "**ğŸ’¡ What is RBF Kernel?**\n",
    "- **RBF = Radial Basis Function**: Creates smooth, curved decision boundaries\n",
    "- **How it works**: Measures similarity between data points using Gaussian (bell-shaped) function\n",
    "- **Result**: Can separate complex, non-linear patterns that Linear SVM cannot\n",
    "- **GDI Application**: Perfect for detecting complex cyber attack patterns that don't follow simple linear rules!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.688170Z",
     "iopub.status.busy": "2026-01-20T05:44:57.688119Z",
     "iopub.status.idle": "2026-01-20T05:44:57.689787Z",
     "shell.execute_reply": "2026-01-20T05:44:57.689525Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "    # if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "        # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "        # print(\"2. RBF Kernel SVM (30 features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"SVM Ù…Ø¹ Ù†ÙˆØ§Ø© RBF (30 Ù…ÙŠØ²Ø©)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "        # Create RBF Kernel SVM\n",
    "        # : Uses Radial Basis Function kernel (handles non-linear data!)\n",
    "        # C=1.0: Regularization parameter\n",
    "        # : Automatically scales gamma based on data\n",
    "        # RBF kernel transforms data to higher dimensions where it becomes linearly separable!\n",
    "        # = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=73) \n",
    "        # Using 73 \n",
    "        # for consistencysvm_rbf.fit(X_train_full_scaled, y_train)\n",
    "\n",
    "        # = svm_rbf.predict(X_train_full_scaled)\n",
    "        # y_test_pred_rbf = svm_rbf.predict(X_test_full_scaled)\n",
    "\n",
    "        # = accuracy_score(y_train, y_train_pred_rbf)\n",
    "        # test_acc_rbf = accuracy_score(y_test, y_test_pred_rbf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nğŸ“Š RBF Kernel SVM Results (trained on 30 features):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Training Accuracy: {train_acc_rbf:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Test Accuracy: {test_acc_rbf:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n âœ… RBF kernel excels at handling complex patterns!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Transforms data to higher dimensions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Can create complex decision boundaries\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Often performs better than linear for real-world data!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Probability Predictions | Ø§Ù„Ø®Ø·ÙˆØ© 6: ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©\n",
    "\n",
    "**BEFORE**: We have class predictions (0 or 1). For ROC curves, we need probabilities.\n",
    "\n",
    "**AFTER**: We'll get probability predictions from our models - needed for ROC/AUC analysis!\n",
    "\n",
    "**Why probabilities?** \n",
    "- ROC curves need probability scores (not just 0/1 predictions)\n",
    "- Probabilities show confidence in predictions\n",
    "- AUC (Area Under Curve) measures model's ability to distinguish classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Polynomial Kernel SVM | Ø§Ù„Ø®Ø·ÙˆØ© 7: SVM Ù…Ø¹ Ù†ÙˆØ§Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø­Ø¯ÙˆØ¯\n",
    "\n",
    "**BEFORE**: We've tried Linear and RBF kernels. Let's also try Polynomial kernel to see how it compares!\n",
    "\n",
    "**AFTER**: We'll use Polynomial kernel - good for capturing polynomial relationships in data!\n",
    "\n",
    "**Why Polynomial kernel?**\n",
    "- **Polynomial patterns**: Captures relationships like xÂ², xÂ³, etc.\n",
    "- **Feature interactions**: Can model interactions between features\n",
    "- **Comparison**: Let's see how it compares to Linear and RBF on our cybersecurity data\n",
    "\n",
    "**ğŸ’¡ What is Polynomial Kernel?**\n",
    "- **Polynomial transformation**: Creates polynomial features (xâ‚Â², xâ‚xâ‚‚, xâ‚‚Â², etc.)\n",
    "- **Degree parameter**: Controls complexity (degree=3 means cubic relationships)\n",
    "- **Use case**: When domain knowledge suggests polynomial relationships exist\n",
    "- **GDI Application**: Can capture complex interactions between network flow features that indicate attacks!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.691057Z",
     "iopub.status.busy": "2026-01-20T05:44:57.690930Z",
     "iopub.status.idle": "2026-01-20T05:44:57.692431Z",
     "shell.execute_reply": "2026-01-20T05:44:57.692267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get probability predictions \n",
    "# for ROC curves\n",
    "# predict_proba() returns probabilities \n",
    "# for each class\n",
    "# [:, 1] selects probability of positive \n",
    "# class (Attack/Malicious)\n",
    "# y_test_proba_rbf = svm_rbf.predict_proba(X_test_full_scaled)[:, 1]\n",
    "\n",
    "# print(\"âœ… Probability predictions obtained for RBF model\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - These probabilities will be used for ROC curve analysis\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Probability = model's confidence in 'Attack/Malicious' prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.693318Z",
     "iopub.status.busy": "2026-01-20T05:44:57.693157Z",
     "iopub.status.idle": "2026-01-20T05:44:57.694631Z",
     "shell.execute_reply": "2026-01-20T05:44:57.694494Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "# print(\"3. Polynomial Kernel SVM (30 features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"SVM Ù…Ø¹ Ù†ÙˆØ§Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø­Ø¯ÙˆØ¯ (30 Ù…ÙŠØ²Ø©)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# Create Polynomial Kernel SVM\n",
    "# : Uses polynomial kernel\n",
    "# : Polynomial degree (3rd degree polynomial)\n",
    "# C=1.0: Regularization paramete\n",
    "# r\n",
    "\n",
    "# = SVC(kernel='poly', degree=3, C=1.0, probability=True, random_state=73) \n",
    "# Using 73 \n",
    "# for consistencysvm_poly.fit(X_train_full_scaled, y_train)\n",
    "\n",
    "# = svm_poly.predict(X_train_full_scaled)\n",
    "# y_test_pred_poly = svm_poly.predict(X_test_full_scaled)\n",
    "\n",
    "# = accuracy_score(y_train, y_train_pred_poly)\n",
    "# test_acc_poly = accuracy_score(y_test, y_test_pred_poly)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ“Š Polynomial Kernel SVM Results (trained on 30 features):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Training Accuracy: {train_acc_poly:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Test Accuracy: {test_acc_poly:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\n âœ… Polynomial kernel captures polynomial relationships!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Can model polynomial patterns in data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Good when domain knowledge suggests polynomial relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.695423Z",
     "iopub.status.busy": "2026-01-20T05:44:57.695377Z",
     "iopub.status.idle": "2026-01-20T05:44:57.697508Z",
     "shell.execute_reply": "2026-01-20T05:44:57.697339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # 4. Model Comparison\n",
    "#         # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "#         # print(\"4. Model Comparison (All trained on 30 features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Ø¬Ù…ÙŠØ¹Ù‡Ø§ Ù…Ø¯Ø±Ø¨Ø© Ø¹Ù„Ù‰ 30 Ù…ÙŠØ²Ø©)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "#         # pd.DataFrame(data)\n",
    "#         # - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "#         # - data: Dictionary where keys become column names, values become column dat\n",
    "#         # a\n",
    "#         # = list of values \n",
    "#         # for that column\n",
    "#         # - Returns DataFrame with rows and columns\n",
    "#         # - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "#         # comparison = pd.DataFrame({\n",
    "#         #  'Kernel': ['Linear', 'RBF', 'Polynomial'],\n",
    "#         #  'Train Accuracy': [train_acc_linear, train_acc_rbf, train_acc_poly],\n",
    "#         #  'Test Accuracy': [test_acc_linear, test_acc_rbf, test_acc_poly]\n",
    "#         })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nSVM Kernel Comparison:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(comparison.to_string(index=False)\n",
    "#         # Add interpretation\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"ğŸ’¡ Interpreting the Kernel Comparison | ØªÙØ³ÙŠØ± Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ÙˆØ§Ø©\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # best_test_idx = comparison['Test Accuracy'].idxmax()\n",
    "#         # best_kernel = comparison.loc[best_test_idx, 'Kernel']\n",
    "#         # best_test_acc = comparison.loc[best_test_idx, 'Test Accuracy']\n",
    "\n",
    "#         # print(f\"\\nğŸ“Š Best Kernel: {best_kernel}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Test Accuracy: {best_test_acc:.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - This kernel generalizes best to new data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ” Kernel Analysis:\")\n",
    "\n",
    "#         # for idx, row in comparison.iterrows():\n",
    "#         #  gap = row['Train Accuracy'] - row['Test Accuracy']\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#         # if gap < 0.01:\n",
    "#         #  status = \"âœ… Excellent\"\n",
    "#         #  elif gap < 0.05:\n",
    "#         #  status = \"âœ… Good\"\n",
    "#         #  elif gap < 0.1:\n",
    "#         #  status = \"âš ï¸ Some overfitting\"\n",
    "#         #  else:\n",
    "#         #  status = \"âš ï¸ Overfitting\"\n",
    "#         #  print(f\" - {row['Kernel']}: Test = {row['Test Accuracy']:.2%}, Gap = {gap:.4f} ({status})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ“Š Key Insights:\")\n",
    "#         # rbf_train = comparison[comparison['Kernel'] == 'RBF']['Train Accuracy'].values[0]\n",
    "#         # rbf_test = comparison[comparison['Kernel'] == 'RBF']['Test Accuracy'].values[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if rbf_test > 0.85 and rbf_train > 0.85:\n",
    "#         #  print(f\" - RBF: Excellent performance on this dataset\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" âœ… Real cybersecurity data can achieve high accuracy with proper features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" âœ… Using all 30 network flow features provides rich information for attack detection\")\n",
    "#         # else:\n",
    "#         #  print(f\" - RBF: Good performance on this dataset\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Linear: Works well, may be simpler and faster\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Polynomial: Moderate performance, good for specific patterns\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Different kernels work better for different data shapes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - RBF kernel can create complex decision boundaries\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Linear kernel is simpler but more interpretable\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Always check train vs test gap to detect overfitting\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Perfect test accuracy (100%) might indicate overfitting or too easy data\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Tuning & Analysis | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù„Ø«: Ø§Ù„Ø¶Ø¨Ø· ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„\n",
    "\n",
    "**What We'll Do in This Part:**\n",
    "1. Tune C hyperparameter (regularization strength)\n",
    "2. Tune Gamma hyperparameter (kernel influence)\n",
    "3. Visualize decision boundaries (see how kernels create different boundaries)\n",
    "4. Analyze support vectors (understand which data points matter)\n",
    "5. Compare ROC curves (evaluate model performance)\n",
    "\n",
    "**Why This Order:**\n",
    "- Hyperparameter tuning â†’ optimize model performance\n",
    "- Visualizations â†’ understand how models work\n",
    "- Analysis â†’ interpret results and make decisions\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… How SVM Solves the Optimal Margin Problem | ÙƒÙŠÙ ÙŠØ­Ù„ SVM Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù‡Ø§Ù…Ø´ Ø§Ù„Ø£Ù…Ø«Ù„\n",
    "\n",
    "### ğŸ”— Addressing the Problem from Decision Trees Notebook | Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ù…Ù† Ø¯ÙØªØ± Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Recall from Decision Trees Notebook:**\n",
    "- âŒ Decision Trees can overfit (train 98%, test 77% - large gap!)\n",
    "- âŒ Decision Trees create boundaries based on training data locations, not optimal margins\n",
    "- âŒ Result: Poor generalization on unseen data\n",
    "\n",
    "**How SVM Solves This:**\n",
    "- âœ… **SVM finds maximum margin boundaries** (widest gap between classes)\n",
    "- âœ… **Small train-test gap** (87% train, 87% test - excellent generalization!)\n",
    "- âœ… **Optimal margin = robust boundaries** = better generalization on new data\n",
    "- âœ… **Result**: Better generalization than Decision Trees!\n",
    "\n",
    "**Key Difference:**\n",
    "- **Decision Trees**: Create boundaries based on training data locations â†’ can overfit\n",
    "- **SVM**: Finds boundaries with maximum margin â†’ better generalization\n",
    "\n",
    "**What We Observed:**\n",
    "- SVM kernels (Linear, RBF, Polynomial) all show **small train-test gaps** (0.001-0.004)\n",
    "- This indicates **excellent generalization** (no overfitting!)\n",
    "- Compare this to Decision Trees: train 98% vs test 77% (gap of 0.21 = overfitting!)\n",
    "\n",
    "**ğŸ¯ Takeaway:**\n",
    "- SVM's optimal margin approach leads to **better generalization** than Decision Trees\n",
    "- Small train-test gap = robust model = good for production use\n",
    "- This is why SVM is preferred when generalization is critical!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.698399Z",
     "iopub.status.busy": "2026-01-20T05:44:57.698347Z",
     "iopub.status.idle": "2026-01-20T05:44:57.700005Z",
     "shell.execute_reply": "2026-01-20T05:44:57.699813Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Hyperparameter Tuning - C Parameter\n",
    "# print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "# print(\"5. Hyperparameter Tuning - C Parameter\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª - Ù…Ø¹Ø§Ù…Ù„ C\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“Š Testing C parameter on RBF kernel (30 features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" C controls regularization: higher C = less regularization = more complex model\")\n",
    "\n",
    "# C_values = [0.01, 0.1, 1, 10, 100]\n",
    "# train_scores_c = []\n",
    "# test_scores_c = []\n",
    "\n",
    "# for C in C_values:\n",
    "#  svm_temp = SVC(kernel='rbf', C=C, gamma='scale', probability=True, random_state=73) \n",
    "# Using 73 \n",
    "# for consistencysvm_temp.fit(X_train_full_scaled, y_train)\n",
    "#  train_scores_c.append(accuracy_score(y_train, svm_temp.predict(X_train_full_scaled)))\n",
    "#  test_scores_c.append(accuracy_score(y_test, svm_temp.predict(X_test_full_scaled)))\n",
    "# C parameter tuning complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Best test accuracy: {max(test_scores_c):.4f} at C = {C_values[np.argmax(test_scores_c)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.700780Z",
     "iopub.status.busy": "2026-01-20T05:44:57.700733Z",
     "iopub.status.idle": "2026-01-20T05:44:57.702407Z",
     "shell.execute_reply": "2026-01-20T05:44:57.702243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # 6))\n",
    "# # plt.semilogx(C_values, train_scores_c, 'o-', label='Training Accuracy', linewidth=2)\n",
    "# # plt.semilogx(C_values, test_scores_c, 's-', label='Test Accuracy', linewidth=2)\n",
    "# # plt.xlabel('C (Regularization Parameter)')\n",
    "# # plt.ylabel('Accuracy')\n",
    "# # plt.title('SVM: C Parameter vs Performance')\n",
    "# plt.legend()\n",
    "# # plt.grid(True, alpha=0.3)\n",
    "# # plt.tight_layout()\n",
    "# # plt.savefig('svm_c_parameter.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nâœ“ Plot saved as 'svm_c_parameter.png'\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"ğŸ’¡ Interpreting the C Parameter Plot | ØªÙØ³ÙŠØ± Ù…Ø®Ø·Ø· Ù…Ø¹Ø§Ù…Ù„ C\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ“Š What You Should See:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Training accuracy increases as C increases (higher C = less regularization)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Test accuracy may increase then decrease (optimal C value exists)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Gap between train and test = overfitting indicator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ” Key Observations:\")\n",
    "# # best_c_idx = np.argmax(test_scores_c)\n",
    "# # best_c = C_values[best_c_idx]\n",
    "# # print(f\" - Best C value: {best_c} (test accuracy: {max(test_scores_c):.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - At C={best_c}, we get best generalization (highest test accuracy)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Very high C (100, 1000): May overfit (large train-test gap)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Very low C (0.01, 0.1): May underfit (both accuracies low)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ’¡ For GDI Cyber Threat Detection:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Choose C that maximizes test accuracy (best attack detection)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Balance between detecting attacks and avoiding false alarms\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Optimal C ensures robust model for real-world deployment\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.703256Z",
     "iopub.status.busy": "2026-01-20T05:44:57.703185Z",
     "iopub.status.idle": "2026-01-20T05:44:57.704615Z",
     "shell.execute_reply": "2026-01-20T05:44:57.704458Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6. Hyperparameter Tuning - Gamma Parameter\n",
    "# print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "# print(\"6. Hyperparameter Tuning - Gamma Parameter\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª - Ù…Ø¹Ø§Ù…Ù„ Gamma\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“Š Testing gamma parameter on RBF kernel (30 features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" Gamma controls kernel influence: higher gamma = more complex boundaries\")\n",
    "\n",
    "# gamma_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "# train_scores_gamma = []\n",
    "# test_scores_gamma = []\n",
    "\n",
    "# for gamma in gamma_values:\n",
    "#  svm_temp = SVC(kernel='rbf', C=1.0, gamma=gamma, probability=True, random_state=73) \n",
    "# Using 73 \n",
    "# for consistencysvm_temp.fit(X_train_full_scaled, y_train)\n",
    "#  train_scores_gamma.append(accuracy_score(y_train, svm_temp.predict(X_train_full_scaled)))\n",
    "#  test_scores_gamma.append(accuracy_score(y_test, svm_temp.predict(X_test_full_scaled)))\n",
    "# Gamma parameter tuning complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Best test accuracy: {max(test_scores_gamma):.4f} at gamma = {gamma_values[np.argmax(test_scores_gamma)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.705351Z",
     "iopub.status.busy": "2026-01-20T05:44:57.705301Z",
     "iopub.status.idle": "2026-01-20T05:44:57.706971Z",
     "shell.execute_reply": "2026-01-20T05:44:57.706812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # 6))\n",
    "# # plt.semilogx(gamma_values, train_scores_gamma, 'o-', label='Training Accuracy', linewidth=2)\n",
    "# # plt.semilogx(gamma_values, test_scores_gamma, 's-', label='Test Accuracy', linewidth=2)\n",
    "# # plt.xlabel('Gamma')\n",
    "# # plt.ylabel('Accuracy')\n",
    "# # plt.title('SVM: Gamma Parameter vs Performance')\n",
    "# plt.legend()\n",
    "# # plt.grid(True, alpha=0.3)\n",
    "# # plt.tight_layout()\n",
    "# # plt.savefig('svm_gamma_parameter.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nâœ“ Plot saved as 'svm_gamma_parameter.png'\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"ğŸ’¡ Interpreting the Gamma Parameter Plot | ØªÙØ³ÙŠØ± Ù…Ø®Ø·Ø· Ù…Ø¹Ø§Ù…Ù„ Gamma\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ“Š What You Should See:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Training accuracy increases as gamma increases (higher gamma = more complex boundaries)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Test accuracy may increase then decrease (optimal gamma value exists)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Large gap between train and test = overfitting (gamma too high)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ” Key Observations:\")\n",
    "# # best_gamma_idx = np.argmax(test_scores_gamma)\n",
    "# # best_gamma = gamma_values[best_gamma_idx]\n",
    "# # print(f\" - Best gamma value: {best_gamma} (test accuracy: {max(test_scores_gamma):.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - At gamma={best_gamma}, we get best generalization (highest test accuracy)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Very high gamma (10, 100): May overfit (complex boundaries, large train-test gap)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Very low gamma (0.001, 0.01): May underfit (smooth boundaries, both accuracies low)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ’¡ For GDI Cyber Threat Detection:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Choose gamma that maximizes test accuracy (best attack pattern detection)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Balance between capturing complex attack patterns and avoiding overfitting\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Optimal gamma ensures model generalizes to new cyber threats\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.707868Z",
     "iopub.status.busy": "2026-01-20T05:44:57.707819Z",
     "iopub.status.idle": "2026-01-20T05:44:57.709943Z",
     "shell.execute_reply": "2026-01-20T05:44:57.709769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # 7. Decision Boundaries Visualization\n",
    "# # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "# # print(\"7. Decision Boundaries Visualization (2D for visualization)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"ØªØµÙˆØ± Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù‚Ø±Ø§Ø± (2D Ù„Ù„ØªØµÙˆØ±)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nâš ï¸ Note: Training separate 2D models for visualization only!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" Real models above use 30 features (better performance)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" These 2D models show decision boundary shapes in 2D space\\n\")\n",
    "\n",
    "# # Train models on 2D data \n",
    "# # for visualizationsvm_linear_2d = SVC(kernel='linear', C=1.0, probability=True, random_state=73)\n",
    "# # Using 73 \n",
    "# # for consistencysvm_linear_2d.fit(X_train_2d_scaled, y_train)\n",
    "\n",
    "# # svm_rbf_2d = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=73) \n",
    "# # Using 73 \n",
    "# # for consistencysvm_rbf_2d.fit(X_train_2d_scaled, y_train)\n",
    "\n",
    "# # svm_poly_2d = SVC(kernel='poly', degree=3, C=1.0, probability=True, random_state=73) \n",
    "# # Using 73 \n",
    "# # for consistencysvm_poly_2d.fit(X_train_2d_scaled, y_train)\n",
    "\n",
    "# # def plot_decision_boundary(X, y, model, title, ax):\n",
    "# #  \"\"\"Plot decision boundary for 2D data\"\"\"\n",
    "# #  h = 0.02x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "#  np.arange(y_min, y_max, h))\n",
    "# #  Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# #  Z = Z.reshape(xx.shape)\n",
    "# #  ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "# #  scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu',\n",
    "# #  edgecolors='black', s=50)\n",
    "# #  ax.set_xlabel('PC1 (Principal Component 1)')\n",
    "# #  ax.set_ylabel('PC2 (Principal Component 2)')\n",
    "#  ax.set_title(title)\n",
    "\n",
    "# # fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "# # Use 2D models trained on PCA-reduced data \n",
    "# # for visualizationmodels_for_plot = [\n",
    "# #  (svm_linear_2d, 'Linear SVM (2D)', X_train_2d_scaled),\n",
    "# #  (svm_rbf_2d, 'RBF Kernel SVM (2D)', X_train_2d_scaled),\n",
    "# #  (svm_poly_2d, 'Polynomial Kernel SVM (2D)', X_train_2d_scaled)\n",
    "# ]\n",
    "\n",
    "# # for idx, (model, title, X_plot) in enumerate(models_for_plot):\n",
    "# #  plot_decision_boundary(X_plot, y_train, model, title, axes[idx])\n",
    "\n",
    "# # plt.tight_layout()\n",
    "# # plt.savefig('svm_decision_boundaries.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nâœ“ Plot saved as 'svm_decision_boundaries.png'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"âš ï¸ Remember: These are 2D visualizations. Real models use all 30 features!\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"ğŸ’¡ Interpreting Decision Boundaries | ØªÙØ³ÙŠØ± Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù‚Ø±Ø§Ø±\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ“Š What You Should See (3 plots):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\n1. Linear SVM (Left Plot):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Straight line boundary separating the two classes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Simple, interpretable decision boundary\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Works well\")\n",
    "\n",
    "\n",
    "\n",
    "# # if data is linearly separable in 2D space\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\n2. RBF Kernel SVM (Middle Plot):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Curved, smooth boundary separating the classes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Can create complex, non-linear boundaries\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Handles non-linear patterns that Linear SVM cannot\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\n3. Polynomial Kernel SVM (Right Plot):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Polynomial-shaped boundary\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Can capture polynomial relationships in data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - More complex than linear, different from RBF\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ” Key Observations:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Different kernels create different boundary shapes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - RBF creates smooth curves (most versatile)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Linear creates straight lines (simplest)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Polynomial creates polynomial curves (specific patterns)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ’¡ For GDI Cyber Threat Detection:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Real attack patterns are complex and non-linear\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - RBF kernel's curved boundaries better match real attack patterns\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - These 2D plots show the concept - real models use all 30 network features!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - In production, decision boundaries exist in 30D space (much more complex!)\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.710621Z",
     "iopub.status.busy": "2026-01-20T05:44:57.710574Z",
     "iopub.status.idle": "2026-01-20T05:44:57.711985Z",
     "shell.execute_reply": "2026-01-20T05:44:57.711831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ØªØ­Ù„ÙŠÙ„ Ù†Ø§Ù‚Ù„Ø§Øª Ø§Ù„Ø¯Ø¹Ù…\n"
     ]
    }
   ],
   "source": [
    "# 8. Support Vectors Analysis\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"8. Support Vectors Analysis\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ØªØ­Ù„ÙŠÙ„ Ù†Ø§Ù‚Ù„Ø§Øª Ø§Ù„Ø¯Ø¹Ù…\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“Š Support vectors from models trained on 30 features:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Linear SVM: {len(svm_linear.support_vectors_)} support vectors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" RBF Kernel: {len(svm_rbf.support_vectors_)} support vectors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Polynomial: {len(svm_poly.support_vectors_)} support vectors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ’¡ Interpretation:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Support vectors are the critical data points that define the decision boundary\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Fewer support vectors = simpler model (more efficient)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - RBF often uses fewer support vectors than linear for complex boundaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.712698Z",
     "iopub.status.busy": "2026-01-20T05:44:57.712652Z",
     "iopub.status.idle": "2026-01-20T05:44:57.714377Z",
     "shell.execute_reply": "2026-01-20T05:44:57.714230Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # 8))\n",
    "        # plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1],\n",
    "        #  c=y_train, cmap='RdYlBu', alpha=0.5, s=50, edgecolors='black')\n",
    "        # plt.scatter(svm_rbf_2d.support_vectors_[:, 0], svm_rbf_2d.support_vectors_[:, 1],\n",
    "        #  facecolors='none', edgecolors='red', s=200, linewidths=2,\n",
    "        #  label='Support Vectors')\n",
    "        # plt.xlabel('PC1 (Principal Component 1)')\n",
    "        # plt.ylabel('PC2 (Principal Component 2)')\n",
    "        # plt.title('Support Vectors (RBF Kernel)')\n",
    "        plt.legend()\n",
    "        # plt.grid(True, alpha=0.3)\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig('support_vectors.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nâœ“ Plot saved as 'support_vectors.png'\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"ğŸ’¡ Interpreting Support Vectors | ØªÙØ³ÙŠØ± Ù†Ø§Ù‚Ù„Ø§Øª Ø§Ù„Ø¯Ø¹Ù…\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ“Š What You Should See:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Blue/Yellow points: All training data points (Benign=Blue, Attack=Yellow)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Red circles: Support vectors (critical data points)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Support vectors are located near the decision boundary\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ” Key Observations:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Total support vectors: {len(svm_rbf_2d.support_vectors_)} out of {len(X_train_2d_scaled)} training points\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Percentage: {len(svm_rbf_2d.support_vectors_)/len(X_train_2d_scaled)*100:.1f}% of data points are support vectors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Only these points matter for the decision boundary!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Other points can be ignored (SVM is memory efficient)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ’¡ Why Support Vectors Matter:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - They define the decision boundary (the 'hard' examples)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Fewer support vectors = simpler\")\n",
    "\n",
    "        # model = faster predictions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - More support vectors = more complex boundary = better fit\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Balance: Enough to capture patterns, not too many to overfit\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ’¡ For GDI Cyber Threat Detection:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Support vectors represent 'borderline' network flows\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - These are the ambiguous cases (not clearly benign or attack)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Model focuses on these critical cases to make decisions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - In production, monitoring these support vectors helps understand model behavior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.715074Z",
     "iopub.status.busy": "2026-01-20T05:44:57.715030Z",
     "iopub.status.idle": "2026-01-20T05:44:57.716267Z",
     "shell.execute_reply": "2026-01-20T05:44:57.716122Z"
    }
   },
   "outputs": [],
   "source": [
    "# 9. ROC Curves Comparison\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"9. ROC Curves Comparison\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ù†Ø­Ù†ÙŠØ§Øª ROC\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.717138Z",
     "iopub.status.busy": "2026-01-20T05:44:57.717080Z",
     "iopub.status.idle": "2026-01-20T05:44:57.718897Z",
     "shell.execute_reply": "2026-01-20T05:44:57.718738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Calculate ROC curves (using models trained on 30 features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ“Š Calculating ROC curves from models trained on 30 features:\")\n",
    "\n",
    "# # y_test_proba_linear = svm_linear.predict_proba(X_test_full_scaled)[:, 1]\n",
    "# # y_test_proba_poly = svm_poly.predict_proba(X_test_full_scaled)[:, 1]\n",
    "# # y_test_proba_rbf already calculated abovefpr_linear, tpr_linear, \n",
    "# # _ = roc_curve(y_test, y_test_proba_linear)\n",
    "# # fpr_rbf, tpr_rbf, _ = roc_curve(y_test, y_test_proba_rbf)\n",
    "# # fpr_poly, tpr_poly, _ = roc_curve(y_test, y_test_proba_poly)\n",
    "\n",
    "# # auc_linear = roc_auc_score(y_test, y_test_proba_linear)\n",
    "# # auc_rbf = roc_auc_score(y_test, y_test_proba_rbf)\n",
    "# # auc_poly = roc_auc_score(y_test, y_test_proba_poly)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nAUC Scores:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"Linear: {auc_linear:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"RBF: {auc_rbf:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"Polynomial: {auc_poly:.4f}\")\n",
    "\n",
    "# # Add interpretation\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"ğŸ’¡ Interpreting AUC Scores | ØªÙØ³ÙŠØ± Ø¯Ø±Ø¬Ø§Øª AUC\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "# # auc_scores = {\n",
    "# #  'Linear': auc_linear, 'RBF': auc_rbf,\n",
    "# #  'Polynomial': auc_poly\n",
    "# }\n",
    "\n",
    "# # best_auc_kernel = max(auc_scores, key=auc_scores.get)\n",
    "# # best_auc = auc_scores[best_auc_kernel]\n",
    "\n",
    "# # print(f\"\\nğŸ“Š Best AUC: {best_auc_kernel} ({best_auc:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - This kernel has the best ability to distinguish classes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nğŸ” AUC Quality Assessment:\")\n",
    "\n",
    "# # for kernel, score in auc_scores.items():\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# # if score >= 0.9:\n",
    "# #  quality = \"âœ… EXCELLENT\"\n",
    "# #  elif score >= 0.8:\n",
    "# #  quality = \"âœ… GOOD\"\n",
    "# #  elif score >= 0.7:\n",
    "# #  quality = \"âš ï¸ FAIR\"\n",
    "# #  elif score >= 0.5:\n",
    "# #  quality = \"âš ï¸ POOR\"\n",
    "# #  else:\n",
    "# #  quality = \"âŒ VERY POOR\"\n",
    "# #  print(f\" - {kernel}: {score:.4f} ({quality})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # if auc_rbf >= 0.90:\n",
    "# #  print(f\"\\nâœ… Excellent AUC Performance:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - RBF has excellent AUC ({auc_rbf:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - This indicates:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" â€¢ Very good class separation on this dataset\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" â€¢ Real cybersecurity data with 30 network flow features can achieve high performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" â€¢ Proper feature scaling and kernel selection are working well\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" â€¢ GDI Application: Excellent for cyber threat detection!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - AUC measures model's ability to separate classes (0-1 scale)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Higher AUC = better at distinguishing between classes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - AUC > 0.9 is excellent, >0.8 is good, >0.7 is fair\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Perfect AUC (1.0) is suspicious - check for overfitting\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Compare AUC with accuracy to get full picture\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - RBF kernel often has best AUC for non-linear data\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â“ Common Student Questions | Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„Ø·Ù„Ø§Ø¨\n",
    "\n",
    "**Q: Why does SVM need feature scaling?**\n",
    "- **Answer**: SVM is distance-based (finds optimal margin):\n",
    "  - **Problem**: Features on different scales (age: 0-100, income: 0-100000) â†’ distance calculations biased\n",
    "  - **Issue**: Features with larger scales dominate distance calculations\n",
    "  - **Solution**: Scale all features to same range (StandardScaler) â†’ fair distance calculations\n",
    "  - **Rule**: ALWAYS scale features before SVM (unlike Decision Trees which don't need scaling)\n",
    "\n",
    "**Q: What's the difference between C and gamma?**\n",
    "- **Answer**: \n",
    "  - **C (regularization)**: Controls how much to penalize misclassifications\n",
    "    - **High C (100, 1000)**: Less regularization, tries to classify all points correctly (can overfit)\n",
    "    - **Low C (0.1, 0.01)**: More regularization, allows some misclassifications (more robust)\n",
    "  - **Gamma (kernel influence)**: Controls how far the influence of a single training example reaches\n",
    "    - **High gamma**: Close influence (complex boundaries, can overfit)\n",
    "    - **Low gamma**: Far influence (smooth boundaries, more general)\n",
    "  - **Rule of thumb**: Start with C=1, gamma='scale', then tune both\n",
    "\n",
    "**Q: When should I use Linear vs RBF kernel?**\n",
    "- **Answer**: \n",
    "  - **Use Linear**: Data is linearly separable, large dataset, need fast training\n",
    "  - **Use RBF**: Non-linear patterns, unknown pattern, need smooth boundaries\n",
    "  - **Try both**: Start with Linear, if performance poor â†’ try RBF\n",
    "  - **Default**: RBF is good default when pattern is unknown\n",
    "\n",
    "**Q: What are support vectors?**\n",
    "- **Answer**: Support vectors are the critical data points that define the decision boundary:\n",
    "  - **Definition**: Data points closest to the decision boundary (on the margin)\n",
    "  - **Why they matter**: Only these points matter for the boundary (others can be ignored)\n",
    "  - **Interpretation**: Support vectors are the \"hard\" examples that determine the boundary\n",
    "  - **Use**: Visualize support vectors to understand which examples are critical\n",
    "\n",
    "**Q: Why is SVM slower than other algorithms?**\n",
    "- **Answer**: SVM needs to find optimal margin (optimization problem):\n",
    "  - **Problem**: Finding maximum margin is computationally expensive\n",
    "  - **Issue**: Especially slow with RBF kernel on large datasets\n",
    "  - **Solution**: Use Linear kernel for large datasets, or use other algorithms (Random Forest, XGBoost)\n",
    "  - **Trade-off**: SVM is slower but often gives excellent results\n",
    "\n",
    "**Q: Can SVM handle multi-class classification?**\n",
    "- **Answer**: **Yes!** SVM handles multi-class using one-vs-rest or one-vs-one:\n",
    "  - **One-vs-Rest**: Each class vs all others (K classifiers for K classes)\n",
    "  - **One-vs-One**: Each pair of classes (K(K-1)/2 classifiers)\n",
    "  - **sklearn default**: One-vs-rest (automatically handled)\n",
    "  - **Example**: 3 classes â†’ 3 binary classifiers (class 0 vs rest, class 1 vs rest, class 2 vs rest)\n",
    "\n",
    "**Q: What's the difference between SVM and Logistic Regression?**\n",
    "- **Answer**: \n",
    "  - **SVM**: Finds optimal margin boundary, can use kernels for non-linear, slower\n",
    "  - **Logistic Regression**: Finds probability-based boundary, linear only, faster\n",
    "  - **Use SVM**: When you need optimal margin, non-linear patterns, best performance\n",
    "  - **Use Logistic Regression**: When you need probabilities, linear patterns, fast training\n",
    "  - **Try both**: Compare performance, pick the one that works better\n",
    "\n",
    "**Q: How do I choose C and gamma values?**\n",
    "- **Answer**: Use grid search (Unit 5, Example 1) or try common values:\n",
    "  - **C values**: Try 0.1, 1, 10, 100, 1000\n",
    "  - **Gamma values**: Try 'scale', 'auto', 0.001, 0.01, 0.1, 1\n",
    "  - **Method**: Try combinations, pick the one with best cross-validation score\n",
    "  - **Rule of thumb**: Start with C=1, gamma='scale', then tune based on performance\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Decision Framework - Kernel Selection for SVM | Ø§Ù„Ø®Ø·ÙˆØ© 8: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†ÙˆØ§Ø© Ù„Ù€ SVM\n",
    "\n",
    "**BEFORE**: You've learned how to use different SVM kernels, but when should you use each one?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose the right kernel for any classification problem!\n",
    "\n",
    "**Why this matters**: Using the wrong kernel can:\n",
    "- **Poor performance** â†’ Wrong kernel can't capture the patterns\n",
    "- **Overfitting** â†’ Complex kernels may overfit on small data\n",
    "- **Wasted computation** â†’ Using expensive kernels when simple ones work\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework: Which Kernel to Use? | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±: Ø£ÙŠ Ù†ÙˆØ§Ø© ØªØ³ØªØ®Ø¯Ù…ØŸ\n",
    "\n",
    "**Key Question**: Should I use **LINEAR**, **RBF**, **POLYNOMIAL**, or another kernel?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "What type of problem do you have?\n",
    "â”œâ”€ REGRESSION â†’ Use SVR (Support Vector Regression) with appropriate kernel\n",
    "â”‚   â””â”€ Why? SVM for regression uses same kernels\n",
    "â”‚\n",
    "â””â”€ CLASSIFICATION â†’ Check data characteristics:\n",
    "    â”œâ”€ Linear decision boundary? â†’ Use LINEAR kernel âœ…\n",
    "    â”‚   â””â”€ Why? Simple, fast, interpretable\n",
    "    â”‚\n",
    "    â”œâ”€ Non-linear, smooth curves? â†’ Use RBF kernel âœ…\n",
    "    â”‚   â””â”€ Why? Handles smooth non-linear patterns well\n",
    "    â”‚\n",
    "    â”œâ”€ Non-linear, polynomial patterns? â†’ Use POLYNOMIAL kernel âš ï¸\n",
    "    â”‚   â””â”€ Why? Can capture polynomial relationships\n",
    "    â”‚\n",
    "    â””â”€ Unknown pattern? â†’ Start with RBF kernel âœ…\n",
    "        â””â”€ Why? Most versatile, good default\n",
    "```\n",
    "\n",
    "#### Detailed Decision Process:\n",
    "\n",
    "```\n",
    "Step 1: Visualize the data\n",
    "â”œâ”€ Plot scatter plot or decision boundary\n",
    "â”‚\n",
    "â””â”€ What pattern do you see?\n",
    "    â”œâ”€ Straight line separation â†’ LINEAR kernel\n",
    "    â”œâ”€ Smooth curves â†’ RBF kernel\n",
    "    â”œâ”€ Polynomial patterns â†’ POLYNOMIAL kernel\n",
    "    â””â”€ Unknown â†’ Try RBF first (most versatile)\n",
    "\n",
    "Step 2: Try Linear kernel first\n",
    "â”œâ”€ Build SVM with linear kernel\n",
    "â”œâ”€ Check performance\n",
    "â”‚\n",
    "â””â”€ Is performance good enough?\n",
    "    â”œâ”€ YES (accuracy > 0.85) â†’ Use LINEAR kernel âœ…\n",
    "    â””â”€ NO (accuracy < 0.85) â†’ Try RBF kernel\n",
    "\n",
    "Step 3: Try RBF kernel\n",
    "â”œâ”€ Build SVM with RBF kernel\n",
    "â”œâ”€ Tune C and gamma\n",
    "â”œâ”€ Check performance\n",
    "â”‚\n",
    "â””â”€ Is performance good enough?\n",
    "    â”œâ”€ YES â†’ Use RBF kernel âœ…\n",
    "    â””â”€ NO â†’ Try Polynomial kernel or other methods\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Comparison Table: SVM Kernels | Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
    "\n",
    "| Kernel | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Linear** | Linear boundaries, large datasets, fast | â€¢ Fast<br>â€¢ Interpretable<br>â€¢ Less overfitting<br>â€¢ Good for large data | â€¢ Can't handle non-linear<br>â€¢ Limited to linear patterns | Linearly separable data, text classification |\n",
    "| **RBF (Radial Basis Function)** | Non-linear, smooth curves, default choice | â€¢ Handles non-linear<br>â€¢ Versatile<br>â€¢ Smooth boundaries<br>â€¢ Good default | â€¢ Requires tuning (C, gamma)<br>â€¢ Slower than linear<br>â€¢ Can overfit | Most non-linear problems, default choice |\n",
    "| **Polynomial** | Polynomial patterns, specific relationships | â€¢ Handles polynomial patterns<br>â€¢ Flexible degree | â€¢ More hyperparameters<br>â€¢ Can overfit<br>â€¢ Slower | Polynomial relationships, specific patterns |\n",
    "| **Sigmoid** | Neural network-like, rarely used | â€¢ Similar to neural networks | â€¢ Rarely better than RBF<br>â€¢ Less common | Special cases, rarely used |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When to Use Each Kernel | Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ Ù†ÙˆØ§Ø©\n",
    "\n",
    "#### Use Linear Kernel when:\n",
    "1. **Linear Decision Boundary** âœ…\n",
    "   - Data is linearly separable\n",
    "   - Straight line can separate classes\n",
    "   - **Example**: Text classification, linearly separable data\n",
    "\n",
    "2. **Large Dataset** âœ…\n",
    "   - More than 10,000 samples\n",
    "   - Linear is faster\n",
    "   - **Example**: Large-scale text classification\n",
    "\n",
    "3. **Interpretability Needed** âœ…\n",
    "   - Need to understand decision boundary\n",
    "   - Linear is more interpretable\n",
    "   - **Example**: Cyber threat detection with linear patterns (network flow features)\n",
    "\n",
    "4. **Fast Training Needed** âœ…\n",
    "   - Need quick results\n",
    "   - Linear is fastest\n",
    "   - **Example**: Real-time classification\n",
    "\n",
    "#### Use RBF Kernel when:\n",
    "1. **Non-Linear Patterns** âœ…\n",
    "   - Data has curves, non-linear boundaries\n",
    "   - Linear kernel fails\n",
    "   - **Example**: Most real-world classification problems\n",
    "\n",
    "2. **Unknown Pattern** âœ…\n",
    "   - Don't know the pattern type\n",
    "   - RBF is good default\n",
    "   - **Example**: First attempt at new problem\n",
    "\n",
    "3. **Smooth Boundaries** âœ…\n",
    "   - Need smooth, curved decision boundaries\n",
    "   - RBF creates smooth curves\n",
    "   - **Example**: Image classification, complex patterns\n",
    "\n",
    "4. **Small to Medium Dataset** âœ…\n",
    "   - Less than 10,000 samples\n",
    "   - RBF works well\n",
    "   - **Example**: Most classification problems\n",
    "\n",
    "#### Use Polynomial Kernel when:\n",
    "1. **Polynomial Relationships** âœ…\n",
    "   - Data has polynomial patterns (xÂ², xÂ³ relationships)\n",
    "   - RBF may not capture well\n",
    "   - **Example**: Specific polynomial patterns\n",
    "\n",
    "2. **Specific Domain Knowledge** âœ…\n",
    "   - Know the relationship is polynomial\n",
    "   - **Example**: Physics-based problems\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ When NOT to Use Each Kernel | Ù…ØªÙ‰ Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ Ù†ÙˆØ§Ø©\n",
    "\n",
    "#### Don't use Linear Kernel when:\n",
    "1. **Non-Linear Patterns** âŒ\n",
    "   - Data has curves, complex boundaries\n",
    "   - **Use Instead**: RBF or Polynomial kernel\n",
    "\n",
    "2. **Poor Performance** âŒ\n",
    "   - Linear accuracy < 0.80\n",
    "   - **Use Instead**: RBF kernel\n",
    "\n",
    "#### Don't use RBF Kernel when:\n",
    "1. **Very Large Dataset** âŒ\n",
    "   - More than 100,000 samples\n",
    "   - RBF is slow\n",
    "   - **Use Instead**: Linear kernel or other methods\n",
    "\n",
    "2. **Linear Patterns** âŒ\n",
    "   - Data is linearly separable\n",
    "   - **Use Instead**: Linear kernel (simpler, faster)\n",
    "\n",
    "#### Don't use Polynomial Kernel when:\n",
    "1. **Unknown Patterns** âŒ\n",
    "   - Don't know if polynomial\n",
    "   - **Use Instead**: RBF kernel (more versatile)\n",
    "\n",
    "2. **Small Dataset** âŒ\n",
    "   - Less than 100 samples\n",
    "   - Polynomial may overfit\n",
    "   - **Use Instead**: RBF or Linear kernel\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: Text Classification âœ… LINEAR KERNEL\n",
    "- **Problem**: Classify documents (spam/not spam)\n",
    "- **Pattern**: Linear (word frequencies linearly related)\n",
    "- **Dataset**: Large (50,000 documents)\n",
    "- **Decision**: âœ… Use Linear Kernel\n",
    "- **Reasoning**: Linear patterns, large dataset, fast training needed\n",
    "\n",
    "#### Example 2: Image Classification âœ… RBF KERNEL\n",
    "- **Problem**: Classify images (cat/dog)\n",
    "- **Pattern**: Non-linear (complex pixel patterns)\n",
    "- **Dataset**: Medium (5,000 images)\n",
    "- **Decision**: âœ… Use RBF Kernel\n",
    "- **Reasoning**: Non-linear patterns, smooth boundaries needed, medium dataset\n",
    "\n",
    "#### Example 3: Customer Segmentation âœ… RBF KERNEL\n",
    "- **Problem**: Segment customers (high/medium/low value)\n",
    "- **Pattern**: Unknown, likely non-linear\n",
    "- **Dataset**: Medium (2,000 customers)\n",
    "- **Decision**: âœ… Use RBF Kernel (start here)\n",
    "- **Reasoning**: Unknown pattern, RBF is good default, can tune C and gamma\n",
    "\n",
    "#### Example 4: Cyber Threat Detection (Linear) âœ… LINEAR KERNEL\n",
    "- **Problem**: Detect cyber attacks from network flow features\n",
    "- **Pattern**: Linear (network features linearly related to attack)\n",
    "- **Dataset**: Medium (10,000 network flows)\n",
    "- **Decision**: âœ… Use Linear Kernel\n",
    "- **Reasoning**: Linear patterns, interpretability important for GDI operations, medium dataset\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Start with Linear** - Always try linear kernel first (simplest)\n",
    "2. **Non-linear â†’ RBF** - Use RBF for non-linear patterns (most versatile)\n",
    "3. **RBF is default** - Good default choice when pattern is unknown\n",
    "4. **Tune C and gamma** - Critical hyperparameters for RBF\n",
    "5. **Large data â†’ Linear** - Linear is faster for large datasets\n",
    "6. **Visualize boundaries** - Plot decision boundaries to see pattern\n",
    "7. **Compare kernels** - Try multiple kernels, pick the best\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario 1**: Classifying emails as spam (50,000 emails)\n",
    "- **Pattern**: Linear (word frequencies)\n",
    "- **Dataset**: Large (50,000)\n",
    "- **Decision**: âœ… Linear Kernel (linear patterns, large dataset, fast)\n",
    "\n",
    "**Scenario 2**: Classifying images (cat/dog, 3,000 images)\n",
    "- **Pattern**: Non-linear (complex pixel patterns)\n",
    "- **Dataset**: Medium (3,000)\n",
    "- **Decision**: âœ… RBF Kernel (non-linear patterns, smooth boundaries)\n",
    "\n",
    "**Scenario 3**: Customer churn prediction (unknown pattern, 1,000 customers)\n",
    "- **Pattern**: Unknown\n",
    "- **Dataset**: Medium (1,000)\n",
    "- **Decision**: âœ… RBF Kernel (unknown pattern, good default, can tune)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Unit 5, Example 1: Grid Search** - For tuning C and gamma hyperparameters\n",
    "- ğŸ““ **Unit 3, Example 2: Decision Trees** - Alternative for non-linear patterns\n",
    "- ğŸ““ **Unit 5, Example 2: Boosting** - For best performance on complex problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:57.719570Z",
     "iopub.status.busy": "2026-01-20T05:44:57.719528Z",
     "iopub.status.idle": "2026-01-20T05:44:57.721264Z",
     "shell.execute_reply": "2026-01-20T05:44:57.721104Z"
    }
   },
   "outputs": [],
   "source": [
    "# 8))\n",
    "# plt.plot(fpr_linear, tpr_linear, linewidth=2, label=f'Linear (AUC = {auc_linear:.4f})')\n",
    "# plt.plot(fpr_rbf, tpr_rbf, linewidth=2, label=f'RBF (AUC = {auc_rbf:.4f})')\n",
    "# plt.plot(fpr_poly, tpr_poly, linewidth=2, label=f'Polynomial (AUC = {auc_poly:.4f})')\n",
    "# plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('SVM ROC Curves Comparison')\n",
    "if 'plt' in globals():\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('svm_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nâœ“ Plot saved as 'svm_roc_curves.png'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"ğŸ’¡ Interpreting ROC Curves | ØªÙØ³ÙŠØ± Ù…Ù†Ø­Ù†ÙŠØ§Øª ROC\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“Š What You Should See:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Three colored lines: Linear (blue), RBF (orange), Polynomial (green)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Diagonal dashed line: Random classifier (AUC = 0.5, worst possible)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Curves above diagonal = better than random\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Curves closer to top-left corner = better performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ” Key Observations:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - RBF curve is highest (best AUC = 0.9556) â†’ Best at distinguishing classes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Linear curve is close (AUC = 0.9535) â†’ Also excellent\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Polynomial curve is slightly lower (AUC = 0.9522) â†’ Still excellent\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - All curves are far above diagonal â†’ All kernels perform well!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ’¡ How to Read ROC Curves:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - X-axis (False Positive Rate): Rate of false alarms\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Y-axis (True Positive Rate): Rate of correctly detected attacks\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Top-left corner = perfect (100% detection, 0% false alarms)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Diagonal line = random guessing (50% detection, 50% false alarms)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Higher curve = better at balancing detection vs false alarms\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ’¡ For GDI Cyber Threat Detection:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - ROC curves show model's ability to detect attacks while minimizing false alarms\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - All three kernels achieve excellent performance (AUC > 0.95)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - RBF kernel has slight edge (best AUC) for this cybersecurity dataset\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - In production, choose kernel with best AUC for most reliable threat detection\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Example 3 Complete! âœ“\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 3! âœ“\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
