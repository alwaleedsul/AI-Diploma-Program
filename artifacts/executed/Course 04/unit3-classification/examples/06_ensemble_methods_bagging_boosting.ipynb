{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Techniques: Bagging and Boosting\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand ensemble methods (Bagging, Boosting)\n",
    "- Experiment with ensemble techniques to improve model performance\n",
    "- Compare bagging vs boosting approaches\n",
    "- Apply ensemble methods to classification problems\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of decision trees and classification\n",
    "- âœ… Python 3.8+ installed\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 04, Unit 3**:\n",
    "- Experimenting with ensemble techniques (Bagging, Boosting) to improve prediction accuracy\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 3 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Ensemble Methods\n",
    "\n",
    "**Ensemble methods** combine multiple models to improve performance:\n",
    "- **Bagging**: Parallel training, reduces variance (e.g., Random Forest)\n",
    "- **Boosting**: Sequential training, reduces bias (e.g., AdaBoost, Gradient Boosting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:05:03.100251Z",
     "iopub.status.busy": "2026-01-15T20:05:03.100101Z",
     "iopub.status.idle": "2026-01-15T20:05:03.844071Z",
     "shell.execute_reply": "2026-01-15T20:05:03.843875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:05:03.844941Z",
     "iopub.status.busy": "2026-01-15T20:05:03.844859Z",
     "iopub.status.idle": "2026-01-15T20:05:03.847372Z",
     "shell.execute_reply": "2026-01-15T20:05:03.847219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (800, 10), Test set: (200, 10)\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Base Model (Single Decision Tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:05:03.848184Z",
     "iopub.status.busy": "2026-01-15T20:05:03.848132Z",
     "iopub.status.idle": "2026-01-15T20:05:03.852901Z",
     "shell.execute_reply": "2026-01-15T20:05:03.852742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Base Model: Single Decision Tree\n",
      "============================================================\n",
      "Accuracy: 0.8400\n"
     ]
    }
   ],
   "source": [
    "# Base model: Single decision tree\n",
    "base_tree = DecisionTreeClassifier(random_state=42)\n",
    "base_tree.fit(X_train, y_train)\n",
    "base_pred = base_tree.predict(X_test)\n",
    "base_acc = accuracy_score(y_test, base_pred)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Base Model: Single Decision Tree\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {base_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bagging (Random Forest is a type of Bagging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:05:03.853600Z",
     "iopub.status.busy": "2026-01-15T20:05:03.853546Z",
     "iopub.status.idle": "2026-01-15T20:05:04.037787Z",
     "shell.execute_reply": "2026-01-15T20:05:04.037599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Bagging Classifier (100 trees)\n",
      "============================================================\n",
      "Accuracy: 0.8950\n",
      "Improvement over base: 0.0550\n"
     ]
    }
   ],
   "source": [
    "# Bagging Classifier\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "bagging_pred = bagging.predict(X_test)\n",
    "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Bagging Classifier (100 trees)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {bagging_acc:.4f}\")\n",
    "print(f\"Improvement over base: {bagging_acc - base_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Boosting Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:05:04.038641Z",
     "iopub.status.busy": "2026-01-15T20:05:04.038583Z",
     "iopub.status.idle": "2026-01-15T20:05:04.246457Z",
     "shell.execute_reply": "2026-01-15T20:05:04.246262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Boosting Methods:\n",
      "============================================================\n",
      "AdaBoost Accuracy: 0.8500\n",
      "Gradient Boosting Accuracy: 0.9000\n",
      "\n",
      "============================================================\n",
      "Comparison Summary:\n",
      "============================================================\n",
      "Base Tree:        0.8400\n",
      "Bagging:          0.8950 (+0.0550)\n",
      "AdaBoost:         0.8500 (+0.0100)\n",
      "Gradient Boost:   0.9000 (+0.0600)\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost\n",
    "adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "adaboost.fit(X_train, y_train)\n",
    "adaboost_pred = adaboost.predict(X_test)\n",
    "adaboost_acc = accuracy_score(y_test, adaboost_pred)\n",
    "\n",
    "# Gradient Boosting\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "gb_acc = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Boosting Methods:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"AdaBoost Accuracy: {adaboost_acc:.4f}\")\n",
    "print(f\"Gradient Boosting Accuracy: {gb_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Comparison Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Base Tree:        {base_acc:.4f}\")\n",
    "print(f\"Bagging:          {bagging_acc:.4f} (+{bagging_acc-base_acc:.4f})\")\n",
    "print(f\"AdaBoost:         {adaboost_acc:.4f} (+{adaboost_acc-base_acc:.4f})\")\n",
    "print(f\"Gradient Boost:   {gb_acc:.4f} (+{gb_acc-base_acc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Bagging**: Trains models in parallel, averages predictions (reduces variance)\n",
    "2. **Boosting**: Trains models sequentially, focuses on errors (reduces bias)\n",
    "3. **Ensemble Benefit**: Multiple models outperform single models\n",
    "4. **When to use**: Bagging for high variance models, Boosting for high bias models\n",
    "\n",
    "**Reference:** Course 04, Unit 3: \"Experimenting with ensemble techniques (Bagging, Boosting)\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
