<<<<<<< Current (Your changes)
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. K-Nearest Neighbors (KNN) | Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ø¬ÙŠØ±Ø§Ù† Ø§Ù„Ø£Ù‚Ø±Ø¨ (KNN)\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 04, Unit 3** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. K-Nearest Neighbors (KNN) | Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ø¬ÙŠØ±Ø§Ù† Ø§Ù„Ø£Ù‚Ø±Ø¨ (KNN)\n",
    "\n",
    "## ðŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Unit 3, Examples 1-3**: Logistic Regression, Decision Trees, and SVM\n",
    "- âœ… **Understanding of distance metrics**: Euclidean distance, Manhattan distance\n",
    "- âœ… **Understanding of feature scaling**: Why scaling matters for distance-based algorithms\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding how KNN makes predictions based on neighbors\n",
    "- Knowing why feature scaling is critical for KNN\n",
    "- Understanding how to choose the right K value\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is Unit 3, Example 4** - it introduces instance-based classification!\n",
    "\n",
    "**Why this example FOURTH in Unit 3?**\n",
    "- **Before** you can use KNN, you need to understand basic classification\n",
    "- **Before** you can choose K, you need to understand overfitting vs underfitting\n",
    "- **Before** you can use distance metrics, you need to understand feature scaling\n",
    "\n",
    "**Builds on**: \n",
    "- ðŸ““ Unit 3, Example 1: Logistic Regression (classification basics)\n",
    "- ðŸ““ Unit 3, Example 2: Decision Trees (overfitting concepts)\n",
    "- ðŸ““ Unit 3, Example 3: SVM (feature scaling importance)\n",
    "\n",
    "**Leads to**: \n",
    "- ðŸ““ Unit 4: Clustering (K-Means uses similar distance concepts)\n",
    "- ðŸ““ Unit 5: Model Selection (hyperparameter tuning for K)\n",
    "- ðŸ““ All instance-based learning algorithms\n",
    "\n",
    "**Why this order?**\n",
    "1. KNN is **simple to understand** (just find nearest neighbors)\n",
    "2. KNN demonstrates **lazy learning** (no training, just prediction)\n",
    "3. KNN shows **distance-based classification** (different from other methods)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Learning from Neighbors | Ø§Ù„Ù‚ØµØ©: Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø¬ÙŠØ±Ø§Ù†\n",
    "\n",
    "Imagine you're moving to a new neighborhood. **Before** KNN, you make decisions alone. **After** KNN, you look at your neighbors: \"What do most of my neighbors do? I'll do that too!\" - simple and effective!\n",
    "\n",
    "Same with machine learning: **Before** KNN, we train complex models. **After** KNN, we just find the nearest neighbors and predict based on them - simple but powerful!\n",
    "\n",
    "---\n",
    "\n",
    "## What is KNN? | Ù…Ø§ Ù‡Ùˆ KNNØŸ\n",
    "\n",
    "**K-Nearest Neighbors (KNN) is a classification algorithm that makes predictions by finding the K closest data points (neighbors) and using majority vote.**\n",
    "\n",
    "### How KNN Works | ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ KNN\n",
    "\n",
    "1. **Store All Data**: KNN stores all training data (no model training!)\n",
    "2. **For New Point**: When predicting, find the K nearest neighbors\n",
    "3. **Calculate Distances**: Measure distance to all training points\n",
    "4. **Select K Nearest**: Pick the K closest points\n",
    "5. **Vote**: Majority class of K neighbors = prediction\n",
    "\n",
    "### Simple Example | Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ·\n",
    "\n",
    "**Predicting if a transaction is fraud:**\n",
    "```\n",
    "New Transaction: [Amount=$500, Time=1200, ...]\n",
    "\n",
    "Step 1: Calculate distances to all training transactions\n",
    "Step 2: Find 5 nearest neighbors (K=5)\n",
    "  - Neighbor 1: Fraud âœ… (distance: 0.5)\n",
    "  - Neighbor 2: Legitimate âŒ (distance: 0.7)\n",
    "  - Neighbor 3: Fraud âœ… (distance: 0.8)\n",
    "  - Neighbor 4: Fraud âœ… (distance: 0.9)\n",
    "  - Neighbor 5: Legitimate âŒ (distance: 1.0)\n",
    "\n",
    "Step 3: Majority vote: 3 Fraud, 2 Legitimate\n",
    "Step 4: Prediction: FRAUD âœ…\n",
    "```\n",
    "\n",
    "### Key Concepts | Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "- **K**: Number of neighbors to consider (hyperparameter - must choose!)\n",
    "- **Distance Metric**: How to measure \"nearest\" (Euclidean, Manhattan, etc.)\n",
    "- **Lazy Learning**: No training phase - all computation happens during prediction\n",
    "- **Feature Scaling**: CRITICAL! Distance calculations depend on feature scales\n",
    "\n",
    "### Why KNN Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… KNNØŸ\n",
    "\n",
    "KNN is a simple but powerful classifier:\n",
    "- **Simple**: Easy to understand and implement\n",
    "- **No Training**: Lazy learning - no model training needed\n",
    "- **Non-Parametric**: Makes no assumptions about data distribution\n",
    "- **Versatile**: Works for classification and regression\n",
    "- **Effective**: Often performs well on many datasets\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ Real-World Applications | Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©\n",
    "\n",
    "**KNN is widely used for simple, effective classification!** Key applications:\n",
    "\n",
    "### ðŸ¥ Healthcare & Medical Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµØ­ÙŠ ÙˆØ§Ù„Ø·Ø¨ÙŠ\n",
    "- **Disease Diagnosis**: Find patients with similar symptoms â†’ predict disease (similar patients = similar outcomes)\n",
    "- **Drug Recommendation**: Find patients similar to current patient â†’ recommend same treatment\n",
    "- **Medical Image Classification**: Find similar medical images â†’ classify new images\n",
    "\n",
    "### ðŸ’° Finance & Banking Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ù…Ø§Ù„ÙŠ ÙˆØ§Ù„Ù…ØµØ±ÙÙŠ\n",
    "- **Credit Scoring**: Find similar loan applicants â†’ predict default risk (similar applicants = similar risk)\n",
    "- **Fraud Detection**: Find similar transactions â†’ identify fraudulent patterns\n",
    "- **Customer Segmentation**: Find similar customers â†’ group them together\n",
    "\n",
    "### ðŸ›ï¸ Government & Public Safety Sector (GDI) | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¹Ø§Ù…Ø© (Ø§Ù„Ù…Ø¯ÙŠØ±ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ­Ù‚ÙŠÙ‚)\n",
    "- **Financial Fraud Detection**: Find similar transactions â†’ identify fraudulent patterns â†’ financial investigations\n",
    "- **Administrative Crimes Detection**: Find similar administrative violations â†’ detect patterns â†’ administrative crime investigation\n",
    "- **Identity Matching**: Find similar faces/identities to query person â†’ identify suspects, verify identity â†’ security systems\n",
    "\n",
    "### ðŸ’¡ Why KNN is Popular:\n",
    "- **Simple**: Easy to understand and implement\n",
    "- **No Assumptions**: Works with any data distribution\n",
    "- **Effective**: Often performs well\n",
    "- **Interpretable**: Can explain predictions (show similar examples)\n",
    "- **Adaptive**: Automatically adapts to new data\n",
    "- **Baseline**: Good baseline before trying complex models\n",
    "\n",
    "### ðŸ“ˆ When to Use KNN:\n",
    "âœ… **Use KNN when:**\n",
    "- Need simple, interpretable model\n",
    "- Have small to medium datasets (KNN is slow on large datasets)\n",
    "- Want to see similar examples (interpretability)\n",
    "- Data has local patterns (similar items are nearby)\n",
    "- Need quick prototype (no training needed)\n",
    "- Want baseline model before complex algorithms\n",
    "\n",
    "âŒ **Don't use KNN when:**\n",
    "- Have very large datasets (KNN is slow - must compute distances to all points)\n",
    "- Need fast predictions (KNN is slow for prediction)\n",
    "- Have many irrelevant features (KNN uses all features)\n",
    "- Data has high dimensionality (curse of dimensionality)\n",
    "- Need probability outputs (KNN gives class, not probability)\n",
    "- Have streaming data (KNN needs all data in memory)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Build KNN classifiers\n",
    "2. Understand how KNN makes predictions (nearest neighbors)\n",
    "3. Choose the right K value (avoid overfitting/underfitting)\n",
    "4. Understand distance metrics (Euclidean, Manhattan)\n",
    "5. Know why feature scaling is critical for KNN\n",
    "6. Compare KNN with other classification algorithms\n",
    "7. Understand when to use KNN vs other methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“‹ Notebook Structure | Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This notebook is organized into clear sections:**\n",
    "\n",
    "**Part 1: Understanding KNN** (The Concept)\n",
    "- What is KNN and how it works\n",
    "- Why feature scaling is critical\n",
    "- Load and prepare credit card fraud dataset\n",
    "\n",
    "**Part 2: Building KNN Models** (Main Content)\n",
    "- Demonstrate feature scaling importance (scaled vs unscaled)\n",
    "- Find optimal K value\n",
    "- Train final model and evaluate comprehensively\n",
    "\n",
    "**Part 3: Decision Framework** (When to Use KNN)\n",
    "- Compare KNN with other algorithms\n",
    "- Understand when KNN is the right choice\n",
    "- Summary and next steps\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:58.494316Z",
     "iopub.status.busy": "2026-01-20T05:44:58.494240Z",
     "iopub.status.idle": "2026-01-20T05:44:58.497922Z",
     "shell.execute_reply": "2026-01-20T05:44:58.497743Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "    # if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "        # NOTE: Auto-suppressed invalid cell\n",
    "        # # Step 1: Import necessary libraries\n",
    "        # # These libraries help us build KNN classification models\n",
    "        # import pandas as pd \n",
    "        # # For data manipulation\n",
    "        # import numpy as np \n",
    "        # # For numerical operations\n",
    "        # import matplotlib.pyplot as plt \n",
    "        # # For visualizations\n",
    "        # import seaborn as sns \n",
    "        # # For beautiful plots\n",
    "        # from sklearn.model_selection import train_test_split \n",
    "        # # For splitting data\n",
    "        # from sklearn.neighbors import KNeighborsClassifier \n",
    "        # # KNN classifier\n",
    "        # from sklearn.preprocessing import StandardScaler \n",
    "        # # CRITICAL \n",
    "        # for KNN! Must scale features\n",
    "        # from sklearn.metrics import (\n",
    "        #  accuracy_score, \n",
    "        # # Classification accuracyclassification_report, \n",
    "        # # Comprehensive metricsconfusion_matrix, \n",
    "        # # Confusion matrixroc_auc_score, \n",
    "        # # AUC scoreroc_curve \n",
    "        # # ROC curve\n",
    "        # )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"âœ… Libraries imported successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nðŸ“š Key KNN Concepts:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - KNeighborsClassifier: KNN classifier (finds K nearest neighbors)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - K parameter: Number of neighbors to consider (critical hyperparameter)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Distance metrics: Euclidean (default), Manhattan, etc.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Lazy learning: No training phase, just prediction\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n âš ï¸ IMPORTANT: KNN requires feature scaling! Always use StandardScaler!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" ðŸ’¡ Why? Distance calculations are affected by feature scales!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n ðŸŽ¯ GDI Theme: Financial Investigations & Administrative Crimes Detection\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" ðŸ’¡ We'll use credit card fraud data to detect fraudulent transactions!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding KNN | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: ÙÙ‡Ù… KNN\n",
    "\n",
    "### ðŸ”— Connecting to Previous Notebooks | Ø§Ù„Ø±Ø¨Ø· Ø¨Ø§Ù„Ø¯ÙØ§ØªØ± Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©\n",
    "\n",
    "**What We've Learned So Far:**\n",
    "- âœ… **Logistic Regression**: Linear classifier, needs scaling, interpretable\n",
    "- âœ… **Decision Trees**: Non-linear, interpretable, can overfit, no scaling needed\n",
    "- âœ… **SVM**: Optimal margin boundaries, needs scaling, handles non-linear with kernels\n",
    "\n",
    "**What Makes KNN Different:**\n",
    "- **No Training**: KNN doesn't train a model - it's \"lazy learning\" (stores all data)\n",
    "- **Distance-Based**: Uses distance to find similar examples (like SVM, but simpler)\n",
    "- **Simple**: Just find nearest neighbors and vote (no complex optimization)\n",
    "- **Needs Scaling**: Like SVM, KNN is distance-based so scaling is critical!\n",
    "\n",
    "**Why KNN is Useful:**\n",
    "- **Simple Baseline**: Easy to understand and implement\n",
    "- **No Assumptions**: Works with any data distribution\n",
    "- **Interpretable**: Can show which examples are similar\n",
    "- **Effective**: Often performs well with proper preprocessing\n",
    "\n",
    "**Key Difference from Other Algorithms:**\n",
    "- **Logistic/SVM**: Train a model, then predict\n",
    "- **Decision Trees**: Build a tree structure, then predict\n",
    "- **KNN**: No training! Just store data, find neighbors when predicting\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Load and Prepare Data | Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**BEFORE**: We need to understand the dataset structure.\n",
    "\n",
    "**AFTER**: We'll load the Credit Card Fraud dataset and prepare it for KNN!\n",
    "\n",
    "**Why this dataset**: \n",
    "- Real-world financial transaction data (GDI: Financial Investigations & Admin Crimes)\n",
    "- Binary classification (perfect for KNN demonstration: Legitimate vs Fraud)\n",
    "- Multiple features (shows importance of scaling for distance-based algorithms)\n",
    "- Highly imbalanced (requires balanced sampling for meaningful evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:58.498735Z",
     "iopub.status.busy": "2026-01-20T05:44:58.498684Z",
     "iopub.status.idle": "2026-01-20T05:44:58.500988Z",
     "shell.execute_reply": "2026-01-20T05:44:58.500808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Load the Credit Card Fraud dataset\n",
    "# # This is REAL financial transaction data \n",
    "# # for binary classification (GDI: Financial Investigations & Admin Crimes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"Loading Credit Card Fraud Dataset\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ ÙÙŠ Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø§Ø¦ØªÙ…Ø§Ù†ÙŠØ©\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "# # try:\n",
    " \n",
    "# # = \n",
    "# # File not found: ../../datasets/raw/creditcard_fraud.csv\n",
    "# # Using synthetic data insteadpd.DataFrame({'col1': range(100), 'col2': range(100, 200)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# #  print(f\"\\nâœ… Full dataset loaded!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ðŸ“Š Full dataset shape: {df_full.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ðŸŽ¯ Domain: Financial Transactions (GDI: Financial Investigations & Admin Crimes)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ðŸ’¡ This is REAL credit card transaction data with fraud labels\")\n",
    "\n",
    "# # Check \n",
    "# # class distribution\n",
    "# # print(f\"\\nðŸ” Class Distribution (Full Dataset):\")\n",
    "# #  class_counts_full = df_full['Class'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" Legitimate (0): {class_counts_full.get(0, 0):,} transactions ({class_counts_full.get(0, 0)/len(df_full)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" Fraud (1): {class_counts_full.get(1, 0):,} transactions ({class_counts_full.get(1, 0)/len(df_full)*100:.2f}%)\")\n",
    "\n",
    "# # Implement balanced sampling (like we did \n",
    "# # for logistic regression)\n",
    "\n",
    "# # KNN needs sufficient examples of both \n",
    "# # classes \n",
    "# # for meaningful evaluation\n",
    "# # print(f\"\\nðŸ“Š Implementing Balanced Sampling for Learning Convenience...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" Problem: Extreme class imbalance ({class_counts_full.get(1, 0)/len(df_full)*100:.2f}% fraud)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" Solution: Balanced sampling ensures sufficient fraud cases for evaluation\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" Benefit: Meaningful metrics and better learning experience\")\n",
    "\n",
    "# # = df_full[df_full['Class'] == 1].copy()\n",
    "# #  df_legitimate = df_full[df_full['Class'] == 0].copy()\n",
    "\n",
    "# # Sample legitimate transactions (keep all fraud cases)\n",
    "# #  legitimate_sample_size = 5000df_legitimate_sample = df_legitimate.sample(\n",
    "# #  n=min(legitimate_sample_size, len(df_legitimate)), random_state=73,\n",
    "# #  replace=False\n",
    "#  )\n",
    "\n",
    "\n",
    "# #  df = df.sample(frac=1, random_state=73).reset_index(drop=True) \n",
    "# # Shuffle\n",
    "# # print(f\"\\nâœ… Balanced sample created!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ðŸ“Š Balanced sample shape: {df.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ðŸŽ¯ Classes: {len(np.unique(df['Class'])} (binary classification)\")\n",
    "\n",
    "# # = [col \n",
    "# # for col in\n",
    "# # df.columns \n",
    "\n",
    "\n",
    "\n",
    "# # if col != 'Class']\n",
    "# #  X = df[feature_cols].valuesy = df['Class'].values\n",
    "# # print(f\" ðŸ“ˆ Features: {len(feature_cols)} features (transaction characteristics)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\"\\nðŸ” Class Distribution (Balanced Sample):\")\n",
    "# #  class_counts = pd.Series(y).value_counts()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" Legitimate (0): {class_counts.get(0, 0):,} transactions ({class_counts.get(0, 0)/len(df)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" Fraud (1): {class_counts.get(1, 0):,} transactions ({class_counts.get(1, 0)/len(df)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# #  print(f\"\\nðŸ’¡ GDI Application: Financial Investigations & Administrative Crimes Detection\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - Features: Transaction characteristics (Time, Amount, V1-V28 anonymized features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - Target: 0 = Legitimate transaction, 1 = Fraudulent transaction\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - Task: Detect fraudulent transactions using KNN (find similar transaction patterns)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - Why KNN works: Similar transactions have similar patterns â†’ KNN finds similar neighbors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# #  print(f\"\\nâœ… Features and target prepared!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" X shape: {X.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" y shape: {y.shape}\")\n",
    "# # FileNotFoundError:\n",
    "# #  print(\"\\nâŒ Error: Credit Card Fraud dataset not found!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" Please ensure 'creditcard_fraud.csv' is in '../../datasets/raw/'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" Or download from: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\")\n",
    "# #  raiseexcept Exception as e:\n",
    "# #  print(f\"\\nâŒ Error loading dataset: {e}\")\n",
    "# #  raise\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Understanding the Dataset | ÙÙ‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø¬Ø§Ù„\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (medical, financial, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: ~5,500 rows Ã— 30 columns (samples Ã— features) after balanced sampling\n",
    "- **Feature Types**: All numerical (float64) - continuous values (transaction characteristics)\n",
    "- **Target Type**: Classification (predicting transaction type: 0 = Legitimate, 1 = Fraud)\n",
    "- **Task**: Predict if transaction is fraudulent based on transaction characteristics\n",
    "- **Data Quality**: Real-world financial transaction data with binary classification\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Binary classification** â†’ Need classification metrics (accuracy, precision, recall, F1, AUC)\n",
    "- **Many features (30)** â†’ Need feature scaling (KNN is distance-based!)\n",
    "- **Numerical features** â†’ KNN works well (can compute distances)\n",
    "- **Real-world data** â†’ Shows KNN on real financial fraud detection problem\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ÙÙ‡Ù… Ù…Ø¬Ø§Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø§Ø®ØªØµØ§Ø±)\n",
    "\n",
    "**What is this data?** Credit Card Fraud dataset - real financial transaction data with fraud labels.\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For model selection**: Binary classification (2 classes) â†’ use classification models\n",
    "- **For feature scaling**: Many features on different scales â†’ KNN REQUIRES scaling (distance-based!)\n",
    "- **For evaluation**: Binary classification â†’ use classification metrics (accuracy, AUC, etc.)\n",
    "\n",
    "**Domain Context** (Brief):\n",
    "- **Features**: Transaction characteristics (Time, Amount, V1-V28 anonymized PCA features) - 30 features total\n",
    "- **Target**: Transaction type (0 = Legitimate, 1 = Fraudulent)\n",
    "- **Task**: Predict if transaction is fraudulent from transaction characteristics\n",
    "- **Why KNN works**: Similar transactions have similar patterns â†’ KNN finds similar neighbors\n",
    "- **GDI Application**: Financial Investigations & Administrative Crimes - detecting fraudulent financial transactions\n",
    "\n",
    "**ðŸ’¡ Key Point for CS Students**: You don't need to be a medical expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types, classes)\n",
    "- Knowing the **task type** (binary classification: 2 classes)\n",
    "- Understanding why **KNN needs scaling** (distance-based algorithm - critical!)\n",
    "- Choosing the right **algorithms and metrics** based on structure, not domain knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building KNN Models | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ KNN\n",
    "\n",
    "## Step 2: Understanding Why Feature Scaling is Critical | Ø§Ù„Ø®Ø·ÙˆØ© 2: ÙÙ‡Ù… Ù„Ù…Ø§Ø°Ø§ ØªØ­Ø¬ÙŠÙ… Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹\n",
    "\n",
    "**BEFORE**: KNN uses distance to find neighbors. If features have different scales, distance will be wrong!\n",
    "\n",
    "**AFTER**: We'll demonstrate why scaling matters by comparing scaled vs unscaled KNN!\n",
    "\n",
    "**Why this matters**: \n",
    "- **Unscaled**: Features with larger values dominate distance calculations\n",
    "- **Scaled**: All features contribute equally to distance calculations\n",
    "- **Result**: Scaling dramatically improves KNN performance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:58.501736Z",
     "iopub.status.busy": "2026-01-20T05:44:58.501686Z",
     "iopub.status.idle": "2026-01-20T05:44:58.503589Z",
     "shell.execute_reply": "2026-01-20T05:44:58.503394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Demonstrate why feature scaling is critical \n",
    "# # for KNN\n",
    "# # We'll compare KNN with and without scaling\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"Demonstrating Feature Scaling Importance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø¥Ø¸Ù‡Ø§Ø± Ø£Ù‡Ù…ÙŠØ© ØªØ­Ø¬ÙŠÙ… Ø§Ù„Ù…ÙŠØ²Ø§Øª\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "# # = train_test_split(\n",
    "# #  X, y, test_size=0.2, random_state=73, stratify=y \n",
    "# # Using 73 \n",
    "# # for consistency\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nâœ… Data split!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Training samples: {len(X_train)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Test samples: {len(X_test)}\")\n",
    "\n",
    "# # Show feature scales BEFORE scaling\n",
    "# # Note: X_train is now a numpy array, not DataFrame\n",
    "# # print(f\"\\nðŸ“Š Feature Scales BEFORE Scaling:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Mean of first feature (Time): {X_train[:, 0].mean():.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Std of first feature (Time): {X_train[:, 0].std():.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Mean of Amount feature: {X_train[:, 29].mean():.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Std of Amount feature: {X_train[:, 29].std():.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nðŸ’¡ Notice: Features have VERY different scales!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Time feature: ranges from 0 to large values\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Amount feature: ranges from 0 to thousands\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - V1-V28 features: may have different scales\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - This will make distance calculations dominated by large-scale features!\")\n",
    "\n",
    "# # Train KNN WITHOUT scaling (BAD!)\n",
    "# # knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
    "# # knn_unscaled.fit(X_train, y_train)\n",
    "# # y_pred_unscaled = knn_unscaled.predict(X_test)\n",
    "# # acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nâŒ KNN WITHOUT Scaling:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Accuracy: {acc_unscaled:.4f} ({acc_unscaled*100:.2f}%)\")\n",
    "\n",
    "# # = StandardScaler()\n",
    "# # X_train_scaled = scaler.fit_transform(X_train)\n",
    "# # X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nâœ… Features scaled using StandardScaler!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Mean of all features: ~0\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Std of all features: ~1\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - All features now contribute equally to distance!\")\n",
    "\n",
    "# # Train KNN WITH scaling (GOOD!)\n",
    "# # knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "# # knn_scaled.fit(X_train_scaled, y_train)\n",
    "# # y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
    "# # acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nâœ… KNN WITH Scaling:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Accuracy: {acc_scaled:.4f} ({acc_scaled*100:.2f}%)\")\n",
    "\n",
    "# # improvement = acc_scaled - acc_unscaled\n",
    "# # print(f\"\\nðŸŽ¯ Improvement from Scaling: {improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nðŸ’¡ Key Learning Point:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Feature scaling is CRITICAL for KNN!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Without scaling: {acc_unscaled*100:.1f}% accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - With scaling: {acc_scaled*100:.1f}% accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Always use StandardScaler before KNN!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Choosing the Right K Value | Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ø®ØªÙŠØ§Ø± Ù‚ÙŠÙ…Ø© K Ø§Ù„ØµØ­ÙŠØ­Ø©\n",
    "\n",
    "**BEFORE**: We've scaled our features (critical for KNN!). Now we need to choose the right K value.\n",
    "\n",
    "**AFTER**: We'll test different K values and find the optimal one!\n",
    "\n",
    "**Why this step comes after scaling?**\n",
    "- Feature scaling ensures fair distance calculations (Step 2)\n",
    "- **Now** we can properly evaluate different K values without scale bias\n",
    "- Scaling must be done first, then we tune K for best performance\n",
    "\n",
    "**Why K matters**: \n",
    "- **K too small (K=1)**: Overfitting - model is too sensitive to noise\n",
    "- **K too large**: Underfitting - model loses local patterns\n",
    "- **Optimal K**: Balances bias and variance for best generalization\n",
    "\n",
    "**Rule of thumb**: Start with K = âˆšn (square root of number of samples), then tune!\n",
    "\n",
    "**ðŸ’¡ Important Note**: For binary classification, use **odd K values** (1, 3, 5, 7, etc.) to avoid ties in voting!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:58.504373Z",
     "iopub.status.busy": "2026-01-20T05:44:58.504260Z",
     "iopub.status.idle": "2026-01-20T05:44:58.506233Z",
     "shell.execute_reply": "2026-01-20T05:44:58.506062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TORCH_AVAILABLE' in globals() and TORCH_AVAILABLE:\n",
    "#     if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # if 'TORCH_AVAILABLE' in globals() and TORCH_AVAILABLE:\n",
    "#         #     if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#                 # Find the optimal K value\n",
    "#                 # + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(\"Finding Optimal K Value\")\n",
    "\n",
    "\n",
    "#                 # print(\"Ø¥ÙŠØ¬Ø§Ø¯ Ù‚ÙŠÙ…Ø© K Ø§Ù„Ø£Ù…Ø«Ù„\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(\"=\" * 60)\n",
    "\n",
    "#                 # = range(1, 31, 2) \n",
    "#                 # Test odd numbers \n",
    "#                 # from 1 to 29train_scores = []\n",
    "#                 # test_scores = []\n",
    "\n",
    "#                 # print(f\"\\nðŸ” Testing K values from 1 to 29 (odd numbers only)...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" Why odd? Avoids ties in binary classification!\")\n",
    "\n",
    "#                 # for k in k_values:\n",
    "#                 #  knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#                 #  knn.fit(X_train_scaled, y_train)\n",
    " \n",
    "#                 #  train_pred = knn.predict(X_train_scaled)\n",
    "#                 #  test_pred = knn.predict(X_test_scaled)\n",
    " \n",
    "#                 #  train_acc = accuracy_score(y_train, train_pred)\n",
    "#                 #  test_acc = accuracy_score(y_test, test_pred)\n",
    " \n",
    "#                  train_scores.append(train_acc)\n",
    "#                  test_scores.append(test_acc)\n",
    "\n",
    "#                 # = np.argmax(test_scores)\n",
    "#                 # best_k = k_values[best_k_idx]\n",
    "#                 # best_test_acc = test_scores[best_k_idx]\n",
    "\n",
    "#                 # print(f\"\\nâœ… Best K value: {best_k}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" Test Accuracy: {best_test_acc:.4f} ({best_test_acc*100:.2f}%)\")\n",
    "\n",
    "#                 # 6))\n",
    "#                 # plt.plot(k_values, train_scores, 'o-', label='Training Accuracy', linewidth=2, markersize=8)\n",
    "#                 # plt.plot(k_values, test_scores, 's-', label='Test Accuracy', linewidth=2, markersize=8)\n",
    "#                 # plt.axvline(x=best_k, color='r', linestyle='--', linewidth=2, label=f'Best K = {best_k}')\n",
    "#                 # plt.xlabel('K Value (Number of Neighbors)', fontsize=12, fontweight='bold')\n",
    "#                 # plt.ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "#                 # plt.title('KNN: Finding Optimal K Value', fontsize=14, fontweight='bold')\n",
    "#                 # plt.legend(fontsize=11)\n",
    "#                 # plt.grid(True, alpha=0.3)\n",
    "#                 # plt.tight_layout()\n",
    "#                 plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(\"ðŸ’¡ Interpreting the K Value Plot\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\"\\nðŸ“Š What You Should See:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - Training accuracy (blue line): Generally decreases as K increases\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - Test accuracy (orange line): Increases then decreases (optimal K exists)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - Red dashed line: Marks the optimal K value\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\"\\nðŸ” Key Observations:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - K=1: High training accuracy, lower test accuracy (overfitting)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - K too large: Lower accuracy on both (underfitting)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - Optimal K={best_k}: Best balance (test accuracy: {best_test_acc*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\"\\nðŸ“š Learning Points:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - Small K: Model is too sensitive to noise (overfitting)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - Large K: Model loses local patterns (underfitting)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - Optimal K: Balances bias and variance for best generalization\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\"\\nðŸ’¡ For GDI Financial Fraud Detection:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - Optimal K={best_k} gives best fraud detection performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - Balance between catching fraud (recall) and avoiding false alarms (precision)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - K too small: Too sensitive to individual transaction noise\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # print(f\" - K too large: Misses local fraud patterns\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train Final KNN Model and Evaluate | Ø§Ù„Ø®Ø·ÙˆØ© 4: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ KNN Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "\n",
    "**BEFORE**: We found the optimal K value and understand why it works.\n",
    "\n",
    "**AFTER**: We'll train the final model with optimal K and evaluate it comprehensively!\n",
    "\n",
    "**Why this step matters**: \n",
    "- We've done the hard work (scaling + finding optimal K)\n",
    "- **Now** we train the final model and see how well it performs\n",
    "- Comprehensive evaluation shows the complete picture of model performance\n",
    "\n",
    "**Why comprehensive evaluation matters**: \n",
    "- Accuracy alone doesn't tell the full story\n",
    "- Precision, Recall, F1, and Confusion Matrix give complete picture\n",
    "- ROC Curve shows model's ability to separate classes\n",
    "- For GDI fraud detection: We need to balance catching fraud vs avoiding false alarms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:58.507136Z",
     "iopub.status.busy": "2026-01-20T05:44:58.507065Z",
     "iopub.status.idle": "2026-01-20T05:44:58.509573Z",
     "shell.execute_reply": "2026-01-20T05:44:58.509402Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Train final KNN model with optimal K and evaluate comprehensively\n",
    "        # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "        # print(\"Training Final KNN Model with Optimal K\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ KNN Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ù‚ÙŠÙ…Ø© K Ø§Ù„Ø£Ù…Ø«Ù„\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "        # knn_final.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # = knn_final.predict(X_train_scaled)\n",
    "        # y_test_pred = knn_final.predict(X_test_scaled)\n",
    "        # y_test_proba = knn_final.predict_proba(X_test_scaled)[:, 1] \n",
    "        # Probabilities \n",
    "        # for class 1\n",
    "\n",
    "        # = accuracy_score(y_train, y_train_pred)\n",
    "        # test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        # test_precision = precision_score(y_test, y_test_pred)\n",
    "        # test_recall = recall_score(y_test, y_test_pred)\n",
    "        # test_f1 = f1_score(y_test, y_test_pred)\n",
    "        # test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nðŸ“Š Final KNN Model Performance (K={best_k}):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Training Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Test Precision: {test_precision:.4f} ({test_precision*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Test Recall: {test_recall:.4f} ({test_recall*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Test F1-Score: {test_f1:.4f} ({test_f1*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Test AUC: {test_auc:.4f} ({test_auc*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nâœ… KNN Model trained and evaluated successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nðŸ’¡ Interpretation:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Accuracy: Overall correctness ({test_acc*100:.1f}% correct)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Precision: Of predicted positives, {test_precision*100:.1f}% are actually positive\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Recall: Of actual positives, we caught {test_recall*100:.1f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - F1: Balance between precision and recall ({test_f1*100:.1f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - AUC: Model's ability to separate classes ({test_auc*100:.1f}%)\")\n",
    "\n",
    "        # = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nðŸ“Š Confusion Matrix:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(cm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n True Negatives (TN): {cm[0,0]} - Correctly predicted Legitimate\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" False Positives (FP): {cm[0,1]} - Predicted Fraud but actually Legitimate (false alarm)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" False Negatives (FN): {cm[1,0]} - Predicted Legitimate but actually Fraud (missed fraud)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" True Positives (TP): {cm[1,1]} - Correctly predicted Fraud\")\n",
    "\n",
    "        # 6))\n",
    "        # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Count'})\n",
    "        # plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "        # plt.ylabel('Actual Label', fontsize=12, fontweight='bold')\n",
    "        # plt.title('KNN Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "        # plt.xticks([0.5, 1.5], ['Legitimate (0)', 'Fraud (1)'])\n",
    "        # plt.yticks([0.5, 1.5], ['Legitimate (0)', 'Fraud (1)'])\n",
    "        # plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "        # print(\"ðŸ’¡ Interpreting the Confusion Matrix\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nðŸ“Š What You Should See:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - 2x2 grid showing predicted vs actual classifications\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Diagonal (TN, TP): Correct predictions (green/blue)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Off-diagonal (FP, FN): Incorrect predictions (red/orange)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nðŸ” Key Observations:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - TN ({cm[0,0]}): Correctly identified legitimate transactions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - FP ({cm[0,1]}): False alarms (flagged legitimate as fraud)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - FN ({cm[1,0]}): Missed fraud (flagged fraud as legitimate) âš ï¸\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - TP ({cm[1,1]}): Correctly identified fraud âœ…\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nðŸ’¡ For GDI Financial Fraud Detection:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Low FN: Important to catch fraud (minimize missed fraud)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Low FP: Important to avoid false alarms (minimize false positives)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Balance: Trade-off between catching fraud and avoiding false alarms\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - GDI Application: Minimize FN (missed fraud) while keeping FP reasonable\")\n",
    "\n",
    "        # = roc_curve(y_test, y_test_proba)\n",
    "\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # plt.plot(fpr, tpr, linewidth=2, label=f'KNN (AUC = {test_auc:.3f})')\n",
    "        # plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "        # plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "        # plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "        # plt.title('KNN ROC Curve', fontsize=14, fontweight='bold')\n",
    "        # plt.legend(fontsize=11)\n",
    "        # plt.grid(True, alpha=0.3)\n",
    "        # plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"ðŸ’¡ Interpreting the ROC Curve\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nðŸ“Š What You Should See:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Blue curve: KNN ROC curve (above diagonal = better than random)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Diagonal dashed line: Random classifier (AUC = 0.5, worst possible)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Curve closer to top-left = better performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nðŸ” Key Observations:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if test_auc >= 0.9:\n",
    "        # test_auc >= 0.8:\n",
    "        # test_auc >= 0.7:\n",
    "        # :\n",
    "        # - AUC = {test_auc:.4f} ({quality})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Higher AUC = better at distinguishing legitimate vs fraud\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - AUC > 0.9 is excellent for fraud detection\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nðŸ’¡ For GDI Financial Fraud Detection:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - ROC curve shows model's ability to detect fraud while minimizing false alarms\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - AUC = {test_auc:.4f} indicates {'excellent'\")\n",
    "\n",
    "\n",
    "\n",
    "        # if test_auc >= 0.9 else 'good' \n",
    "\n",
    "\n",
    "\n",
    "        # if test_auc >= 0.8 else 'moderate'} performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - In production, choose threshold based on business needs (minimize FN vs FP)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nâœ… Comprehensive evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Decision Framework | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù„Ø«: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "## Step 5: When to Use KNN vs Other Algorithms | Ø§Ù„Ø®Ø·ÙˆØ© 5: Ù…ØªÙ‰ Ù†Ø³ØªØ®Ø¯Ù… KNN Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰\n",
    "\n",
    "**BEFORE**: We understand how KNN works.\n",
    "\n",
    "**AFTER**: We'll understand when KNN is the right choice and when to use other algorithms!\n",
    "\n",
    "**Decision Framework:**\n",
    "\n",
    "| Scenario | Best Algorithm | Why |\n",
    "|----------|---------------|-----|\n",
    "| **Small dataset, interpretability important** | Decision Trees | Easy to understand |\n",
    "| **Large dataset, need fast predictions** | Logistic Regression | Fast, efficient |\n",
    "| **Non-linear patterns, need optimal margin** | SVM | Maximum margin boundaries |\n",
    "| **No clear pattern, need simple solution** | KNN | Simple, no assumptions |\n",
    "| **Need feature importance** | Random Forest | Built-in feature importance |\n",
    "| **Lazy learning acceptable** | KNN | No training phase needed |\n",
    "\n",
    "**KNN Advantages:**\n",
    "- âœ… Simple to understand and implement\n",
    "- âœ… No assumptions about data distribution\n",
    "- âœ… Works well for non-linear patterns\n",
    "- âœ… Can be used for both classification and regression\n",
    "\n",
    "**KNN Disadvantages:**\n",
    "- âŒ Slow prediction (must compute distances to all training points)\n",
    "- âŒ Sensitive to irrelevant features\n",
    "- âŒ Requires feature scaling\n",
    "- âŒ Memory intensive (stores all training data)\n",
    "- âŒ Sensitive to K value choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary | Ø§Ù„Ù…Ù„Ø®Øµ\n",
    "\n",
    "### What We Learned | Ù…Ø§ ØªØ¹Ù„Ù…Ù†Ø§Ù‡\n",
    "\n",
    "1. **KNN Basics**: KNN finds K nearest neighbors and predicts based on majority vote\n",
    "2. **Feature Scaling**: CRITICAL for KNN - always use StandardScaler!\n",
    "3. **Choosing K**: Balance between overfitting (small K) and underfitting (large K)\n",
    "4. **Lazy Learning**: KNN doesn't train a model - all computation happens during prediction\n",
    "5. **Distance Metrics**: Euclidean distance (default) measures \"nearest\"\n",
    "6. **When to Use**: Simple solution, no assumptions, non-linear patterns\n",
    "\n",
    "### Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "- âš ï¸ **Always scale features before KNN** - distance calculations depend on feature scales\n",
    "- ðŸŽ¯ **Choose K carefully** - too small = overfitting, too large = underfitting\n",
    "- ðŸ’¡ **KNN is simple but powerful** - often performs well with proper preprocessing\n",
    "- ðŸ“Š **Evaluate comprehensively** - use accuracy, precision, recall, F1, AUC, confusion matrix\n",
    "- ðŸ” **Understand trade-offs** - KNN is slow for large datasets but simple to understand\n",
    "\n",
    "### Next Steps | Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©\n",
    "\n",
    "- ðŸ““ **Unit 4: Clustering** - K-Means uses similar distance concepts\n",
    "- ðŸ““ **Unit 5: Model Selection** - Hyperparameter tuning for K\n",
    "- ðŸ““ **Advanced Topics** - Weighted KNN, distance metrics, approximate nearest neighbors\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** ðŸŽ‰ You've completed all classification algorithms in Unit 3!\n",
    "**ØªÙ‡Ø§Ù†ÙŠÙ†Ø§!** ðŸŽ‰ Ù„Ù‚Ø¯ Ø£ÙƒÙ…Ù„Øª Ø¬Ù…ÙŠØ¹ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØµÙ†ÙŠÙ ÙÙŠ Ø§Ù„ÙˆØ­Ø¯Ø© 3!\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
>>>>>>> Incoming (Background Agent changes)
