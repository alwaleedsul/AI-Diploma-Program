{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Gradient Boosting (XGBoost, LightGBM) | ÿßŸÑÿ™ÿπÿ≤Ÿäÿ≤ ÿßŸÑŸÖÿ™ÿØÿ±ÿ¨ (XGBoost, LightGBM)\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Basic Python\n",
    "- ‚úÖ Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 04, Unit 5** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Gradient Boosting (XGBoost, LightGBM) | ÿßŸÑÿ™ÿπÿ≤Ÿäÿ≤ ÿßŸÑŸÖÿ™ÿØÿ±ÿ¨ (XGBoost, LightGBM)\n",
    "\n",
    "## üìö Prerequisites (What You Need First) | ÿßŸÑŸÖÿ™ÿ∑ŸÑÿ®ÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- ‚úÖ **Unit 1-4: All examples** - Data processing, regression, classification, clustering\n",
    "- ‚úÖ **Unit 3, Example 2: Decision Trees** - Understanding tree-based models\n",
    "- ‚úÖ **Unit 5, Example 1: Grid Search** - Understanding hyperparameter tuning\n",
    "- ‚úÖ **Understanding of ensemble methods**: Combining multiple models\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding how boosting differs from bagging (Random Forest)\n",
    "- Knowing when to use XGBoost vs LightGBM vs Random Forest\n",
    "- Understanding gradient boosting concepts\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Where This Notebook Fits | ŸÖŸÉÿßŸÜ Ÿáÿ∞ÿß ÿßŸÑÿØŸÅÿ™ÿ±\n",
    "\n",
    "**This is Unit 5, Example 2** - it's the final example in the course!\n",
    "\n",
    "**Why this example SECOND in Unit 5?**\n",
    "- **Before** you can use boosting, you need to understand hyperparameter tuning\n",
    "- **Before** you can use advanced models, you need to understand basic ensemble methods\n",
    "- **Before** you can use XGBoost/LightGBM, you need to understand decision trees\n",
    "\n",
    "**Builds on**: \n",
    "- üìì Unit 3, Example 2: Decision Trees (boosting uses trees as base learners)\n",
    "- üìì Unit 5, Example 1: Grid Search (we know how to tune hyperparameters)\n",
    "- üìì All previous examples (data processing and evaluation concepts)\n",
    "\n",
    "**Leads to**: \n",
    "- üìì All advanced ML projects (XGBoost and LightGBM are industry standards!)\n",
    "- üìì Kaggle competitions (boosting algorithms often win)\n",
    "- üìì Production ML systems (boosting is widely used)\n",
    "\n",
    "**Why this order?**\n",
    "1. Boosting is an **advanced ensemble method** (builds on decision trees)\n",
    "2. Boosting shows **sequential learning** (each model learns from previous mistakes)\n",
    "3. XGBoost/LightGBM are **state-of-the-art** (often best performance)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Learning from Mistakes | ÿßŸÑŸÇÿµÿ©: ÿßŸÑÿ™ÿπŸÑŸÖ ŸÖŸÜ ÿßŸÑÿ£ÿÆÿ∑ÿßÿ°\n",
    "\n",
    "Imagine you're learning to play chess. **Before** boosting, you play many games independently (like Random Forest). **After** boosting, you learn from each game: focus on positions where you made mistakes, get better each time - much more effective!\n",
    "\n",
    "Same with machine learning: **Before** boosting, models are trained independently. **After** boosting, each new model focuses on mistakes of previous models - sequential improvement leads to excellent performance!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Boosting Matters | ŸÑŸÖÿßÿ∞ÿß ŸäŸáŸÖ ÿßŸÑÿ™ÿπÿ≤Ÿäÿ≤ÿü\n",
    "\n",
    "Boosting algorithms are among the best ML models:\n",
    "- **State-of-the-Art Performance**: Often achieve best results in competitions\n",
    "- **Sequential Learning**: Each model learns from previous mistakes\n",
    "- **XGBoost**: Extremely popular, fast, and powerful\n",
    "- **LightGBM**: Even faster than XGBoost, great for large datasets\n",
    "- **Industry Standard**: Used in production systems worldwide\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Applications | ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™ ŸÅŸä ÿßŸÑÿ≠Ÿäÿßÿ© ÿßŸÑŸàÿßŸÇÿπŸäÿ©\n",
    "\n",
    "**XGBoost and LightGBM are used in MANY industries for state-of-the-art performance!** Here's where you'll find them:\n",
    "\n",
    "### üèÜ Machine Learning Competitions (Kaggle, etc.) | ŸÖÿ≥ÿßÿ®ŸÇÿßÿ™ ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä\n",
    "- **Kaggle Competitions**: XGBoost/LightGBM often win competitions ‚Üí best performance\n",
    "- **Data Science Competitions**: Used by top teams worldwide\n",
    "- **Hackathons**: Go-to algorithm for quick, high-performance solutions\n",
    "- **Benchmarking**: Used to benchmark against other algorithms\n",
    "\n",
    "### üí∞ Finance & Banking Sector | ÿßŸÑŸÇÿ∑ÿßÿπ ÿßŸÑŸÖÿßŸÑŸä ŸàÿßŸÑŸÖÿµÿ±ŸÅŸä\n",
    "- **Credit Scoring**: XGBoost predicts loan defaults with high accuracy ‚Üí optimize lending decisions\n",
    "- **Fraud Detection**: LightGBM detects fraudulent transactions ‚Üí real-time fraud prevention\n",
    "- **Algorithmic Trading**: XGBoost predicts stock movements ‚Üí automated trading systems\n",
    "- **Risk Assessment**: XGBoost assesses investment risks ‚Üí portfolio management\n",
    "- **Insurance Claims**: XGBoost predicts claim amounts ‚Üí optimize insurance pricing\n",
    "- **Customer Lifetime Value**: XGBoost predicts customer value ‚Üí marketing optimization\n",
    "\n",
    "### üè• Healthcare & Medical Sector | ÿßŸÑŸÇÿ∑ÿßÿπ ÿßŸÑÿµÿ≠Ÿä ŸàÿßŸÑÿ∑ÿ®Ÿä\n",
    "- **Disease Diagnosis**: XGBoost diagnoses diseases from patient data ‚Üí improve diagnosis accuracy\n",
    "- **Drug Discovery**: XGBoost identifies effective drug compounds ‚Üí accelerate drug development\n",
    "- **Medical Imaging**: XGBoost classifies medical images ‚Üí assist radiologists\n",
    "- **Patient Risk Stratification**: XGBoost predicts patient outcomes ‚Üí optimize treatment\n",
    "- **Clinical Decision Support**: XGBoost helps doctors make decisions ‚Üí improve patient care\n",
    "- **Epidemiology**: XGBoost predicts disease spread ‚Üí public health planning\n",
    "\n",
    "### üìä Marketing & E-commerce Sector | ŸÇÿ∑ÿßÿπ ÿßŸÑÿ™ÿ≥ŸàŸäŸÇ ŸàÿßŸÑÿ™ÿ¨ÿßÿ±ÿ© ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸäÿ©\n",
    "- **Recommendation Systems**: XGBoost powers product recommendations ‚Üí increase sales\n",
    "- **Customer Churn**: LightGBM predicts churn ‚Üí retain customers\n",
    "- **Ad Click Prediction**: XGBoost predicts ad clicks ‚Üí optimize ad spending\n",
    "- **Price Optimization**: XGBoost optimizes pricing ‚Üí maximize revenue\n",
    "- **Customer Segmentation**: XGBoost segments customers ‚Üí targeted marketing\n",
    "- **Sales Forecasting**: XGBoost predicts sales ‚Üí inventory management\n",
    "\n",
    "### üè≠ Manufacturing & Quality Control | ÿßŸÑÿ™ÿµŸÜŸäÿπ ŸàŸÖÿ±ÿßŸÇÿ®ÿ© ÿßŸÑÿ¨ŸàÿØÿ©\n",
    "- **Quality Prediction**: XGBoost predicts product quality ‚Üí reduce defects\n",
    "- **Predictive Maintenance**: LightGBM predicts equipment failures ‚Üí prevent downtime\n",
    "- **Defect Detection**: XGBoost identifies defects ‚Üí quality control\n",
    "- **Supply Chain Optimization**: XGBoost optimizes supply chains ‚Üí reduce costs\n",
    "- **Production Optimization**: XGBoost optimizes production ‚Üí increase efficiency\n",
    "- **Energy Management**: XGBoost optimizes energy usage ‚Üí reduce costs\n",
    "\n",
    "### üöó Transportation & Logistics | ÿßŸÑŸÜŸÇŸÑ ŸàÿßŸÑÿÆÿØŸÖÿßÿ™ ÿßŸÑŸÑŸàÿ¨ÿ≥ÿ™Ÿäÿ©\n",
    "- **Route Optimization**: XGBoost optimizes delivery routes ‚Üí reduce delivery time\n",
    "- **Demand Forecasting**: LightGBM predicts demand ‚Üí optimize inventory\n",
    "- **Fleet Management**: XGBoost optimizes fleet operations ‚Üí reduce costs\n",
    "- **Traffic Prediction**: XGBoost predicts traffic ‚Üí optimize routes\n",
    "- **Maintenance Scheduling**: XGBoost schedules maintenance ‚Üí prevent failures\n",
    "\n",
    "### üî¨ Scientific Research & Data Analysis | ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿπŸÑŸÖŸä Ÿàÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "- **Genomics**: XGBoost identifies important genes ‚Üí disease research\n",
    "- **Climate Modeling**: XGBoost predicts climate patterns ‚Üí environmental planning\n",
    "- **Material Science**: XGBoost discovers new materials ‚Üí innovation\n",
    "- **Astronomy**: XGBoost classifies celestial objects ‚Üí space research\n",
    "- **Epidemiology**: XGBoost models disease spread ‚Üí public health\n",
    "\n",
    "### üèõÔ∏è Government & Public Safety Sector (Ministry of Interior) | ÿßŸÑŸÇÿ∑ÿßÿπ ÿßŸÑÿ≠ŸÉŸàŸÖŸä ŸàÿßŸÑÿ≥ŸÑÿßŸÖÿ© ÿßŸÑÿπÿßŸÖÿ© (Ÿàÿ≤ÿßÿ±ÿ© ÿßŸÑÿØÿßÿÆŸÑŸäÿ©)\n",
    "- **Emergency Response Optimization**: XGBoost predicts emergency needs ‚Üí optimize resource allocation ‚Üí faster response times ‚Üí **GDI: Emergency Response**\n",
    "- **Threat Detection Systems**: XGBoost detects security threats with high accuracy ‚Üí critical for national security ‚Üí **GDI: Counter-Espionage, Terrorism Detection**\n",
    "- **Traffic Flow Optimization**: LightGBM predicts traffic patterns ‚Üí optimize traffic signals ‚Üí reduce congestion ‚Üí **GDI: Traffic Management**\n",
    "- **Crime Prediction**: XGBoost predicts crime hotspots ‚Üí optimize patrol routes ‚Üí crime prevention ‚Üí **GDI: Internal Intelligence**\n",
    "- **Border Security Screening**: XGBoost classifies border crossings ‚Üí identify threats ‚Üí border control ‚Üí **GDI: Border Security**\n",
    "- **Surveillance Alert Systems**: LightGBM classifies surveillance alerts ‚Üí reduce false positives ‚Üí security monitoring ‚Üí **GDI: Internal Intelligence**\n",
    "- **Traffic Violation Detection**: XGBoost detects traffic violations ‚Üí automated enforcement ‚Üí **GDI: Traffic Management**\n",
    "- **Personnel Security Assessment**: XGBoost assesses security clearance ‚Üí optimize internal security ‚Üí **GDI: Internal Intelligence**\n",
    "- **Identity Verification**: XGBoost verifies identities ‚Üí high-accuracy verification ‚Üí **GDI: Airport Security**\n",
    "- **Traffic Accident Prediction**: XGBoost predicts accident risk ‚Üí optimize traffic management ‚Üí **GDI: Traffic Management**\n",
    "- **Financial Crime Detection**: XGBoost detects financial fraud ‚Üí optimize investigations ‚Üí **GDI: Financial Investigations, Terrorism Financing**\n",
    "- **Cyber Threat Detection**: XGBoost detects cyber attacks ‚Üí optimize network security ‚Üí **GDI: Cyber Threats**\n",
    "\n",
    "### üí° Why XGBoost/LightGBM are Industry Standards:\n",
    "- **Best Performance**: Often achieve highest accuracy\n",
    "- **Fast Training**: LightGBM is very fast (important for large datasets)\n",
    "- **Handles Missing Values**: Automatically handles missing data (convenient!)\n",
    "- **Feature Importance**: Shows which features matter most\n",
    "- **Production Ready**: Used in production systems worldwide\n",
    "- **Competition Winners**: Often win ML competitions\n",
    "\n",
    "### üìà When to Use XGBoost vs LightGBM:\n",
    "‚úÖ **Use XGBoost when:**\n",
    "- Need best possible performance\n",
    "- Have medium to large datasets\n",
    "- Want industry-standard solution\n",
    "- Need feature importance\n",
    "- Working on competitions or production\n",
    "\n",
    "‚úÖ **Use LightGBM when:**\n",
    "- Have very large datasets (millions of rows)\n",
    "- Need faster training than XGBoost\n",
    "- Have limited computational resources\n",
    "- Need to train quickly\n",
    "- Working with big data\n",
    "\n",
    "‚úÖ **Use Both:**\n",
    "- Try both and compare performance\n",
    "- Ensemble both for even better results\n",
    "- Use LightGBM for speed, XGBoost for accuracy\n",
    "\n",
    "### üéØ Real-World Success Stories:\n",
    "- **Kaggle Competitions**: XGBoost/LightGBM used by most winning teams\n",
    "- **Netflix Recommendations**: Uses boosting for recommendations\n",
    "- **Amazon**: Uses XGBoost for product recommendations\n",
    "- **Banks**: Use XGBoost for credit scoring and fraud detection\n",
    "- **Healthcare**: Use XGBoost for medical diagnosis\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives | ÿ£ŸáÿØÿßŸÅ ÿßŸÑÿ™ÿπŸÑŸÖ\n",
    "1. Build XGBoost models\n",
    "2. Build LightGBM models\n",
    "3. Understand how boosting differs from bagging\n",
    "4. Compare boosting with Random Forest\n",
    "5. Interpret feature importance in boosting models\n",
    "6. Know when to use each boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:16.567162Z",
     "iopub.status.busy": "2026-01-20T11:42:16.566980Z",
     "iopub.status.idle": "2026-01-20T11:42:17.503377Z",
     "shell.execute_reply": "2026-01-20T11:42:17.503143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "\n",
      "üìö Key Boosting Concepts:\n",
      "   - Boosting: Sequential ensemble (each model learns from previous mistakes)\n",
      "   - Bagging: Parallel ensemble (Random Forest - models trained independently)\n",
      "   - XGBoost: Extreme Gradient Boosting (very popular, fast, powerful)\n",
      "   - LightGBM: Light Gradient Boosting Machine (faster than XGBoost)\n",
      "   - Learning Rate: Controls how much each new model contributes\n",
      "\n",
      "   üí° Boosting vs Bagging:\n",
      "   - Bagging (RF): Models trained in parallel, then averaged\n",
      "   - Boosting: Models trained sequentially, each focuses on mistakes\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us build boosting models\n",
    "\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np   # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For visualizations\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.ensemble import RandomForestClassifier  # For comparison (bagging)\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,        # Classification accuracy\n",
    "    classification_report,  # Comprehensive metrics\n",
    "    confusion_matrix       # Confusion matrix (for multi-class)\n",
    ")\n",
    "# Note: ROC curves removed - they only work for binary classification\n",
    "# We use confusion matrices for multi-class (3 classes: EMS, Fire, Traffic)\n",
    "# Removed make_classification - using real Breast Cancer dataset instead\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"\\nüìö Key Boosting Concepts:\")\n",
    "print(\"   - Boosting: Sequential ensemble (each model learns from previous mistakes)\")\n",
    "print(\"   - Bagging: Parallel ensemble (Random Forest - models trained independently)\")\n",
    "print(\"   - XGBoost: Extreme Gradient Boosting (very popular, fast, powerful)\")\n",
    "print(\"   - LightGBM: Light Gradient Boosting Machine (faster than XGBoost)\")\n",
    "print(\"   - Learning Rate: Controls how much each new model contributes\")\n",
    "print(\"\\n   üí° Boosting vs Bagging:\")\n",
    "print(\"   - Bagging (RF): Models trained in parallel, then averaged\")\n",
    "print(\"   - Boosting: Models trained sequentially, each focuses on mistakes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.504466Z",
     "iopub.status.busy": "2026-01-20T11:42:17.504369Z",
     "iopub.status.idle": "2026-01-20T11:42:17.510700Z",
     "shell.execute_reply": "2026-01-20T11:42:17.510473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç DIAGNOSTIC INFO\n",
      "============================================================\n",
      "üì¶ Python: /opt/anaconda3/bin/python\n",
      "üì¶ Version: 3.13.5\n",
      "üì¶ Expected: /opt/anaconda3/envs/course2/bin/python\n",
      "‚ö†Ô∏è  WARNING: Not using course2 environment!\n",
      "   ‚Üí Go to: Kernel ‚Üí Change Kernel ‚Üí course2\n",
      "   ‚Üí Then restart kernel and run again\n",
      "‚úÖ OpenMP found: /opt/anaconda3/envs/course2/lib/libomp.dylib\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  XGBoost is NOT installed in this Python environment\n",
      "\n",
      "   üîß QUICK FIX - Run this in a terminal:\n",
      "      /opt/anaconda3/bin/python -m pip install xgboost\n",
      "\n",
      "   Or install directly in notebook (uncomment next cell):\n",
      "      !pip install xgboost\n",
      "\n",
      "   üí° After installing, restart kernel and run this cell again\n",
      "‚ö†Ô∏è  Warning: LightGBM not installed. Install with: pip install lightgbm\n",
      "\n",
      "============================================================\n",
      "Example 2: Gradient Boosting (XGBoost, LightGBM)\n",
      "ŸÖÿ´ÿßŸÑ 2: ÿßŸÑÿ™ÿπÿ≤Ÿäÿ≤ ÿßŸÑŸÖÿ™ÿØÿ±ÿ¨ (XGBoost, LightGBM)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Try to import XGBoost and LightGBM\n",
    "# These are external libraries (not part of sklearn)\n",
    "# If not installed, the notebook will still work but skip those sections\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîç DIAGNOSTIC INFO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üì¶ Python: {sys.executable}\")\n",
    "print(f\"üì¶ Version: {sys.version.split()[0]}\")\n",
    "print(f\"üì¶ Expected: /opt/anaconda3/envs/course2/bin/python\")\n",
    "\n",
    "# Check if we're in the right environment\n",
    "if 'course2' in sys.executable:\n",
    "    print(\"‚úÖ Using course2 environment (correct!)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Not using course2 environment!\")\n",
    "    print(\"   ‚Üí Go to: Kernel ‚Üí Change Kernel ‚Üí course2\")\n",
    "    print(\"   ‚Üí Then restart kernel and run again\")\n",
    "\n",
    "# Check for OpenMP\n",
    "omp_path = \"/opt/anaconda3/envs/course2/lib/libomp.dylib\"\n",
    "if os.path.exists(omp_path):\n",
    "    print(f\"‚úÖ OpenMP found: {omp_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå OpenMP missing: {omp_path}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"‚úÖ XGBoost imported successfully!\")\n",
    "    print(f\"   Version: {xgb.__version__}\")\n",
    "except (ImportError, Exception) as e:\n",
    "    # Catch both ImportError (not installed) and XGBoostError (OpenMP issues)\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    error_str = str(e)\n",
    "    error_type = str(type(e).__name__)\n",
    "    \n",
    "    # Check if it's an OpenMP/runtime issue (XGBoost IS installed but can't run)\n",
    "    if 'XGBoostError' in error_type or 'OpenMP' in error_str or 'libomp' in error_str or 'Library not loaded' in error_str:\n",
    "        print(\"‚ö†Ô∏è  XGBoost import failed (OpenMP or kernel issue)\")\n",
    "        print(\"\\n   üîç DIAGNOSIS:\")\n",
    "        print(\"   - XGBoost is installed in course2 environment\")\n",
    "        print(\"   - OpenMP libraries are available\")\n",
    "        print(\"   - This might be a kernel/environment mismatch\")\n",
    "        print(\"\\n   üîß SOLUTION:\")\n",
    "        print(\"   1. Make sure you're using 'course2' kernel in Jupyter\")\n",
    "        print(\"      (Check: Kernel ‚Üí Change Kernel ‚Üí course2)\")\n",
    "        print(\"   2. Restart kernel: Kernel ‚Üí Restart Kernel\")\n",
    "        print(\"   3. Run this cell again\")\n",
    "        print(\"\\n   üí° If still failing, try in terminal:\")\n",
    "        print(\"      conda activate course2\")\n",
    "        print(\"      python -c 'import xgboost; print(xgboost.__version__)'\")\n",
    "        print(\"\\n   ‚úÖ The notebook will still work without XGBoost (Random Forest will run)\")\n",
    "    elif 'No module named' in error_str or 'ImportError' in error_type:\n",
    "        print(\"‚ö†Ô∏è  XGBoost is NOT installed in this Python environment\")\n",
    "        print(f\"\\n   üîß QUICK FIX - Run this in a terminal:\")\n",
    "        print(f\"      {sys.executable} -m pip install xgboost\")\n",
    "        print(\"\\n   Or install directly in notebook (uncomment next cell):\")\n",
    "        print(\"      !pip install xgboost\")\n",
    "        print(\"\\n   üí° After installing, restart kernel and run this cell again\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Warning: XGBoost error: {error_str[:100]}\")\n",
    "        print(\"   Try: pip install --upgrade --force-reinstall xgboost\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "    print(\"‚úÖ LightGBM imported successfully!\")\n",
    "except (ImportError, Exception) as e:\n",
    "    # Catch both ImportError (not installed) and other errors\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    if 'OpenMP' in str(e) or 'libomp' in str(e):\n",
    "        print(\"‚ö†Ô∏è  Warning: LightGBM installed but OpenMP runtime missing.\")\n",
    "        print(\"   Mac users: Run 'brew install libomp' to install OpenMP\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Warning: LightGBM not installed. Install with: pip install lightgbm\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 2: Gradient Boosting (XGBoost, LightGBM)\")\n",
    "print(\"ŸÖÿ´ÿßŸÑ 2: ÿßŸÑÿ™ÿπÿ≤Ÿäÿ≤ ÿßŸÑŸÖÿ™ÿØÿ±ÿ¨ (XGBoost, LightGBM)\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.511583Z",
     "iopub.status.busy": "2026-01-20T11:42:17.511524Z",
     "iopub.status.idle": "2026-01-20T11:42:17.519094Z",
     "shell.execute_reply": "2026-01-20T11:42:17.518899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading Montgomery 911 Calls dataset...\n",
      "ÿ™ÿ≠ŸÖŸäŸÑ ŸÖÿ¨ŸÖŸàÿπÿ© ÿ®ŸäÿßŸÜÿßÿ™ ŸÖŸÉÿßŸÑŸÖÿßÿ™ ÿßŸÑÿ∑Ÿàÿßÿ±ÿ¶ ŸÅŸä ŸÖŸàŸÜÿ™ÿ∫ŸàŸÖÿ±Ÿä...\n",
      "\n",
      "‚ö†Ô∏è  Dataset file not found!\n",
      "   Please ensure 'montgomery_911_calls.csv' is in '../../datasets/raw/'\n",
      "   Creating minimal structure for demonstration...\n",
      "   ‚ö†Ô∏è  Using synthetic data - please download the real dataset!\n"
     ]
    }
   ],
   "source": [
    "# Load real-world Montgomery 911 Calls dataset\n",
    "# This is REAL emergency response data perfect for demonstrating boosting algorithms!\n",
    "\n",
    "print(\"\\nüì• Loading Montgomery 911 Calls dataset...\")\n",
    "print(\"ÿ™ÿ≠ŸÖŸäŸÑ ŸÖÿ¨ŸÖŸàÿπÿ© ÿ®ŸäÿßŸÜÿßÿ™ ŸÖŸÉÿßŸÑŸÖÿßÿ™ ÿßŸÑÿ∑Ÿàÿßÿ±ÿ¶ ŸÅŸä ŸÖŸàŸÜÿ™ÿ∫ŸàŸÖÿ±Ÿä...\")\n",
    "\n",
    "try:\n",
    "    # Load 911 calls data\n",
    "    df_full = pd.read_csv('../../datasets/raw/montgomery_911_calls.csv')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Real-world Montgomery 911 Calls dataset loaded!\")\n",
    "    print(f\"   üìä This is REAL emergency response data from Montgomery County\")\n",
    "    print(f\"   üìà Contains {len(df_full):,} emergency call records\")\n",
    "    print(f\"   üéØ GDI Application: Emergency Response - analyzing emergency call patterns\")\n",
    "    \n",
    "    # Sample data for faster processing (boosting can be slow on very large datasets)\n",
    "    # Use 20,000 samples for demonstration (good balance between speed and representativeness)\n",
    "    sample_size = min(20000, len(df_full))\n",
    "    df_sample = df_full.sample(n=sample_size, random_state=73)\n",
    "    \n",
    "    # Prepare features for classification\n",
    "    # Target: Predict emergency category (EMS, Fire, Traffic)\n",
    "    # Features: lat, lng, zip, and date/time features\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    # Extract category from title (EMS, Fire, Traffic)\n",
    "    df_sample['category'] = df_sample['title'].str.split(':').str[0]\n",
    "    \n",
    "    # Encode target\n",
    "    le_category = LabelEncoder()\n",
    "    df_sample['target'] = le_category.fit_transform(df_sample['category'])\n",
    "    \n",
    "    # Extract date/time features\n",
    "    df_sample['timeStamp'] = pd.to_datetime(df_sample['timeStamp'], errors='coerce')\n",
    "    df_sample['hour'] = df_sample['timeStamp'].dt.hour\n",
    "    df_sample['day_of_week'] = df_sample['timeStamp'].dt.dayofweek\n",
    "    df_sample['month'] = df_sample['timeStamp'].dt.month\n",
    "    \n",
    "    # Select features for classification\n",
    "    feature_cols = ['lat', 'lng', 'zip', 'hour', 'day_of_week', 'month']\n",
    "    X_data = df_sample[feature_cols].copy()\n",
    "    y_data = df_sample['target'].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    X_data = X_data.fillna(X_data.median())\n",
    "    \n",
    "    # Get category names for reference\n",
    "    category_names = le_category.classes_\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Summary:\")\n",
    "    print(f\"   - Samples: {len(X_data):,}\")\n",
    "    print(f\"   - Features: {len(feature_cols)} ({', '.join(feature_cols)})\")\n",
    "    print(f\"   - Target: Multi-class classification ({len(category_names)} classes)\")\n",
    "    print(f\"   - Categories: {', '.join(category_names)}\")\n",
    "    print(f\"\\nüìä Target distribution:\")\n",
    "    print(y_data.value_counts().sort_index())\n",
    "    print(f\"\\nüîç Notice:\")\n",
    "    print(f\"   - This is REAL emergency response data\")\n",
    "    print(f\"   - We'll compare Random Forest (bagging) vs XGBoost/LightGBM (boosting)\")\n",
    "    print(f\"   - Boosting can perform better or worse than bagging; results depend on data and tuning\")\n",
    "    print(f\"   - Perfect for demonstrating the power of boosting algorithms!\")\n",
    "    print(f\"   - GDI Application: Emergency Response - optimizing emergency prediction models\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\n‚ö†Ô∏è  Dataset file not found!\")\n",
    "    print(\"   Please ensure 'montgomery_911_calls.csv' is in '../../datasets/raw/'\")\n",
    "    print(\"   Creating minimal structure for demonstration...\")\n",
    "    # Fallback: Create minimal structure\n",
    "    np.random.seed(73)\n",
    "    n_samples = 2000\n",
    "    X_data = pd.DataFrame({\n",
    "        'lat': np.random.uniform(39.9, 40.4, n_samples),\n",
    "        'lng': np.random.uniform(-75.7, -75.0, n_samples),\n",
    "        'zip': np.random.randint(18000, 19600, n_samples),\n",
    "        'hour': np.random.randint(0, 24, n_samples),\n",
    "        'day_of_week': np.random.randint(0, 7, n_samples),\n",
    "        'month': np.random.randint(1, 13, n_samples)\n",
    "    })\n",
    "    y_data = pd.Series(np.random.randint(0, 3, n_samples))\n",
    "    category_names = ['EMS', 'Fire', 'Traffic']\n",
    "    print(\"   ‚ö†Ô∏è  Using synthetic data - please download the real dataset!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.519890Z",
     "iopub.status.busy": "2026-01-20T11:42:17.519834Z",
     "iopub.status.idle": "2026-01-20T11:42:17.522742Z",
     "shell.execute_reply": "2026-01-20T11:42:17.522558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data split successfully!\n",
      "   Training set: 1600 samples (80.0%)\n",
      "   Testing set: 400 samples (20.0%)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "# train_test_split(X, y, test_size=0.2, random_state=73, stratify=y)\n",
    "# - Splits data into training and testing sets\n",
    "# - X: Features (input variables), y: Target (output variable)\n",
    "# - test_size=0.2: 20% for testing, 80% for training\n",
    "# - random_state=73: Seed for reproducibility (same split every time)\n",
    "# - stratify=y: Maintains class distribution in train/test (for classification)\n",
    "# - Returns: X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=73, stratify=y_data\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data split successfully!\")\n",
    "print(f\"   Training set: {len(X_train)} samples ({len(X_train)/len(X_data)*100:.1f}%)\")\n",
    "print(f\"   Testing set: {len(X_test)} samples ({len(X_test)/len(X_data)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Understanding the Dataset | ŸÅŸáŸÖ ŸÖÿ¨ŸÖŸàÿπÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | ŸÑŸÑÿ∑ŸÑÿßÿ® ŸÅŸä ÿπŸÑŸàŸÖ ÿßŸÑÿ≠ÿßÿ≥Ÿàÿ® - ÿ±ŸÉÿ≤ ÿπŸÑŸâ ŸáŸäŸÉŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ÿå ŸàŸÑŸäÿ≥ ÿßŸÑŸÖÿ¨ÿßŸÑ\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (emergency response, financial, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: ~20,000 rows √ó 6 columns (samples √ó features)\n",
    "- **Feature Types**: Mix of numerical (lat, lng, zip, hour, day_of_week, month)\n",
    "- **Target Type**: Classification (predicting emergency category: EMS, Fire, Traffic)\n",
    "- **Task**: Predict emergency category based on location and time features\n",
    "- **Data Quality**: Real-world emergency response data with multi-class classification\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Multi-class classification** ‚Üí Need classification metrics (accuracy, precision, recall, F1)\n",
    "- **Mixed feature types** ‚Üí Boosting works well (can handle numerical data)\n",
    "- **Real-world data** ‚Üí Shows boosting on real classification problem\n",
    "- **Good for boosting** ‚Üí Demonstrates how boosting algorithms learn complex patterns\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ŸÅŸáŸÖ ŸÖÿ¨ÿßŸÑ ŸÖÿ¨ŸÖŸàÿπÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ (ÿ®ÿßÿÆÿ™ÿµÿßÿ±)\n",
    "\n",
    "**What is this data?** Montgomery County 911 Calls dataset - emergency call records with location and time information.\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For boosting**: Shows how boosting algorithms perform on real emergency data\n",
    "- **For feature importance**: Boosting will show which features matter most (location, time, etc.)\n",
    "- **For evaluation**: Multi-class classification ‚Üí use classification metrics (accuracy, precision, recall, F1)\n",
    "\n",
    "**Domain Context** (Brief):\n",
    "- **Features**: Location (lat, lng, zip), Time (hour, day_of_week, month) - 6 features total\n",
    "- **Target**: Emergency category (0 = EMS, 1 = Fire, 2 = Traffic)\n",
    "- **Task**: Predict emergency category from location and time features\n",
    "- **Why boosting works**: Boosting can learn complex patterns from location and time features\n",
    "- **GDI Application**: Emergency Response - optimizing emergency prediction models for faster response\n",
    "\n",
    "**üí° Key Point for CS Students**: You don't need to be an emergency response expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types, classes)\n",
    "- Knowing the **task type** (multi-class classification: 3 classes)\n",
    "- Understanding how **boosting algorithms** learn from data\n",
    "- Choosing the right **boosting algorithm** (XGBoost vs LightGBM) based on structure, not domain knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ£ŸàŸÑ: ÿ•ÿπÿØÿßÿØ ÿßŸÑŸÖÿ¥ŸáÿØ\n",
    "\n",
    "**BEFORE**: We've used Random Forest (bagging) where models are trained independently.\n",
    "\n",
    "**AFTER**: We'll use boosting where models are trained sequentially, each learning from previous mistakes!\n",
    "\n",
    "**Why this matters**: Boosting can achieve better performance than bagging, but it depends on the data and tuning. XGBoost and LightGBM are state-of-the-art algorithms used in production systems!\n",
    "\n",
    "**Note on results**: If Random Forest scores higher in your run, that's normal. Performance depends on data, sample size, and hyperparameters. The goal here is to compare methods, not to guarantee a winner. Re-run the training cells to refresh the printed messages with your current results.\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: What's the difference between boosting and bagging (Random Forest)?**\n",
    "  - Answer: Boosting = sequential (each model learns from previous mistakes), Bagging = parallel (models trained independently)\n",
    "  - Boosting: Model 1 ‚Üí Model 2 fixes Model 1's mistakes ‚Üí Model 3 fixes Model 2's mistakes ‚Üí ...\n",
    "  - Bagging: Model 1, Model 2, Model 3 trained independently ‚Üí average predictions\n",
    "  - Boosting can perform better but is not guaranteed; bagging is often more stable and less prone to overfitting\n",
    "- **Q: Why is XGBoost so popular?**\n",
    "  - Answer: XGBoost = fast, powerful, handles missing values, regularization built-in\n",
    "  - Often achieves best performance in competitions\n",
    "  - Easy to use, well-documented, widely supported\n",
    "  - Industry standard for tabular data (structured data)\n",
    "- **Q: What's the difference between XGBoost and LightGBM?**\n",
    "  - Answer: LightGBM = faster than XGBoost (especially on large datasets)\n",
    "  - XGBoost = level-wise tree growth (grows all leaves at same level)\n",
    "  - LightGBM = leaf-wise tree growth (grows best leaves first) ‚Üí faster, uses less memory\n",
    "  - Rule of thumb: Use LightGBM for large datasets (> 10k samples), XGBoost for smaller datasets\n",
    "- **Q: Why use boosting instead of just one decision tree?**\n",
    "  - Answer: Single tree = can overfit, boosting = many trees combined ‚Üí less overfitting, better performance\n",
    "  - Boosting combines weak learners (simple trees) into strong learner (powerful model)\n",
    "  - Each tree corrects previous mistakes ‚Üí sequential improvement ‚Üí excellent performance\n",
    "- **Q: What is learning rate in boosting?**\n",
    "  - Answer: Learning rate = how much each new tree contributes (usually 0.01 to 0.3)\n",
    "  - Low learning rate (0.01): Small steps, many trees needed, less overfitting\n",
    "  - High learning rate (0.3): Large steps, fewer trees needed, can overfit\n",
    "  - Rule of thumb: Lower learning rate + more trees = better performance (but slower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.523551Z",
     "iopub.status.busy": "2026-01-20T11:42:17.523493Z",
     "iopub.status.idle": "2026-01-20T11:42:17.600501Z",
     "shell.execute_reply": "2026-01-20T11:42:17.600279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "1. Random Forest (Baseline)\n",
      "ÿßŸÑÿ∫ÿßÿ®ÿ© ÿßŸÑÿπÿ¥Ÿàÿßÿ¶Ÿäÿ© (ÿÆÿ∑ ÿßŸÑÿ£ÿ≥ÿßÿ≥)\n",
      "============================================================\n",
      "\n",
      "üí° Transition: First, we'll train Random Forest (bagging) as a baseline!\n",
      "   This gives us a reference point to compare boosting algorithms against.\n",
      "\n",
      "üìä Random Forest (Bagging) Results:\n",
      "   Training Accuracy: 0.6000\n",
      "   Test Accuracy: 0.3500\n",
      "\n",
      "   üí° Random Forest (Bagging):\n",
      "   - All 100 trees trained independently (in parallel)\n",
      "   - Final prediction = average of all trees\n",
      "   - This is our baseline to compare with boosting!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. Random Forest (Baseline)\")\n",
    "print(\"ÿßŸÑÿ∫ÿßÿ®ÿ© ÿßŸÑÿπÿ¥Ÿàÿßÿ¶Ÿäÿ© (ÿÆÿ∑ ÿßŸÑÿ£ÿ≥ÿßÿ≥)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Transition: First, we'll train Random Forest (bagging) as a baseline!\")\n",
    "print(\"   This gives us a reference point to compare boosting algorithms against.\")\n",
    "\n",
    "# Random Forest: Bagging (parallel ensemble)\n",
    "# All trees trained independently, then predictions averaged\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=73)  # Using 73 for consistency\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_rf = rf.predict(X_train)\n",
    "y_test_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_acc_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "test_acc_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(f\"\\nüìä Random Forest (Bagging) Results:\")\n",
    "print(f\"   Training Accuracy: {train_acc_rf:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc_rf:.4f}\")\n",
    "\n",
    "print(f\"\\n   üí° Random Forest (Bagging):\")\n",
    "print(f\"   - All 100 trees trained independently (in parallel)\")\n",
    "print(f\"   - Final prediction = average of all trees\")\n",
    "print(f\"   - This is our baseline to compare with boosting!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.601574Z",
     "iopub.status.busy": "2026-01-20T11:42:17.601502Z",
     "iopub.status.idle": "2026-01-20T11:42:17.604392Z",
     "shell.execute_reply": "2026-01-20T11:42:17.604222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. XGBoost\n",
      "XGBoost\n",
      "============================================================\n",
      "\n",
      "üí° Transition: Now we'll train XGBoost (boosting) to compare with Random Forest.\n",
      "   XGBoost trains trees sequentially, each learning from previous mistakes.\n",
      "\n",
      "üìö Teaching Note: We're using TUNED parameters for boosting to demonstrate its potential:\n",
      "   - More trees (200 vs 100) - boosting benefits from more iterations\n",
      "   - Lower learning rate (0.05 vs 0.1) - more stable, less overfitting\n",
      "   - Regularization (subsample, colsample, reg_alpha, reg_lambda) - prevents overfitting\n",
      "   - This shows WHY tuning matters for boosting - it can significantly improve performance!\n",
      "‚ö†Ô∏è  XGBoost not available - skipping this section\n",
      "   ‚úÖ Don't worry! The notebook still works perfectly.\n",
      "   ‚úÖ You can still learn from Random Forest (Cell 6) and other examples.\n",
      "   üí° XGBoost is optional - you can continue learning without it.\n",
      "\n",
      "   üîß To use XGBoost (optional):\n",
      "   - If you saw 'OpenMP runtime missing' earlier:\n",
      "     1. Run: brew install libomp\n",
      "     2. Run: pip install --upgrade --force-reinstall xgboost\n",
      "   - Otherwise: pip install xgboost\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. XGBoost\")\n",
    "print(\"XGBoost\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Transition: Now we'll train XGBoost (boosting) to compare with Random Forest.\")\n",
    "print(\"   XGBoost trains trees sequentially, each learning from previous mistakes.\")\n",
    "print(\"\\nüìö Teaching Note: We're using TUNED parameters for boosting to demonstrate its potential:\")\n",
    "print(\"   - More trees (200 vs 100) - boosting benefits from more iterations\")\n",
    "print(\"   - Lower learning rate (0.05 vs 0.1) - more stable, less overfitting\")\n",
    "print(\"   - Regularization (subsample, colsample, reg_alpha, reg_lambda) - prevents overfitting\")\n",
    "print(\"   - This shows WHY tuning matters for boosting - it can significantly improve performance!\")\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    # XGBoost: Boosting (sequential ensemble)\n",
    "    # XGBoost with tuned parameters to demonstrate boosting's potential\n",
    "    # Key: More trees + lower learning rate + regularization = better performance\n",
    "    # This showcases why tuning matters for boosting!\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=200,      # More trees for better performance (boosting benefits from more iterations)\n",
    "        max_depth=6,           # Slightly deeper trees to capture more patterns\n",
    "        learning_rate=0.05,    # Lower learning rate = more stable, less overfitting (needs more trees)\n",
    "        subsample=0.8,         # Use 80% of data per tree (reduces overfitting)\n",
    "        colsample_bytree=0.8,  # Use 80% of features per tree (reduces overfitting)\n",
    "        reg_alpha=0.1,         # L1 regularization (reduces overfitting)\n",
    "        reg_lambda=1.0,        # L2 regularization (reduces overfitting)\n",
    "        random_state=73,       # Using 73 for consistency\n",
    "        eval_metric='logloss'  # Evaluation metric\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "    y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_acc_xgb = accuracy_score(y_train, y_train_pred_xgb)\n",
    "    test_acc_xgb = accuracy_score(y_test, y_test_pred_xgb)\n",
    "    \n",
    "    print(f\"\\nüìä XGBoost (Boosting) Results:\")\n",
    "    print(f\"   Training Accuracy: {train_acc_xgb:.4f}\")\n",
    "    print(f\"   Test Accuracy: {test_acc_xgb:.4f}\")\n",
    "    \n",
    "    # Probability predictions (for multi-class classification)\n",
    "    y_test_proba_xgb = xgb_model.predict_proba(X_test)\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ XGBoost (Boosting) - Tuned Parameters:\")\n",
    "    print(f\"   - Trees trained sequentially (one after another)\")\n",
    "    print(f\"   - Each tree focuses on mistakes of previous trees\")\n",
    "    print(f\"   - Using 200 trees with lower learning rate (0.05) for better performance\")\n",
    "    print(f\"   - Added regularization (subsample, colsample, reg_alpha, reg_lambda) to prevent overfitting\")\n",
    "    print(f\"   - This demonstrates why tuning matters for boosting!\")\n",
    "    print(f\"   - Compare test accuracy vs Random Forest to see boosting's potential\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  XGBoost not available - skipping this section\")\n",
    "    print(\"   ‚úÖ Don't worry! The notebook still works perfectly.\")\n",
    "    print(\"   ‚úÖ You can still learn from Random Forest (Cell 6) and other examples.\")\n",
    "    print(\"   üí° XGBoost is optional - you can continue learning without it.\")\n",
    "    print(\"\\n   üîß To use XGBoost (optional):\")\n",
    "    print(\"   - If you saw 'OpenMP runtime missing' earlier:\")\n",
    "    print(\"     1. Run: brew install libomp\")\n",
    "    print(\"     2. Run: pip install --upgrade --force-reinstall xgboost\")\n",
    "    print(\"   - Otherwise: pip install xgboost\")\n",
    "    train_acc_xgb = test_acc_xgb = 0\n",
    "    y_test_proba_xgb = None\n",
    "\n",
    "# Initialize LightGBM variables in case it's not available\n",
    "if not LIGHTGBM_AVAILABLE:\n",
    "    train_acc_lgb = test_acc_lgb = 0\n",
    "    y_test_proba_lgb = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.605216Z",
     "iopub.status.busy": "2026-01-20T11:42:17.605159Z",
     "iopub.status.idle": "2026-01-20T11:42:17.607721Z",
     "shell.execute_reply": "2026-01-20T11:42:17.607543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. LightGBM\n",
      "LightGBM\n",
      "============================================================\n",
      "\n",
      "üí° Transition: Finally, we'll train LightGBM - faster alternative to XGBoost!\n",
      "   LightGBM uses leaf-wise growth, making it faster especially on large datasets.\n",
      "\n",
      "üìö Teaching Note: Using TUNED parameters (same as XGBoost) to show boosting's potential:\n",
      "   - More trees (200 vs 100) - boosting benefits from more iterations\n",
      "   - Lower learning rate (0.05 vs 0.1) - more stable, less overfitting\n",
      "   - Regularization - prevents overfitting while maintaining performance\n",
      "‚ö†Ô∏è  LightGBM not available - skipping this section\n",
      "   ‚úÖ Don't worry! The notebook still works perfectly.\n",
      "   ‚úÖ You can still learn from Random Forest (Cell 6) and XGBoost (Cell 7).\n",
      "   üí° LightGBM is optional - you can continue learning without it.\n",
      "\n",
      "   üîß To use LightGBM (optional):\n",
      "   - Install with: pip install lightgbm\n",
      "   - If you see OpenMP errors on Mac: brew install libomp\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. LightGBM\")\n",
    "print(\"LightGBM\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Transition: Finally, we'll train LightGBM - faster alternative to XGBoost!\")\n",
    "print(\"   LightGBM uses leaf-wise growth, making it faster especially on large datasets.\")\n",
    "print(\"\\nüìö Teaching Note: Using TUNED parameters (same as XGBoost) to show boosting's potential:\")\n",
    "print(\"   - More trees (200 vs 100) - boosting benefits from more iterations\")\n",
    "print(\"   - Lower learning rate (0.05 vs 0.1) - more stable, less overfitting\")\n",
    "print(\"   - Regularization - prevents overfitting while maintaining performance\")\n",
    "\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    # LightGBM: Boosting (sequential ensemble)\n",
    "    # Faster than XGBoost, great for large datasets\n",
    "    # Uses leaf-wise tree growth (more efficient than level-wise)\n",
    "    # LightGBM with tuned parameters to demonstrate boosting's potential\n",
    "    # Key: More trees + lower learning rate + regularization = better performance\n",
    "    # This showcases why tuning matters for boosting!\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=200,      # More trees for better performance (boosting benefits from more iterations)\n",
    "        max_depth=6,           # Slightly deeper trees to capture more patterns\n",
    "        learning_rate=0.05,    # Lower learning rate = more stable, less overfitting (needs more trees)\n",
    "        subsample=0.8,         # Use 80% of data per tree (reduces overfitting)\n",
    "        colsample_bytree=0.8,  # Use 80% of features per tree (reduces overfitting)\n",
    "        reg_alpha=0.1,         # L1 regularization (reduces overfitting)\n",
    "        reg_lambda=1.0,        # L2 regularization (reduces overfitting)\n",
    "        random_state=73,       # Using 73 for consistency\n",
    "        verbose=-1             # Suppress output (-1 = no output)\n",
    "    )\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred_lgb = lgb_model.predict(X_train)\n",
    "    y_test_pred_lgb = lgb_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_acc_lgb = accuracy_score(y_train, y_train_pred_lgb)\n",
    "    test_acc_lgb = accuracy_score(y_test, y_test_pred_lgb)\n",
    "    \n",
    "    print(f\"\\nüìä LightGBM (Boosting) Results:\")\n",
    "    print(f\"   Training Accuracy: {train_acc_lgb:.4f}\")\n",
    "    print(f\"   Test Accuracy: {test_acc_lgb:.4f}\")\n",
    "    \n",
    "    # Probability predictions (for multi-class classification)\n",
    "    y_test_proba_lgb = lgb_model.predict_proba(X_test)\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ LightGBM (Boosting) - Tuned Parameters:\")\n",
    "    print(f\"   - Faster than XGBoost, great for large datasets\")\n",
    "    print(f\"   - Uses leaf-wise tree growth (more efficient)\")\n",
    "    print(f\"   - Using 200 trees with lower learning rate (0.05) for better performance\")\n",
    "    print(f\"   - Added regularization (subsample, colsample, reg_alpha, reg_lambda) to prevent overfitting\")\n",
    "    print(f\"   - This demonstrates why tuning matters for boosting!\")\n",
    "    print(f\"   - Can match or exceed XGBoost performance depending on data and tuning\")\n",
    "    print(f\"   - Compare test accuracy across models to see boosting's potential\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  LightGBM not available - skipping this section\")\n",
    "    print(\"   ‚úÖ Don't worry! The notebook still works perfectly.\")\n",
    "    print(\"   ‚úÖ You can still learn from Random Forest (Cell 6) and XGBoost (Cell 7).\")\n",
    "    print(\"   üí° LightGBM is optional - you can continue learning without it.\")\n",
    "    print(\"\\n   üîß To use LightGBM (optional):\")\n",
    "    print(\"   - Install with: pip install lightgbm\")\n",
    "    print(\"   - If you see OpenMP errors on Mac: brew install libomp\")\n",
    "    train_acc_lgb = test_acc_lgb = 0\n",
    "    y_test_proba_lgb = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.608527Z",
     "iopub.status.busy": "2026-01-20T11:42:17.608473Z",
     "iopub.status.idle": "2026-01-20T11:42:17.610843Z",
     "shell.execute_reply": "2026-01-20T11:42:17.610652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üí° Interpreting Random Forest Results | ÿ™ŸÅÿ≥Ÿäÿ± ŸÜÿ™ÿßÿ¶ÿ¨ Random Forest\n",
      "============================================================\n",
      "\n",
      "üìä Random Forest Performance:\n",
      "   - Training Accuracy: 0.6000 (60.00%)\n",
      "   - Test Accuracy: 0.3500 (35.00%)\n",
      "   - Gap: 0.2500 (25.00 percentage points)\n",
      "   - Status: ‚ö†Ô∏è  Warning\n",
      "   - Meaning: Possible overfitting - large gap between train and test\n",
      "\n",
      "üìä Random Forest (Bagging):\n",
      "   - Bagging: Models trained in parallel, then averaged\n",
      "   - Reduces variance by combining multiple models\n",
      "   - Good baseline for comparison with boosting\n",
      "\n",
      "üìö What This Teaches Us:\n",
      "   - Random Forest uses bagging (parallel ensemble)\n",
      "   - Small gap = good generalization\n",
      "   - Large gap = possible overfitting\n",
      "   - Test accuracy is what matters for real-world performance\n"
     ]
    }
   ],
   "source": [
    "# Add interpretation after Random Forest results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° Interpreting Random Forest Results | ÿ™ŸÅÿ≥Ÿäÿ± ŸÜÿ™ÿßÿ¶ÿ¨ Random Forest\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gap_rf = abs(train_acc_rf - test_acc_rf)\n",
    "\n",
    "print(f\"\\nüìä Random Forest Performance:\")\n",
    "print(f\"   - Training Accuracy: {train_acc_rf:.4f} ({train_acc_rf*100:.2f}%)\")\n",
    "print(f\"   - Test Accuracy: {test_acc_rf:.4f} ({test_acc_rf*100:.2f}%)\")\n",
    "print(f\"   - Gap: {gap_rf:.4f} ({gap_rf*100:.2f} percentage points)\")\n",
    "\n",
    "if gap_rf < 0.01:\n",
    "    status = \"‚úÖ Excellent\"\n",
    "    meaning = \"No overfitting - model generalizes well\"\n",
    "elif gap_rf < 0.05:\n",
    "    status = \"‚úÖ Good\"\n",
    "    meaning = \"Minimal overfitting\"\n",
    "else:\n",
    "    status = \"‚ö†Ô∏è  Warning\"\n",
    "    meaning = \"Possible overfitting - large gap between train and test\"\n",
    "\n",
    "print(f\"   - Status: {status}\")\n",
    "print(f\"   - Meaning: {meaning}\")\n",
    "\n",
    "print(f\"\\nüìä Random Forest (Bagging):\")\n",
    "print(f\"   - Bagging: Models trained in parallel, then averaged\")\n",
    "print(f\"   - Reduces variance by combining multiple models\")\n",
    "print(f\"   - Good baseline for comparison with boosting\")\n",
    "\n",
    "print(f\"\\nüìö What This Teaches Us:\")\n",
    "print(f\"   - Random Forest uses bagging (parallel ensemble)\")\n",
    "print(f\"   - Small gap = good generalization\")\n",
    "print(f\"   - Large gap = possible overfitting\")\n",
    "print(f\"   - Test accuracy is what matters for real-world performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.611637Z",
     "iopub.status.busy": "2026-01-20T11:42:17.611582Z",
     "iopub.status.idle": "2026-01-20T11:42:17.614510Z",
     "shell.execute_reply": "2026-01-20T11:42:17.614301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. Model Comparison\n",
      "ŸÖŸÇÿßÿ±ŸÜÿ© ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨\n",
      "============================================================\n",
      "\n",
      "üí° Transition: Now let's compare all models using test accuracy.\n",
      "   There is no guaranteed winner; results depend on data and tuning.\n",
      "\n",
      "Model Comparison:\n",
      "        Model  Train Accuracy  Test Accuracy\n",
      "Random Forest             0.6           0.35\n"
     ]
    }
   ],
   "source": [
    "# 4. Model Comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Model Comparison\")\n",
    "print(\"ŸÖŸÇÿßÿ±ŸÜÿ© ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Transition: Now let's compare all models using test accuracy.\")\n",
    "print(\"   There is no guaranteed winner; results depend on data and tuning.\")\n",
    "comparison_data = {\n",
    "    'Model': ['Random Forest'], 'Train Accuracy': [train_acc_rf],\n",
    "    'Test Accuracy': [test_acc_rf]\n",
    "}\n",
    "if XGBOOST_AVAILABLE:\n",
    "    comparison_data['Model'].append('XGBoost')\n",
    "    comparison_data['Train Accuracy'].append(train_acc_xgb)\n",
    "    comparison_data['Test Accuracy'].append(test_acc_xgb)\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    comparison_data['Model'].append('LightGBM')\n",
    "    comparison_data['Train Accuracy'].append(train_acc_lgb)\n",
    "    comparison_data['Test Accuracy'].append(test_acc_lgb)\n",
    "# pd.DataFrame(data)\n",
    "# - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "# - data: Dictionary where keys become column names, values become column data\n",
    "#   - Each key-value pair: key = column name, value = list of values for that column\n",
    "# - Returns DataFrame with rows and columns\n",
    "# - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.615337Z",
     "iopub.status.busy": "2026-01-20T11:42:17.615285Z",
     "iopub.status.idle": "2026-01-20T11:42:17.617330Z",
     "shell.execute_reply": "2026-01-20T11:42:17.617125Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add interpretation after XGBoost results (if available)\n",
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üí° Interpreting XGBoost Results | ÿ™ŸÅÿ≥Ÿäÿ± ŸÜÿ™ÿßÿ¶ÿ¨ XGBoost\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    gap_xgb = abs(train_acc_xgb - test_acc_xgb)\n",
    "    \n",
    "    print(f\"\\nüìä XGBoost Performance:\")\n",
    "    print(f\"   - Training Accuracy: {train_acc_xgb:.4f} ({train_acc_xgb*100:.2f}%)\")\n",
    "    print(f\"   - Test Accuracy: {test_acc_xgb:.4f} ({test_acc_xgb*100:.2f}%)\")\n",
    "    print(f\"   - Gap: {gap_xgb:.4f} ({gap_xgb*100:.2f} percentage points)\")\n",
    "    \n",
    "    if gap_xgb < 0.01:\n",
    "        status = \"‚úÖ Excellent\"\n",
    "        meaning = \"No overfitting - model generalizes well\"\n",
    "    elif gap_xgb < 0.05:\n",
    "        status = \"‚úÖ Good\"\n",
    "        meaning = \"Minimal overfitting\"\n",
    "    else:\n",
    "        status = \"‚ö†Ô∏è  Warning\"\n",
    "        meaning = \"Possible overfitting - large gap between train and test\"\n",
    "    \n",
    "    print(f\"   - Status: {status}\")\n",
    "    print(f\"   - Meaning: {meaning}\")\n",
    "    \n",
    "    print(f\"\\nüìä XGBoost (Boosting):\")\n",
    "    print(f\"   - Boosting: Models trained sequentially, each learns from mistakes\")\n",
    "    print(f\"   - XGBoost: Extremely popular, fast, and powerful\")\n",
    "    print(f\"   - Can achieve top performance in competitions (depends on data and tuning)\")\n",
    "    print(f\"   - Not guaranteed to beat bagging on every dataset\")\n",
    "    \n",
    "    print(f\"\\nüìö What This Teaches Us:\")\n",
    "    print(f\"   - XGBoost uses boosting (sequential ensemble)\")\n",
    "    print(f\"   - Each new model focuses on previous mistakes\")\n",
    "    print(f\"   - Can achieve very high training accuracy\")\n",
    "    print(f\"   - Check test accuracy to ensure good generalization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Comparison | ŸÖŸÇÿßÿ±ŸÜÿ© ÿ£ŸáŸÖŸäÿ© ÿßŸÑŸÖŸäÿ≤ÿßÿ™\n",
    "\n",
    "**BEFORE**: We've compared model performance, but which features matter most?\n",
    "\n",
    "**AFTER**: We'll visualize feature importance to understand what drives predictions!\n",
    "\n",
    "**Why this matters**: Feature importance shows which location/time features are most predictive of emergency types - valuable for GDI emergency response planning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.618164Z",
     "iopub.status.busy": "2026-01-20T11:42:17.618110Z",
     "iopub.status.idle": "2026-01-20T11:42:17.620115Z",
     "shell.execute_reply": "2026-01-20T11:42:17.619936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add interpretation after LightGBM results (if available)\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üí° Interpreting LightGBM Results | ÿ™ŸÅÿ≥Ÿäÿ± ŸÜÿ™ÿßÿ¶ÿ¨ LightGBM\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    gap_lgb = abs(train_acc_lgb - test_acc_lgb)\n",
    "    \n",
    "    print(f\"\\nüìä LightGBM Performance:\")\n",
    "    print(f\"   - Training Accuracy: {train_acc_lgb:.4f} ({train_acc_lgb*100:.2f}%)\")\n",
    "    print(f\"   - Test Accuracy: {test_acc_lgb:.4f} ({test_acc_lgb*100:.2f}%)\")\n",
    "    print(f\"   - Gap: {gap_lgb:.4f} ({gap_lgb*100:.2f} percentage points)\")\n",
    "    \n",
    "    if gap_lgb < 0.01:\n",
    "        status = \"‚úÖ Excellent\"\n",
    "        meaning = \"No overfitting - model generalizes well\"\n",
    "    elif gap_lgb < 0.05:\n",
    "        status = \"‚úÖ Good\"\n",
    "        meaning = \"Minimal overfitting\"\n",
    "    else:\n",
    "        status = \"‚ö†Ô∏è  Warning\"\n",
    "        meaning = \"Possible overfitting - large gap between train and test\"\n",
    "    \n",
    "    print(f\"   - Status: {status}\")\n",
    "    print(f\"   - Meaning: {meaning}\")\n",
    "    \n",
    "    print(f\"\\nüìä LightGBM (Boosting):\")\n",
    "    print(f\"   - LightGBM: Faster than XGBoost, great for large datasets\")\n",
    "    print(f\"   - Uses leaf-wise tree growth (more efficient)\")\n",
    "    print(f\"   - Can match or exceed XGBoost performance depending on data and tuning\")\n",
    "    print(f\"   - Not guaranteed to beat Random Forest on every dataset\")\n",
    "    \n",
    "    print(f\"\\nüìö What This Teaches Us:\")\n",
    "    print(f\"   - LightGBM is optimized for speed and memory\")\n",
    "    print(f\"   - Great choice for large datasets\")\n",
    "    print(f\"   - Test accuracy validates model quality\")\n",
    "    print(f\"   - Perfect test accuracy may indicate overfitting (check carefully)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.620977Z",
     "iopub.status.busy": "2026-01-20T11:42:17.620908Z",
     "iopub.status.idle": "2026-01-20T11:42:17.622841Z",
     "shell.execute_reply": "2026-01-20T11:42:17.622646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. Feature Importance Comparison\n",
      "ŸÖŸÇÿßÿ±ŸÜÿ© ÿ£ŸáŸÖŸäÿ© ÿßŸÑŸÖŸäÿ≤ÿßÿ™\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. Feature Importance Comparison\")\n",
    "print(\"ŸÖŸÇÿßÿ±ŸÜÿ© ÿ£ŸáŸÖŸäÿ© ÿßŸÑŸÖŸäÿ≤ÿßÿ™\")\n",
    "print(\"=\" * 60)\n",
    "if XGBOOST_AVAILABLE or LIGHTGBM_AVAILABLE:\n",
    "    importance_data = {\n",
    "        'Feature': X_data.columns, 'Random Forest': rf.feature_importances_\n",
    "    }\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        importance_data['XGBoost'] = xgb_model.feature_importances_\n",
    "    if LIGHTGBM_AVAILABLE:\n",
    "        # LightGBM feature importance: normalize to 0-1 range for comparison\n",
    "        # LightGBM uses gain-based importance (can be large numbers)\n",
    "        lgb_importance = lgb_model.feature_importances_\n",
    "        # Normalize to sum to 1 (like Random Forest and XGBoost)\n",
    "        lgb_importance_normalized = lgb_importance / lgb_importance.sum()\n",
    "        importance_data['LightGBM'] = lgb_importance_normalized\n",
    "    # pd.DataFrame(data)\n",
    "    # - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "    # - data: Dictionary where keys become column names, values become column data\n",
    "    #   - Each key-value pair: key = column name, value = list of values for that column\n",
    "    # - Returns DataFrame with rows and columns\n",
    "    # - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "    importance_df = pd.DataFrame(importance_data)\n",
    "    importance_df = importance_df.sort_values('Random Forest', ascending=False)\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance_df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.623720Z",
     "iopub.status.busy": "2026-01-20T11:42:17.623670Z",
     "iopub.status.idle": "2026-01-20T11:42:17.629003Z",
     "shell.execute_reply": "2026-01-20T11:42:17.628830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üí° Interpreting Model Comparison | ÿ™ŸÅÿ≥Ÿäÿ± ŸÖŸÇÿßÿ±ŸÜÿ© ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨\n",
      "============================================================\n",
      "\n",
      "üìä Model Performance Summary:\n",
      "   - Best Model (Test Accuracy): Random Forest\n",
      "   - Best Test Accuracy: 0.3500 (35.00%)\n",
      "\n",
      "üîç Bagging vs Boosting Comparison:\n",
      "\n",
      "üìä Overfitting Analysis:\n",
      "   - Random Forest: Gap=0.2500 (‚ö†Ô∏è  Possible overfitting)\n",
      "\n",
      "üìö What This Teaches Us:\n",
      "   - Bagging (Random Forest): Parallel ensemble, reduces variance\n",
      "   - Boosting (XGBoost/LightGBM): Sequential ensemble, reduces bias\n",
      "   - Test accuracy is the most important metric (real-world performance)\n",
      "   - Small train-test gap = good generalization\n",
      "   - Either method can win: performance depends on data, tuning, and problem type\n",
      "   - Boosting may achieve higher accuracy with tuning but can overfit (not guaranteed)\n",
      "   - Bagging is often more stable and can win on some datasets without tuning\n",
      "   - Choose model based on test performance, not training performance\n",
      "\n",
      "üí° Why These Results? Understanding Performance Differences:\n",
      "   ============================================================\n",
      "\n",
      "   üéØ Boosting Won - Here's Why:\n",
      "   ------------------------------------------------------------\n",
      "\n",
      "   üìä General Factors That Affect Performance:\n",
      "   ------------------------------------------------------------\n",
      "   ‚Ä¢ Dataset size: Small datasets favor Random Forest\n",
      "   ‚Ä¢ Data quality: Noisy data favors Random Forest's averaging\n",
      "   ‚Ä¢ Feature complexity: Complex patterns favor boosting\n",
      "   ‚Ä¢ Hyperparameter tuning: Tuned boosting often beats Random Forest\n",
      "   ‚Ä¢ Overfitting risk: Random Forest is more stable\n",
      "   ‚Ä¢ Training time: Random Forest trains faster\n",
      "   \n",
      "   üéì Remember: These results are specific to THIS dataset.\n",
      "      Always test both methods on YOUR data to find the best fit!\n"
     ]
    }
   ],
   "source": [
    "# Add interpretation after comparison table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° Interpreting Model Comparison | ÿ™ŸÅÿ≥Ÿäÿ± ŸÖŸÇÿßÿ±ŸÜÿ© ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Model Performance Summary:\")\n",
    "\n",
    "# Find best test accuracy\n",
    "best_test_idx = comparison_df['Test Accuracy'].idxmax()\n",
    "best_model = comparison_df.loc[best_test_idx, 'Model']\n",
    "best_test_acc = comparison_df.loc[best_test_idx, 'Test Accuracy']\n",
    "\n",
    "print(f\"   - Best Model (Test Accuracy): {best_model}\")\n",
    "print(f\"   - Best Test Accuracy: {best_test_acc:.4f} ({best_test_acc*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüîç Bagging vs Boosting Comparison:\")\n",
    "\n",
    "# Compare Random Forest with boosting models\n",
    "rf_test = comparison_df[comparison_df['Model'] == 'Random Forest']['Test Accuracy'].values[0]\n",
    "rf_train = comparison_df[comparison_df['Model'] == 'Random Forest']['Train Accuracy'].values[0]\n",
    "\n",
    "if 'XGBoost' in comparison_df['Model'].values:\n",
    "    xgb_test = comparison_df[comparison_df['Model'] == 'XGBoost']['Test Accuracy'].values[0]\n",
    "    xgb_train = comparison_df[comparison_df['Model'] == 'XGBoost']['Train Accuracy'].values[0]\n",
    "    print(f\"   - Random Forest (Bagging): Test={rf_test:.4f}, Train={rf_train:.4f}\")\n",
    "    print(f\"   - XGBoost (Boosting): Test={xgb_test:.4f}, Train={xgb_train:.4f}\")\n",
    "    \n",
    "    if xgb_test > rf_test:\n",
    "        print(f\"   - ‚úÖ XGBoost outperforms Random Forest on test data\")\n",
    "    elif xgb_test < rf_test:\n",
    "        print(f\"   - Random Forest outperforms XGBoost on test data\")\n",
    "    else:\n",
    "        print(f\"   - Both models perform similarly on test data\")\n",
    "\n",
    "if 'LightGBM' in comparison_df['Model'].values:\n",
    "    lgb_test = comparison_df[comparison_df['Model'] == 'LightGBM']['Test Accuracy'].values[0]\n",
    "    lgb_train = comparison_df[comparison_df['Model'] == 'LightGBM']['Train Accuracy'].values[0]\n",
    "    print(f\"   - LightGBM (Boosting): Test={lgb_test:.4f}, Train={lgb_train:.4f}\")\n",
    "    \n",
    "    if lgb_test > rf_test:\n",
    "        print(f\"   - ‚úÖ LightGBM outperforms Random Forest on test data\")\n",
    "    elif lgb_test < rf_test:\n",
    "        print(f\"   - Random Forest outperforms LightGBM on test data\")\n",
    "    else:\n",
    "        print(f\"   - LightGBM and Random Forest perform similarly on test data\")\n",
    "\n",
    "print(f\"\\nüìä Overfitting Analysis:\")\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    gap = abs(row['Train Accuracy'] - row['Test Accuracy'])\n",
    "    model = row['Model']\n",
    "    if gap < 0.01:\n",
    "        status = \"‚úÖ No overfitting\"\n",
    "    elif gap < 0.05:\n",
    "        status = \"‚úÖ Minimal overfitting\"\n",
    "    else:\n",
    "        status = \"‚ö†Ô∏è  Possible overfitting\"\n",
    "    print(f\"   - {model}: Gap={gap:.4f} ({status})\")\n",
    "\n",
    "print(f\"\\nüìö What This Teaches Us:\")\n",
    "print(f\"   - Bagging (Random Forest): Parallel ensemble, reduces variance\")\n",
    "print(f\"   - Boosting (XGBoost/LightGBM): Sequential ensemble, reduces bias\")\n",
    "print(f\"   - Test accuracy is the most important metric (real-world performance)\")\n",
    "print(f\"   - Small train-test gap = good generalization\")\n",
    "print(f\"   - Either method can win: performance depends on data, tuning, and problem type\")\n",
    "print(f\"   - Boosting may achieve higher accuracy with tuning but can overfit (not guaranteed)\")\n",
    "print(f\"   - Bagging is often more stable and can win on some datasets without tuning\")\n",
    "print(f\"   - Choose model based on test performance, not training performance\")\n",
    "\n",
    "# Add dynamic explanation based on actual results\n",
    "print(f\"\\nüí° Why These Results? Understanding Performance Differences:\")\n",
    "print(f\"   {'='*60}\")\n",
    "\n",
    "# Check if Random Forest won\n",
    "rf_won = False\n",
    "if 'XGBoost' in comparison_df['Model'].values:\n",
    "    xgb_test = comparison_df[comparison_df['Model'] == 'XGBoost']['Test Accuracy'].values[0]\n",
    "    if rf_test > xgb_test:\n",
    "        rf_won = True\n",
    "elif 'LightGBM' in comparison_df['Model'].values:\n",
    "    lgb_test = comparison_df[comparison_df['Model'] == 'LightGBM']['Test Accuracy'].values[0]\n",
    "    if rf_test > lgb_test:\n",
    "        rf_won = True\n",
    "\n",
    "if rf_won:\n",
    "    print(f\"\\n   üéØ Random Forest (Bagging) Won - Here's Why:\")\n",
    "    print(f\"   {'-'*60}\")\n",
    "    print(f\"   1. ‚úÖ Better Generalization:\")\n",
    "    rf_gap = abs(rf_train - rf_test)\n",
    "    print(f\"      - Random Forest has smaller train-test gap ({rf_gap:.4f})\")\n",
    "    print(f\"      - This means it generalizes better to unseen data\")\n",
    "    print(f\"      - Less overfitting = more reliable predictions\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   2. ‚úÖ Stability Without Tuning:\")\n",
    "    print(f\"      - Random Forest works well with default parameters\")\n",
    "    print(f\"      - Boosting needs careful hyperparameter tuning to excel\")\n",
    "    print(f\"      - Without tuning, Random Forest often performs better\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   3. ‚úÖ Variance Reduction:\")\n",
    "    print(f\"      - Bagging reduces variance by averaging many models\")\n",
    "    print(f\"      - This dataset may benefit more from variance reduction\")\n",
    "    print(f\"      - Random Forest's parallel approach handles noise better\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   4. ‚úÖ Dataset Characteristics:\")\n",
    "    print(f\"      - This dataset may have:\")\n",
    "    print(f\"        ‚Ä¢ High variance (noisy data)\")\n",
    "    print(f\"        ‚Ä¢ Features that benefit from averaging\")\n",
    "    print(f\"        ‚Ä¢ Patterns that don't require sequential learning\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   üí° Key Takeaway: Random Forest's stability and variance reduction\")\n",
    "    print(f\"      make it the better choice for this dataset without extensive tuning!\")\n",
    "else:\n",
    "    print(f\"\\n   üéØ Boosting Won - Here's Why:\")\n",
    "    print(f\"   {'-'*60}\")\n",
    "    if 'XGBoost' in comparison_df['Model'].values:\n",
    "        xgb_test = comparison_df[comparison_df['Model'] == 'XGBoost']['Test Accuracy'].values[0]\n",
    "        xgb_gap = abs(comparison_df[comparison_df['Model'] == 'XGBoost']['Train Accuracy'].values[0] - xgb_test)\n",
    "        print(f\"   1. ‚úÖ Sequential Learning Advantage:\")\n",
    "        print(f\"      - Boosting learns from mistakes sequentially\")\n",
    "        print(f\"      - Each tree corrects previous errors\")\n",
    "        print(f\"      - Better at capturing complex patterns\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   2. ‚úÖ Bias Reduction:\")\n",
    "        print(f\"      - Boosting reduces bias by focusing on hard examples\")\n",
    "        print(f\"      - This dataset may have complex patterns\")\n",
    "        print(f\"      - Sequential learning captures these better\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   3. ‚ö†Ô∏è  But Watch Out for Overfitting:\")\n",
    "        print(f\"      - XGBoost train-test gap: {xgb_gap:.4f}\")\n",
    "        print(f\"      - Larger gap indicates possible overfitting\")\n",
    "        print(f\"      - Still performs better on test data, but monitor closely\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   üí° Key Takeaway: Boosting's sequential learning captured\")\n",
    "        print(f\"      complex patterns better, but needs monitoring for overfitting!\")\n",
    "\n",
    "print(f\"\\n   üìä General Factors That Affect Performance:\")\n",
    "print(f\"   {'-'*60}\")\n",
    "print(f\"   ‚Ä¢ Dataset size: Small datasets favor Random Forest\")\n",
    "print(f\"   ‚Ä¢ Data quality: Noisy data favors Random Forest's averaging\")\n",
    "print(f\"   ‚Ä¢ Feature complexity: Complex patterns favor boosting\")\n",
    "print(f\"   ‚Ä¢ Hyperparameter tuning: Tuned boosting often beats Random Forest\")\n",
    "print(f\"   ‚Ä¢ Overfitting risk: Random Forest is more stable\")\n",
    "print(f\"   ‚Ä¢ Training time: Random Forest trains faster\")\n",
    "print(f\"   \")\n",
    "print(f\"   üéì Remember: These results are specific to THIS dataset.\")\n",
    "print(f\"      Always test both methods on YOUR data to find the best fit!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Decision Framework: When to Choose Boosting vs Bagging | ÿ•ÿ∑ÿßÿ± ÿßŸÑŸÇÿ±ÿßÿ±: ŸÖÿ™Ÿâ ÿ™ÿÆÿ™ÿßÿ± ÿßŸÑÿ™ÿπÿ≤Ÿäÿ≤ ŸÖŸÇÿßÿ®ŸÑ ÿßŸÑÿ™ÿ¨ŸÖŸäÿπ\n",
    "\n",
    "Based on the comparison above, here's a clear guide for when to choose each method:\n",
    "\n",
    "### ‚úÖ Choose **Boosting** (XGBoost/LightGBM) when:\n",
    "\n",
    "1. **You Need Maximum Performance** üèÜ\n",
    "   - **When**: Competitions, critical applications, production systems needing best accuracy\n",
    "   - **Why**: Boosting can achieve higher accuracy through sequential learning and careful tuning\n",
    "   - **Trade-off**: Requires more time for hyperparameter tuning\n",
    "\n",
    "2. **You Have Time for Tuning** ‚öôÔ∏è\n",
    "   - **When**: You can invest time in grid search, cross-validation, and hyperparameter optimization\n",
    "   - **Why**: Boosting performance heavily depends on proper tuning (learning rate, n_estimators, max_depth, etc.)\n",
    "   - **Trade-off**: More complex tuning process\n",
    "\n",
    "3. **You Have Tabular/Structured Data** üìä\n",
    "   - **When**: Working with CSV files, databases, structured datasets\n",
    "   - **Why**: Boosting algorithms excel at finding complex patterns in tabular data\n",
    "   - **Trade-off**: Not ideal for images, text, or unstructured data\n",
    "\n",
    "4. **You Can Handle Potential Overfitting** ‚ö†Ô∏è\n",
    "   - **When**: You have monitoring systems, validation sets, and can detect overfitting\n",
    "   - **Why**: Boosting can overfit if not properly tuned or if data is small\n",
    "   - **Trade-off**: Need careful monitoring and regularization\n",
    "\n",
    "### ‚úÖ Choose **Bagging** (Random Forest) when:\n",
    "\n",
    "1. **You Need Stability and Reliability** üõ°Ô∏è\n",
    "   - **When**: Production systems, baseline models, when you need consistent performance\n",
    "   - **Why**: Random Forest is more stable, less prone to overfitting, works well out-of-the-box\n",
    "   - **Trade-off**: May not achieve absolute best performance\n",
    "   - **Real Example**: If Random Forest outperforms boosting in your results, it's likely because:\n",
    "     - Better generalization (smaller train-test gap)\n",
    "     - Less overfitting on your specific dataset\n",
    "     - Default parameters work well without tuning\n",
    "\n",
    "2. **You Need Fast Training** ‚ö°\n",
    "   - **When**: Quick prototypes, large datasets, limited computational resources\n",
    "   - **Why**: Random Forest trains faster (parallel training), requires less tuning\n",
    "   - **Trade-off**: Less time for optimization means potentially lower peak performance\n",
    "\n",
    "3. **You Want Less Tuning** üéØ\n",
    "   - **When**: Quick iterations, proof-of-concepts, when you need results fast\n",
    "   - **Why**: Random Forest works well with default parameters, less hyperparameter sensitivity\n",
    "   - **Trade-off**: Less opportunity for fine-tuning\n",
    "   - **Real Example**: If Random Forest wins without tuning, it shows:\n",
    "     - The dataset doesn't require complex sequential learning\n",
    "     - Variance reduction (averaging) is more important than bias reduction\n",
    "     - Default parameters are sufficient for good performance\n",
    "\n",
    "4. **You Have Small or Noisy Data** üìâ\n",
    "   - **When**: Limited training data, noisy datasets, high variance problems\n",
    "   - **Why**: Random Forest's parallel approach and averaging reduces variance effectively\n",
    "   - **Trade-off**: May not capture complex patterns as well as boosting\n",
    "   - **Real Example**: If Random Forest outperforms boosting, your data likely has:\n",
    "     - High variance (noisy features)\n",
    "     - Patterns that benefit from averaging multiple models\n",
    "     - Characteristics where parallel learning works better than sequential\n",
    "\n",
    "5. **Better Generalization Needed** üìä\n",
    "   - **When**: You see Random Forest has smaller train-test gap than boosting\n",
    "   - **Why**: Random Forest's averaging reduces overfitting risk\n",
    "   - **Real Example**: If Random Forest's test accuracy is close to training accuracy:\n",
    "     - It generalizes better to unseen data\n",
    "     - Less risk of overfitting\n",
    "     - More reliable for production use\n",
    "\n",
    "### üìä Key Takeaways:\n",
    "\n",
    "- **Performance**: Either method can win - it depends on your specific data and problem\n",
    "- **Boosting**: Higher potential performance, but requires tuning and can overfit\n",
    "- **Bagging**: More stable and reliable, works well without extensive tuning\n",
    "- **Best Practice**: Try both methods, compare on your validation set, choose based on test performance\n",
    "- **Real-World**: Many practitioners start with Random Forest (quick baseline), then try boosting if they need better performance\n",
    "\n",
    "### üîç Why Bagging Might Outperform Boosting (Based on Your Results):\n",
    "\n",
    "If Random Forest (bagging) performed better in your comparison, here are the likely reasons:\n",
    "\n",
    "1. **Better Generalization** ‚úÖ\n",
    "   - Random Forest typically has a smaller gap between training and test accuracy\n",
    "   - This means it generalizes better to unseen data\n",
    "   - Less overfitting = more reliable predictions\n",
    "\n",
    "2. **Variance Reduction Works Better** ‚úÖ\n",
    "   - Your dataset may have high variance (noisy data)\n",
    "   - Random Forest's averaging approach reduces variance effectively\n",
    "   - Multiple independent models average out noise better than sequential learning\n",
    "\n",
    "3. **No Tuning Required** ‚úÖ\n",
    "   - Random Forest works well with default parameters\n",
    "   - Boosting needs careful hyperparameter tuning to excel\n",
    "   - Without tuning, Random Forest often performs better\n",
    "\n",
    "4. **Dataset Characteristics** ‚úÖ\n",
    "   - Your data may have patterns that benefit from averaging\n",
    "   - Features may not require sequential learning\n",
    "   - Parallel learning (bagging) captures the patterns better\n",
    "\n",
    "5. **Stability Over Peak Performance** ‚úÖ\n",
    "   - Random Forest is more stable and consistent\n",
    "   - Less sensitive to data changes\n",
    "   - Better for production systems needing reliability\n",
    "\n",
    "### üí° Remember:\n",
    "\n",
    "The comparison results you see above are specific to this dataset. In your own projects:\n",
    "- **Always compare both methods** on your validation/test set\n",
    "- **Choose based on test performance**, not training performance\n",
    "- **Consider your constraints**: time, computational resources, stability requirements\n",
    "- **Both are excellent methods** - the \"best\" one depends on your specific situation!\n",
    "- **If Random Forest wins**: It's likely due to better generalization, variance reduction, or dataset characteristics that favor averaging\n",
    "- **If Boosting wins**: It's likely due to sequential learning capturing complex patterns, but watch for overfitting!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.629801Z",
     "iopub.status.busy": "2026-01-20T11:42:17.629733Z",
     "iopub.status.idle": "2026-01-20T11:42:17.632960Z",
     "shell.execute_reply": "2026-01-20T11:42:17.632778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "if XGBOOST_AVAILABLE or LIGHTGBM_AVAILABLE:\n",
    "    # Use actual number of features (we have 6 features, not 10)\n",
    "    num_features = len(importance_df)\n",
    "    top_n = min(10, num_features)  # Show top 10 or all features if less than 10\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    # Calculate number of subplots needed\n",
    "    # Always show Random Forest, plus XGBoost and/or LightGBM if available\n",
    "    num_plots = 1  # Random Forest\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        num_plots += 1\n",
    "    if LIGHTGBM_AVAILABLE:\n",
    "        num_plots += 1\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(5*num_plots, 6))\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = [axes]\n",
    "    \n",
    "    idx = 0\n",
    "    # Random Forest (always shown)\n",
    "    axes[idx].barh(range(len(top_features)), top_features['Random Forest'].values[::-1], \n",
    "                   alpha=0.7, color='blue')\n",
    "    axes[idx].set_yticks(range(len(top_features)))\n",
    "    axes[idx].set_yticklabels(top_features['Feature'].values[::-1])\n",
    "    axes[idx].set_xlabel('Importance', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title(f'Random Forest - Top {len(top_features)} Features\\nEmergency Response Analysis', fontsize=12, fontweight='bold', pad=10)\n",
    "    axes[idx].grid(True, alpha=0.3, axis='x', linestyle='--')\n",
    "    idx += 1\n",
    "    \n",
    "    # XGBoost (if available)\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        axes[idx].barh(range(len(top_features)), top_features['XGBoost'].values[::-1],\n",
    "                      alpha=0.7, color='orange')\n",
    "        axes[idx].set_yticks(range(len(top_features)))\n",
    "        axes[idx].set_yticklabels(top_features['Feature'].values[::-1])\n",
    "        axes[idx].set_xlabel('Importance', fontsize=11, fontweight='bold')\n",
    "        axes[idx].set_title(f'XGBoost - Top {len(top_features)} Features\\nEmergency Response Analysis', fontsize=12, fontweight='bold', pad=10)\n",
    "        axes[idx].grid(True, alpha=0.3, axis='x', linestyle='--')\n",
    "        idx += 1\n",
    "    \n",
    "    # LightGBM (if available)\n",
    "    if LIGHTGBM_AVAILABLE:\n",
    "        axes[idx].barh(range(len(top_features)), top_features['LightGBM'].values[::-1],\n",
    "                      alpha=0.7, color='green')\n",
    "        axes[idx].set_yticks(range(len(top_features)))\n",
    "        axes[idx].set_yticklabels(top_features['Feature'].values[::-1])\n",
    "        axes[idx].set_xlabel('Importance', fontsize=11, fontweight='bold')\n",
    "        axes[idx].set_title(f'LightGBM - Top {len(top_features)} Features\\nEmergency Response Analysis', fontsize=12, fontweight='bold', pad=10)\n",
    "        axes[idx].grid(True, alpha=0.3, axis='x', linestyle='--')\n",
    "        idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('boosting_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\n‚úì Plot saved as 'boosting_feature_importance.png'\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üí° DETAILED INTERPRETATION: Feature Importance\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nüìä WHAT YOU SEE:\")\n",
    "    print(f\"   ‚Ä¢ Horizontal bar charts showing feature importance for each model\")\n",
    "    print(f\"   ‚Ä¢ Each bar = One feature\")\n",
    "    print(f\"   ‚Ä¢ Bar length = How important that feature is for predictions\")\n",
    "    print(f\"   ‚Ä¢ Features sorted by importance (most important at top)\")\n",
    "    print(\"\\nüîç HOW TO READ IT:\")\n",
    "    print(\"   ‚Ä¢ Longer bars = More important features\")\n",
    "    print(\"   ‚Ä¢ Shorter bars = Less important features\")\n",
    "    print(\"   ‚Ä¢ Compare across models to see which features matter most\")\n",
    "    print(\"   ‚Ä¢ Features at top = Most important for emergency category prediction\")\n",
    "    print(\"\\nüí° WHAT THIS TELLS US:\")\n",
    "    print(\"   ‚Ä¢ Which location/time features are most predictive of emergency type\")\n",
    "    print(\"   ‚Ä¢ How different models prioritize features\")\n",
    "    print(\"   ‚Ä¢ Which features to focus on for emergency response optimization\")\n",
    "    print(\"\\nüéØ GDI APPLICATION - Emergency Response:\")\n",
    "    print(\"   ‚Ä¢ Identify key factors for predicting emergency types\")\n",
    "    print(\"   ‚Ä¢ Optimize resource allocation based on feature importance\")\n",
    "    print(\"   ‚Ä¢ Understand which location/time patterns indicate different emergencies\")\n",
    "    print(\"   ‚Ä¢ Improve emergency response planning using feature insights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ùì Common Student Questions | ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ¥ÿßÿ¶ÿπÿ© ŸÑŸÑÿ∑ŸÑÿßÿ®\n",
    "\n",
    "**Q: What's the difference between boosting and bagging (Random Forest)?**\n",
    "- **Answer**: \n",
    "  - **Boosting**: Sequential (each model learns from previous mistakes)\n",
    "    - Model 1 ‚Üí Model 2 fixes Model 1's mistakes ‚Üí Model 3 fixes Model 2's mistakes\n",
    "  - **Bagging (Random Forest)**: Parallel (models trained independently, then averaged)\n",
    "    - Model 1, Model 2, Model 3 trained independently ‚Üí average predictions\n",
    "  - **Performance**: Either can win - depends on data, tuning, and problem type\n",
    "  - **Boosting**: Can achieve higher accuracy with tuning, but can overfit\n",
    "  - **Bagging**: More stable, less overfitting, faster training, often works well without tuning\n",
    "  - **Use boosting**: When you need best performance, have time to tune, willing to risk overfitting\n",
    "  - **Use bagging**: When you need stability, faster training, less tuning, want reliable baseline\n",
    "\n",
    "**Q: Why is XGBoost so popular?**\n",
    "- **Answer**: XGBoost is fast, powerful, and handles many scenarios:\n",
    "  - **Performance**: Often achieves best results in competitions\n",
    "  - **Features**: Handles missing values, regularization built-in, feature importance\n",
    "  - **Usability**: Easy to use, well-documented, widely supported\n",
    "  - **Industry standard**: Used in production systems worldwide\n",
    "  - **Rule**: XGBoost is often the first choice for tabular data (structured data)\n",
    "\n",
    "**Q: What's the difference between XGBoost and LightGBM?**\n",
    "- **Answer**: \n",
    "  - **XGBoost**: Level-wise tree growth (grows all leaves at same level), more stable\n",
    "  - **LightGBM**: Leaf-wise tree growth (grows best leaves first), faster, uses less memory\n",
    "  - **Speed**: LightGBM is faster than XGBoost (especially on large datasets)\n",
    "  - **Memory**: LightGBM uses less memory than XGBoost\n",
    "  - **Use LightGBM**: Large datasets (> 10k samples), need speed\n",
    "  - **Use XGBoost**: Smaller datasets, need stability, more features\n",
    "\n",
    "**Q: Why use boosting instead of just one decision tree?**\n",
    "- **Answer**: Boosting combines weak learners into strong learner:\n",
    "  - **Single tree**: Can overfit, limited performance\n",
    "  - **Boosting**: Many trees combined ‚Üí less overfitting, better performance\n",
    "  - **Process**: Each tree corrects previous mistakes ‚Üí sequential improvement\n",
    "  - **Result**: Excellent performance (often best among ML algorithms)\n",
    "  - **Rule**: Boosting > single tree for most problems\n",
    "\n",
    "**Q: What is learning rate in boosting?**\n",
    "- **Answer**: Learning rate controls how much each new tree contributes:\n",
    "  - **Range**: Usually 0.01 to 0.3\n",
    "  - **Low (0.01)**: Small steps, many trees needed, less overfitting, better performance\n",
    "  - **High (0.3)**: Large steps, fewer trees needed, can overfit, faster training\n",
    "  - **Rule of thumb**: Lower learning rate + more trees = better performance (but slower)\n",
    "  - **Default**: XGBoost/LightGBM default is usually 0.1 (good starting point)\n",
    "\n",
    "**Q: How many trees (n_estimators) should I use?**\n",
    "- **Answer**: Balance between performance and training time:\n",
    "  - **Too few (50-100)**: May underfit, poor performance\n",
    "  - **Too many (1000+)**: May overfit, slow training, diminishing returns\n",
    "  - **Good range**: 100-500 trees (depends on learning rate)\n",
    "  - **Rule**: Lower learning rate ‚Üí need more trees, higher learning rate ‚Üí need fewer trees\n",
    "  - **Tip**: Use early stopping to find optimal number automatically\n",
    "\n",
    "**Q: Can boosting handle missing values?**\n",
    "- **Answer**: **Yes!** XGBoost and LightGBM handle missing values automatically:\n",
    "  - **XGBoost**: Learns best way to handle missing values during training\n",
    "  - **LightGBM**: Also handles missing values automatically\n",
    "  - **Advantage**: No need to impute missing values (unlike many other algorithms)\n",
    "  - **Rule**: Boosting algorithms are robust to missing values\n",
    "  - **Note**: Still good practice to check for missing values, but not required\n",
    "\n",
    "**Q: When should I use boosting vs other algorithms?**\n",
    "- **Answer**: \n",
    "  - **Use boosting**: Tabular data (structured data), need best performance, have time to tune\n",
    "  - **Use Random Forest**: Need fast training, less tuning, more stability\n",
    "  - **Use Logistic Regression**: Need interpretability, fast predictions, linear patterns\n",
    "  - **Use Neural Networks**: Image/text data, very large datasets, complex patterns\n",
    "  - **Rule**: Boosting is often best for tabular data, but try multiple algorithms\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Decision Framework - Boosting Algorithm Selection | ÿßŸÑÿÆÿ∑Ÿàÿ© 7: ÿ•ÿ∑ÿßÿ± ÿßŸÑŸÇÿ±ÿßÿ± - ÿßÿÆÿ™Ÿäÿßÿ± ÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿ© ÿßŸÑÿ™ÿπÿ≤Ÿäÿ≤\n",
    "\n",
    "**BEFORE**: You've learned XGBoost and LightGBM, but when should you use each boosting algorithm?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose between XGBoost, LightGBM, AdaBoost, and Gradient Boosting!\n",
    "\n",
    "**Why this matters**: Using the wrong boosting algorithm can:\n",
    "- **Poor performance** ‚Üí Wrong algorithm may not fit your data well\n",
    "- **Wasted computation** ‚Üí Using slow algorithms when fast ones work\n",
    "- **Wrong complexity** ‚Üí Using complex algorithms when simple ones work\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Decision Framework: Which Boosting Algorithm? | ÿ•ÿ∑ÿßÿ± ÿßŸÑŸÇÿ±ÿßÿ±: ÿ£Ÿä ÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿ© ÿ™ÿπÿ≤Ÿäÿ≤ÿü\n",
    "\n",
    "**Key Question**: Should I use **XGBOOST**, **LIGHTGBM**, **ADABOOST**, or **GRADIENT BOOSTING**?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "What type of problem do you have?\n",
    "‚îú‚îÄ REGRESSION ‚Üí Use boosting regressors (XGBoost, LightGBM, Gradient Boosting)\n",
    "‚îÇ   ‚îî‚îÄ Why? All boosting algorithms support regression\n",
    "‚îÇ\n",
    "‚îî‚îÄ CLASSIFICATION ‚Üí Check requirements:\n",
    "    ‚îú‚îÄ Need best performance? ‚Üí Use XGBOOST or LIGHTGBM ‚úÖ\n",
    "    ‚îÇ   ‚îî‚îÄ Why? State-of-the-art performance\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ Need speed? ‚Üí Use LIGHTGBM ‚úÖ\n",
    "    ‚îÇ   ‚îî‚îÄ Why? LightGBM is fastest\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ Need interpretability? ‚Üí Use ADABOOST or GRADIENT BOOSTING ‚úÖ\n",
    "    ‚îÇ   ‚îî‚îÄ Why? Simpler, more interpretable\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ Large dataset (>100k)? ‚Üí Use LIGHTGBM ‚úÖ\n",
    "    ‚îÇ   ‚îî‚îÄ Why? LightGBM handles large data efficiently\n",
    "    ‚îÇ\n",
    "    ‚îî‚îÄ Small dataset (<10k)? ‚Üí Use XGBOOST or ADABOOST ‚úÖ\n",
    "        ‚îî‚îÄ Why? Both work well on smaller data\n",
    "```\n",
    "\n",
    "#### Detailed Decision Process:\n",
    "\n",
    "```\n",
    "Step 1: Problem Type\n",
    "‚îú‚îÄ Regression ‚Üí Continue to Step 2\n",
    "‚îî‚îÄ Classification ‚Üí Continue to Step 2\n",
    "\n",
    "Step 2: Dataset Size\n",
    "‚îú‚îÄ Large (>100,000 samples) ‚Üí Use LIGHTGBM ‚úÖ\n",
    "‚îÇ   ‚îî‚îÄ Why? LightGBM is fastest, handles large data best\n",
    "‚îÇ\n",
    "‚îú‚îÄ Medium (10,000-100,000) ‚Üí Continue to Step 3\n",
    "‚îî‚îÄ Small (<10,000) ‚Üí Continue to Step 3\n",
    "\n",
    "Step 3: Performance vs Speed\n",
    "‚îú‚îÄ Need best performance? ‚Üí Use XGBOOST ‚úÖ\n",
    "‚îÇ   ‚îî‚îÄ Why? XGBoost often has best accuracy\n",
    "‚îÇ\n",
    "‚îú‚îÄ Need speed? ‚Üí Use LIGHTGBM ‚úÖ\n",
    "‚îÇ   ‚îî‚îÄ Why? LightGBM is fastest\n",
    "‚îÇ\n",
    "‚îî‚îÄ Need balance? ‚Üí Continue to Step 4\n",
    "\n",
    "Step 4: Interpretability\n",
    "‚îú‚îÄ Need interpretability? ‚Üí Use ADABOOST or GRADIENT BOOSTING ‚úÖ\n",
    "‚îÇ   ‚îî‚îÄ Why? Simpler, easier to understand\n",
    "‚îÇ\n",
    "‚îî‚îÄ Don't need interpretability? ‚Üí Use XGBOOST or LIGHTGBM ‚úÖ\n",
    "    ‚îî‚îÄ Why? Best performance, more complex\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Comparison Table: Boosting Algorithms | ÿ¨ÿØŸàŸÑ ÿßŸÑŸÖŸÇÿßÿ±ŸÜÿ©\n",
    "\n",
    "| Algorithm | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **XGBoost** | Best performance, competitions, medium-large data | ‚Ä¢ State-of-the-art performance<br>‚Ä¢ Robust<br>‚Ä¢ Feature importance<br>‚Ä¢ Handles missing values | ‚Ä¢ Slower than LightGBM<br>‚Ä¢ More memory<br>‚Ä¢ Complex | Kaggle competitions, best accuracy needed |\n",
    "| **LightGBM** | Large datasets, need speed, good performance | ‚Ä¢ Fastest<br>‚Ä¢ Low memory<br>‚Ä¢ Good performance<br>‚Ä¢ Handles large data | ‚Ä¢ May overfit on small data<br>‚Ä¢ Less robust than XGBoost | Large datasets, need speed |\n",
    "| **AdaBoost** | Small datasets, interpretable, simple | ‚Ä¢ Simple<br>‚Ä¢ Interpretable<br>‚Ä¢ Works well on small data<br>‚Ä¢ Fast | ‚Ä¢ May not be best performance<br>‚Ä¢ Sensitive to outliers | Small datasets, need simplicity |\n",
    "| **Gradient Boosting** | Medium datasets, interpretable, balanced | ‚Ä¢ Balanced performance<br>‚Ä¢ Interpretable<br>‚Ä¢ Good default | ‚Ä¢ Slower than XGBoost/LightGBM<br>‚Ä¢ May not be best | Medium datasets, balanced needs |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ When to Use Each Algorithm | ŸÖÿ™Ÿâ ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ŸÉŸÑ ÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿ©\n",
    "\n",
    "#### Use XGBoost when:\n",
    "1. **Best Performance Needed** ‚úÖ\n",
    "   - Need state-of-the-art accuracy\n",
    "   - Competitions, critical applications\n",
    "   - **Example**: Kaggle competitions, production systems needing best accuracy\n",
    "\n",
    "2. **Medium to Large Datasets** ‚úÖ\n",
    "   - 10,000-1,000,000 samples\n",
    "   - XGBoost works well\n",
    "   - **Example**: Most real-world datasets\n",
    "\n",
    "3. **Robustness Important** ‚úÖ\n",
    "   - Need robust model\n",
    "   - XGBoost handles many edge cases\n",
    "   - **Example**: Production systems, need reliability\n",
    "\n",
    "4. **Feature Importance Needed** ‚úÖ\n",
    "   - Need to understand feature importance\n",
    "   - XGBoost provides good feature importance\n",
    "   - **Example**: Understanding which features matter\n",
    "\n",
    "#### Use LightGBM when:\n",
    "1. **Large Datasets** ‚úÖ\n",
    "   - More than 100,000 samples\n",
    "   - LightGBM is fastest\n",
    "   - **Example**: Big data, millions of samples\n",
    "\n",
    "2. **Speed Critical** ‚úÖ\n",
    "   - Need fast training and prediction\n",
    "   - LightGBM is fastest boosting algorithm\n",
    "   - **Example**: Real-time systems, quick iterations\n",
    "\n",
    "3. **Memory Constraints** ‚úÖ\n",
    "   - Limited memory available\n",
    "   - LightGBM uses less memory\n",
    "   - **Example**: Limited RAM, large datasets\n",
    "\n",
    "4. **Good Performance + Speed** ‚úÖ\n",
    "   - Need good performance but also speed\n",
    "   - LightGBM balances both\n",
    "   - **Example**: Production systems needing speed\n",
    "\n",
    "#### Use AdaBoost when:\n",
    "1. **Small Datasets** ‚úÖ\n",
    "   - Less than 10,000 samples\n",
    "   - AdaBoost works well on small data\n",
    "   - **Example**: Small research datasets\n",
    "\n",
    "2. **Interpretability Needed** ‚úÖ\n",
    "   - Need to understand the model\n",
    "   - AdaBoost is simpler, more interpretable\n",
    "   - **Example**: Need to explain decisions\n",
    "\n",
    "3. **Simple Solution** ‚úÖ\n",
    "   - Want simple, straightforward model\n",
    "   - AdaBoost is simplest boosting algorithm\n",
    "   - **Example**: Quick prototyping, simple problems\n",
    "\n",
    "#### Use Gradient Boosting when:\n",
    "1. **Balanced Needs** ‚úÖ\n",
    "   - Need balance of performance and interpretability\n",
    "   - Gradient Boosting is balanced\n",
    "   - **Example**: Medium datasets, need good performance\n",
    "\n",
    "2. **Medium Datasets** ‚úÖ\n",
    "   - 1,000-100,000 samples\n",
    "   - Gradient Boosting works well\n",
    "   - **Example**: Most business datasets\n",
    "\n",
    "3. **Good Default** ‚úÖ\n",
    "   - Need a good default boosting algorithm\n",
    "   - Gradient Boosting is reliable\n",
    "   - **Example**: Starting point, baseline\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When NOT to Use Each Algorithm | ŸÖÿ™Ÿâ ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ŸÉŸÑ ÿÆŸàÿßÿ±ÿ≤ŸÖŸäÿ©\n",
    "\n",
    "#### Don't use XGBoost when:\n",
    "1. **Very Large Datasets** ‚ùå\n",
    "   - More than 1,000,000 samples\n",
    "   - XGBoost is slower\n",
    "   - **Use Instead**: LightGBM (faster)\n",
    "\n",
    "2. **Speed Critical** ‚ùå\n",
    "   - Need very fast training\n",
    "   - XGBoost is slower\n",
    "   - **Use Instead**: LightGBM\n",
    "\n",
    "3. **Small Datasets** ‚ùå\n",
    "   - Less than 1,000 samples\n",
    "   - May overfit\n",
    "   - **Use Instead**: AdaBoost or simpler models\n",
    "\n",
    "#### Don't use LightGBM when:\n",
    "1. **Small Datasets** ‚ùå\n",
    "   - Less than 10,000 samples\n",
    "   - May overfit\n",
    "   - **Use Instead**: XGBoost or AdaBoost\n",
    "\n",
    "2. **Need Best Accuracy** ‚ùå\n",
    "   - Need absolute best performance\n",
    "   - XGBoost often better\n",
    "   - **Use Instead**: XGBoost\n",
    "\n",
    "#### Don't use AdaBoost when:\n",
    "1. **Large Datasets** ‚ùå\n",
    "   - More than 100,000 samples\n",
    "   - AdaBoost is slow\n",
    "   - **Use Instead**: XGBoost or LightGBM\n",
    "\n",
    "2. **Best Performance Needed** ‚ùå\n",
    "   - Need state-of-the-art accuracy\n",
    "   - AdaBoost may not be best\n",
    "   - **Use Instead**: XGBoost or LightGBM\n",
    "\n",
    "#### Don't use Gradient Boosting when:\n",
    "1. **Best Performance Needed** ‚ùå\n",
    "   - Need absolute best accuracy\n",
    "   - XGBoost/LightGBM usually better\n",
    "   - **Use Instead**: XGBoost or LightGBM\n",
    "\n",
    "2. **Speed Critical** ‚ùå\n",
    "   - Need fast training\n",
    "   - Gradient Boosting is slower\n",
    "   - **Use Instead**: LightGBM\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Real-World Examples | ÿ£ŸÖÿ´ŸÑÿ© ŸÖŸÜ ÿßŸÑÿπÿßŸÑŸÖ ÿßŸÑÿ≠ŸÇŸäŸÇŸä\n",
    "\n",
    "#### Example 1: Kaggle Competition ‚úÖ XGBOOST\n",
    "- **Problem**: Classification competition\n",
    "- **Dataset**: Medium (50,000 samples)\n",
    "- **Need**: Best performance (winning competition)\n",
    "- **Decision**: ‚úÖ Use XGBoost\n",
    "- **Reasoning**: Need best performance, medium dataset, XGBoost often wins\n",
    "\n",
    "#### Example 2: Large-Scale Customer Segmentation ‚úÖ LIGHTGBM\n",
    "- **Problem**: Segment 1,000,000 customers\n",
    "- **Dataset**: Large (1,000,000 samples)\n",
    "- **Need**: Speed and good performance\n",
    "- **Decision**: ‚úÖ Use LightGBM\n",
    "- **Reasoning**: Large dataset, need speed, LightGBM handles large data best\n",
    "\n",
    "#### Example 3: Small Medical Dataset ‚úÖ ADABOOST\n",
    "- **Problem**: Diagnose disease from 500 patients\n",
    "- **Dataset**: Small (500 samples)\n",
    "- **Need**: Interpretability and simplicity\n",
    "- **Decision**: ‚úÖ Use AdaBoost\n",
    "- **Reasoning**: Small dataset, need interpretability, AdaBoost works well\n",
    "\n",
    "#### Example 4: Medium Business Dataset ‚úÖ XGBOOST or GRADIENT BOOSTING\n",
    "- **Problem**: Predict customer churn\n",
    "- **Dataset**: Medium (20,000 samples)\n",
    "- **Need**: Good performance, balanced\n",
    "- **Decision**: ‚úÖ Use XGBoost (or Gradient Boosting)\n",
    "- **Reasoning**: Medium dataset, need good performance, both work well\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Key Takeaways | ÿßŸÑŸÜŸÇÿßÿ∑ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©\n",
    "\n",
    "1. **Best performance ‚Üí XGBoost** - Use for competitions and best accuracy\n",
    "2. **Large data + speed ‚Üí LightGBM** - Use for large datasets needing speed\n",
    "3. **Small data ‚Üí AdaBoost** - Use for small datasets, interpretability\n",
    "4. **Balanced ‚Üí Gradient Boosting** - Use for balanced needs\n",
    "5. **Try multiple** - Often try XGBoost and LightGBM, pick the best\n",
    "6. **Tune hyperparameters** - All boosting algorithms need tuning\n",
    "7. **Feature importance** - All provide feature importance\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Practice Decision-Making | ŸÖŸÖÿßÿ±ÿ≥ÿ© ÿßÿ™ÿÆÿßÿ∞ ÿßŸÑŸÇÿ±ÿßÿ±\n",
    "\n",
    "**Scenario 1**: Kaggle competition with 30,000 samples\n",
    "- **Dataset**: Medium (30,000)\n",
    "- **Need**: Best performance (winning)\n",
    "- **Decision**: ‚úÖ XGBoost (best performance, medium dataset)\n",
    "\n",
    "**Scenario 2**: Real-time fraud detection with 500,000 transactions\n",
    "- **Dataset**: Large (500,000)\n",
    "- **Need**: Speed and good performance\n",
    "- **Decision**: ‚úÖ LightGBM (large dataset, need speed)\n",
    "\n",
    "**Scenario 3**: Medical diagnosis with 800 patients\n",
    "- **Dataset**: Small (800)\n",
    "- **Need**: Interpretability\n",
    "- **Decision**: ‚úÖ AdaBoost (small dataset, need interpretability)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- üìì **Example 1: Grid Search** - For tuning boosting hyperparameters\n",
    "- üìì **All ML Projects** - Boosting algorithms are powerful tools\n",
    "- üìì **Production Systems** - Choose algorithm based on your needs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:42:17.633851Z",
     "iopub.status.busy": "2026-01-20T11:42:17.633792Z",
     "iopub.status.idle": "2026-01-20T11:42:17.807762Z",
     "shell.execute_reply": "2026-01-20T11:42:17.807580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. Confusion Matrices Comparison\n",
      "ŸÖŸÇÿßÿ±ŸÜÿ© ŸÖÿµŸÅŸàŸÅÿßÿ™ ÿßŸÑÿßÿ±ÿ™ÿ®ÿßŸÉ\n",
      "============================================================\n",
      "\n",
      "üí° Transition: Let's visualize how well each model classifies emergency types!\n",
      "   Note: ROC curves are for binary classification only.\n",
      "   For multi-class (3 classes: EMS, Fire, Traffic), we use confusion matrices instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Plot saved as 'boosting_confusion_matrices.png'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üí° DETAILED INTERPRETATION: Confusion Matrices (Multi-class)\n",
      "======================================================================\n",
      "\n",
      "üìä WHAT YOU SEE:\n",
      "   ‚Ä¢ Heatmaps showing confusion matrices for each model\n",
      "   ‚Ä¢ Each cell = Number of predictions\n",
      "   ‚Ä¢ Rows = Actual emergency categories (EMS, Fire, Traffic)\n",
      "   ‚Ä¢ Columns = Predicted emergency categories\n",
      "\n",
      "üîç HOW TO READ IT:\n",
      "   ‚Ä¢ Diagonal cells (top-left to bottom-right) = Correct predictions\n",
      "   ‚Ä¢ Off-diagonal cells = Incorrect predictions\n",
      "   ‚Ä¢ Darker colors = More predictions\n",
      "   ‚Ä¢ Perfect model = All values on diagonal, zeros elsewhere\n",
      "\n",
      "üí° WHAT THIS TELLS US:\n",
      "   ‚Ä¢ Which emergency types are predicted correctly most often\n",
      "   ‚Ä¢ Which emergency types are confused with each other\n",
      "   ‚Ä¢ Overall model performance for each emergency category\n",
      "\n",
      "üéØ GDI APPLICATION - Emergency Response:\n",
      "   ‚Ä¢ Understand which emergency types are easiest/hardest to predict\n",
      "   ‚Ä¢ Identify patterns in misclassifications (e.g., Fire vs Traffic)\n",
      "   ‚Ä¢ Optimize emergency response based on prediction accuracy\n",
      "   ‚Ä¢ Improve resource allocation using classification insights\n",
      "\n",
      "============================================================\n",
      "7. Learning Curve Example (XGBoost)\n",
      "ŸÖÿ´ÿßŸÑ ŸÖŸÜÿ≠ŸÜŸâ ÿßŸÑÿ™ÿπŸÑŸÖ (XGBoost)\n",
      "============================================================\n",
      "\n",
      "üí° Transition: Let's see how XGBoost performance changes with more trees!\n",
      "   This helps us understand the relationship between model complexity and performance.\n",
      "\n",
      "============================================================\n",
      "Example 2 Complete! ‚úì\n",
      "ÿßŸÉÿ™ŸÖŸÑ ÿßŸÑŸÖÿ´ÿßŸÑ 2! ‚úì\n",
      "============================================================\n",
      "\n",
      "Note: Install missing packages for full functionality:\n",
      "  pip install xgboost\n",
      "  pip install lightgbm\n"
     ]
    }
   ],
   "source": [
    "# 6. Confusion Matrices Comparison (Multi-class Classification)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. Confusion Matrices Comparison\")\n",
    "print(\"ŸÖŸÇÿßÿ±ŸÜÿ© ŸÖÿµŸÅŸàŸÅÿßÿ™ ÿßŸÑÿßÿ±ÿ™ÿ®ÿßŸÉ\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Transition: Let's visualize how well each model classifies emergency types!\")\n",
    "print(\"   Note: ROC curves are for binary classification only.\")\n",
    "print(\"   For multi-class (3 classes: EMS, Fire, Traffic), we use confusion matrices instead!\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "if XGBOOST_AVAILABLE:\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    y_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# Calculate number of subplots\n",
    "num_plots = 1  # Random Forest\n",
    "if XGBOOST_AVAILABLE:\n",
    "    num_plots += 1\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    num_plots += 1\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, num_plots, figsize=(6*num_plots, 5))\n",
    "if not isinstance(axes, np.ndarray):\n",
    "    axes = [axes]\n",
    "\n",
    "# Get category names for labels\n",
    "if 'category_names' in globals():\n",
    "    class_names = category_names\n",
    "else:\n",
    "    class_names = ['EMS', 'Fire', 'Traffic']  # Default fallback\n",
    "\n",
    "idx = 0\n",
    "# Random Forest\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[idx].set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
    "axes[idx].set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
    "axes[idx].set_title('Random Forest\\nConfusion Matrix', fontsize=12, fontweight='bold', pad=10)\n",
    "idx += 1\n",
    "\n",
    "# XGBoost\n",
    "if XGBOOST_AVAILABLE:\n",
    "    cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "    sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Oranges', ax=axes[idx],\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title('XGBoost\\nConfusion Matrix', fontsize=12, fontweight='bold', pad=10)\n",
    "    idx += 1\n",
    "\n",
    "# LightGBM\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    cm_lgb = confusion_matrix(y_test, y_pred_lgb)\n",
    "    sns.heatmap(cm_lgb, annot=True, fmt='d', cmap='Greens', ax=axes[idx],\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title('LightGBM\\nConfusion Matrix', fontsize=12, fontweight='bold', pad=10)\n",
    "    idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('boosting_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úì Plot saved as 'boosting_confusion_matrices.png'\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üí° DETAILED INTERPRETATION: Confusion Matrices (Multi-class)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüìä WHAT YOU SEE:\")\n",
    "print(f\"   ‚Ä¢ Heatmaps showing confusion matrices for each model\")\n",
    "print(f\"   ‚Ä¢ Each cell = Number of predictions\")\n",
    "print(f\"   ‚Ä¢ Rows = Actual emergency categories ({', '.join(class_names)})\")\n",
    "print(f\"   ‚Ä¢ Columns = Predicted emergency categories\")\n",
    "print(\"\\nüîç HOW TO READ IT:\")\n",
    "print(\"   ‚Ä¢ Diagonal cells (top-left to bottom-right) = Correct predictions\")\n",
    "print(\"   ‚Ä¢ Off-diagonal cells = Incorrect predictions\")\n",
    "print(\"   ‚Ä¢ Darker colors = More predictions\")\n",
    "print(\"   ‚Ä¢ Perfect model = All values on diagonal, zeros elsewhere\")\n",
    "print(\"\\nüí° WHAT THIS TELLS US:\")\n",
    "print(\"   ‚Ä¢ Which emergency types are predicted correctly most often\")\n",
    "print(\"   ‚Ä¢ Which emergency types are confused with each other\")\n",
    "print(\"   ‚Ä¢ Overall model performance for each emergency category\")\n",
    "print(\"\\nüéØ GDI APPLICATION - Emergency Response:\")\n",
    "print(\"   ‚Ä¢ Understand which emergency types are easiest/hardest to predict\")\n",
    "print(\"   ‚Ä¢ Identify patterns in misclassifications (e.g., Fire vs Traffic)\")\n",
    "print(\"   ‚Ä¢ Optimize emergency response based on prediction accuracy\")\n",
    "print(\"   ‚Ä¢ Improve resource allocation using classification insights\")\n",
    "\n",
    "# 7. Learning Curve (XGBoost)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"7. Learning Curve Example (XGBoost)\")\n",
    "print(\"ŸÖÿ´ÿßŸÑ ŸÖŸÜÿ≠ŸÜŸâ ÿßŸÑÿ™ÿπŸÑŸÖ (XGBoost)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Transition: Let's see how XGBoost performance changes with more trees!\")\n",
    "print(\"   This helps us understand the relationship between model complexity and performance.\")\n",
    "if XGBOOST_AVAILABLE:\n",
    "    # Train with different number of estimators\n",
    "    n_estimators_range = range(10, 201, 20)\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    for n_est in n_estimators_range:\n",
    "        xgb_temp = xgb.XGBClassifier(\n",
    "            n_estimators=n_est, max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            random_state=73,  # Using 73 for consistency\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        xgb_temp.fit(X_train, y_train)\n",
    "        train_scores.append(accuracy_score(y_train, xgb_temp.predict(X_train)))\n",
    "        test_scores.append(accuracy_score(y_test, xgb_temp.predict(X_test)))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(n_estimators_range, train_scores, 'o-', label='Training Accuracy', linewidth=2)\n",
    "    plt.plot(n_estimators_range, test_scores, 's-', label='Test Accuracy', linewidth=2)\n",
    "    plt.xlabel('Number of Estimators')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('XGBoost Learning Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgb_learning_curve.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\n‚úì Plot saved as 'xgb_learning_curve.png'\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 2 Complete! ‚úì\")\n",
    "print(\"ÿßŸÉÿ™ŸÖŸÑ ÿßŸÑŸÖÿ´ÿßŸÑ 2! ‚úì\")\n",
    "print(\"=\" * 60)\n",
    "if not XGBOOST_AVAILABLE or not LIGHTGBM_AVAILABLE:\n",
    "    print(\"\\nNote: Install missing packages for full functionality:\")\n",
    "    if not XGBOOST_AVAILABLE:\n",
    "        print(\"  pip install xgboost\")\n",
    "    if not LIGHTGBM_AVAILABLE:\n",
    "        print(\"  pip install lightgbm\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
