{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 1 - Exercise 4: Data Preprocessing Practice\n",
    "## Ø£Ø³Ø§Ù„ÙŠØ¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - ØªÙ…Ø±ÙŠÙ† 4: Ù…Ù…Ø§Ø±Ø³Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**GDI Theme**: Financial Investigations - Preparing transaction data for fraud detection models\n",
    "\n",
    "## Instructions:\n",
    "1. Load the sample dataset provided below\n",
    "2. Identify numerical and categorical features\n",
    "3. Apply feature scaling (StandardScaler and MinMaxScaler)\n",
    "4. Encode categorical variables (LabelEncoder and OneHotEncoder)\n",
    "5. Split data into training and testing sets\n",
    "6. Compare different preprocessing methods\n",
    "7. Create visualizations comparing before/after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:16.183422Z",
     "iopub.status.busy": "2026-01-15T20:04:16.183250Z",
     "iopub.status.idle": "2026-01-15T20:04:16.993696Z",
     "shell.execute_reply": "2026-01-15T20:04:16.993492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Generate Sample Dataset\n",
    "### Ø§Ù„Ù…Ù‡Ù…Ø© 1: Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:16.994633Z",
     "iopub.status.busy": "2026-01-15T20:04:16.994536Z",
     "iopub.status.idle": "2026-01-15T20:04:16.998361Z",
     "shell.execute_reply": "2026-01-15T20:04:16.998168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Generating sample financial transaction data...\n",
      "Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø§Ù„ÙŠØ© Ù†Ù…ÙˆØ°Ø¬ÙŠØ©...\n",
      "   GDI Theme: Financial Investigations / Terrorism Financing Detection\n",
      "\n",
      "âœ… Sample dataset created!\n",
      "   ğŸ“Š Shape: (500, 8)\n",
      "   ğŸ“Š Columns: ['transaction_amount', 'transaction_time', 'merchant_category', 'payment_method', 'transaction_region', 'customer_age', 'account_balance', 'risk_level']\n",
      "\n",
      "ğŸ“‹ Data Types:\n",
      "transaction_amount    float64\n",
      "transaction_time      float64\n",
      "merchant_category      object\n",
      "payment_method         object\n",
      "transaction_region     object\n",
      "customer_age            int64\n",
      "account_balance       float64\n",
      "risk_level             object\n",
      "dtype: object\n",
      "\n",
      "ğŸ“‹ First few rows:\n",
      "   transaction_amount  transaction_time merchant_category payment_method  \\\n",
      "0         6430.472259         22.919279            Retail        Digital   \n",
      "1         5390.823100          9.455304            Online           Cash   \n",
      "2         5123.860845          0.873294        Restaurant          Debit   \n",
      "3         6106.469699         23.680339        Restaurant        Digital   \n",
      "4         4934.959946         14.650378            Retail         Credit   \n",
      "\n",
      "  transaction_region  customer_age  account_balance risk_level  \n",
      "0               West            40     44671.429320        Low  \n",
      "1               West            48     41744.384505       High  \n",
      "2               West            36      4363.250304     Medium  \n",
      "3            Central            25      1132.990502     Medium  \n",
      "4              North            35     32159.032751        Low  \n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(73)\n",
    "\n",
    "# Generate sample financial transaction data (GDI Theme: Financial Investigations)\n",
    "print(\"ğŸ“¥ Generating sample financial transaction data...\")\n",
    "print(\"Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø§Ù„ÙŠØ© Ù†Ù…ÙˆØ°Ø¬ÙŠØ©...\")\n",
    "print(\"   GDI Theme: Financial Investigations / Terrorism Financing Detection\\n\")\n",
    "\n",
    "# Create sample dataset with mixed data types\n",
    "n_samples = 500\n",
    "data = {\n",
    "    'transaction_amount': np.random.uniform(10, 10000, n_samples),\n",
    "    'transaction_time': np.random.uniform(0, 24, n_samples),\n",
    "    'merchant_category': np.random.choice(['Retail', 'Online', 'Gas', 'Restaurant', 'Travel'], n_samples),\n",
    "    'payment_method': np.random.choice(['Credit', 'Debit', 'Cash', 'Digital'], n_samples),\n",
    "    'transaction_region': np.random.choice(['North', 'South', 'East', 'West', 'Central'], n_samples),\n",
    "    'customer_age': np.random.randint(18, 80, n_samples),\n",
    "    'account_balance': np.random.uniform(100, 50000, n_samples),\n",
    "    'risk_level': np.random.choice(['Low', 'Medium', 'High'], n_samples)  # Categorical target\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"âœ… Sample dataset created!\")\n",
    "print(f\"   ğŸ“Š Shape: {df.shape}\")\n",
    "print(f\"   ğŸ“Š Columns: {list(df.columns)}\")\n",
    "print(f\"\\nğŸ“‹ Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nğŸ“‹ First few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Identify Feature Types\n",
    "### Ø§Ù„Ù…Ù‡Ù…Ø© 2: ØªØ­Ø¯ÙŠØ¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:16.999231Z",
     "iopub.status.busy": "2026-01-15T20:04:16.999155Z",
     "iopub.status.idle": "2026-01-15T20:04:17.000597Z",
     "shell.execute_reply": "2026-01-15T20:04:17.000421Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Separate numerical and categorical features\n",
    "# Hint: Use df.select_dtypes(include=['number']) for numerical\n",
    "# Hint: Use df.select_dtypes(include=['object']) for categorical\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Feature Scaling - StandardScaler\n",
    "### Ø§Ù„Ù…Ù‡Ù…Ø© 3: ØªØ­Ø¬ÙŠÙ… Ø§Ù„Ù…ÙŠØ²Ø§Øª - StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:17.001356Z",
     "iopub.status.busy": "2026-01-15T20:04:17.001302Z",
     "iopub.status.idle": "2026-01-15T20:04:17.002641Z",
     "shell.execute_reply": "2026-01-15T20:04:17.002468Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Apply StandardScaler to numerical features\n",
    "# Steps:\n",
    "# 1. Select numerical features\n",
    "# 2. Create StandardScaler object\n",
    "# 3. Fit and transform the features\n",
    "# 4. Display before/after statistics (mean, std)\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Feature Scaling - MinMaxScaler\n",
    "### Ø§Ù„Ù…Ù‡Ù…Ø© 4: ØªØ­Ø¬ÙŠÙ… Ø§Ù„Ù…ÙŠØ²Ø§Øª - MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:17.003399Z",
     "iopub.status.busy": "2026-01-15T20:04:17.003344Z",
     "iopub.status.idle": "2026-01-15T20:04:17.004719Z",
     "shell.execute_reply": "2026-01-15T20:04:17.004562Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Apply MinMaxScaler to numerical features\n",
    "# Steps:\n",
    "# 1. Create MinMaxScaler object\n",
    "# 2. Fit and transform numerical features\n",
    "# 3. Display before/after statistics (min, max)\n",
    "# 4. Compare with StandardScaler results\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Categorical Encoding - LabelEncoder\n",
    "### Ø§Ù„Ù…Ù‡Ù…Ø© 5: ØªØ±Ù…ÙŠØ² Ø§Ù„ÙØ¦Ø§Øª - LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:17.005434Z",
     "iopub.status.busy": "2026-01-15T20:04:17.005384Z",
     "iopub.status.idle": "2026-01-15T20:04:17.006736Z",
     "shell.execute_reply": "2026-01-15T20:04:17.006577Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Apply LabelEncoder to ordinal categorical features\n",
    "# Steps:\n",
    "# 1. Select ordinal categorical feature (e.g., risk_level)\n",
    "# 2. Create LabelEncoder object\n",
    "# 3. Fit and transform the feature\n",
    "# 4. Display mapping (original values -> encoded values)\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Categorical Encoding - OneHotEncoder\n",
    "### Ø§Ù„Ù…Ù‡Ù…Ø© 6: ØªØ±Ù…ÙŠØ² Ø§Ù„ÙØ¦Ø§Øª - OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:17.007443Z",
     "iopub.status.busy": "2026-01-15T20:04:17.007394Z",
     "iopub.status.idle": "2026-01-15T20:04:17.008718Z",
     "shell.execute_reply": "2026-01-15T20:04:17.008553Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Apply OneHotEncoder to nominal categorical features\n",
    "# Steps:\n",
    "# 1. Select nominal categorical features\n",
    "# 2. Create OneHotEncoder object\n",
    "# 3. Fit and transform the features\n",
    "# 4. Convert to DataFrame for better visualization\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Train-Test Split\n",
    "### Ø§Ù„Ù…Ù‡Ù…Ø© 7: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:17.009502Z",
     "iopub.status.busy": "2026-01-15T20:04:17.009443Z",
     "iopub.status.idle": "2026-01-15T20:04:17.010787Z",
     "shell.execute_reply": "2026-01-15T20:04:17.010649Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Split data into training and testing sets\n",
    "# Steps:\n",
    "# 1. Prepare features (X) - all columns except target\n",
    "# 2. Prepare target (y) - risk_level\n",
    "# 3. Use train_test_split with test_size=0.2, random_state=73\n",
    "# 4. Display shapes of train/test sets\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Complete Preprocessing Pipeline\n",
    "### Ø§Ù„Ù…Ù‡Ù…Ø© 8: Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø§Ù„ÙƒØ§Ù…Ù„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:17.011510Z",
     "iopub.status.busy": "2026-01-15T20:04:17.011462Z",
     "iopub.status.idle": "2026-01-15T20:04:17.012859Z",
     "shell.execute_reply": "2026-01-15T20:04:17.012715Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create a complete preprocessing pipeline\n",
    "# Steps:\n",
    "# 1. Split data first (train/test)\n",
    "# 2. Scale numerical features on training data\n",
    "# 3. Apply scaling to test data (using scaler fitted on train)\n",
    "# 4. Encode categorical features\n",
    "# 5. Combine scaled numerical + encoded categorical features\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Visualization - Compare Before/After Scaling\n",
    "### Ø§Ù„Ù…Ù‡Ù…Ø© 9: Ø§Ù„ØªØµÙˆØ± - Ù…Ù‚Ø§Ø±Ù†Ø© Ù‚Ø¨Ù„ ÙˆØ¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø¬ÙŠÙ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:17.013559Z",
     "iopub.status.busy": "2026-01-15T20:04:17.013512Z",
     "iopub.status.idle": "2026-01-15T20:04:17.014863Z",
     "shell.execute_reply": "2026-01-15T20:04:17.014681Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create visualizations comparing original vs scaled features\n",
    "# Steps:\n",
    "# 1. Plot distribution of original numerical feature\n",
    "# 2. Plot distribution after StandardScaler\n",
    "# 3. Plot distribution after MinMaxScaler\n",
    "# 4. Use subplots to show side-by-side comparison\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Exercise Complete!\n",
    "\n",
    "**What You Learned:**\n",
    "- âœ… Feature scaling (StandardScaler vs MinMaxScaler)\n",
    "- âœ… Categorical encoding (LabelEncoder vs OneHotEncoder)\n",
    "- âœ… Train-test split for proper evaluation\n",
    "- âœ… Complete preprocessing pipeline\n",
    "- âœ… GDI context: Preparing financial data for fraud detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
