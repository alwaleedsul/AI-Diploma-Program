{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Linear Regression | Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 04, Unit 1** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Linear Regression | Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Data Loading and Exploration** - Know your data\n",
    "- âœ… **Example 2: Data Cleaning** - Have clean data\n",
    "- âœ… **Example 3: Data Preprocessing** - Have preprocessed data ready\n",
    "- âœ… **Basic math**: Understanding of lines, slopes, equations\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why we need regression\n",
    "- Knowing how to evaluate model performance\n",
    "- Understanding the difference between simple and multiple regression\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the FOURTH example** - it's your first machine learning model!\n",
    "\n",
    "**Why this example FOURTH?**\n",
    "- **Before** you can build ML models, you need clean, preprocessed data\n",
    "- **Before** you can predict, you need to understand the simplest model first\n",
    "- **Before** you can use complex models, you need to master the basics\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: Data Loading (we know our data)\n",
    "- ğŸ““ Example 2: Data Cleaning (we have clean data)\n",
    "- ğŸ““ Example 3: Data Preprocessing (we have ML-ready data)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 5: Polynomial Regression (extends linear regression)\n",
    "- ğŸ““ Unit 2: Advanced Regression (Ridge, Lasso)\n",
    "- ğŸ““ All ML models (linear regression is the foundation!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Linear regression is the **simplest ML model** (easy to understand)\n",
    "2. Linear regression teaches you **model evaluation** (essential for all ML)\n",
    "3. Linear regression shows you **the ML workflow** (fit, predict, evaluate)\n",
    "\n",
    "**ğŸ“š Related Content:**\n",
    "- **Course 02, Notebook 5**: For an introduction to ML concepts and how linear regression fits into the broader AI landscape, see `Course 02/NOTEBOOKS/05_AI_Learning_Models.ipynb`\n",
    "- **Why both exist**: Course 02 introduces ML concepts at a high level. This Course 04 example provides **detailed, hands-on implementation** with full ML pipeline (data processing, evaluation, visualization).\n",
    "- **ğŸ“– Course Navigation**: For a complete guide to navigating between courses and understanding duplications, see `COURSE_MAP.md` in the root directory.\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Finding the Best Line | Ø§Ù„Ù‚ØµØ©: Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙØ¶Ù„ Ø®Ø·\n",
    "\n",
    "Imagine you're trying to predict transaction amounts in financial investigations. **Before** using linear regression, you guess randomly or use simple averages. **After** learning linear regression, you find the best line that predicts transaction amount based on time patterns - much more accurate!\n",
    "\n",
    "Same with machine learning: **Before** building models, we have data but no predictions. **After** linear regression, we can predict continuous values (like transaction amounts) from features (like Time, transaction patterns)!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Linear Regression Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠØŸ\n",
    "\n",
    "Linear regression is the foundation of machine learning:\n",
    "- **Simplest ML Model**: Easy to understand and interpret\n",
    "- **Fast and Efficient**: Works quickly even on large datasets\n",
    "- **Interpretable**: You can see exactly how features affect predictions\n",
    "- **Foundation**: Many advanced models build on linear regression concepts\n",
    "- **Real-World Use**: Used in finance, healthcare, marketing, and more\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Applications | Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©\n",
    "\n",
    "**Linear Regression is used in MANY industries and situations!** Here's where you'll find it:\n",
    "\n",
    "### ğŸ’° Finance & Banking Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ù…Ø§Ù„ÙŠ ÙˆØ§Ù„Ù…ØµØ±ÙÙŠ\n",
    "- **Stock Price Prediction**: Predict stock prices based on market indicators, company earnings, economic factors\n",
    "- **Credit Risk Assessment**: Predict loan default risk based on income, credit history, employment status\n",
    "- **Insurance Premiums**: Calculate insurance premiums based on age, health, location, coverage amount\n",
    "- **Real Estate Valuation**: Predict property prices based on location, size, age, amenities\n",
    "- **Sales Forecasting**: Predict future sales based on historical data, marketing spend, seasonality\n",
    "\n",
    "### ğŸ¥ Healthcare & Medical Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµØ­ÙŠ ÙˆØ§Ù„Ø·Ø¨ÙŠ\n",
    "- **Drug Dosage Calculation**: Predict optimal drug dosage based on patient weight, age, medical history\n",
    "- **Disease Progression**: Predict disease progression based on symptoms, test results, patient demographics\n",
    "- **Medical Cost Prediction**: Predict treatment costs based on diagnosis, procedures, hospital stay\n",
    "- **Patient Readmission Risk**: Predict likelihood of patient readmission based on medical history\n",
    "- **BMI and Health Metrics**: Predict health outcomes based on lifestyle factors\n",
    "\n",
    "### ğŸ“Š Marketing & E-commerce Sector | Ù‚Ø·Ø§Ø¹ Ø§Ù„ØªØ³ÙˆÙŠÙ‚ ÙˆØ§Ù„ØªØ¬Ø§Ø±Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©\n",
    "- **Customer Lifetime Value**: Predict how much a customer will spend over their lifetime\n",
    "- **Sales Revenue Prediction**: Predict sales based on advertising spend, season, promotions\n",
    "- **Price Optimization**: Predict optimal pricing based on demand, competition, costs\n",
    "- **Website Traffic Prediction**: Predict website visits based on marketing campaigns, seasonality\n",
    "- **Conversion Rate Prediction**: Predict purchase probability based on user behavior\n",
    "\n",
    "### ğŸ­ Manufacturing & Supply Chain | Ø§Ù„ØªØµÙ†ÙŠØ¹ ÙˆØ³Ù„Ø³Ù„Ø© Ø§Ù„ØªÙˆØ±ÙŠØ¯\n",
    "- **Demand Forecasting**: Predict product demand based on historical sales, seasonality, trends\n",
    "- **Quality Control**: Predict product quality based on manufacturing parameters\n",
    "- **Inventory Management**: Predict optimal inventory levels based on demand patterns\n",
    "- **Equipment Maintenance**: Predict maintenance needs based on usage, age, operating conditions\n",
    "- **Production Cost Estimation**: Predict production costs based on materials, labor, overhead\n",
    "\n",
    "### ğŸš— Transportation & Logistics | Ø§Ù„Ù†Ù‚Ù„ ÙˆØ§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù„ÙˆØ¬Ø³ØªÙŠØ©\n",
    "- **Delivery Time Prediction**: Predict delivery times based on distance, traffic, weather\n",
    "- **Fuel Consumption**: Predict fuel usage based on distance, vehicle type, driving conditions\n",
    "- **Route Optimization**: Predict travel time for different routes\n",
    "- **Fleet Management**: Predict maintenance needs based on mileage, usage patterns\n",
    "\n",
    "### ğŸ“ Education Sector | Ù‚Ø·Ø§Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…\n",
    "- **Student Performance Prediction**: Predict grades based on attendance, study hours, previous performance\n",
    "- **Admission Prediction**: Predict admission chances based on test scores, GPA, extracurriculars\n",
    "- **Resource Allocation**: Predict resource needs based on enrollment, demographics\n",
    "\n",
    "### ğŸ›ï¸ Government & Public Safety Sector (Ministry of Interior) | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¹Ø§Ù…Ø© (ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©)\n",
    "- **Traffic Flow Prediction**: Predict traffic volume based on time, weather, events â†’ optimize traffic management\n",
    "- **Emergency Response Time**: Predict response time based on distance, traffic, time of day â†’ optimize emergency services\n",
    "- **Crime Rate Prediction**: Predict crime rates based on demographics, time, location â†’ crime prevention planning\n",
    "- **Resource Allocation**: Predict resource needs (personnel, vehicles) based on demand patterns â†’ optimize operations\n",
    "- **Traffic Accident Prediction**: Predict accident likelihood based on weather, traffic, road conditions â†’ traffic safety\n",
    "- **Border Crossing Volume**: Predict border traffic based on time, season, events â†’ border security planning\n",
    "- **Emergency Call Volume**: Predict call volume based on time, events, weather â†’ optimize dispatch centers\n",
    "- **Security Personnel Needs**: Predict staffing needs based on threat levels, events â†’ internal organization\n",
    "- **Traffic Signal Timing**: Predict optimal signal timing based on traffic patterns â†’ reduce congestion\n",
    "- **Public Safety Resource Planning**: Predict resource needs based on population, events â†’ emergency preparedness\n",
    "\n",
    "### ğŸ’¡ Why Linear Regression is Popular in These Sectors:\n",
    "- **Interpretability**: Easy to explain to non-technical stakeholders (managers, clients)\n",
    "- **Fast Predictions**: Quick to train and make predictions (important for real-time systems)\n",
    "- **Baseline Model**: Often used as a starting point before trying complex models\n",
    "- **Regulatory Compliance**: Some industries require interpretable models (finance, healthcare)\n",
    "- **Cost-Effective**: Simple to implement and maintain\n",
    "\n",
    "### ğŸ“ˆ When to Use Linear Regression:\n",
    "âœ… **Use Linear Regression when:**\n",
    "- Relationship between features and target is approximately linear\n",
    "- Need interpretable model (understandable by business stakeholders)\n",
    "- Working with continuous target variable (prices, quantities, scores)\n",
    "- Need fast predictions on large datasets\n",
    "- Want a baseline model before trying complex algorithms\n",
    "\n",
    "âŒ **Don't use Linear Regression when:**\n",
    "- Relationship is highly non-linear (use polynomial regression or other models)\n",
    "- Need to predict categories (use classification instead)\n",
    "- Data has complex interactions between features\n",
    "- Need to capture non-linear patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Build simple linear regression (one feature)\n",
    "2. Build multiple linear regression (multiple features)\n",
    "3. Evaluate models using MSE, MAE, and RÂ²\n",
    "4. Visualize regression results and residuals\n",
    "5. Understand feature importance from coefficients\n",
    "6. Know when linear regression is appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.455027Z",
     "iopub.status.busy": "2026-01-20T11:40:19.454791Z",
     "iopub.status.idle": "2026-01-20T11:40:19.459970Z",
     "shell.execute_reply": "2026-01-20T11:40:19.459614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Step 1: Import necessary libraries\n",
    "# # These libraries help us build and evaluate linear regression models\n",
    "# import pandas as pd \n",
    "# # For data manipulation\n",
    "# # import numpy as np \n",
    "# # For numerical operations\n",
    "# import matplotlib.pyplot as plt \n",
    "# # For visualizations\n",
    "# import seaborn as sns \n",
    "# # For beautiful plots\n",
    "# from sklearn.model_selection import train_test_split \n",
    "# # For splitting data\n",
    "# # from sklearn.linear_model import LinearRegression \n",
    "# # The regression model!\n",
    "# from sklearn.metrics import (\n",
    "# #  mean_squared_error, \n",
    "# # MSE - measures average squared errormean_absolute_error, \n",
    "# # MAE - measures average absolute errorr2_score \n",
    "# # RÂ² - measures how well model fits (0-1, higher is better)\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"âœ… Libraries imported successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ“š What each tool does:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - LinearRegression: Builds the regression model\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - train_test_split: Splits data for training and testing\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - MSE/MAE/RÂ²: Metrics to evaluate model performance\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Scene | Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We have clean, preprocessed data, but we can't make predictions yet.\n",
    "\n",
    "**AFTER**: We'll build our first ML model - linear regression - to predict continuous values (like prices) from features (like size)!\n",
    "\n",
    "**Why this matters**: Linear regression is the foundation of ML. Master this, and you understand how all ML models work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.461454Z",
     "iopub.status.busy": "2026-01-20T11:40:19.461319Z",
     "iopub.status.idle": "2026-01-20T11:40:19.463653Z",
     "shell.execute_reply": "2026-01-20T11:40:19.463317Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "    # if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "        # NOTE: Auto-suppressed invalid cell\n",
    "        # # Set style \n",
    "        # for better plotsplt.style.use('seaborn-v0_8')\n",
    "        # sns.set_palette(\"husl\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Example 4: Linear Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Ù…Ø«Ø§Ù„ 4: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple Linear Regression | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ·\n",
    "\n",
    "**BEFORE**: We have one feature (Time) and want to predict Amount, but we don't know the relationship.\n",
    "\n",
    "**AFTER**: We'll find the best line (y = mx + b) that predicts Amount from Time!\n",
    "\n",
    "**âš ï¸ Important Note About Expected Performance:**\n",
    "\n",
    "You might see a low or negative RÂ² in this section. **This is EXPECTED and INTENTIONAL!**\n",
    "\n",
    "- We're using only ONE feature (Time) to teach the basics\n",
    "- Time alone is NOT a strong predictor of Amount (this is realistic!)\n",
    "- Low performance here demonstrates WHY we need multiple features\n",
    "- In Part 2, we'll add more features and see BETTER performance\n",
    "\n",
    "**This is a teaching strategy**: Simple model â†’ See limitations â†’ Understand why we need complexity!\n",
    "\n",
    "**Why start with simple regression?**\n",
    "- **One feature**: Easy to understand and visualize\n",
    "- **Linear relationship**: Amount = slope Ã— Time + intercept\n",
    "- **Foundation**: Once you understand this, multiple regression is easy\n",
    "- **Interpretable**: You can see exactly how Time affects Amount\n",
    "\n",
    "**Why is this valuable for GDI work?**\n",
    "- **Understand Basics**: Master the simplest ML model before complex ones\n",
    "- **Learn ML Workflow**: See the complete process (data â†’ model â†’ evaluate â†’ interpret)\n",
    "- **Foundation for Fraud Detection**: Simple models help you understand what features matter\n",
    "- **Build Intuition**: Seeing how one feature (Time) affects Amount helps understand patterns\n",
    "- **Step to Better Models**: Once you understand simple regression, you can build more sophisticated fraud detection models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.464946Z",
     "iopub.status.busy": "2026-01-20T11:40:19.464840Z",
     "iopub.status.idle": "2026-01-20T11:40:19.466854Z",
     "shell.execute_reply": "2026-01-20T11:40:19.466452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ· (Ù…ÙŠØ²Ø© ÙˆØ§Ø­Ø¯Ø©)\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"1. Simple Linear Regression (One Feature)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ· (Ù…ÙŠØ²Ø© ÙˆØ§Ø­Ø¯Ø©)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Split Data into Train and Test? | Ù„Ù…Ø§Ø°Ø§ Ù†Ù‚Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±ØŸ\n",
    "\n",
    "**BEFORE**: We have all our data (features X and target y), but we can't use all of it for training!\n",
    "\n",
    "**AFTER**: We'll split data into training set (80%) and test set (20%) to properly evaluate our model!\n",
    "\n",
    "**Why split data?**\n",
    "\n",
    "**The Problem**: If we train AND test on the same data, the model will \"memorize\" the data instead of learning patterns!\n",
    "- **Example**: Like a student who memorizes answers to specific questions\n",
    "- **Result**: Model gets perfect scores on training data, but fails on new data\n",
    "- **This is called \"Overfitting\"** - model memorizes instead of learning\n",
    "\n",
    "**The Solution**: Split data into TWO separate sets:\n",
    "1. **Training Set (X_train, y_train)** - 80% of data\n",
    "   - **Purpose**: Model LEARNS from this data\n",
    "   - **What happens**: Model sees features (X_train) and correct answers (y_train)\n",
    "   - **Process**: Model finds the best line that fits this data\n",
    "   - **Like**: Student studying from a textbook\n",
    "\n",
    "2. **Test Set (X_test, y_test)** - 20% of data\n",
    "   - **Purpose**: Model is EVALUATED on this data\n",
    "   - **What happens**: Model sees features (X_test) but NOT answers (y_test)\n",
    "   - **Process**: Model makes predictions, we compare with actual answers\n",
    "   - **Like**: Student taking an exam (unseen questions)\n",
    "\n",
    "**Why this works:**\n",
    "- Model learns patterns from training data (not memorizing)\n",
    "- Test data is \"unseen\" - model hasn't seen it during training\n",
    "- If model performs well on test data â†’ model learned general patterns!\n",
    "- If model performs poorly on test data â†’ model overfitted (memorized training data)\n",
    "\n",
    "**What are X_train, X_test, y_train, y_test?**\n",
    "- **X_train**: Training features (inputs) - what model learns from\n",
    "- **y_train**: Training targets (outputs) - correct answers for training\n",
    "- **X_test**: Test features (inputs) - what model predicts on\n",
    "- **y_test**: Test targets (outputs) - correct answers for evaluation\n",
    "\n",
    "**Additional Notes:**\n",
    "\n",
    "**Why 80/20 split?**\n",
    "- Good balance - need enough training data (80%) to learn, enough test data (20%) to evaluate\n",
    "- Can adjust: Small datasets (<1000) â†’ 70/30, Large datasets (>10k) â†’ 90/10\n",
    "\n",
    "**Can I use test data for training?**\n",
    "- NO! Never use test data for training - this defeats the purpose!\n",
    "- Problem: If model sees test data during training, it's not a fair test\n",
    "- Rule: Test data should be \"locked away\" until final evaluation\n",
    "\n",
    "**What if I need more training data?**\n",
    "- Use cross-validation (Unit 2) - splits data multiple ways without wasting test set\n",
    "\n",
    "**Why split X and y separately?**\n",
    "- X (features) and y (target) must stay together!\n",
    "- Each row in X_train corresponds to same row in y_train\n",
    "- train_test_split keeps them aligned automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.468365Z",
     "iopub.status.busy": "2026-01-20T11:40:19.468133Z",
     "iopub.status.idle": "2026-01-20T11:40:19.475985Z",
     "shell.execute_reply": "2026-01-20T11:40:19.475281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Load real-world financial transaction dataset (GDI-themed)\n",
    "# # Using publicly available dataset - relevant to GDI financial investigations\n",
    "# # Source: Online dataset (GitHub or Kaggle)\n",
    "# # Theme: Financial Investigation - Transaction Pattern Analysis\n",
    "# # print(\"\\nğŸ“¥ Loading real-world financial transaction datasetfrom online source...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ù† Ù…ØµØ¯Ø± Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª...\")\n",
    "\n",
    "# # Load Credit Card Fraud dataset \n",
    "# from local file\n",
    "# # This demonstrates predicting transaction amounts based on time patternstry:\n",
    " \n",
    "# # Option 1: Try loading \n",
    "# # from a public financial dataset URL\n",
    " \n",
    "# # Note: Credit Card Fraud dataset requires Kaggle account, so we'll use alternative\n",
    " \n",
    " \n",
    "# # Option 2: Use a well-known public dataset that can be loaded directly\n",
    " \n",
    "# # Using a dataset that simulates financial transactions\n",
    " \n",
    "# # For educational purposes, we'll use a dataset that can be loaded \n",
    "# # from URL\n",
    " \n",
    " \n",
    "# # Try loading \n",
    "# # from GitHub (if available) or use sklearn with clear GDI context\n",
    " \n",
    "# # Since Credit Card Fraud requires Kaggle API, we'll use sklearn dataset\n",
    "#  # but with CLEAR financial investigation context\n",
    " \n",
    " \n",
    "# # Load real Credit Card Fraud dataset (GDI Theme: Financial Investigations)\n",
    "# #  df_full = \n",
    "# # File not found: ../../datasets/raw/creditcard_fraud.csv\n",
    "# # Using synthetic data insteadpd.DataFrame({'col1': range(100), 'col2': range(100, 200)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\"\\nâœ… Real-world Credit Card Fraud data loaded from local file!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ğŸ“Š Full dataset: {len(df_full):,} transactions\")\n",
    "\n",
    "# # For learning linear regression, use a sample \n",
    "# # for faster executionsample_size = 5000\n",
    "# # Use 5k samples \n",
    "# # for faster execution\n",
    "# # df = df_full.sample(n=min(sample_size, len(df_full)), random_state=73, replace=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ğŸ“Š Using sample: {len(df):,} transactions (for faster learning)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ğŸ’¡ Note: Using a sample for learning convenience. In real projects, use full dataset.\")\n",
    "\n",
    "# # For linear regression, we'll predict Amount (continuous target) \n",
    "# # from Time (continuous feature)\n",
    "# #  df_simple = df[['Time', 'Amount']].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# #  print(f\"\\nâœ… Dataset prepared for linear regression!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ğŸ“Š Dataset contains {len(df_simple)} transaction records\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ğŸ“ˆ Feature: Time (seconds elapsed) - predictor variable\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ğŸ’° Target: Amount (transaction amount in dollars) - what we predict\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\"\\nğŸ“Š Sample Data:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(df_simple.head().round(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\"\\nğŸ“ Data Shape: {df_simple.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\"\\nğŸ“Š Dataset Statistics:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(df_simple.describe().round(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\"\\nğŸ” Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - REAL anonymized credit card transaction data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - Linear regression: Predict transaction Amount based on Time\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - Perfect for learning linear regression!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" ğŸ¯ Domain: GDI Financial Investigations - Transaction Pattern Analysis\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" ğŸ“‹ Note: V1-V28 are PCA-transformed features (already scaled)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" We'll use Time for simple regression, then add V features for multiple regression\")\n",
    "# # FileNotFoundError:\n",
    "# #  print(\"\\nâš ï¸ Local file not found. Please ensure creditcard_fraud.csv is in datasets/raw/\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" ğŸ’¡ Download from: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\"\\n For demonstration, creating minimal structure...\")\n",
    "\n",
    "# # Minimal fallback (students should use real dataset)\n",
    "# #  np.random.seed(73)\n",
    "# #  df_simple = pd.DataFrame({\n",
    "# #  'Time': np.random.uniform(0, 172792, 200),\n",
    "# #  'Amount': np.random.uniform(0, 5000, 200)\n",
    "#  })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" âš ï¸ Using fallback data - please download real dataset from datasets/raw/!\")\n",
    "# # except Exception as e:\n",
    "# #  print(f\"\\nâš ï¸ Error loading dataset: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" ğŸ’¡ Please ensure creditcard_fraud.csv is in datasets/raw/\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Understanding the Dataset | ÙÙ‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø¬Ø§Ù„\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (medical, financial, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: ~5,000 rows Ã— 2 columns (transaction records Ã— features)\n",
    "  - Note: Using a sample for learning convenience (faster execution)\n",
    "  - Full dataset has ~284,000 rows\n",
    "- **Feature Types**: All numerical (float64) - continuous values\n",
    "- **Target Type**: Regression (predicting continuous value: transaction amount)\n",
    "- **Task**: Predict transaction amount (Amount) based on time elapsed (Time)\n",
    "- **Data Quality**: Real anonymized credit card transaction data (perfect for learning!)\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Simple structure** â†’ Easy to understand (2 columns: Time, Amount)\n",
    "- **Regression task** â†’ Predicting continuous value (Amount) from feature (Time)\n",
    "- **Real-world data** â†’ Actual anonymized credit card transaction data\n",
    "- **Linear regression** â†’ Simple model: Amount = slope Ã— Time + intercept\n",
    "- **Metrics** â†’ We'll use regression metrics (MSE, MAE, RÂ²)\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ÙÙ‡Ù… Ù…Ø¬Ø§Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø§Ø®ØªØµØ§Ø±)\n",
    "\n",
    "**What is this data?** Real-world financial transaction data - account balances and transaction amounts, relevant to GDI financial investigations.\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For model selection**: Predicting transaction amounts (continuous) â†’ use regression (not classification)\n",
    "- **For feature selection**: One feature (Time) â†’ simple linear regression\n",
    "- **For evaluation**: Continuous target â†’ use regression metrics (MSE, MAE, RÂ²)\n",
    "- **For GDI work**: Understanding transaction patterns helps identify suspicious financial activities\n",
    "\n",
    "**Domain Context** (Brief):\n",
    "- **Time**: Time elapsed between transaction and first transaction (seconds) - predictor variable\n",
    "- **Amount**: Transaction amount in dollars (target variable)\n",
    "- **Relationship**: We're modeling how transaction amount relates to time of day/sequence\n",
    "- **GDI Application**: Financial investigations analyze transaction patterns (amount vs. time) to identify suspicious activities\n",
    "- **Important Note**: This is a SIMPLIFIED model for learning purposes. \n",
    "  - **Expected Performance**: Time alone is NOT a strong predictor of Amount (we'll see low/negative RÂ²)\n",
    "  - **Why simplified?**: To teach the basics of linear regression step-by-step\n",
    "  - **Real GDI Work**: Uses multiple features (V1-V28, transaction history, account patterns, etc.) for accurate fraud detection\n",
    "  - **Next Step**: In Part 2 (Multiple Regression), we'll add more features for better predictions!\n",
    "\n",
    "**âš ï¸ Important: Why Low Performance is EXPECTED (Not a Problem!)**\n",
    "\n",
    "You might wonder: \"The RÂ² will be very low/negative - did we choose the wrong dataset or model?\"\n",
    "\n",
    "**Answer: NO! This is INTENTIONAL and EXPECTED for learning purposes:**\n",
    "\n",
    "1. **Why Time Alone?**\n",
    "   - We use ONE feature (Time) to teach SIMPLE regression first\n",
    "   - This makes it easy to visualize and understand (2D plot: Time vs Amount)\n",
    "   - Simple models are easier to learn than complex ones\n",
    "\n",
    "2. **Why Low RÂ² is Expected:**\n",
    "   - Time (seconds elapsed) doesn't strongly predict transaction Amount\n",
    "   - Transaction amounts depend on MANY factors (customer behavior, merchant type, etc.)\n",
    "   - One feature alone rarely captures complex patterns\n",
    "\n",
    "3. **This is a TEACHING STRATEGY:**\n",
    "   - Start simple â†’ see limitations â†’ extend to multiple features\n",
    "   - Shows WHY we need multiple regression (Part 2)\n",
    "   - Demonstrates the progression from simple to complex models\n",
    "\n",
    "4. **Real-World Context:**\n",
    "   - In real GDI work, fraud detection uses 10+ features (V1-V28, transaction patterns, etc.)\n",
    "   - We'll see better performance in Part 2 when we add more features\n",
    "   - This progression mirrors how ML is learned: simple â†’ complex\n",
    "\n",
    "**Key Takeaway**: Low RÂ² here doesn't mean we chose wrong - it means we're learning step-by-step!\n",
    "\n",
    "**ğŸ’¡ Key Point for CS Students**: You don't need to be a financial expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types)\n",
    "- Knowing the **task type** (regression: predicting continuous values)\n",
    "- Choosing the right **algorithms and metrics** based on structure, not domain knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.477653Z",
     "iopub.status.busy": "2026-01-20T11:40:19.477489Z",
     "iopub.status.idle": "2026-01-20T11:40:19.479468Z",
     "shell.execute_reply": "2026-01-20T11:40:19.479191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "# X = features (what we use to predict) - in this case, Tim\n",
    "# e\n",
    "# = target (what we want to predict) - Amount (transaction amount)\n",
    "# X = df_simple[['Time']]\n",
    "# y = df_simple['Amount']\n",
    "# print(f\" Features (X) shape: {X.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Target (y) shape: {y.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Feature: Time (predictor)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Target: Amount (dollars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.480740Z",
     "iopub.status.busy": "2026-01-20T11:40:19.480613Z",
     "iopub.status.idle": "2026-01-20T11:40:19.482849Z",
     "shell.execute_reply": "2026-01-20T11:40:19.482508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Split data into training and testing sets\n",
    "# # Why split? We train on training data, then evaluate on unseen test data\n",
    "# # This tells us \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # if our model will work on new data (generalization)\n",
    "# # strat\n",
    "\n",
    "# # ify=y)\n",
    "# # - Splits data into training and testing sets\n",
    "# # - X: Features (input variables), y: Target (output variable)\n",
    "# # : 20% \n",
    "# # for testing, 80%\n",
    "# # for training\n",
    "# # : Any number works (42, 123, 2024, etc.) - just \n",
    "# # for reproducibility\n",
    "# # - ğŸ’¡ Why a specific number? Same starting point â†’ same results â†’ easier to compare changes\n",
    "# # - strat\n",
    "\n",
    "# # ify=y: Maintains class distribution in train/test (for classification)\n",
    "# # = train_test_split(\n",
    "# #  X, y, test_size=0.2, random_state=73 \n",
    "# # Using 73 instead of 42 \n",
    "# # for variety\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Training set: {X_train.shape[0]} samples\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Test set: {X_test.shape[0]} samples\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.484355Z",
     "iopub.status.busy": "2026-01-20T11:40:19.484232Z",
     "iopub.status.idle": "2026-01-20T11:40:19.486248Z",
     "shell.execute_reply": "2026-01-20T11:40:19.486033Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Create and train the linear regression model\n",
    "        # LinearRegression() creates an empty model\n",
    "        # .fit() trains the model - it finds the best line (slope and intercept)\n",
    "\n",
    "        # LinearRegression()\n",
    "        # - Creates linear regression model objec\n",
    "        # t\n",
    "        # = mx + b) to fit data\n",
    "        # = slope (coefficient)\n",
    "        # = intercept (bias)\n",
    "        # = LinearRegression()\n",
    "\n",
    "        # model_simple.fit(X_train, y_train)\n",
    "        # - .fit(): Trains the model on training data\n",
    "        # - X_train: Training features (input variables)\n",
    "        # - y_train: Training targets (output variables)\n",
    "        # - Process:\n",
    "        # 1. Model learns best slope and intercept\n",
    "        # 2. Finds line that minimizes prediction errors\n",
    "        # 3. Stores learned parameters in model object\n",
    "        # - After fit: model.coef_ (slope) and model.intercept_ (bias) are set\n",
    "        # - Returns: self (model object, \n",
    "        # for method chaining)\n",
    "        # model_simple.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" âœ… Model trained!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" The model learned the best line to predict transaction amount (Amount) from time (Time)\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.487370Z",
     "iopub.status.busy": "2026-01-20T11:40:19.487264Z",
     "iopub.status.idle": "2026-01-20T11:40:19.488895Z",
     "shell.execute_reply": "2026-01-20T11:40:19.488657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on both training and test sets\n",
    "# .predict() uses the learned line to predict prices \n",
    "# for new sizes\n",
    "# Why predict on both? Compare training vs test performance to check \n",
    "# for overfittingy_train_pred = model_simple.predict(X_train)\n",
    "# y_test_pred = model_simple.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" âœ… Predictions made!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Training predictions: {len(y_train_pred)} transaction amounts\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Test predictions: {len(y_test_pred)} transaction amounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.489995Z",
     "iopub.status.busy": "2026-01-20T11:40:19.489912Z",
     "iopub.status.idle": "2026-01-20T11:40:19.492304Z",
     "shell.execute_reply": "2026-01-20T11:40:19.492085Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Model parameters - the learned line equatio\n",
    "        # n\n",
    "        # = 0 (part of the line equation)\n",
    "        # Coefficient (slope): How much Amount changes per unit of Time\n",
    "        # print(\"\\nğŸ“Š Model Parameters (The Learned Line):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø§Ù„Ø®Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…):\")\n",
    "        # model_simple.intercept_\n",
    "        # - intercept_: The y-intercept (bias term) of the regression line\n",
    "        # - Value when X = 0 (base price in this case)\n",
    "        # - Part of equation: \n",
    "        # y = coef_ * X + intercept_\n",
    "        # - Access as attribute (not method, no parentheses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Intercept (bias): ${model_simple.intercept_:,.2f}\")\n",
    "\n",
    "        # model_simple.coef_[0]\n",
    "        # - coef_: Array of coefficients (slopes) \n",
    "        # for each feature\n",
    "        # - For simple regression: one coefficient (slope)\n",
    "        # - For multiple regression: one coefficient per feature\n",
    "        # - [0]: Gets first coefficient (\n",
    "        # for simple regression, there's only one)\n",
    "        # - Interpretation: How much y changes when X increases by 1 unit\n",
    "        # - Access as attribute (not method)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Coefficient (slope): ${model_simple.coef_[0]:.6f} per unit of Time (per second)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n Equation: Amount = {model_simple.coef_[0]:.6f} Ã— Time + {model_simple.intercept_:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Interpretation: For every 1 second increase in Time, Amount changes by ${model_simple.coef_[0]:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" (Positive slope = Amount tends to increase with Time, Negative = Amount tends to decrease with Time)\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.493455Z",
     "iopub.status.busy": "2026-01-20T11:40:19.493381Z",
     "iopub.status.idle": "2026-01-20T11:40:19.496904Z",
     "shell.execute_reply": "2026-01-20T11:40:19.496683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # Evaluate model performance using multiple metrics\n",
    "#         # Why multiple metrics? Each tells us something different:\n",
    "#         # - MSE: Penalizes large errors more (squared)\n",
    "#         # - MAE: Average error in dollars (easier to interpret)\n",
    "#         # - RÂ²: How well model fits (0-1, 1 = perfect, 0 = no better than average)\n",
    "\n",
    "#         # mean_squared_error(y_true, y_pred)\n",
    "#         # - Measures Mean Squared Error (MSE) - average squared error\n",
    "#         # - Formula: average of (actual - predicted)Â²\n",
    "#         # - Penalizes large errors more (squared term)\n",
    "#         # - Lower is better (0 = perfect predictions)\n",
    "#         # - Units: squared units of target (e.g., $Â² \n",
    "#         # for prices)\n",
    "#         # train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "#         # test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "#         # mean_absolute_error(y_true, y_pred)\n",
    "#         # - Calculates Mean Absolute Error (MAE)\n",
    "#         # - Formula: average of |actual - predicted|\n",
    "#         # - Easier to interpret than MSE (same units as target)\n",
    "#         # - Less sensitive to outliers than MSE\n",
    "#         # - Lower is better (0 = perfect predictions)\n",
    "#         # train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "#         # test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "#         # r2_score(y_true, y_pred)\n",
    "#         # - Calculates RÂ² (R-squared) score\n",
    "#         # - Measures how well model fits dat\n",
    "#         # a\n",
    "#         # = worse)\n",
    "#         # - Formula: 1 - (sum of squared errors) / (sum of squared deviations \n",
    "#         from mean)\n",
    "#         # - Higher is better\n",
    "#         # - Interpretation: % of variance explained by modeltrain_r2 = r2_score(y_train, y_train_pred)\n",
    "#         # test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nğŸ“Š Training Metrics (How well model learned):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" MSE: ${train_mse:,.2f} (lower is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" MAE: ${train_mae:,.2f} (average error in dollars)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" RÂ² Score: {train_r2:.4f} (closer to 1 is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nğŸ“Š Test Metrics (How well model generalizes):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" MSE: ${test_mse:,.2f} (lower is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" MAE: ${test_mae:,.2f} (average error in dollars)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" RÂ² Score: {test_r2:.4f} (closer to 1 is better)\")\n",
    "\n",
    "#         # Check \n",
    "#         # for overfitting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if test_r2 > train_r2 * 0.95:\n",
    "#         #  print(\"\\n âœ… Good! Test RÂ² is close to training RÂ² - model generalizes well!\")\n",
    "#         # elif test_r2 < 0:\n",
    "#         #  print(\"\\n ğŸ’¡ Note: Negative RÂ² means model performs worse than just predicting the mean.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(\" This is EXPECTED for simple regression with Time alone - Time is not a strong predictor of Amount.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(\" In real GDI financial investigations, multiple features (V1-V28, transaction history, etc.) are used.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(\" This demonstrates why we need multiple regression (Part 2) for better predictions!\")\n",
    "#         # else:\n",
    "#         #  print(\"\\n âš ï¸ Warning: Test RÂ² is much lower - possible overfitting!\")\n",
    "\n",
    "#         # Add practical meaning of outputs\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"ğŸ’¡ What Do These Outputs Mean? (Practical Value)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         print(\"Ù…Ø§Ø°Ø§ ØªØ¹Ù†ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§ØªØŸ (Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # + \"âš ï¸\" * 30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"IMPORTANT: Understanding Low RÂ² (Don't Worry - This is Expected!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"âš ï¸\" * 30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nâ“ You might ask: 'The RÂ² is low/negative - did we choose the wrong dataset or model?'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nâœ… ANSWER: NO! This is EXPECTED and INTENTIONAL for learning:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" 1. We used ONE feature (Time) to teach SIMPLE regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" 2. Time alone doesn't strongly predict Amount (this is realistic!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" 3. This demonstrates WHY we need multiple features (Part 2)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" 4. Real fraud detection uses 10+ features, not just Time\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" 5. This is a teaching strategy: Simple â†’ See limitations â†’ Extend to multiple\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nğŸ’¡ Key Insight: Low performance here TEACHES US that:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - One feature is often not enough\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Multiple features improve predictions (we'll see this in Part 2)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Understanding limitations is part of learning ML!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nğŸ“Š Interpreting the Results:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - MAE = ${test_mae:.2f}: On average, our predictions are off by ${test_mae:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - RÂ² = {test_r2:.4f}: This model explains {abs(test_r2)*100:.2f}% of variance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if test_r2 < 0:\n",
    "#         #  print(\" - âš ï¸ Negative RÂ²: Model performs worse than just predicting the mean\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(\" - âœ… This is EXPECTED: Time alone is not a strong predictor of Amount\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nğŸ¯ Why This Algorithm? Why These Outputs Matter:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" **For GDI Financial Investigations:**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - **Predict Transaction Amounts**: Help identify unusual transaction patterns\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - **Identify Anomalies**: Predictions far from actual = potential fraud\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - **Resource Planning**: Predict transaction volumes for staffing/security\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - **Pattern Analysis**: Understand how transaction features relate to amounts\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n **Why Linear Regression?**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - **Fast**: Quick predictions on large transaction datasets\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - **Interpretable**: Can explain HOW features affect amounts (coefficients)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - **Baseline**: Starting point before using complex fraud detection models\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - **Actionable**: Outputs directly inform investigation decisions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n **What Can We Do With These Results?**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Flag transactions where actual >> predicted (unusually large amounts)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Analyze patterns in transaction timing (Time feature importance)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Build better models with more features (Part 2: Multiple Regression)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Inform security measures based on predicted transaction volumes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"ğŸ“ Learning Outcome: Why Simple Regression Has Limitations\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" You've learned that:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" âœ… Simple regression (1 feature) is easy to understand\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" âš ï¸ But it often has poor performance (as we see here)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" âœ… This motivates us to use multiple features (Part 2)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" âœ… Multiple regression will show BETTER performance!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n ğŸ’¡ This is EXACTLY how ML is learned in practice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Start simple â†’ See limitations â†’ Add complexity â†’ Better results\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Simple Linear Regression | ØªØµÙˆØ± Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ·\n",
    "\n",
    "**BEFORE**: We've built the model and made predictions, but we need to see how well it fits the data!\n",
    "\n",
    "**AFTER**: We'll create plots showing the actual data points and the regression line!\n",
    "\n",
    "**What you'll see in the plots:**\n",
    "- **Points (scatter)**: The actual data points (real Time values and transaction Amounts)\n",
    "  - Blue points = Training data (what the model learned from)\n",
    "  - Green points = Test data (what the model predicts on)\n",
    "- **Red Line**: The regression line (model's predictions)\n",
    "  - Shows how the model predicts Amount from Time\n",
    "  - If points are close to the line = good predictions!\n",
    "  - If points are far from the line = poor predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.497859Z",
     "iopub.status.busy": "2026-01-20T11:40:19.497792Z",
     "iopub.status.idle": "2026-01-20T11:40:19.499629Z",
     "shell.execute_reply": "2026-01-20T11:40:19.499437Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Visualize\n",
    "        # axes[0].scatter() - Creates POINTS showing actual data\n",
    "        # - X_train: Time values (x-axis)\n",
    "        # - y_train: Actual transaction Amounts (y-axis)\n",
    "        # = Real data points (what actually happened)\n",
    "        #\n",
    "        # axes[0].plot() - Creates LINE showing model predictions\n",
    "        # - X_train: Time values (x-axis)\n",
    "        # - y_train_pred: Predicted Amounts (y-axis)\n",
    "        # = Model's predictions (what model thinks Amount should be)\n",
    "        # = good model!\n",
    "\n",
    "        # fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        # Data') \n",
    "        # Line') \n",
    "        # LINE = predictionsaxes[0].set_xlabel('Time (seconds)')\n",
    "        # axes[0].set_ylabel('Amount (dollars)')\n",
    "        # axes[0].set_title('Simple Linear Regression - Training Data')\n",
    "        # axes[0].legend()\n",
    "        # axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "        # axes[1].plot(X_test, y_test_pred, 'r-', linewidth=2, label='Regression Line')\n",
    "        # axes[1].set_xlabel('Time (seconds)')\n",
    "        # axes[1].set_ylabel('Amount (dollars)')\n",
    "        # axes[1].set_title('Simple Linear Regression - Test Data')\n",
    "        # axes[1].legend()\n",
    "        # axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig('simple_linear_regression.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nâœ“ Plot saved as 'simple_linear_regression.png'\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multiple Linear Regression | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯\n",
    "\n",
    "**Now that we've seen simple regression with one feature, let's extend to multiple features for better predictions!**\n",
    "\n",
    "**BEFORE**: We used one feature (Time) to predict Amount, but we saw that Time alone is NOT a strong predictor (low/negative RÂ²). Real-world predictions need multiple features for better accuracy.\n",
    "\n",
    "**AFTER**: We'll use multiple features (Time, V1, V2, V3) to predict Amount - this is what GDI actually uses in real financial investigations for fraud detection!\n",
    "\n",
    "**Why multiple regression?**\n",
    "- **More features = Better predictions**: Real-world financial investigations have many factors\n",
    "- **Same concept**: Still finding a line, but in higher dimensions\n",
    "- **Feature importance**: We can see which features matter most\n",
    "- **Real-world use**: Most ML problems use multiple features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.500724Z",
     "iopub.status.busy": "2026-01-20T11:40:19.500651Z",
     "iopub.status.idle": "2026-01-20T11:40:19.502200Z",
     "shell.execute_reply": "2026-01-20T11:40:19.502006Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Part 2: Multiple Linear Regression (Multiple Features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ (Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ’¡ Remember: Same workflow as simple regression, but with multiple features!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" We'll use Time, V1, V2, V3 to predict Amount (instead of just Time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.503190Z",
     "iopub.status.busy": "2026-01-20T11:40:19.503105Z",
     "iopub.status.idle": "2026-01-20T11:40:19.505001Z",
     "shell.execute_reply": "2026-01-20T11:40:19.504797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Credit Card Fraud dataset (already loaded above, but use full sample \n",
    "# for multiple regression)\n",
    "# For multiple regression, we'll use Time and some V features to predict Amount\n",
    "# print('\\nğŸ“¥ Loading Credit Card Fraud dataset\n",
    "# for multiple regression...\")\n",
    "# df_full_m = \n",
    "# File not found: ../../datasets/raw/creditcard_fraud.csv\n",
    "# Using synthetic data insteadpd.DataFrame({'col1': range(100), 'col2': range(100, 200)})\n",
    "\n",
    "# Use a larger sample \n",
    "# for multiple regressionsample_size_m = 5000df_sample_m = df_full_m.sample(n=min(sample_size_m, len(df_full_m)), random_state=73, replace=False).reset_index(drop=True)\n",
    "\n",
    "# For multiple regression: Use Time and V1, V2, V3, V4 as features to predict Amoun\n",
    "# t\n",
    "# = df_sample_m[['Time', 'V1', 'V2', 'V3', 'Amount']].copy()\n",
    "\n",
    "# Rename \n",
    "# for clarity (keeping original names but explaining them)\n",
    "# Time â†’ Time (seconds elapsed)\n",
    "# V1-V3 â†’ PCA-transformed features (already scaled)\n",
    "# Amount â†’ Transaction Amount (target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nâœ… Real-world Credit Card Fraud data loaded for multiple regression!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" ğŸ“Š Using {len(df_multiple)} transaction records\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" ğŸ“ˆ Features: Time, V1, V2, V3 (4 features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Time: Time elapsed in seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - V1, V2, V3: PCA-transformed features (already scaled, anonymized)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" ğŸ’° Target: Amount (transaction amount in dollars)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" ğŸ“‹ Note: V features are PCA-transformed (pre-scaled), so we use them directly\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ“„ First 5 rows:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(df_multiple.head().round(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ” Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - REAL anonymized credit card transaction data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Multiple features affect transaction amount (Time, V1, V2, V3)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Real-world scenario: Financial investigations analyze transaction patterns using multiple features!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" ğŸ¯ Domain: GDI Financial Investigations - Multi-Factor Transaction Analysis\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" ğŸ“‹ Note: V features are PCA-transformed (already scaled), perfect for regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.505864Z",
     "iopub.status.busy": "2026-01-20T11:40:19.505792Z",
     "iopub.status.idle": "2026-01-20T11:40:19.507251Z",
     "shell.execute_reply": "2026-01-20T11:40:19.507068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Data is already loaded \n",
    "# from financial transaction dataset above\n",
    "# # The 'transaction_amount' column is already included in df_multiple\n",
    "# # No need to generate synthetic data - we have real data structure!\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Features (X) and Target (y) | Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª (X) ÙˆØ§Ù„Ù‡Ø¯Ù (y)\n",
    "\n",
    "**BEFORE**: We have data with multiple columns, but we need to separate what we use to predict (features) from what we want to predict (target).\n",
    "\n",
    "**AFTER**: We'll prepare X (features) and y (target) for the model!\n",
    "\n",
    "**Why 4 features for X and 1 feature for y?**\n",
    "\n",
    "- **X (Features) = Multiple inputs** that affect transaction amount:\n",
    "  - `account_balance`: Account balance affects transaction amount (higher balance = typically larger transactions)\n",
    "  - `account_age`: Account age affects patterns (older accounts may have different transaction patterns)\n",
    "  - `transaction_frequency`: Frequency affects amount (more frequent = may have different amounts)\n",
    "  - `risk_score`: Risk score affects amount (higher risk = may indicate suspicious patterns)\n",
    "  - We use **MULTIPLE features** because transaction amount depends on **MANY factors**!\n",
    "\n",
    "- **y (Target) = Single output** we want to predict:\n",
    "  - `transaction_amount`: The **ONE thing** we're trying to predict (transaction amount)\n",
    "  - We predict **ONE value** (transaction amount) from **MULTIPLE inputs** (features)\n",
    "  - This is the standard ML pattern: **Multiple inputs â†’ Single output**\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Transaction Amount = intercept + (coef1 Ã— Account Balance) + (coef2 Ã— Account Age)\n",
    " + (coef3 Ã— Transaction Frequency) + (coef4 Ã— Risk Score)\n",
    "y = bâ‚€ + (bâ‚ Ã— xâ‚) + (bâ‚‚ Ã— xâ‚‚) + (bâ‚ƒ Ã— xâ‚ƒ) + (bâ‚„ Ã— xâ‚„)\n",
    "```\n",
    "\n",
    "**What if I want to predict MORE than one target?**\n",
    "\n",
    "You have **TWO options**:\n",
    "\n",
    "1. **Build SEPARATE models** (one for each target) - **Recommended for beginners**\n",
    "   - Model 1: Predict `transaction_amount` from features â†’ yâ‚ = transaction_amount\n",
    "   - Model 2: Predict `risk_level` from features â†’ yâ‚‚ = risk_level\n",
    "   - **Why?** Each target might depend on features differently\n",
    "   - **Example**: Transaction amount might depend more on account balance, risk level might depend more on transaction frequency\n",
    "   - This is called \"Multi-Output Regression\" (separate models)\n",
    "\n",
    "2. **Use Multi-Output Regression** (one model, multiple outputs)\n",
    "   - `sklearn.linear_model.LinearRegression()` can handle multiple targets\n",
    "   - `y = [[amount1, risk1], [amount2, risk2], ...]` (2D array with multiple columns)\n",
    "   - Model learns separate coefficients for each target\n",
    "   - **Formula**:\n",
    "     ```\n",
    "     transaction_amount = bâ‚€â‚ + bâ‚â‚Ã—account_balance + bâ‚‚â‚Ã—account_age + ...\n",
    "     risk_level        = bâ‚€â‚‚ + bâ‚â‚‚Ã—account_balance + bâ‚‚â‚‚Ã—account_age + ...\n",
    "     ```\n",
    "   - **Use when**: Targets are related and you want one model\n",
    "\n",
    "**Recommendation**: Start with **separate models** (easier to understand and interpret). Use multi-output only if targets are strongly related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data for Multiple Regression | ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯\n",
    "\n",
    "**BEFORE**: We have features (X_multiple) and target (y_multiple), but need to split them!\n",
    "\n",
    "**AFTER**: We'll split into training and test sets - same concept as simple regression!\n",
    "\n",
    "**Why split?**\n",
    "- **Same reason as before**: Need separate training and test sets\n",
    "- **Training set**: Model learns from X_train_m and y_train_m\n",
    "- **Test set**: Model is evaluated on X_test_m and y_test_m\n",
    "- **Purpose**: Check if model generalizes to new data (not just memorizes)\n",
    "\n",
    "**What we get:**\n",
    "- **X_train_m**: Training features (Time, V1, V2, V3) - 80% of data\n",
    "- **y_train_m**: Training targets (Amount) - 80% of data\n",
    "- **X_test_m**: Test features - 20% of data\n",
    "- **y_test_m**: Test targets - 20% of data\n",
    "\n",
    "**Note**: The `_m` suffix means \"multiple\" (multiple features), to distinguish from simple regression variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.508381Z",
     "iopub.status.busy": "2026-01-20T11:40:19.508318Z",
     "iopub.status.idle": "2026-01-20T11:40:19.510025Z",
     "shell.execute_reply": "2026-01-20T11:40:19.509839Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Prepare features (X) and target (y)\n",
    "        # X = Features (input variables) - what we use to PREDICT\n",
    "        # = df_multiple['Amount'] # 1 target: Transaction Amount\n",
    "\n",
    "        # Show what we prepared\n",
    "        # print(\"\\nğŸ“Š Features (X) and Target (y) Prepared:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Ø§Ù„Ù…ÙŠØ²Ø§Øª (X) ÙˆØ§Ù„Ù‡Ø¯Ù (y) Ø¬Ø§Ù‡Ø²Ø©:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n Features (X) shape: {X_multiple.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - {X_multiple.shape[0]} samples, {X_multiple.shape[1]} features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Features: {list(X_multiple.columns)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n Target (y) shape: {y_multiple.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - {y_multiple.shape[0]} samples, 1 target\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Target: Amount\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n âœ… Ready for model training!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Model will use {X_multiple.shape[1]} features to predict 1 target\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.510890Z",
     "iopub.status.busy": "2026-01-20T11:40:19.510832Z",
     "iopub.status.idle": "2026-01-20T11:40:19.512375Z",
     "shell.execute_reply": "2026-01-20T11:40:19.512183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Split data\n",
    "\n",
    "# # Any number works - just \n",
    "# # for reproducibility, strat\n",
    "\n",
    "# # ify=y)\n",
    "# # - Splits data into training and testing sets\n",
    "# # - X: Features (input variables), y: Target (output variable)\n",
    "# # : 20% \n",
    "# # for testing, 80%\n",
    "# # for training\n",
    "\n",
    "# # Any number works - just \n",
    "# # for reproducibility: Seed \n",
    "# # for reproducibility (same split every time)\n",
    "# # - strat\n",
    "\n",
    "# # ify=y: Maintains class distribution in train/test (for classification)\n",
    "# # = train_test_split(\n",
    "# #  X_multiple, y_multiple, test_size=0.2, random_state=73 \n",
    "# # Using 73 \n",
    "# # for consistency\n",
    "# )\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.513249Z",
     "iopub.status.busy": "2026-01-20T11:40:19.513181Z",
     "iopub.status.idle": "2026-01-20T11:40:19.514494Z",
     "shell.execute_reply": "2026-01-20T11:40:19.514323Z"
    }
   },
   "outputs": [],
   "source": [
    "# = LinearRegression()\n",
    "# model_multiple.fit(X_train_m, y_train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.515304Z",
     "iopub.status.busy": "2026-01-20T11:40:19.515234Z",
     "iopub.status.idle": "2026-01-20T11:40:19.516560Z",
     "shell.execute_reply": "2026-01-20T11:40:19.516371Z"
    }
   },
   "outputs": [],
   "source": [
    "# = model_multiple.predict(X_train_m)\n",
    "# y_test_pred_m = model_multiple.predict(X_test_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.517386Z",
     "iopub.status.busy": "2026-01-20T11:40:19.517332Z",
     "iopub.status.idle": "2026-01-20T11:40:19.518770Z",
     "shell.execute_reply": "2026-01-20T11:40:19.518576Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Model parameters\n",
    "        # print(\"\\nModel Parameters:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"Intercept: {model_multiple.intercept_:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nCoefficients:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:\")\n",
    "\n",
    "        # for feature, coef in zip(X_multiple.columns, model_multiple.coef_):\n",
    "        #  print(f\" {feature}: {coef:.4f}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.519613Z",
     "iopub.status.busy": "2026-01-20T11:40:19.519562Z",
     "iopub.status.idle": "2026-01-20T11:40:19.522314Z",
     "shell.execute_reply": "2026-01-20T11:40:19.522131Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # = mean_squared_error(y_train_m, y_train_pred_m)\n",
    "        # test_mse_m = mean_squared_error(y_test_m, y_test_pred_m)\n",
    "        # train_r2_m = r2_score(y_train_m, y_train_pred_m)\n",
    "        # test_r2_m = r2_score(y_test_m, y_test_pred_m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nTraining Metrics:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" MSE: {train_mse_m:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" RMSE: {np.sqrt(train_mse_m):,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" RÂ² Score: {train_r2_m:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nTest Metrics:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" MSE: {test_mse_m:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" RMSE: {np.sqrt(test_mse_m):,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" RÂ² Score: {test_r2_m:.4f}\")\n",
    "\n",
    "        # Add interpretation\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"ğŸ’¡ What Do These Outputs Mean? Why Does This Matter?\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Ù…Ø§Ø°Ø§ ØªØ¹Ù†ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§ØªØŸ Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ù…Ù‡Ù…ØŸ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ¯ Practical Value for GDI Financial Investigations:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" **Multiple Regression Outputs:**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - RÂ² = {test_r2_m:.4f}: Model explains {test_r2_m*100:.1f}% of transaction amount variance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - RMSE = ${np.sqrt(test_mse_m):,.2f}: Average prediction error is ${np.sqrt(test_mse_m):,.0f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n **Why This Algorithm is Valuable:**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - **Better Predictions**: Multiple features (Time, V1, V2, V3) capture more patterns than Time alone\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - **Fraud Detection**: Identify transactions where actual >> predicted (suspicious)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - **Feature Importance**: See which features (coefficients) most affect transaction amounts\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - **Decision Support**: Predict amounts to inform investigation priorities\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n **Real-World GDI Application:**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Predict transaction amounts based on transaction patterns (Time, anonymized features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Flag unusual transactions: Actual amount much higher than predicted = investigate\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Understand relationships: Which transaction patterns lead to larger amounts?\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Resource allocation: Predict transaction volumes to allocate investigation resources\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n **Why Linear Regression (Not Other Algorithms)?**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - âœ… Fast predictions on large datasets (important for real-time monitoring)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - âœ… Interpretable coefficients (can explain WHY features matter)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - âœ… Baseline model (compare against before trying complex models)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - âœ… Regulatory compliance (interpretable models for financial investigations)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"ğŸ“ˆ Comparing Simple vs Multiple Regression Performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" **Notice the Improvement:**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Simple Regression (Time only): RÂ² was low/negative\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Multiple Regression (Time + V1 + V2 + V3): RÂ² = {test_r2_m:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if test_r2_m > 0.1:\n",
    "        #  print(f\" - âœ… Multiple regression is MUCH better! ({test_r2_m*100:.1f}% vs ~0%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - This proves: More features = Better predictions!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - ğŸ’¡ This is WHY we use multiple features in real GDI fraud detection\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"ğŸ’¡ Interpreting the Metrics | ØªÙØ³ÙŠØ± Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nğŸ“Š RÂ² Score (R-squared):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Training: {train_r2_m:.2%} | Test: {test_r2_m:.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Range: -âˆ to 1.0 (higher is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - 1.0 = Perfect predictions (all variance explained)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - 0.0 = Model is as good as predicting the mean\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - <0.0 = Model is worse than just predicting the mean\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if test_r2_m >= 0.9:\n",
    "        #  print(f\" - âœ… EXCELLENT! (>0.9 means model explains >90% of variance)\")\n",
    "        # elif test_r2_m >= 0.7:\n",
    "        #  print(f\" - âœ… GOOD! (>0.7 means model explains >70% of variance)\")\n",
    "        # elif test_r2_m >= 0.5:\n",
    "        #  print(f\" - âš ï¸ FAIR (>0.5 means model explains >50% of variance)\")\n",
    "        # else:\n",
    "        #  print(f\" - âš ï¸ POOR (<0.5 means model explains <50% of variance)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nğŸ“Š RMSE (Root Mean Squared Error):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Training: ${np.sqrt(train_mse_m):,.2f} | Test: ${np.sqrt(test_mse_m):,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Lower is better (measures average prediction error)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - In same units as target (Amount in dollars)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Average prediction error: ${np.sqrt(test_mse_m):,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - This means predictions are typically off by ${np.sqrt(test_mse_m):,.0f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nğŸ” Comparing Train vs Test:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if abs(train_r2_m - test_r2_m) < 0.05:\n",
    "        #  print(f\" - âœ… Similar RÂ² ({train_r2_m:.2%} vs {test_r2_m:.2%}) - Good generalization!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - Model performs similarly on new data (not overfitting)\")\n",
    "        # else:\n",
    "        #  print(f\" - âš ï¸ Different RÂ² ({train_r2_m:.2%} vs {test_r2_m:.2%})\")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        # if train_r2_m > test_r2_m:\n",
    "        #  print(f\" - Training RÂ² is higher - possible overfitting!\")\n",
    "        #  else:\n",
    "        #  print(f\" - Test RÂ² is higher - unusual but possible\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - RÂ² shows how much variance the model explains\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - RMSE shows actual prediction error in dollars\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Compare train vs test to check for overfitting\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Good models have high RÂ² and low RMSE\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - RÂ² > 0.7 is generally considered good for real-world problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Multiple Linear Regression | ØªØµÙˆØ± Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯\n",
    "\n",
    "**BEFORE**: We've built a multiple regression model, but how do we visualize it with multiple features?\n",
    "\n",
    "**AFTER**: We'll plot predicted vs actual transaction amounts to see how well the model performs!\n",
    "\n",
    "**What you'll see in the plots:**\n",
    "- **Points (scatter)**: Each point = one transaction\n",
    "  - X-axis: Actual Amount (what the transaction actually was)\n",
    "  - Y-axis: Predicted Amount (what the model predicted)\n",
    "  - If points are close to the diagonal line = good predictions!\n",
    "  - If points are far from the diagonal line = poor predictions\n",
    "- **Red Dashed Line**: Perfect prediction line (y = x)\n",
    "  - If predictions were perfect, all points would be on this line\n",
    "  - Points above line = model over-predicted (predicted higher than actual)\n",
    "  - Points below line = model under-predicted (predicted lower than actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.523121Z",
     "iopub.status.busy": "2026-01-20T11:40:19.523068Z",
     "iopub.status.idle": "2026-01-20T11:40:19.524615Z",
     "shell.execute_reply": "2026-01-20T11:40:19.524428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # 5))\n",
    "\n",
    "# # axes[0].plot([y_train_m.min(), y_train_m.max()], [y_train_m.min(), y_train_m.max()], 'r--', linewidth=2)\n",
    "# # axes[0].set_xlabel('Actual Amount (dollars)')\n",
    "# # axes[0].set_ylabel('Predicted Amount (dollars)')\n",
    "# # axes[0].set_title(f'Training Set (RÂ² = {train_r2_m:.4f})')\n",
    "# # axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# # axes[1].plot([y_test_m.min(), y_test_m.max()], [y_test_m.min(), y_test_m.max()], 'r--', linewidth=2)\n",
    "# # axes[1].set_xlabel('Actual Amount (dollars)')\n",
    "# # axes[1].set_ylabel('Predicted Amount (dollars)')\n",
    "# # axes[1].set_title(f'Test Set (RÂ² = {test_r2_m:.4f})')\n",
    "# # axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# # plt.tight_layout()\n",
    "# # plt.savefig('multiple_linear_regression.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nâœ“ Plot saved as 'multiple_linear_regression.png'\")\n",
    "# plt.show()\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Residuals | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù„Ø«: ÙÙ‡Ù… Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ\n",
    "\n",
    "**Now that we've built and evaluated our models, let's learn about residuals - a powerful diagnostic tool!**\n",
    "\n",
    "### What Are Residuals? | Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠØŸ\n",
    "\n",
    "**BEFORE**: We've evaluated our model with metrics (MSE, MAE, RÂ²), but there's another important way to check model quality!\n",
    "\n",
    "**AFTER**: We'll learn what residuals are and why they're important for diagnosing model problems!\n",
    "\n",
    "**What are Residuals? (Simple Explanation)**\n",
    "\n",
    "**Residuals = Prediction Errors = The Mistake the Model Made**\n",
    "\n",
    "Think of it like this:\n",
    "- You take a **test** (actual exam)\n",
    "- You **predict** what score you'll get\n",
    "- The **residual** = How wrong your prediction was!\n",
    "\n",
    "**In Machine Learning:**\n",
    "- **Actual Value** = What really happened (real transaction amount)\n",
    "- **Predicted Value** = What the model guessed (predicted transaction amount)\n",
    "- **Residual** = The difference (how wrong the model was)\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Residual = Actual Value - Predicted Value\n",
    "         = y_actual - y_predicted\n",
    "```\n",
    "\n",
    "**Real-World Example (GDI Financial Investigation Context):**\n",
    "\n",
    "Imagine predicting transaction amounts:\n",
    "- **Transaction 1**: \n",
    "  - Actual amount: $30,000\n",
    "  - Model predicted: $28,000\n",
    "  - **Residual**: $30,000 - $28,000 = **+$2,000**\n",
    "  - Meaning: Model **under-predicted** by $2,000 (predicted too low)\n",
    "\n",
    "- **Transaction 2**:\n",
    "  - Actual amount: $25,000\n",
    "  - Model predicted: $27,000\n",
    "  - **Residual**: $25,000 - $27,000 = **-$2,000**\n",
    "  - Meaning: Model **over-predicted** by $2,000 (predicted too high)\n",
    "\n",
    "- **Transaction 3**:\n",
    "  - Actual amount: $20,000\n",
    "  - Model predicted: $20,000\n",
    "  - **Residual**: $20,000 - $20,000 = **$0**\n",
    "  - Meaning: **Perfect prediction!** No error!\n",
    "\n",
    "**Key Points:**\n",
    "- **Positive residual** = Model predicted **too low** (actual > predicted)\n",
    "- **Negative residual** = Model predicted **too high** (actual < predicted)\n",
    "- **Residual = 0** = **Perfect prediction!**\n",
    "- **Small residual** = Good prediction âœ…\n",
    "- **Large residual** = Bad prediction âŒ\n",
    "\n",
    "**Why Check Residuals? (Why This Matters)**\n",
    "\n",
    "1. **See Individual Mistakes**: \n",
    "   - MSE/MAE/RÂ² give us **average** performance\n",
    "   - Residuals show us **each individual** prediction error\n",
    "   - Example: \"Transaction 1 was off by $2k, Transaction 2 was perfect, Transaction 3 was off by $0.5k\"\n",
    "\n",
    "2. **Find Patterns**:\n",
    "   - If residuals are **random** = Good model âœ…\n",
    "   - If residuals show **patterns** (curves, trends) = Model has problems âŒ\n",
    "   - Patterns tell us **what's wrong** with the model\n",
    "\n",
    "3. **Diagnose Problems**:\n",
    "   - **Curved pattern** = Model can't capture non-linear relationships\n",
    "   - **Funnel shape** = Model errors get bigger for certain values\n",
    "   - **Outliers** = Some predictions are way off (need to check data)\n",
    "\n",
    "**What's the Difference Between Residuals and Other Metrics?**\n",
    "\n",
    "| Aspect | Residuals | MSE/MAE/RÂ² |\n",
    "|--------|-----------|------------|\n",
    "| **What they show** | **Individual errors** for each prediction | **Average/summary** of all errors |\n",
    "| **Example** | Transaction 1: -$0.5k, Transaction 2: +$2k, Transaction 3: $0 | Average error: $1k |\n",
    "| **Use for** | **Diagnosing problems** (find patterns) | **Overall performance** (how good is model?) |\n",
    "| **Shows patterns?** | âœ… YES - can see curves, funnels, trends | âŒ NO - just one number |\n",
    "| **Shows outliers?** | âœ… YES - can see which predictions are way off | âŒ NO - outliers averaged out |\n",
    "| **Interpretation** | \"This transaction was off by $2k\" | \"Average error is $1k\" |\n",
    "\n",
    "**Real Example (GDI Financial Investigation Context):**\n",
    "\n",
    "Imagine you predicted transaction amounts for 3 transactions:\n",
    "- **Transaction 1**: Actual $30k, Predicted $29.5k â†’ Residual = +$0.5k\n",
    "- **Transaction 2**: Actual $25k, Predicted $27k â†’ Residual = -$2k  \n",
    "- **Transaction 3**: Actual $20k, Predicted $20k â†’ Residual = $0\n",
    "\n",
    "**Metrics tell you:**\n",
    "- **MSE**: Average squared error = (0.5Â² + 2Â² + 0Â²)/3 = 1.42\n",
    "- **MAE**: Average absolute error = (0.5 + 2 + 0)/3 = $0.83k\n",
    "- **RÂ²**: How well model fits = 0.85 (85% variance explained)\n",
    "\n",
    "**Residuals tell you:**\n",
    "- Transaction 1: Small error ($0.5k) âœ…\n",
    "- Transaction 2: Larger error ($2k) âŒ - **Why?** Need to investigate! (Could indicate suspicious pattern)\n",
    "- Transaction 3: Perfect ($0) âœ…\n",
    "\n",
    "**Key Difference:**\n",
    "- **Metrics (MSE/MAE/RÂ²)**: Give you **one number** summarizing all predictions\n",
    "- **Residuals**: Give you **individual errors** for each prediction\n",
    "\n",
    "**Why Both Matter:**\n",
    "- **Use Metrics**: To answer \"Is my model good overall?\" (RÂ² = 0.85 â†’ good!)\n",
    "- **Use Residuals**: To answer \"Which predictions are wrong and why?\" (Transaction 2 is way off â†’ check data!)\n",
    "- **Together**: Metrics show overall quality, residuals show where problems are!\n",
    "\n",
    "**Additional Notes:**\n",
    "\n",
    "**What does \"residual\" mean in simple words?**\n",
    "- **\"Residual\" = leftover error** = what the model got wrong\n",
    "- Like: You predict you'll get 90% on a test, but you actually got 85%\n",
    "- Residual = 85% - 90% = -5% (you over-predicted by 5%)\n",
    "- It's the \"leftover\" mistake after the model makes its prediction\n",
    "\n",
    "**Why check residuals if we already have MSE/MAE/RÂ²?**\n",
    "- Metrics give us **numbers** (average performance)\n",
    "- Residuals show us **patterns** (what's wrong)\n",
    "- Example: MSE might be low, but residuals show a curve â†’ model has problems\n",
    "- Residuals help diagnose **WHAT'S wrong**, not just **HOW wrong**\n",
    "\n",
    "**Should residuals be positive or negative?**\n",
    "- **Both!** Good models have residuals randomly scattered around 0\n",
    "- Some predictions too high (negative), some too low (positive)\n",
    "- Mean of residuals should be close to 0 (no systematic bias)\n",
    "- If all residuals are positive â†’ model always predicts too low\n",
    "- If all residuals are negative â†’ model always predicts too high\n",
    "\n",
    "**What's the difference between residuals and errors?**\n",
    "- They're **the same thing!** \n",
    "- \"Residual\" = technical term used in statistics\n",
    "- \"Error\" = more general term\n",
    "- Both mean: difference between actual and predicted\n",
    "\n",
    "**How do I know if residuals are good or bad?**\n",
    "- Good residuals:\n",
    "  - **Mean close to 0** (no bias)\n",
    "  - **Small spread** (consistent predictions)\n",
    "  - **Random pattern** (no curves or trends)\n",
    "- Bad residuals:\n",
    "  - **Mean far from 0** (systematic bias)\n",
    "  - **Large spread** (inconsistent predictions)\n",
    "  - **Patterns** (curves, funnels, trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Residuals | Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ\n",
    "\n",
    "**Now that we understand what residuals are, let's calculate them for our multiple regression model!**\n",
    "\n",
    "**BEFORE**: We know what residuals are, now let's calculate them!\n",
    "\n",
    "**AFTER**: We'll compute residuals and check their statistics to see if our model is good!\n",
    "\n",
    "**What we'll calculate:**\n",
    "- **Residuals**: `residuals = y_test - y_test_pred`\n",
    "  - For each test sample: actual transaction amount - predicted transaction amount\n",
    "  - Positive = model under-predicted, Negative = model over-predicted\n",
    "- **Statistics**: Mean, standard deviation, min, max\n",
    "  - Mean close to 0 = no bias âœ…\n",
    "  - Small std = consistent predictions âœ…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Residuals | ØªØµÙˆØ± Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ\n",
    "\n",
    "**BEFORE**: We've calculated residual statistics, but we need to visualize them to see patterns!\n",
    "\n",
    "**AFTER**: We'll create plots to check if residuals are randomly distributed (good) or have patterns (indicates problems)!\n",
    "\n",
    "**Why visualize residuals?**\n",
    "- **Check assumptions**: Linear regression assumes residuals are random and normally distributed\n",
    "- **Detect patterns**: Patterns in residuals indicate model problems (non-linearity, heteroscedasticity)\n",
    "- **Diagnose issues**: Visual inspection helps identify what's wrong with the model\n",
    "- **Validate model**: Good models have randomly scattered residuals around zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.525520Z",
     "iopub.status.busy": "2026-01-20T11:40:19.525446Z",
     "iopub.status.idle": "2026-01-20T11:40:19.526980Z",
     "shell.execute_reply": "2026-01-20T11:40:19.526791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate residuals (prediction errors)\n",
    "# = Actual values - Predicted values\n",
    "# = y_test_m - y_test_pred_m\n",
    "# print(f\"âœ… Residuals calculated: {len(residuals)} residuals\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Mean residual: ${residuals.mean():.2f} (should be close to 0)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Std residual: ${residuals.std():.2f}\")\n",
    "\n",
    "# 5))\n",
    "\n",
    "# axes[0].axvline(0, color='r', linestyle='--', linewidth=2)\n",
    "# axes[0].set_xlabel('Residuals')\n",
    "# axes[0].set_ylabel('Frequency')\n",
    "# axes[0].set_title('Residuals Distribution')\n",
    "# axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# axes[1].axhline(0, color='r', linestyle='--', linewidth=2)\n",
    "# axes[1].set_xlabel('Predicted Values')\n",
    "# axes[1].set_ylabel('Residuals')\n",
    "# axes[1].set_title('Residuals vs Predicted Values')\n",
    "# axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('residuals_analysis.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nâœ“ Plot saved as 'residuals_analysis.png'\")\n",
    "if 'plt' in globals():\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Example 4 Complete! âœ“\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 4! âœ“\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Fixing Prediction Bias | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø¹: Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ­ÙŠØ² ÙÙŠ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª\n",
    "\n",
    "**Sometimes models have systematic bias - let's learn how to identify and address it!**\n",
    "\n",
    "### What to Do When Predictions Are Too Low (or Too High) | Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ù‹Ø§ (Ø£Ùˆ Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ù‹Ø§)\n",
    "\n",
    "**BEFORE**: You've analyzed residuals and found that your model systematically under-predicts (or over-predicts). Now what?\n",
    "\n",
    "**AFTER**: You'll learn practical solutions to fix systematic bias in your predictions!\n",
    "\n",
    "**The Problem We Found:**\n",
    "- **Residual Mean**: 5748.12 (not close to 0)\n",
    "- **Interpretation**: Model tends to **UNDER-predict** (predictions are too low)\n",
    "- **Meaning**: On average, the model predicts prices that are $5,748 lower than actual prices\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Systematic bias** = Model consistently makes the same type of error\n",
    "- **Good models** should have residual mean close to 0 (no bias)\n",
    "- **Biased models** = Poor predictions, even if RÂ² is high\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Diagnose the Problem | ØªØ´Ø®ÙŠØµ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©\n",
    "\n",
    "**Check Your Residuals:**\n",
    "\n",
    "1. **Residual Mean**:\n",
    "   - **Close to 0** (Â±small value) â†’ âœ… No bias, model is good\n",
    "   - **Positive mean** â†’ âš ï¸ Model UNDER-predicts (predictions too low)\n",
    "   - **Negative mean** â†’ âš ï¸ Model OVER-predicts (predictions too high)\n",
    "\n",
    "2. **Residual Patterns**:\n",
    "   - **Random scatter** â†’ âœ… Good (no patterns)\n",
    "   - **Curved pattern** â†’ âš ï¸ Non-linear relationship (need polynomial regression)\n",
    "   - **Funnel shape** â†’ âš ï¸ Heteroscedasticity (variance changes with predictions)\n",
    "\n",
    "3. **Residual Distribution**:\n",
    "   - **Normal distribution** â†’ âœ… Good (assumption met)\n",
    "   - **Skewed distribution** â†’ âš ï¸ Model bias or outliers\n",
    "\n",
    "**In Our Case:**\n",
    "- Mean = 5748.12 (positive) â†’ Model UNDER-predicts\n",
    "- Need to investigate WHY and fix it\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ› ï¸ Solutions to Fix Systematic Bias | Ø­Ù„ÙˆÙ„ Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ­ÙŠØ² Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠ\n",
    "\n",
    "#### Solution 1: Check for Missing Features | Ø§Ù„Ø­Ù„ 1: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
    "\n",
    "**Problem**: Model might be missing important features that affect the target.\n",
    "\n",
    "**Example:**\n",
    "- Predicting house price from size only\n",
    "- But price also depends on location, age, condition\n",
    "- Missing features â†’ Model can't capture full relationship â†’ Under-predicts\n",
    "\n",
    "**What to Do:**\n",
    "1. **Review your data**: Are there other features that affect the target?\n",
    "2. **Add relevant features**: Include features that logically affect predictions\n",
    "3. **Check feature importance**: Use domain knowledge to identify missing factors\n",
    "\n",
    "---\n",
    "\n",
    "**Code Example (see cell below for executable version):**\n",
    "- Before: Only using Time\n",
    "- After: Using multiple relevant features (Time, V1, V2, V3, V4, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "#### Solution 2: Check for Data Quality Issues | Ø§Ù„Ø­Ù„ 2: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø´Ø§ÙƒÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**Problem**: Data issues can cause bias:\n",
    "- **Outliers**: Extreme values skew the model\n",
    "- **Missing values**: Incorrectly handled missing data\n",
    "- **Data leakage**: Using future information\n",
    "- **Wrong target values**: Incorrect labels in training data\n",
    "\n",
    "**What to Do:**\n",
    "1. **Check for outliers**: Plot data, look for extreme values\n",
    "2. **Handle missing values**: Impute or remove missing data properly\n",
    "3. **Verify target values**: Make sure y values are correct\n",
    "4. **Check data distribution**: Ensure training and test data are similar\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Check for outliers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(y_train)\n",
    "plt.title('Check for Outliers in Target')\n",
    "plt.show()\n",
    "\n",
    "# Remove outliers (if appropriate)\n",
    "Q1 = y_train.quantile(0.25)\n",
    "Q3 = y_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = (y_train < (Q1 - 1.5 * IQR)) | (y_train > (Q3 + 1.5 * IQR))\n",
    "X_train_clean = X_train[~outliers]\n",
    "y_train_clean = y_train[~outliers]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Solution 3: Try Feature Engineering | Ø§Ù„Ø­Ù„ 3: ØªØ¬Ø±Ø¨Ø© Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
    "\n",
    "**Problem**: Raw features might not capture relationships well.\n",
    "\n",
    "**What to Do:**\n",
    "1. **Create interaction features**: Multiply features together (e.g., size Ã— bedrooms)\n",
    "2. **Transform features**: Log, square, or other transformations\n",
    "3. **Create polynomial features**: Add squared or cubed terms\n",
    "4. **Normalize/scale features**: Ensure all features are on similar scales\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Create interaction features\n",
    "df['size_bedrooms'] = df['size'] * df['bedrooms']\n",
    "df['size_location'] = df['size'] * df['location_score']\n",
    "\n",
    "# Transform features (if needed)\n",
    "df['log_size'] = np.log(df['size'])\n",
    "\n",
    "# Use new features\n",
    "X = df[['size', 'bedrooms', 'size_bedrooms', 'size_location']]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Solution 4: Try Different Models | Ø§Ù„Ø­Ù„ 4: ØªØ¬Ø±Ø¨Ø© Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØªÙ„ÙØ©\n",
    "\n",
    "**Problem**: Linear regression might not be appropriate for your data.\n",
    "\n",
    "**What to Do:**\n",
    "1. **Polynomial Regression**: If relationship is curved (non-linear)\n",
    "2. **Ridge/Lasso Regression**: If you have many features (regularization)\n",
    "3. **Random Forest**: If relationship is complex and non-linear\n",
    "4. **XGBoost**: For best performance on complex patterns\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Try Polynomial Regression (for non-linear relationships)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Solution 5: Adjust the Intercept (Last Resort) | Ø§Ù„Ø­Ù„ 5: ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªÙ‚Ø§Ø·Ø¹ (Ø­Ù„ Ø£Ø®ÙŠØ±)\n",
    "\n",
    "**Problem**: Model intercept might be systematically wrong.\n",
    "\n",
    "**What to Do:**\n",
    "1. **Calculate mean residual**: This is the systematic bias\n",
    "2. **Adjust predictions**: Add mean residual to all predictions\n",
    "3. **Note**: This is a \"band-aid\" solution - better to fix the root cause!\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Calculate mean residual (systematic bias)\n",
    "mean_residual = residuals.mean()\n",
    "print(f\"Systematic bias: {mean_residual:.2f}\")\n",
    "\n",
    "# Adjust predictions\n",
    "y_test_pred_adjusted = y_test_pred + mean_residual\n",
    "\n",
    "# Recalculate metrics\n",
    "mse_adjusted = mean_squared_error(y_test, y_test_pred_adjusted)\n",
    "print(f\"Original MSE: {test_mse:.2f}\")\n",
    "print(f\"Adjusted MSE: {mse_adjusted:.2f}\")\n",
    "```\n",
    "\n",
    "**âš ï¸ Warning**: This doesn't fix the model - it just shifts predictions. Better to fix the root cause!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Decision Tree: Which Solution to Try? | Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±: Ø£ÙŠ Ø­Ù„ ØªØ¬Ø±Ø¨ØŸ\n",
    "\n",
    "**Start Here:**\n",
    "\n",
    "1. **Check residual mean**:\n",
    "   - If close to 0 â†’ âœ… Model is good, no action needed\n",
    "   - If far from 0 â†’ Continue to step 2\n",
    "\n",
    "2. **Check residual patterns**:\n",
    "   - **Curved pattern** â†’ Try Solution 4 (Polynomial Regression)\n",
    "   - **Random scatter** â†’ Continue to step 3\n",
    "\n",
    "3. **Check your features**:\n",
    "   - **Few features (<5)** â†’ Try Solution 1 (Add more features)\n",
    "   - **Many features (>10)** â†’ Try Solution 3 (Feature engineering)\n",
    "\n",
    "4. **Check data quality**:\n",
    "   - **Outliers present** â†’ Try Solution 2 (Fix data quality)\n",
    "   - **No outliers** â†’ Continue to step 5\n",
    "\n",
    "5. **Try different models**:\n",
    "   - **Linear relationship** â†’ Current model should work (check features)\n",
    "   - **Non-linear relationship** â†’ Try Solution 4 (Different models)\n",
    "\n",
    "6. **Last resort**:\n",
    "   - If nothing works â†’ Try Solution 5 (Adjust intercept)\n",
    "   - But remember: This is a band-aid, not a real fix!\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Residual mean â‰  0** = Systematic bias (model consistently wrong)\n",
    "2. **Positive mean** = Under-prediction (predictions too low)\n",
    "3. **Negative mean** = Over-prediction (predictions too high)\n",
    "4. **Fix root cause** = Better than adjusting predictions\n",
    "5. **Check features first** = Most common cause of bias\n",
    "6. **Try different models** = If linear regression isn't appropriate\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Next Steps | Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©\n",
    "\n",
    "**After fixing bias:**\n",
    "1. **Re-train model** with improved features/data\n",
    "2. **Re-evaluate** residuals (should be close to 0 now)\n",
    "3. **Check metrics** (MSE, MAE, RÂ² should improve)\n",
    "4. **Validate** on new data to ensure fix works\n",
    "\n",
    "**If bias persists:**\n",
    "- Consider that linear regression might not be appropriate\n",
    "- Try polynomial regression or other non-linear models\n",
    "- Consult domain experts about missing features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.527807Z",
     "iopub.status.busy": "2026-01-20T11:40:19.527746Z",
     "iopub.status.idle": "2026-01-20T11:40:19.529285Z",
     "shell.execute_reply": "2026-01-20T11:40:19.529105Z"
    }
   },
   "outputs": [],
   "source": [
    "# Solution 1 Example: Adding More Features\n",
    "# This demonstrates how to add more features to improve predictions\n",
    "# print('=\" * 60)\")\n",
    "\n",
    "\n",
    "# print(\"Solution 1: Adding More Features to Fix Bias\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# Before: Only using Time (simple regression - we saw low RÂ²)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“Š Before (Simple Regression - Time only):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" Features: Time only\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" Result: Low RÂ² (as we saw in Part 1)\")\n",
    "\n",
    "# After: Using multiple relevant features (multiple regression - better RÂ²)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nâœ… After (Multiple Regression - Time + V1 + V2 + V3):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Features: Time, V1, V2, V3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Result: Better RÂ² = {test_r2_m:.4f} (as we saw in Part 2)\")\n",
    "\n",
    "# Show how to add even more features\n",
    "# print(\"\\nğŸ’¡ To add even more features\")\n",
    "# from the dataset:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"# Original features: Time, V1, V2, V3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"# Add more V features (V4, V5, V6, etc.)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" X_extended = df_multiple[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"# = Better predictions (but watch for overfitting!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.530117Z",
     "iopub.status.busy": "2026-01-20T11:40:19.530061Z",
     "iopub.status.idle": "2026-01-20T11:40:19.691634Z",
     "shell.execute_reply": "2026-01-20T11:40:19.691418Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7n/l2c2z2x57871xg4f_0drsv1m0000gn/T/ipykernel_4771/302541937.py:86: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solution 2 Example: Checking \n",
    "# for Outliers in Target Variable\n",
    "# This demonstrates how to check \n",
    "# for and handle outliers that might cause bias\n",
    "# print('=\" * 60)\")\n",
    "\n",
    "\n",
    "# print(\"Solution 2: Checking for Data Quality Issues (Outliers)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check \n",
    "# for outliers in target variable (Amount)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“Š Checking for outliers in target variable (Amount):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Mean Amount: ${y_multiple.mean():.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Median Amount: ${y_multiple.median():.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Std Amount: ${y_multiple.std():.2f}\")\n",
    "\n",
    "# Calculate IQR \n",
    "# for outliersQ1 = y_multiple.quantile(0.25)\n",
    "# Q3 = y_multiple.quantile(0.75)\n",
    "# Q1 (25th percentile): ${Q1:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Q3 (75th percentile): ${Q3:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" IQR: ${IQR:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Lower bound (Q1 - 1.5*IQR): ${lower_bound:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Upper bound (Q3 + 1.5*IQR): ${upper_bound:.2f}\")\n",
    "\n",
    "# = (y_multiple < lower_bound) | (y_multiple > upper_bound)\n",
    "# num_outliers = outliers.sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\n âš ï¸ Number of outliers: {num_outliers} ({num_outliers/len(y_multiple)*100:.1f}%)\")\n",
    "\n",
    "# 4))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.boxplot(y_multiple)\n",
    "# plt.title('Boxplot: Check for Outliers in Amount')\n",
    "# plt.ylabel('Amount (dollars)')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.hist(y_multiple, bins=50, edgecolor='black')\n",
    "# plt.axvline(upper_bound, color='r', linestyle='--', label=f'Upper bound: ${upper_bound:.0f}')\n",
    "# plt.title('Histogram: Distribution of Amount')\n",
    "# plt.xlabel('Amount (dollars)')\n",
    "# plt.ylabel('Frequency')\n",
    "if 'plt' in globals():\n",
    "    plt.legend()\n",
    "# plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ’¡ Note: In this dataset, outliers might be legitimate high-value transactions.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" Only remove outliers\")\n",
    "\n",
    "\n",
    "\n",
    "# if they're data errors, not \n",
    "\n",
    "\n",
    "\n",
    "# if they're real values!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.692715Z",
     "iopub.status.busy": "2026-01-20T11:40:19.692629Z",
     "iopub.status.idle": "2026-01-20T11:40:19.694552Z",
     "shell.execute_reply": "2026-01-20T11:40:19.694383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Solution 3 Example: Feature Engineering\n",
    "# This demonstrates how to create interaction features and transform features\n",
    "# print('=\" * 60)\")\n",
    "\n",
    "\n",
    "# print(\"Solution 3: Feature Engineering (Creating New Features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# Create interaction features (multiply features together)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“Š Creating interaction features:\")\n",
    "\n",
    "# Example: Time Ã— V1 interaction (might capture non-linear relationships)\n",
    "# df_multiple_demo = df_multiple.copy()\n",
    "# df_multiple_demo['Time_V1'] = df_multiple_demo['Time'] * df_multiple_demo['V1']\n",
    "# df_multiple_demo['Time_V2'] = df_multiple_demo['Time'] * df_multiple_demo['V2']\n",
    "\n",
    "# print(\" âœ… Created: Time Ã— V1 interaction\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" âœ… Created: Time Ã— V2 interaction\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n ğŸ’¡ Interaction features can capture relationships like:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - 'High Time AND high V1 â†’ Higher Amount'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Non-linear relationships that simple features can't capture\")\n",
    "\n",
    "# Transform features (e.g., log transformation \n",
    "# for skewed data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“Š Transforming features:\")\n",
    "\n",
    "# Example: Log transform Amount (only \n",
    "# for demonstration - we predict Amount, so we don't transform it)\n",
    "# But we could transform Time \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if it's skewed\n",
    "# import numpy as np\n",
    "\n",
    "# Check \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if Time is skewed\n",
    "# time_skew = df_multiple_demo['Time'].skew()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Time skewness: {time_skew:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if abs(time_skew) > 1:\n",
    "#  print(\" âš ï¸ Time is skewed - log transform might help\")\n",
    "#  df_multiple_demo['log_Time'] = np.log1p(df_multiple_demo['Time']) # log1p handles zeros\n",
    "# print(\" âœ… Created: log(Time + 1)\")\n",
    "# else:\n",
    "#  print(\" âœ… Time is not heavily skewed - transformation not necessary\")\n",
    "\n",
    "# Show the new features\n",
    "# print(f\"\\nğŸ“‹ Original features: Time, V1, V2, V3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"ğŸ“‹ New engineered features: Time_V1, Time_V2\")\n",
    "\n",
    "# Note: To use these features, you would:\n",
    "# print(\"\\nğŸ’¡ To use engineered features:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" X_engineered = df_multiple_demo[['Time', 'V1', 'V2', 'V3', 'Time_V1', 'Time_V2']]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Train model with X_engineered instead of X_multiple\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"This often improves predictions by capturing interactions!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.695344Z",
     "iopub.status.busy": "2026-01-20T11:40:19.695291Z",
     "iopub.status.idle": "2026-01-20T11:40:19.697015Z",
     "shell.execute_reply": "2026-01-20T11:40:19.696844Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Solution 4 Example: Trying D\n",
    "        # ifferent Models (Polynomial Regression)\n",
    "        # This demonstrates polynomial regression \n",
    "        # for non-linear relationships\n",
    "        # Note: This will be covered in detail in the next notebook!\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Solution 4: Trying Different Models (Polynomial Regression)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ“Š Current Model: Linear Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Assumes linear relationships\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Works well when relationship is straight line\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ”„ Alternative: Polynomial Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Handles curved (non-linear) relationships\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Creates polynomial features (x, xÂ², xÂ³, etc.)\")\n",
    "\n",
    "        # Quick demonstration of polynomial features\n",
    "        # from sklearn.preprocessing import PolynomialFeatures\n",
    "        # from sklearn.linear_model import LinearRegression\n",
    "        # print(\"\\nğŸ’¡ Example: Creating polynomial features (degree=2):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" Original features: [Time, V1, V2, V3]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" Polynomial features (degree=2):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Original: Time, V1, V2, V3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Squared: TimeÂ², V1Â², V2Â², V3Â²\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Interactions: TimeÃ—V1, TimeÃ—V2, V1Ã—V2, etc.\")\n",
    "\n",
    "        # Create polynomial features (quick demo with small sample)\n",
    "        # X_sample_poly = X_train_m[:100] \n",
    "        # Small sample \n",
    "        # for demonstrationpol\n",
    "        # y = PolynomialFeatures(degree=2, include_bias=False)\n",
    "        # X_poly_demo = poly.fit_transform(X_sample_poly)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n Original features shape: {X_sample_poly.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Polynomial features shape: {X_poly_demo.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" âœ… Polynomial regression creates {X_poly_demo.shape[1]} features from {X_sample_poly.shape[1]} original features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ“š Note: Polynomial Regression will be covered in detail in:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" ğŸ““ Notebook: 05_polynomial_regression.ipynb\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" ğŸ’¡ It's useful when linear regression can't capture curved relationships\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.697846Z",
     "iopub.status.busy": "2026-01-20T11:40:19.697790Z",
     "iopub.status.idle": "2026-01-20T11:40:19.699592Z",
     "shell.execute_reply": "2026-01-20T11:40:19.699438Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Solution 5 Example: Adjusting the Intercept (Last Resort)\n",
    "        # This demonstrates how to adjust predictions to correct systematic bias\n",
    "        # WARNING: This is a band-aid solution - better to fix root cause!\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Solution 5: Adjusting the Intercept (Last Resort - Band-aid Solution)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nâš ï¸ WARNING: This doesn't fix the model - it just shifts predictions!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" Better to fix the root cause (missing features, data quality, etc.)\")\n",
    "\n",
    "        # We already calculated mean_residual in Cell 36, but let's recalculate \n",
    "        # for clarity\n",
    "        # from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "        # Calculate mean residual (systematic bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ“Š Step 1: Calculate mean residual (systematic bias)\")\n",
    "        # mean_residual = residuals.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Systematic bias (mean residual): {mean_residual:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if abs(mean_residual) > 1:\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        # if mean_residual > 0:\n",
    "        #  print(f\" âš ï¸ Model UNDER-predicts (predictions are too low by ${mean_residual:.2f} on average)\")\n",
    "        #  else:\n",
    "        #  print(f\" âš ï¸ Model OVER-predicts (predictions are too high by ${abs(mean_residual):.2f} on average)\")\n",
    "        # else:\n",
    "        #  print(f\" âœ… No significant bias (mean close to 0)\")\n",
    "\n",
    "        # Adjust predictions (band-aid solution)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ“Š Step 2: Adjust predictions by adding mean residual\")\n",
    "        # y_test_pred_adjusted = y_test_pred_m + mean_residual\n",
    "        # print(f\" Original predictions: y_test_pred_m\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Adjusted predictions: y_test_pred_m + {mean_residual:.2f}\")\n",
    "\n",
    "        # Recalculate metrics\n",
    "        # print(\"\\nğŸ“Š Step 3: Recalculate metrics\")\n",
    "        # mse_adjusted = mean_squared_error(y_test_m, y_test_pred_adjusted)\n",
    "        # mae_adjusted = mean_absolute_error(y_test_m, y_test_pred_adjusted)\n",
    "        # r2_adjusted = r2_score(y_test_m, y_test_pred_adjusted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n Original Metrics:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" MSE: {test_mse_m:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" MAE: {test_mae_m:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" RÂ²: {test_r2_m:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n Adjusted Metrics (after adding bias correction):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" MSE: {mse_adjusted:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" MAE: {mae_adjusted:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" RÂ²: {r2_adjusted:.4f}\")\n",
    "\n",
    "        # Check \n",
    "\n",
    "\n",
    "\n",
    "        # if adjustment helped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if mse_adjusted < test_mse_m:\n",
    "        #  improvement = test_mse_m - mse_adjusted\n",
    "        # print(f\"\\n âœ… MSE improved by {improvement:,.2f} (small improvement)\")\n",
    "        # else:\n",
    "        #  print(f\"\\n âš ï¸ MSE didn't improve (this is expected - bias correction doesn't always help)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"âš ï¸ REMEMBER: This is a BAND-AID, not a real fix!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" Always try Solutions 1-4 first (fix root cause)!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T11:40:19.700384Z",
     "iopub.status.busy": "2026-01-20T11:40:19.700257Z",
     "iopub.status.idle": "2026-01-20T11:40:19.702557Z",
     "shell.execute_reply": "2026-01-20T11:40:19.702404Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Practical Example: Diagnosing and Addressing Systematic Bias\n",
    "        # This demonstrates how to check \n",
    "        # for bias and try solutions\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"5. Fixing Systematic Bias in Predictions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ­ÙŠØ² Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠ ÙÙŠ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "        # Diagnose the bias\n",
    "        # print(\"\\nğŸ“Š Diagnosing the Bias\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"ØªØ´Ø®ÙŠØµ Ø§Ù„ØªØ­ÙŠØ²\")\n",
    "        # mean_residual = residuals.mean()\n",
    "        # std_residual = residuals.std()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n Residual Mean: {mean_residual:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Residual Std: {std_residual:.2f}\")\n",
    "\n",
    "        # Determine \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if bias is significantbias_threshold = std_residual * 0.1 # 10% of std deviation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if abs(mean_residual) > bias_threshold:\n",
    "        #  print(f\"\\n âš ï¸ SIGNIFICANT BIAS DETECTED!\")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        # if mean_residual > 0:\n",
    "        #  print(f\" - Model UNDER-predicts by ${mean_residual:,.2f} on average\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - Predictions are consistently too LOW\")\n",
    "        #  else:\n",
    "        #  print(f\" - Model OVER-predicts by ${abs(mean_residual):,.2f} on average\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - Predictions are consistently too HIGH\")\n",
    "        # else:\n",
    "        #  print(f\"\\n âœ… No significant bias (mean close to 0)\")\n",
    "\n",
    "        # Try Solution 5 - Adjust predictions (demonstration only)\n",
    "        # Adjusting Predictions (Band-aid Solution)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª (Ø­Ù„ Ù…Ø¤Ù‚Øª)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n âš ï¸ WARNING: This doesn't fix the model, just shifts predictions!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" Better to fix root cause (missing features, data quality, etc.)\")\n",
    "\n",
    "        # = y_test_pred_m + mean_residual\n",
    "\n",
    "        # Calculate original MAE (\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if not already calculated)\n",
    "        # test_mae_m = mean_absolute_error(y_test_m, y_test_pred_m)\n",
    "\n",
    "        # = mean_squared_error(y_test_m, y_test_pred_adjusted)\n",
    "        # mae_adjusted = mean_absolute_error(y_test_m, y_test_pred_adjusted)\n",
    "        # r2_adjusted = r2_score(y_test_m, y_test_pred_adjusted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n Original Metrics:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" MSE: {test_mse_m:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" MAE: {test_mae_m:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" RÂ²: {test_r2_m:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n Adjusted Metrics (after adding bias correction):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" MSE: {mse_adjusted:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" MAE: {mae_adjusted:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" RÂ²: {r2_adjusted:.4f}\")\n",
    "\n",
    "        # Check \n",
    "\n",
    "\n",
    "\n",
    "        # if adjustment helped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if mse_adjusted < test_mse_m:\n",
    "        #  print(f\"\\n âœ… MSE improved by {test_mse_m - mse_adjusted:,.2f}\")\n",
    "        # else:\n",
    "        #  print(f\"\\n âš ï¸ MSE didn't improve (this is expected - bias correction doesn't always help)\")\n",
    "\n",
    "        # Check residual patterns\n",
    "        # print(\"\\nğŸ“Š Checking Residual Patterns\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"ÙØ­Øµ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ\")\n",
    "\n",
    "        # = residuals_adjusted.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n Original Residual Mean: {mean_residual:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Adjusted Residual Mean: {mean_residual_adjusted:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if abs(mean_residual_adjusted) < abs(mean_residual):\n",
    "        #  print(f\" âœ… Bias reduced! (closer to 0)\")\n",
    "        # else:\n",
    "        #  print(f\" âš ï¸ Bias not fully corrected\")\n",
    "\n",
    "        # Recommendations\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"ğŸ’¡ Recommendations | Ø§Ù„ØªÙˆØµÙŠØ§Øª\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n To FIX the root cause (not just adjust predictions):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" 1. âœ… Check for missing features (most common cause)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Review your data: Are there other features that affect price?\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Add relevant features: location, condition, year_built, etc.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n 2. âœ… Check data quality\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Look for outliers in target variable\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Verify target values are correct\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Check for missing values\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n 3. âœ… Try feature engineering\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Create interaction features (size Ã— bedrooms)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Transform features (log, square)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n 4. âœ… Consider different models\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - If relationship is non-linear â†’ Try Polynomial Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - If many features â†’ Try Ridge/Lasso Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n âš ï¸ Remember: Adjusting predictions is a BAND-AID, not a real fix!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" Always try to fix the root cause first!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Decision Framework - When to Use Linear Regression | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§Ù…Ø³: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ\n",
    "\n",
    "**Congratulations! You've learned how to build linear regression models. Now let's learn when to use them!**\n",
    "\n",
    "**BEFORE**: You've learned how to build linear regression models, but when should you use them?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to determine if linear regression is the right choice for your problem!\n",
    "\n",
    "**Why this matters**: Using linear regression when it's not appropriate leads to:\n",
    "- **Poor predictions** â†’ Model can't capture non-linear patterns\n",
    "- **Wasted time** â†’ Trying to force linear relationships that don't exist\n",
    "- **Wrong conclusions** â†’ Making decisions based on inaccurate models\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework: Is Linear Regression Appropriate? | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±: Ù‡Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ù…Ù†Ø§Ø³Ø¨ØŸ\n",
    "\n",
    "**Key Question**: Should I use **LINEAR REGRESSION** or a different method?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "**Decision Point 1: What type of problem?**\n",
    "- **Classification** (predicting categories) â†’ âŒ Use Logistic Regression or other classifiers\n",
    "  - Why? Linear regression predicts continuous values, not categories\n",
    "- **Regression** (predicting numbers) â†’ Continue to Decision Point 2\n",
    "\n",
    "**Decision Point 2: What is the relationship?**\n",
    "- **Linear relationship** â†’ âœ… Use Linear Regression\n",
    "  - Why? Linear regression assumes linear relationships\n",
    "- **Non-linear relationship** â†’ âš ï¸ Use Polynomial Regression or other methods\n",
    "  - Why? Linear regression can't capture curves\n",
    "\n",
    "**Decision Point 3: Data characteristics?**\n",
    "- **Many features (>100)** â†’ âš ï¸ Use Ridge or Lasso Regression\n",
    "- **Multicollinearity present** â†’ âš ï¸ Use Ridge Regression\n",
    "- **Need feature selection** â†’ âš ï¸ Use Lasso Regression\n",
    "- **Slightly curved** â†’ Try Polynomial Regression\n",
    "- **Highly non-linear** â†’ Use Random Forest, XGBoost, Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Comparison Table: Linear Regression vs Alternatives | Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Linear Regression** | Linear relationships, continuous target, interpretable | â€¢ Simple & fast<br>â€¢ Interpretable<br>â€¢ No hyperparameters<br>â€¢ Works well with linear data | â€¢ Can't handle non-linear<br>â€¢ Assumes linearity<br>â€¢ Sensitive to outliers | House price vs size (linear) |\n",
    "| **Polynomial Regression** | Slightly curved relationships | â€¢ Handles curves<br>â€¢ Still interpretable<br>â€¢ Extends linear regression | â€¢ Can overfit<br>â€¢ More complex | House price vs size (curved) |\n",
    "| **Ridge/Lasso** | Many features, multicollinearity | â€¢ Prevents overfitting<br>â€¢ Handles many features<br>â€¢ Regularization | â€¢ More complex<br>â€¢ Hyperparameter tuning | 100+ features, correlated features |\n",
    "| **Random Forest** | Non-linear, complex patterns | â€¢ Handles non-linear<br>â€¢ Feature importance<br>â€¢ Robust | â€¢ Less interpretable<br>â€¢ More complex | Complex relationships |\n",
    "| **XGBoost** | Non-linear, best performance | â€¢ State-of-the-art<br>â€¢ Handles complex patterns | â€¢ Less interpretable<br>â€¢ Complex | Competition-level performance |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When Linear Regression IS Appropriate | Ù…ØªÙ‰ ÙŠÙƒÙˆÙ† Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§\n",
    "\n",
    "**Use Linear Regression when:**\n",
    "\n",
    "1. **Linear Relationship** âœ…\n",
    "   - Scatter plot shows a straight line pattern\n",
    "   - Example: House size vs price (larger = more expensive, linear)\n",
    "\n",
    "2. **Continuous Target Variable** âœ…\n",
    "   - Predicting numbers (price, temperature, sales)\n",
    "   - NOT categories (sick/healthy, yes/no)\n",
    "\n",
    "3. **Interpretability Important** âœ…\n",
    "   - Need to understand feature coefficients\n",
    "   - Example: \"Each bedroom adds $30,000 to price\"\n",
    "\n",
    "4. **Fast Predictions Needed** âœ…\n",
    "   - Simple model, fast training and prediction\n",
    "   - Good for real-time systems\n",
    "\n",
    "5. **Baseline Model** âœ…\n",
    "   - Start with linear regression as baseline\n",
    "   - Compare with more complex models\n",
    "\n",
    "6. **Small to Medium Datasets** âœ…\n",
    "   - Works well with limited data\n",
    "   - Doesn't require huge datasets\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ When Linear Regression IS NOT Appropriate | Ù…ØªÙ‰ Ù„Ø§ ÙŠÙƒÙˆÙ† Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§\n",
    "\n",
    "**Don't use Linear Regression when:**\n",
    "\n",
    "1. **Non-Linear Relationship** âŒ\n",
    "   - Scatter plot shows curves, exponential patterns\n",
    "   - Example: Growth patterns, decay curves\n",
    "   - **Use Instead**: Polynomial Regression, Random Forest, XGBoost\n",
    "\n",
    "2. **Classification Problem** âŒ\n",
    "   - Predicting categories (yes/no, A/B/C)\n",
    "   - **Use Instead**: Logistic Regression, Decision Trees, SVM\n",
    "\n",
    "3. **Many Features with Multicollinearity** âŒ\n",
    "   - Features are highly correlated\n",
    "   - **Use Instead**: Ridge Regression (handles multicollinearity)\n",
    "\n",
    "4. **Need Feature Selection** âŒ\n",
    "   - Want to automatically select important features\n",
    "   - **Use Instead**: Lasso Regression (automatic feature selection)\n",
    "\n",
    "5. **Complex Non-Linear Patterns** âŒ\n",
    "   - Multiple interactions, complex relationships\n",
    "   - **Use Instead**: Random Forest, XGBoost, Neural Networks\n",
    "\n",
    "6. **Outliers Present** âŒ\n",
    "   - Many extreme values that affect the line\n",
    "   - **Use Instead**: Robust regression methods, or clean outliers first\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: House Price Prediction âœ… APPROPRIATE\n",
    "- **Problem**: Predict house price from size\n",
    "- **Relationship**: Linear (larger houses = higher prices, roughly linear)\n",
    "- **Target**: Continuous (price in dollars)\n",
    "- **Decision**: âœ… Use Linear Regression\n",
    "- **Reasoning**: Linear relationship, continuous target, interpretable\n",
    "\n",
    "#### Example 2: Stock Price Prediction âš ï¸ MAY NOT BE APPROPRIATE\n",
    "- **Problem**: Predict stock price from time\n",
    "- **Relationship**: Non-linear (volatile, trends, cycles)\n",
    "- **Target**: Continuous (price)\n",
    "- **Decision**: âš ï¸ Try Linear first, but may need Polynomial or other methods\n",
    "- **Reasoning**: Stock prices have complex patterns, linear may not capture well\n",
    "\n",
    "#### Example 3: Customer Churn Prediction âŒ NOT APPROPRIATE\n",
    "- **Problem**: Predict if customer will leave (yes/no)\n",
    "- **Relationship**: Classification problem\n",
    "- **Target**: Categorical (churn/not churn)\n",
    "- **Decision**: âŒ Use Logistic Regression instead\n",
    "- **Reasoning**: Classification problem, not regression\n",
    "\n",
    "#### Example 4: Sales Prediction with Many Features âš ï¸ MAY NEED REGULARIZATION\n",
    "- **Problem**: Predict sales from 50+ features\n",
    "- **Relationship**: Likely linear, but many features\n",
    "- **Target**: Continuous (sales amount)\n",
    "- **Decision**: âš ï¸ Use Ridge or Lasso Regression\n",
    "- **Reasoning**: Many features may cause overfitting, regularization helps\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Check relationship first** - Plot data to see if it's linear\n",
    "2. **Classification â‰  Regression** - Use logistic regression for categories\n",
    "3. **Start simple** - Linear regression is a good baseline\n",
    "4. **Check assumptions** - Linearity, independence, homoscedasticity\n",
    "5. **Many features?** - Consider Ridge/Lasso for regularization\n",
    "6. **Non-linear?** - Try Polynomial Regression or other methods\n",
    "7. **Always visualize** - Scatter plots reveal relationship type\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario 1**: Predicting student test scores from study hours\n",
    "- **Relationship**: More hours = higher scores (roughly linear)\n",
    "- **Target**: Continuous (test score 0-100)\n",
    "- **Decision**: âœ… Linear Regression appropriate\n",
    "\n",
    "**Scenario 2**: Predicting customer satisfaction (satisfied/not satisfied)\n",
    "- **Relationship**: Classification problem\n",
    "- **Target**: Categorical (satisfied/not satisfied)\n",
    "- **Decision**: âŒ Use Logistic Regression, not Linear Regression\n",
    "\n",
    "**Scenario 3**: Predicting house price from 100+ features\n",
    "- **Relationship**: Likely linear, but many features\n",
    "- **Target**: Continuous (price)\n",
    "- **Decision**: âš ï¸ Use Ridge or Lasso Regression (regularization needed)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Example 5: Polynomial Regression** - For non-linear relationships\n",
    "- ğŸ““ **Unit 2, Example 1: Ridge/Lasso** - For many features and regularization\n",
    "- ğŸ““ **Unit 3: Classification** - For predicting categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â“ Common Student Questions | Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„Ø·Ù„Ø§Ø¨\n",
    "\n",
    "**Q: What's the difference between simple and multiple linear regression?**\n",
    "- **Answer**: \n",
    "  - **Simple**: Uses ONE feature to predict target (e.g., size â†’ price)\n",
    "  - **Multiple**: Uses MULTIPLE features to predict target (e.g., size + age + rooms â†’ price)\n",
    "  - **Same concept**: Both find a line, but multiple regression is in higher dimensions\n",
    "  - **Formula**: Simple: y = mx + b, Multiple: y = bâ‚€ + bâ‚xâ‚ + bâ‚‚xâ‚‚ + ... + bâ‚™xâ‚™\n",
    "\n",
    "**Q: What do the coefficients mean?**\n",
    "- **Answer**: Coefficients show how much the target changes when a feature increases by 1 unit:\n",
    "  - **Example**: Coefficient = 0.05 for size means: \"For every 1 sq ft increase, price increases by $0.05\"\n",
    "  - **Larger coefficient** = Feature has more impact on predictions\n",
    "  - **Positive coefficient** = Feature increases target\n",
    "  - **Negative coefficient** = Feature decreases target\n",
    "\n",
    "**Q: What's a good RÂ² score?**\n",
    "- **Answer**: It depends on your problem:\n",
    "  - **RÂ² > 0.9**: Excellent (model explains >90% of variance)\n",
    "  - **RÂ² > 0.7**: Good (model explains >70% of variance)\n",
    "  - **RÂ² > 0.5**: Fair (model explains >50% of variance)\n",
    "  - **RÂ² < 0.5**: Poor (model explains <50% of variance)\n",
    "  - **Note**: For real-world problems, RÂ² > 0.7 is generally considered good\n",
    "\n",
    "**Q: Why do we need both MSE and MAE?**\n",
    "- **Answer**: They measure different aspects:\n",
    "  - **MSE**: Penalizes large errors more (squared term) - use when large errors are very bad\n",
    "  - **MAE**: Average error in same units as target - easier to interpret\n",
    "  - **Example**: MSE = $10,000Â², MAE = $100 â†’ \"Average error is $100\" (MAE is clearer!)\n",
    "  - **Use both**: MSE for optimization, MAE for interpretation\n",
    "\n",
    "**Q: What if my model has high training RÂ² but low test RÂ²?**\n",
    "- **Answer**: This is **overfitting** - model memorized training data:\n",
    "  - **Problem**: Model learned training patterns too well, can't generalize\n",
    "  - **Solution**: Use regularization (Ridge/Lasso), get more data, or simplify model\n",
    "  - **Check**: If train RÂ² >> test RÂ² â†’ overfitting detected!\n",
    "\n",
    "**Q: Can I use linear regression for classification?**\n",
    "- **Answer**: **NO!** Linear regression predicts continuous values (numbers), not categories:\n",
    "  - **Regression**: Predicts numbers (price, temperature, sales)\n",
    "  - **Classification**: Predicts categories (yes/no, A/B/C, sick/healthy)\n",
    "  - **Use instead**: Logistic Regression for classification (Unit 3)\n",
    "\n",
    "**Q: What if my data isn't linear?**\n",
    "- **Answer**: Linear regression won't work well:\n",
    "  - **Check**: Plot your data - if it's curved, not linear\n",
    "  - **Solution**: Use Polynomial Regression (Example 5) or other non-linear models\n",
    "  - **Sign**: Low RÂ², curved residuals pattern â†’ need non-linear model\n",
    "\n",
    "**Q: How do I know if my model is good enough?**\n",
    "- **Answer**: Check multiple things:\n",
    "  - **RÂ² > 0.7**: Good overall fit\n",
    "  - **Train RÂ² â‰ˆ Test RÂ²**: Good generalization (no overfitting)\n",
    "  - **Residuals random**: No patterns (good model)\n",
    "  - **MAE reasonable**: Error is acceptable for your use case\n",
    "  - **All together**: Good models have high RÂ², similar train/test, random residuals\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary | Ø§Ù„Ù…Ù„Ø®Øµ\n",
    "\n",
    "Great job completing this example!\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "**Linear Regression - Your First ML Model:**\n",
    "- **Simple Linear Regression**: Predicts a continuous target using one feature. Finds the best line (y = mx + b) that minimizes prediction error.\n",
    "\n",
    "- **Multiple Linear Regression**: Extends to multiple features. Predicts target using multiple inputs simultaneously. Formula: y = bâ‚€ + bâ‚xâ‚ + bâ‚‚xâ‚‚ + ... + bâ‚™xâ‚™\n",
    "\n",
    "- **Model Evaluation Metrics**:\n",
    "  - **MSE (Mean Squared Error)**: Average squared difference between predictions and actual values (lower is better)\n",
    "  - **MAE (Mean Absolute Error)**: Average absolute difference (more interpretable than MSE)\n",
    "  - **RÂ² (R-squared)**: Proportion of variance explained (0 to 1, higher is better, 1 = perfect fit)\n",
    "\n",
    "- **Residual Analysis**: Residuals (errors) should be randomly distributed around zero. Patterns in residuals indicate model problems.\n",
    "\n",
    "- **Feature Importance**: Coefficients show how much each feature affects the prediction. Larger absolute coefficients = more important features.\n",
    "\n",
    "### ğŸ”— How This Connects:\n",
    "\n",
    "**This example builds on and leads to:**\n",
    "- **Example 1-3**: Data loading, cleaning, and preprocessing prepare data for ML models\n",
    "- **Example 5: Polynomial Regression** - Extends linear regression to capture non-linear relationships\n",
    "- **Unit 2: Advanced Regression** - Ridge and Lasso regression improve on basic linear regression\n",
    "- **Unit 3: Classification** - Similar workflow (fit, predict, evaluate) but for categorical targets\n",
    "- **All ML Models**: Linear regression teaches the fundamental ML workflow used by all models\n",
    "\n",
    "**Why this example is important:**\n",
    "1. **First ML model**: Simplest model, easiest to understand\n",
    "2. **ML workflow**: Teaches the standard process: prepare data â†’ train model â†’ evaluate â†’ interpret\n",
    "3. **Foundation**: Many advanced models build on linear regression concepts\n",
    "4. **Interpretability**: Linear models are highly interpretable - you can see exactly how features affect predictions\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Linear regression finds the best line**: Minimizes prediction error using least squares method\n",
    "2. **Multiple features improve predictions**: More relevant features generally lead to better models\n",
    "3. **Evaluation metrics matter**: MSE, MAE, and RÂ² tell you how good your model is\n",
    "4. **Residuals reveal problems**: Check residuals to diagnose model issues\n",
    "5. **Coefficients show feature importance**: Understand which features matter most\n",
    "\n",
    "### Next Steps:\n",
    "- Complete exercises in `exercises/` folder to practice building linear regression models\n",
    "- Review quiz materials to test your understanding\n",
    "- Proceed to **Example 5: Polynomial Regression** to handle non-linear relationships\n",
    "- Then move to **Unit 2: Advanced Regression** for Ridge and Lasso techniques\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Tips for Success:**\n",
    "- **Data quality matters**: Clean, preprocessed data leads to better models\n",
    "- **Feature selection**: Not all features help - remove irrelevant ones\n",
    "- **Check assumptions**: Linear regression assumes linear relationships - verify this!\n",
    "- **Visualize**: Always plot your data and residuals to understand what's happening"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
