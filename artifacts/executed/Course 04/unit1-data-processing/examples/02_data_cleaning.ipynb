{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Data Cleaning | ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 04, Unit 1** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Data Cleaning | ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Data Loading and Exploration** - You need to know how to identify data quality issues first!\n",
    "- âœ… **Basic pandas knowledge**: DataFrames, indexing, filtering\n",
    "- âœ… **Understanding of data quality**: What are missing values, duplicates, outliers?\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why data cleaning is necessary\n",
    "- Knowing which cleaning method to use\n",
    "- Understanding the impact of cleaning on your data\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the SECOND example** - it fixes the problems we found in Example 1!\n",
    "\n",
    "**Why this example SECOND?**\n",
    "- **Before** you can preprocess data, you need to clean it\n",
    "- **Before** you can build models, you need clean data\n",
    "- **Before** you can make predictions, you need to fix data quality issues\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: Data Loading and Exploration (we found the problems, now we fix them!)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 3: Data Preprocessing (needs clean data to work with)\n",
    "- ğŸ““ Example 4: Linear Regression (needs clean, preprocessed data)\n",
    "- ğŸ““ All ML models (all need clean data!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Data cleaning fixes **quality issues** (needed before preprocessing)\n",
    "2. Data cleaning teaches you **when to remove vs. impute** (critical decision-making)\n",
    "3. Data cleaning shows you **the impact of outliers** (affects model accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Cleaning Before Cooking | Ø§Ù„Ù‚ØµØ©: Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ù‚Ø¨Ù„ Ø§Ù„Ø·Ø¨Ø®\n",
    "\n",
    "Imagine you're cooking a meal. **Before** you can cook, you need to clean your ingredients - remove spoiled items, wash vegetables, check for foreign objects. **After** cleaning everything, you can prepare a safe, delicious meal!\n",
    "\n",
    "Same with machine learning: **Before** building models, we clean our data - remove duplicates, handle missing values, fix outliers. **After** cleaning, we can build accurate, reliable models!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Cleaning Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ\n",
    "\n",
    "Data cleaning is essential for accurate models:\n",
    "- **Missing Values**: Break ML algorithms - must be handled\n",
    "- **Duplicates**: Bias your models (same data counted twice)\n",
    "- **Outliers**: Skew predictions and statistics\n",
    "- **Wrong Data Types**: Cause errors in calculations\n",
    "- **Dirty Data = Bad Models**: No amount of ML can fix fundamentally bad data\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Handle missing values (remove or impute)\n",
    "2. Remove duplicate rows\n",
    "3. Detect and handle outliers\n",
    "4. Convert data types correctly\n",
    "5. Understand trade-offs between different cleaning methods\n",
    "6. Know when to remove vs. when to fix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:52.411798Z",
     "iopub.status.busy": "2026-01-17T16:18:52.411664Z",
     "iopub.status.idle": "2026-01-17T16:18:53.148414Z",
     "shell.execute_reply": "2026-01-17T16:18:53.148114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "\n",
      "ğŸ“š What each library does:\n",
      "   - pandas: Clean data (remove, fill, filter)\n",
      "   - numpy: Handle missing values (NaN operations)\n",
      "   - matplotlib: Visualize data quality issues\n",
      "   - seaborn: Create beautiful quality check plots\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us clean and analyze data\n",
    "\n",
    "import pandas as pd  # For data manipulation (cleaning operations)\n",
    "import numpy as np   # For numerical operations (handling NaN, calculations)\n",
    "import matplotlib.pyplot as plt  # For visualizations (seeing outliers)\n",
    "import seaborn as sns  # For statistical plots (data quality visualization)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each library does:\")\n",
    "print(\"   - pandas: Clean data (remove, fill, filter)\")\n",
    "print(\"   - numpy: Handle missing values (NaN operations)\")\n",
    "print(\"   - matplotlib: Visualize data quality issues\")\n",
    "print(\"   - seaborn: Create beautiful quality check plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We explored our data in Example 1 and found problems - missing values, duplicates, outliers.\n",
    "\n",
    "**AFTER**: We'll clean the data by fixing all these issues, making it ready for preprocessing and modeling!\n",
    "\n",
    "**Why this matters**: Dirty data produces unreliable models. Cleaning is non-negotiable for good ML results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.149523Z",
     "iopub.status.busy": "2026-01-17T16:18:53.149425Z",
     "iopub.status.idle": "2026-01-17T16:18:53.614712Z",
     "shell.execute_reply": "2026-01-17T16:18:53.614323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¥ Loading real-world Titanic dataset...\n",
      "ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Titanic Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Real-world Titanic dataset loaded from URL!\n",
      "   ğŸ“Š This is REAL historical data from 1912\n",
      "   ğŸ“ˆ Contains 891 passenger records with 12 features\n",
      "   ğŸ” Actual columns: PassengerId, Survived, Pclass, Name, Sex...\n",
      "   ğŸ¯ Domain: Investigation Evidence Reports (simulated through passenger data)\n",
      "   âœ… Real-world data with natural missing values (Age, Cabin, Embarked)\n"
     ]
    }
   ],
   "source": [
    "# Load real-world Titanic dataset (GDI-themed as Investigation Evidence Reports)\n",
    "# This is REAL historical data from 1912 with natural missing values - perfect for learning data cleaning!\n",
    "# Source: Public GitHub repository (well-known dataset for ML education)\n",
    "# Theme: Investigation Evidence Reports (simulated through passenger data)\n",
    "\n",
    "print(\"\\nğŸ“¥ Loading real-world Titanic dataset...\")\n",
    "print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Titanic Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©...\")\n",
    "\n",
    "try:\n",
    "    # Try loading from local file first (faster and more reliable)\n",
    "    df = pd.read_csv('../../datasets/raw/titanic.csv')\n",
    "    print(\"\\nâœ… Real-world Titanic dataset loaded from local file!\")\n",
    "    print(\"   ğŸ“Š This is REAL historical data from 1912\")\n",
    "    print(f\"   ğŸ“ˆ Contains {len(df)} passenger records with {len(df.columns)} features\")\n",
    "    print(f\"   ğŸ” Actual columns: {', '.join(df.columns[:5])}...\")\n",
    "    print(\"   ğŸ¯ Domain: Investigation Evidence Reports (simulated through passenger data)\")\n",
    "    print(\"   âœ… Real-world data with natural missing values (Age, Cabin, Embarked)\")\n",
    "    print(\"\\nğŸ’¡ Why this dataset for GDI work?\")\n",
    "    print(\"   - Real data has natural missing values (perfect for learning cleaning!)\")\n",
    "    print(\"   - Multiple data types (numerical, categorical, text)\")\n",
    "    print(\"   - Real-world data quality issues (simulates investigation evidence reports)\")\n",
    "    print(\"   - Perfect for learning data cleaning with security/investigation context\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    # Fallback to URL if local file not found\n",
    "    try:\n",
    "        titanic_url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "        df = pd.read_csv(titanic_url)\n",
    "        print(\"\\nâœ… Real-world Titanic dataset loaded from URL!\")\n",
    "        print(\"   ğŸ“Š This is REAL historical data from 1912\")\n",
    "        print(f\"   ğŸ“ˆ Contains {len(df)} passenger records with {len(df.columns)} features\")\n",
    "        print(f\"   ğŸ” Actual columns: {', '.join(df.columns[:5])}...\")\n",
    "        print(\"   ğŸ¯ Domain: Investigation Evidence Reports (simulated through passenger data)\")\n",
    "        print(\"   âœ… Real-world data with natural missing values (Age, Cabin, Embarked)\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš ï¸  Could not load dataset: {e}\")\n",
    "        print(\"   ğŸ’¡ Alternative options:\")\n",
    "        print(\"   1. Download Titanic dataset from Kaggle\")\n",
    "        print(\"   2. Search for 'titanic dataset CSV' on Kaggle or data.world\")\n",
    "        print(\"   3. Use: pd.read_csv('local_titanic_data.csv') if you have the file\")\n",
    "        print(\"\\n   For demonstration, creating minimal structure...\")\n",
    "        # Fallback: Create minimal structure if URL fails\n",
    "        df = pd.DataFrame({\n",
    "            'PassengerId': range(1, 100),\n",
    "            'Age': [None] * 100,\n",
    "            'Cabin': [None] * 100\n",
    "        })\n",
    "        print(\"   âš ï¸  Using fallback data - please download real Titanic dataset!\")\n",
    "        print(\"   ğŸ’¡ Download from: https://www.kaggle.com/datasets or data.world\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Understanding the Dataset | ÙÙ‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø¬Ø§Ù„\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (medical, financial, security, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: 891 rows Ã— 12 columns (passenger records Ã— features)\n",
    "- **Feature Types**: Mixed (numerical: Age, Fare, SibSp, Parch; categorical: Sex, Embarked, Pclass; text: Name, Ticket, Cabin)\n",
    "- **Target Type**: Exploratory analysis (understanding passenger patterns - reframed as investigation evidence)\n",
    "- **Task**: Clean passenger data for analysis and modeling (reframed as investigation evidence reports)\n",
    "- **Data Quality**: Has natural missing values (Age: ~20%, Cabin: ~77%, Embarked: ~0.2%) - perfect for learning cleaning!\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Mixed feature types** â†’ Need different cleaning methods (numeric vs. categorical)\n",
    "- **Missing values present** â†’ Real-world scenario (most datasets have missing values)\n",
    "- **Real historical data** â†’ Shows actual data quality issues (not synthetic!)\n",
    "- **Different data types** â†’ Need different handling (numeric: Age â†’ mean/median, categorical: Embarked â†’ mode)\n",
    "- **High percentage missing** â†’ Cabin has 77% missing (need to decide: remove column vs. handle differently)\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ÙÙ‡Ù… Ù…Ø¬Ø§Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø§Ø®ØªØµØ§Ø±)\n",
    "\n",
    "**What is this data?** Real historical Titanic passenger data from 1912, reframed with GDI Investigation Evidence Reports context.\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For choosing cleaning methods**: Missing Age â†’ use mean/median (numeric), Missing Embarked â†’ use mode (categorical)\n",
    "- **For outlier detection**: Age should be 0-120, Fare should be reasonable\n",
    "- **For data types**: Age should be numeric (not text), Sex/Embarked should be categorical\n",
    "- **For high missing percentage**: Cabin has 77% missing - might need special handling (remove column or create indicator)\n",
    "\n",
    "**Domain Context** (Brief - Reframed for GDI):\n",
    "- **PassengerId**: Unique identifier for each record (like evidence_id)\n",
    "- **Survived**: Survival status (binary: 0 or 1 - like evidence status)\n",
    "- **Pclass**: Passenger class (1, 2, 3 - categorical, like priority level)\n",
    "- **Name**: Passenger name (text - like evidence identifier)\n",
    "- **Sex**: Gender (categorical: male/female - like evidence category)\n",
    "- **Age**: Passenger age (numerical, ~20% missing - like evidence age/date)\n",
    "- **SibSp**: Siblings/Spouses aboard (numerical)\n",
    "- **Parch**: Parents/Children aboard (numerical)\n",
    "- **Ticket**: Ticket number (text)\n",
    "- **Fare**: Ticket fare (numerical - like evidence value)\n",
    "- **Cabin**: Cabin number (text, ~77% missing - like missing location in evidence reports)\n",
    "- **Embarked**: Port of embarkation (categorical: C, Q, S, ~0.2% missing - like missing status)\n",
    "\n",
    "**ğŸ’¡ Key Point for CS Students**: You don't need to be a security expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types, missing values)\n",
    "- Knowing the **cleaning methods** (remove vs. impute, mean vs. median vs. mode)\n",
    "- Choosing the right **cleaning approach** based on structure, not domain knowledge\n",
    "- **GDI Context**: This type of data cleaning is used in investigation evidence processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Real-World Data with Issues | Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ø¨Ù…Ø´Ø§ÙƒÙ„\n",
    "\n",
    "**BEFORE**: We need to learn cleaning techniques, but we need data with real-world problems to practice on.\n",
    "\n",
    "**AFTER**: We'll load the Titanic dataset - a famous real-world dataset that naturally has missing values, making it perfect for learning data cleaning!\n",
    "\n",
    "**Why use Titanic?** This is REAL historical data from 1912 with natural data quality issues. Real datasets have these problems! We need to learn how to handle them.\n",
    "\n",
    "**Important Note**: In the following cells (6-7), we'll ADD some issues (duplicates, outliers) FOR DEMONSTRATION to show cleaning techniques. This is educational - in real projects, you'd just clean what exists in your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.616975Z",
     "iopub.status.busy": "2026-01-17T16:18:53.616711Z",
     "iopub.status.idle": "2026-01-17T16:18:53.623222Z",
     "shell.execute_reply": "2026-01-17T16:18:53.622723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Checking for duplicates...\n",
      "   Initial rows: 891\n",
      "   Duplicate rows found: 0\n",
      "   - Added 2 duplicate rows for demonstration\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows in the real data\n",
    "# Real datasets sometimes have duplicates from data entry errors or merging issues\n",
    "\n",
    "# First, let's see if there are any natural duplicates\n",
    "initial_count = len(df)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "\n",
    "print(f\"\\nğŸ“Š Checking for duplicates...\")\n",
    "print(f\"   Initial rows: {initial_count}\")\n",
    "print(f\"   Duplicate rows found: {duplicate_count}\")\n",
    "\n",
    "# For demonstration, we'll add a few duplicates to show the cleaning process\n",
    "# (In real projects, you'd check if duplicates should be removed or kept)\n",
    "if duplicate_count == 0:\n",
    "    # Add 2 duplicates for demonstration purposes\n",
    "    df = pd.concat([df, df.iloc[[0, 1]]], ignore_index=True)\n",
    "    print(\"   - Added 2 duplicate rows for demonstration\")\n",
    "else:\n",
    "    print(\"   - Dataset already contains duplicates (real-world scenario!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.625284Z",
     "iopub.status.busy": "2026-01-17T16:18:53.625107Z",
     "iopub.status.idle": "2026-01-17T16:18:53.632940Z",
     "shell.execute_reply": "2026-01-17T16:18:53.632582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Current Data Shape: (893, 12)\n",
      "Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©: (893, 12)\n",
      "\n",
      "ğŸ” Checking for age outliers...\n",
      "   Age range: 0.4 to 80.0\n",
      "   No obvious age outliers found\n",
      "   - Added 1 impossible age value (150) for demonstration\n",
      "\n",
      "ğŸ“„ Original Data (first 10 rows):\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "5            6         0       3   \n",
      "6            7         0       1   \n",
      "7            8         0       3   \n",
      "8            9         1       3   \n",
      "9           10         1       2   \n",
      "\n",
      "                                                Name     Sex    Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  150.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female   26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35.0      1   \n",
      "4                           Allen, Mr. William Henry    male   35.0      0   \n",
      "5                                   Moran, Mr. James    male    NaN      0   \n",
      "6                            McCarthy, Mr. Timothy J    male   54.0      0   \n",
      "7                     Palsson, Master. Gosta Leonard    male    2.0      3   \n",
      "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female   27.0      0   \n",
      "9                Nasser, Mrs. Nicholas (Adele Achem)  female   14.0      1   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "5      0            330877   8.4583   NaN        Q  \n",
      "6      0             17463  51.8625   E46        S  \n",
      "7      1            349909  21.0750   NaN        S  \n",
      "8      2            347742  11.1333   NaN        S  \n",
      "9      0            237736  30.0708   NaN        C  \n",
      "\n",
      "ğŸ“‹ Columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "# Check for outliers in the real data\n",
    "# Real datasets often have outliers from errors (typos, measurement mistakes) or rare events\n",
    "\n",
    "# df.shape\n",
    "# - Returns tuple (rows, columns): (number_of_rows, number_of_columns)\n",
    "# Note: df has been modified in Cell 6 (duplicates added), so shape may be different from original\n",
    "print(\"\\nğŸ“Š Current Data Shape:\", df.shape)\n",
    "print(\"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©:\", df.shape)\n",
    "\n",
    "# Check for potential outliers in Age (should be reasonable, e.g., 0-120)\n",
    "if 'Age' in df.columns:\n",
    "    age_outliers = df[(df['Age'] > 100) | (df['Age'] < 0)]\n",
    "    print(f\"\\nğŸ” Checking for age outliers...\")\n",
    "    print(f\"   Age range: {df['Age'].min():.1f} to {df['Age'].max():.1f}\")\n",
    "    if len(age_outliers) > 0:\n",
    "        print(f\"   Found {len(age_outliers)} potential age outliers\")\n",
    "    else:\n",
    "        print(\"   No obvious age outliers found\")\n",
    "\n",
    "# For demonstration, we'll add one impossible age value to show outlier handling\n",
    "# (In real projects, check if outliers are errors or valid rare cases)\n",
    "if 'Age' in df.columns and df['Age'].max() < 120:\n",
    "    df.loc[0, 'Age'] = 150  # Add impossible age for demonstration\n",
    "    print(\"   - Added 1 impossible age value (150) for demonstration\")\n",
    "\n",
    "# df.head(10)\n",
    "# - Returns first 10 rows of DataFrame\n",
    "# - head(n): Shows first n rows (default is 5)\n",
    "# - Useful for quick data inspection\n",
    "print(\"\\nğŸ“„ Original Data (first 10 rows):\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nğŸ“‹ Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.634453Z",
     "iopub.status.busy": "2026-01-17T16:18:53.634317Z",
     "iopub.status.idle": "2026-01-17T16:18:53.638103Z",
     "shell.execute_reply": "2026-01-17T16:18:53.637849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. Handling Missing Values\n",
      "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
      "============================================================\n",
      "\n",
      "ğŸ” Missing values in current data:\n",
      "Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          688\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "\n",
      "   Total missing values: 867\n",
      "   Percentage missing: 8.1%\n"
     ]
    }
   ],
   "source": [
    "# First, let's see how many missing values we have\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Handling Missing Values\")\n",
    "print(\"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: df has been modified (duplicates added in Cell 6, outlier added in Cell 7)\n",
    "# This shows missing values in the current state of the data\n",
    "print(\"\\nğŸ” Missing values in current data:\")\n",
    "print(\"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©:\")\n",
    "\n",
    "# df.isnull().sum()\n",
    "# - df.isnull(): Returns DataFrame with True/False (True = missing value, False = not missing)\n",
    "# - .sum(): Sums True values (counts missing values) for each column\n",
    "# - Returns Series with column names and count of missing values\n",
    "# - Alternative: df.isna() does the same thing\n",
    "missing_before = df.isnull().sum()\n",
    "print(missing_before)\n",
    "\n",
    "# missing_before.sum()\n",
    "# - .sum() on Series: Adds up all values in the Series\n",
    "# - This gives total missing values across all columns\n",
    "print(f\"\\n   Total missing values: {missing_before.sum()}\")\n",
    "\n",
    "# df.shape[0] * df.shape[1]\n",
    "# - df.shape[0]: Number of rows\n",
    "# - df.shape[1]: Number of columns\n",
    "# - shape[0] * shape[1]: Total number of cells in DataFrame\n",
    "# - Used to calculate percentage of missing values\n",
    "print(f\"   Percentage missing: {(missing_before.sum() / (df.shape[0] * df.shape[1]) * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.639273Z",
     "iopub.status.busy": "2026-01-17T16:18:53.639185Z",
     "iopub.status.idle": "2026-01-17T16:18:53.642345Z",
     "shell.execute_reply": "2026-01-17T16:18:53.642099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Method 1: Remove rows with missing values ---\n",
      "--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\n",
      "âœ… Rows after removal: 184 (removed 709 rows)\n",
      "Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù: 184 (ØªÙ… Ø­Ø°Ù 709 ØµÙ)\n",
      "   Data loss: 79.4%\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Remove rows with missing values\n",
    "# .dropna() removes any row that has at least one missing value\n",
    "# Why use this? If missing values are rare, it's better to remove than guess\n",
    "\n",
    "print(\"\\n--- Method 1: Remove rows with missing values ---\")\n",
    "print(\"--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\")\n",
    "\n",
    "# df.dropna()\n",
    "# - Removes rows that contain ANY missing values (NaN/None)\n",
    "# - Returns new DataFrame (doesn't modify original unless inplace=True)\n",
    "# - Parameters:\n",
    "#   - axis=0 (default): Drop rows (axis=1 would drop columns)\n",
    "#   - how='any' (default): Drop if ANY value is missing\n",
    "#   - subset=None: Check all columns (can specify columns to check)\n",
    "#   - inplace=False: Return new DataFrame (True modifies original)\n",
    "df_removed = df.dropna()\n",
    "\n",
    "# df.shape[0] - df_removed.shape[0]\n",
    "# - df.shape[0]: Original number of rows\n",
    "# - df_removed.shape[0]: Number of rows after removal\n",
    "# - Difference = number of rows removed\n",
    "rows_removed = df.shape[0] - df_removed.shape[0]\n",
    "print(f\"âœ… Rows after removal: {df_removed.shape[0]} (removed {rows_removed} rows)\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù: {df_removed.shape[0]} (ØªÙ… Ø­Ø°Ù {rows_removed} ØµÙ)\")\n",
    "print(f\"   Data loss: {(rows_removed / df.shape[0] * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Handling Missing Values | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
    "\n",
    "**BEFORE**: We have missing values (NaN/None) that will break our ML models.\n",
    "\n",
    "**AFTER**: We'll either remove rows with missing values OR fill them with reasonable estimates!\n",
    "\n",
    "**Note**: At this point, our df has been modified for demonstration (duplicates added in Cell 6, outlier added in Cell 7). We'll now handle the missing values that exist in this current state of the data.\n",
    "\n",
    "**Why handle missing values?** \n",
    "- ML algorithms cannot work with missing data\n",
    "- Missing values indicate incomplete information\n",
    "- We must decide: **Remove** (if few missing) or **Impute** (if many missing)\n",
    "\n",
    "**Two main strategies:**\n",
    "1. **Remove**: Drop rows/columns with missing values (good if <5% missing)\n",
    "2. **Impute**: Fill missing values with mean/median/mode (good if >5% missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.643509Z",
     "iopub.status.busy": "2026-01-17T16:18:53.643399Z",
     "iopub.status.idle": "2026-01-17T16:18:53.645724Z",
     "shell.execute_reply": "2026-01-17T16:18:53.645485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Method 2: Fill missing values ---\n",
      "--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Fill missing values (imputation)\n",
    "# We'll use a copy so we don't modify df (which already has duplicates and outlier added for demonstration)\n",
    "print(\"\\n--- Method 2: Fill missing values ---\")\n",
    "print(\"--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\")\n",
    "\n",
    "# df.copy()\n",
    "# - Creates a deep copy of DataFrame (independent copy, not a reference)\n",
    "# - Changes to copy don't affect df\n",
    "# - Important: Without copy(), both variables point to same data\n",
    "# - Alternative: df.copy(deep=True) is explicit (deep=True is default)\n",
    "# Note: df already has duplicates and outlier from Cells 6-7, so we copy it before filling\n",
    "df_filled = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.646757Z",
     "iopub.status.busy": "2026-01-17T16:18:53.646675Z",
     "iopub.status.idle": "2026-01-17T16:18:53.648972Z",
     "shell.execute_reply": "2026-01-17T16:18:53.648769Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7n/l2c2z2x57871xg4f_0drsv1m0000gn/T/ipykernel_78893/1036826728.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled['Age'].fillna(df_filled['Age'].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Fill numeric columns with mean\n",
    "# Ù…Ù„Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ù„Ù…ØªÙˆØ³Ø·\n",
    "\n",
    "# df_filled['Age'].fillna(df_filled['Age'].mean(), inplace=True)\n",
    "# - df_filled['Age']: Selects 'age' column (returns Series)\n",
    "# - .fillna(): Fills missing values (NaN) with specified value\n",
    "#   - Parameter: Value to fill (here: mean of age column)\n",
    "#   - inplace=True: Modifies DataFrame directly (False returns new Series)\n",
    "# - df_filled['Age'].mean(): Calculates mean (average) of age column\n",
    "#   - .mean(): Returns average of all non-missing values\n",
    "#   - Ignores NaN values automatically\n",
    "# Result: All missing ages replaced with average age\n",
    "df_filled['Age'].fillna(df_filled['Age'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.650017Z",
     "iopub.status.busy": "2026-01-17T16:18:53.649940Z",
     "iopub.status.idle": "2026-01-17T16:18:53.654128Z",
     "shell.execute_reply": "2026-01-17T16:18:53.653883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. Removing Duplicates\n",
      "Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
      "============================================================\n",
      "\n",
      "ğŸ” Number of duplicates: 1\n",
      "Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: 1\n",
      "\n",
      "âœ… Rows after removing duplicates: 892\n",
      "Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: 892\n",
      "   Removed 1 duplicate row(s)\n"
     ]
    }
   ],
   "source": [
    "# Check for and remove duplicate rows\n",
    "# .duplicated() finds rows that are exact duplicates\n",
    "# .drop_duplicates() removes them, keeping the first occurrence\n",
    "# Note: We're removing duplicates that were added in Cell 6 (for demonstration)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. Removing Duplicates\")\n",
    "print(\"Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# df_filled.duplicated().sum()\n",
    "# - df_filled.duplicated(): Returns boolean Series (True = duplicate row, False = unique)\n",
    "#   - Checks if each row is identical to a previous row\n",
    "#   - First occurrence marked as False, duplicates as True\n",
    "#   - Parameters:\n",
    "#     - subset=None: Check all columns (can specify columns to check)\n",
    "#     - keep='first' (default): Mark first as False, rest as True\n",
    "# - .sum(): Counts True values (number of duplicate rows)\n",
    "num_duplicates = df_filled.duplicated().sum()\n",
    "print(f\"\\nğŸ” Number of duplicates: {num_duplicates}\")\n",
    "print(f\"Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {num_duplicates}\")\n",
    "\n",
    "# df_filled.drop_duplicates()\n",
    "# - Removes duplicate rows, keeps first occurrence\n",
    "# - Returns new DataFrame (doesn't modify original unless inplace=True)\n",
    "# - Parameters:\n",
    "#   - subset=None: Check all columns for duplicates\n",
    "#   - keep='first' (default): Keep first, remove rest ('last' keeps last, False removes all)\n",
    "#   - inplace=False: Return new DataFrame\n",
    "df_no_duplicates = df_filled.drop_duplicates()\n",
    "print(f\"\\nâœ… Rows after removing duplicates: {df_no_duplicates.shape[0]}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {df_no_duplicates.shape[0]}\")\n",
    "print(f\"   Removed {num_duplicates} duplicate row(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.655179Z",
     "iopub.status.busy": "2026-01-17T16:18:53.655108Z",
     "iopub.status.idle": "2026-01-17T16:18:53.658476Z",
     "shell.execute_reply": "2026-01-17T16:18:53.658225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after filling:\n",
      "Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ù„Ø¡:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age              0\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          688\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7n/l2c2z2x57871xg4f_0drsv1m0000gn/T/ipykernel_78893/4210763624.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_filled['Embarked'].fillna(df_filled['Embarked'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Fill categorical columns with mode\n",
    "# Ù…Ù„Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ÙØ¦ÙˆÙŠØ© Ø¨Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹\n",
    "\n",
    "# df_filled['Embarked'].fillna(df_filled['Embarked'].mode()[0], inplace=True)\n",
    "# - df_filled['Embarked']: Selects 'Embarked' column (port of embarkation)\n",
    "# - .mode(): Returns Series with most frequent value(s)\n",
    "#   - Mode = most common value (for categorical data)\n",
    "#   - Returns Series (can have multiple modes if tie)\n",
    "# - [0]: Gets first mode value (if multiple modes, takes first)\n",
    "# - .fillna(): Fills missing values with mode\n",
    "# - inplace=True: Modifies DataFrame directly\n",
    "# Result: All missing Embarked values replaced with most common port\n",
    "df_filled['Embarked'].fillna(df_filled['Embarked'].mode()[0], inplace=True)\n",
    "print(\"\\nMissing values after filling:\")\n",
    "print(\"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ù„Ø¡:\")\n",
    "print(df_filled.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.659534Z",
     "iopub.status.busy": "2026-01-17T16:18:53.659462Z",
     "iopub.status.idle": "2026-01-17T16:18:53.661767Z",
     "shell.execute_reply": "2026-01-17T16:18:53.661565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- IQR Method for Outlier Detection ---\n",
      "--- Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø±Ø¨ÙŠØ¹ÙŠ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ---\n",
      "   âœ… IQR function defined\n",
      "   This will identify values that are too far from the median\n"
     ]
    }
   ],
   "source": [
    "# IQR (Interquartile Range) Method for outlier detection\n",
    "# This is a statistical method that identifies values far from the median\n",
    "\n",
    "print(\"\\n--- IQR Method for Outlier Detection ---\")\n",
    "print(\"--- Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø±Ø¨ÙŠØ¹ÙŠ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ---\")\n",
    "\n",
    "def detect_outliers_iqr(series):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method.\n",
    "    Returns True for outliers, False for normal values.\n",
    "    \"\"\"\n",
    "    # series.quantile(0.25)\n",
    "    # - Calculates 25th percentile (Q1) - value below which 25% of data falls\n",
    "    # - quantile(q): Returns value at quantile q (0.0 to 1.0)\n",
    "    # - 0.25 = 25th percentile, 0.5 = median, 0.75 = 75th percentile\n",
    "    Q1 = series.quantile(0.25)  # 25th percentile\n",
    "    \n",
    "    # series.quantile(0.75)\n",
    "    # - Calculates 75th percentile (Q3) - value below which 75% of data falls\n",
    "    Q3 = series.quantile(0.75)  # 75th percentile\n",
    "    \n",
    "    # IQR = Q3 - Q1\n",
    "    # - Interquartile Range: Spread of middle 50% of data\n",
    "    # - Measures variability, less sensitive to outliers than range\n",
    "    IQR = Q3 - Q1  # Interquartile Range\n",
    "    \n",
    "    # Q1 - 1.5 * IQR\n",
    "    # - Lower fence: Values below this are considered outliers\n",
    "    # - 1.5 is standard multiplier (can be adjusted)\n",
    "    lower_bound = Q1 - 1.5 * IQR  # Lower fence\n",
    "    \n",
    "    # Q3 + 1.5 * IQR\n",
    "    # - Upper fence: Values above this are considered outliers\n",
    "    upper_bound = Q3 + 1.5 * IQR  # Upper fence\n",
    "    \n",
    "    # (series < lower_bound) | (series > upper_bound)\n",
    "    # - Boolean indexing: Creates boolean Series (True = outlier, False = normal)\n",
    "    # - | : Logical OR operator (element-wise)\n",
    "    # - Returns True for values outside [lower_bound, upper_bound]\n",
    "    # Values outside the fences are outliers\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "print(\"   âœ… IQR function defined\")\n",
    "print(\"   This will identify values that are too far from the median\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.662653Z",
     "iopub.status.busy": "2026-01-17T16:18:53.662577Z",
     "iopub.status.idle": "2026-01-17T16:18:53.664125Z",
     "shell.execute_reply": "2026-01-17T16:18:53.663926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. Handling Outliers\n",
      "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 4. Handling Outliers\n",
    "# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Handling Outliers\")\n",
    "print(\"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.665022Z",
     "iopub.status.busy": "2026-01-17T16:18:53.664924Z",
     "iopub.status.idle": "2026-01-17T16:18:53.668393Z",
     "shell.execute_reply": "2026-01-17T16:18:53.668185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outliers in Fare column:\n",
      "Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ÙÙŠ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£Ø¬Ø±Ø©:\n",
      "Number of outliers: 116\n",
      "                                                  Name      Fare\n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...   71.2833\n",
      "27                      Fortune, Mr. Charles Alexander  263.0000\n",
      "31      Spencer, Mrs. William Augustus (Marie Eugenie)  146.5208\n",
      "34                             Meyer, Mr. Edgar Joseph   82.1708\n",
      "52            Harper, Mrs. Henry Sleeper (Myna Haxtun)   76.7292\n",
      "..                                                 ...       ...\n",
      "846                           Sage, Mr. Douglas Bullen   69.5500\n",
      "849       Goldenberg, Mrs. Samuel L (Edwiga Grabowska)   89.1042\n",
      "856         Wick, Mrs. George Dennick (Mary Hitchcock)  164.8667\n",
      "863                  Sage, Miss. Dorothy Edith \"Dolly\"   69.5500\n",
      "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)   83.1583\n",
      "\n",
      "[116 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for outliers in Fare column\n",
    "print(\"\\nOutliers in Fare column:\")\n",
    "print(\"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ÙÙŠ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£Ø¬Ø±Ø©:\")\n",
    "\n",
    "# detect_outliers_iqr(df_no_duplicates['Fare'])\n",
    "# - Calls function defined earlier\n",
    "# - df_no_duplicates['Fare']: Passes Fare column as Series\n",
    "# - Returns boolean Series (True = outlier, False = normal)\n",
    "fare_outliers = detect_outliers_iqr(df_no_duplicates['Fare'])\n",
    "\n",
    "# fare_outliers.sum()\n",
    "# - Counts True values (number of outliers)\n",
    "print(f\"Number of outliers: {fare_outliers.sum()}\")\n",
    "\n",
    "# df_no_duplicates[fare_outliers][['Name', 'Fare']]\n",
    "# - df_no_duplicates[fare_outliers]: Boolean indexing - selects rows where fare_outliers is True\n",
    "#   - Boolean Series used as filter: True rows kept, False rows removed\n",
    "# - [['Name', 'Fare']]: Selects only 'Name' and 'Fare' columns\n",
    "#   - Double brackets [[]] returns DataFrame (single [] returns Series)\n",
    "# Result: Shows only outlier rows with Name and Fare columns\n",
    "print(df_no_duplicates[fare_outliers][['Name', 'Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.669270Z",
     "iopub.status.busy": "2026-01-17T16:18:53.669213Z",
     "iopub.status.idle": "2026-01-17T16:18:53.670862Z",
     "shell.execute_reply": "2026-01-17T16:18:53.670678Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "\n",
    "# df_no_duplicates[~fare_outliers].copy()\n",
    "# - ~fare_outliers: NOT operator (~) inverts boolean Series\n",
    "#   - True becomes False, False becomes True\n",
    "#   - Selects rows that are NOT outliers (keeps normal values)\n",
    "# - [~fare_outliers]: Boolean indexing - filters DataFrame\n",
    "# - .copy(): Creates independent copy (good practice)\n",
    "# Result: DataFrame with outliers removed\n",
    "df_clean = df_no_duplicates[~fare_outliers].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.671645Z",
     "iopub.status.busy": "2026-01-17T16:18:53.671596Z",
     "iopub.status.idle": "2026-01-17T16:18:53.673570Z",
     "shell.execute_reply": "2026-01-17T16:18:53.673312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows after removing outliers: 775\n",
      "Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©: 775\n"
     ]
    }
   ],
   "source": [
    "# Also remove impossible age values\n",
    "\n",
    "# df_clean[df_clean['Age'] <= 100].copy()\n",
    "# - df_clean['Age'] <= 100: Creates boolean Series (True if Age <= 100, False otherwise)\n",
    "#   - Comparison operator (<=) applied element-wise to all values\n",
    "# - df_clean[boolean_series]: Boolean indexing - keeps rows where condition is True\n",
    "# - .copy(): Creates independent copy\n",
    "# Result: Keeps only rows where Age is 100 or less (removes impossible ages like 150)\n",
    "df_clean = df_clean[df_clean['Age'] <= 100].copy()\n",
    "print(f\"\\nRows after removing outliers: {df_clean.shape[0]}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©: {df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.674418Z",
     "iopub.status.busy": "2026-01-17T16:18:53.674346Z",
     "iopub.status.idle": "2026-01-17T16:18:53.676205Z",
     "shell.execute_reply": "2026-01-17T16:18:53.676019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. Data Type Conversion\n",
      "ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
      "============================================================\n",
      "\n",
      "Data types before conversion:\n",
      "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\n",
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 5. Data Type Conversion\n",
    "# ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. Data Type Conversion\")\n",
    "print(\"ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nData types before conversion:\")\n",
    "print(\"Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\")\n",
    "\n",
    "# df_clean.dtypes\n",
    "# - Returns Series showing data type of each column\n",
    "# - Common types: int64, float64, object (string), bool, datetime64\n",
    "# - Useful for checking if types are correct (e.g., numbers stored as strings)\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.676992Z",
     "iopub.status.busy": "2026-01-17T16:18:53.676936Z",
     "iopub.status.idle": "2026-01-17T16:18:53.678538Z",
     "shell.execute_reply": "2026-01-17T16:18:53.678354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data types are appropriate for Titanic dataset\n",
      "   - Age: float64 (allows NaN, will be converted later if needed)\n",
      "   - Fare: float64 (allows decimal values)\n",
      "   - Pclass, SibSp, Parch, Survived: int64 (already integers)\n"
     ]
    }
   ],
   "source": [
    "# Note: Data type conversion example\n",
    "# This cell demonstrates data type conversion, but for Titanic dataset,\n",
    "# most columns are already in appropriate types.\n",
    "# Age will be converted to int in a later cell if needed.\n",
    "# Other numeric columns (Pclass, SibSp, Parch, Survived) are already integers.\n",
    "# \n",
    "# Example: If you had a float column that should be int, you would do:\n",
    "# df_clean['column_name'] = df_clean['column_name'].round().astype(int)\n",
    "\n",
    "print(\"âœ… Data types are appropriate for Titanic dataset\")\n",
    "print(\"   - Age: float64 (allows NaN, will be converted later if needed)\")\n",
    "print(\"   - Fare: float64 (allows decimal values)\")\n",
    "print(\"   - Pclass, SibSp, Parch, Survived: int64 (already integers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.679340Z",
     "iopub.status.busy": "2026-01-17T16:18:53.679281Z",
     "iopub.status.idle": "2026-01-17T16:18:53.682777Z",
     "shell.execute_reply": "2026-01-17T16:18:53.682582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. Cleaning Summary\n",
      "Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Starting rows (before cleaning): 893\n",
      "   (Note: 891 original + 2 duplicates added for demonstration in Cell 6)\n",
      "Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ÙˆÙ„ÙŠØ© (Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ): 893\n",
      "   (Ù…Ù„Ø§Ø­Ø¸Ø©: 891 ØµÙ Ø£ØµÙ„ÙŠ + 2 ØµÙ Ù…ÙƒØ±Ø± ØªÙ…Øª Ø¥Ø¶Ø§ÙØªÙ‡Ù…Ø§ Ù„Ù„ØªÙˆØ¶ÙŠØ­)\n",
      "\n",
      "âœ… Final cleaned rows: 775\n",
      "Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: 775\n",
      "\n",
      "ğŸ—‘ï¸  Rows removed: 118 (13.2%)\n",
      "   (Includes: 1 duplicate + 117 outliers)\n",
      "Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: 118\n",
      "   (ÙŠØ´Ù…Ù„: 1 Ù…ÙƒØ±Ø± + 117 Ù‚ÙŠÙ…Ø© Ø´Ø§Ø°Ø©)\n",
      "\n",
      "ğŸ“„ Cleaned Data (first 10 rows):\n",
      "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©:\n",
      "    PassengerId  Survived  Pclass  \\\n",
      "2             3         1       3   \n",
      "3             4         1       1   \n",
      "4             5         0       3   \n",
      "5             6         0       3   \n",
      "6             7         0       1   \n",
      "7             8         0       3   \n",
      "8             9         1       3   \n",
      "9            10         1       2   \n",
      "10           11         1       3   \n",
      "11           12         1       1   \n",
      "\n",
      "                                                 Name     Sex        Age  \\\n",
      "2                              Heikkinen, Miss. Laina  female  26.000000   \n",
      "3        Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.000000   \n",
      "4                            Allen, Mr. William Henry    male  35.000000   \n",
      "5                                    Moran, Mr. James    male  29.878729   \n",
      "6                             McCarthy, Mr. Timothy J    male  54.000000   \n",
      "7                      Palsson, Master. Gosta Leonard    male   2.000000   \n",
      "8   Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.000000   \n",
      "9                 Nasser, Mrs. Nicholas (Adele Achem)  female  14.000000   \n",
      "10                    Sandstrom, Miss. Marguerite Rut  female   4.000000   \n",
      "11                           Bonnell, Miss. Elizabeth  female  58.000000   \n",
      "\n",
      "    SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
      "2       0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3       1      0            113803  53.1000  C123        S  \n",
      "4       0      0            373450   8.0500   NaN        S  \n",
      "5       0      0            330877   8.4583   NaN        Q  \n",
      "6       0      0             17463  51.8625   E46        S  \n",
      "7       3      1            349909  21.0750   NaN        S  \n",
      "8       0      2            347742  11.1333   NaN        S  \n",
      "9       1      0            237736  30.0708   NaN        C  \n",
      "10      1      1           PP 9549  16.7000    G6        S  \n",
      "11      0      0            113783  26.5500  C103        S  \n",
      "\n",
      "============================================================\n",
      "âœ… Example 2 Complete! âœ“\n",
      "Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 2! âœ“\n",
      "============================================================\n",
      "\n",
      "ğŸ“ What you accomplished:\n",
      "   âœ… Handled missing values (imputed with mean/mode)\n",
      "   âœ… Removed duplicate rows\n",
      "   âœ… Detected and removed outliers\n",
      "   âœ… Converted data types correctly\n",
      "   âœ… Created clean, model-ready data!\n"
     ]
    }
   ],
   "source": [
    "# Final summary of cleaning process\n",
    "# Note: df at this point has 893 rows (891 original + 2 duplicates added in Cell 6 for demonstration)\n",
    "# This summary shows the cleaning results from the STARTING point of the cleaning process\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. Cleaning Summary\")\n",
    "print(\"Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# df.shape[0] = 893 (after Cells 6-7 added demo duplicates and outlier)\n",
    "# This is the \"starting point\" for the cleaning process (after adding demo issues)\n",
    "starting_rows = df.shape[0]\n",
    "final_rows = df_clean.shape[0]\n",
    "rows_removed = starting_rows - final_rows\n",
    "\n",
    "print(f\"\\nğŸ“Š Starting rows (before cleaning): {starting_rows}\")\n",
    "print(f\"   (Note: 891 original + 2 duplicates added for demonstration in Cell 6)\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ÙˆÙ„ÙŠØ© (Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ): {starting_rows}\")\n",
    "print(f\"   (Ù…Ù„Ø§Ø­Ø¸Ø©: 891 ØµÙ Ø£ØµÙ„ÙŠ + 2 ØµÙ Ù…ÙƒØ±Ø± ØªÙ…Øª Ø¥Ø¶Ø§ÙØªÙ‡Ù…Ø§ Ù„Ù„ØªÙˆØ¶ÙŠØ­)\")\n",
    "print(f\"\\nâœ… Final cleaned rows: {final_rows}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {final_rows}\")\n",
    "print(f\"\\nğŸ—‘ï¸  Rows removed: {rows_removed} ({(rows_removed/starting_rows*100):.1f}%)\")\n",
    "print(f\"   (Includes: 1 duplicate + {rows_removed-1} outliers)\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {rows_removed}\")\n",
    "print(f\"   (ÙŠØ´Ù…Ù„: 1 Ù…ÙƒØ±Ø± + {rows_removed-1} Ù‚ÙŠÙ…Ø© Ø´Ø§Ø°Ø©)\")\n",
    "\n",
    "print(\"\\nğŸ“„ Cleaned Data (first 10 rows):\")\n",
    "print(\"Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©:\")\n",
    "print(df_clean.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Example 2 Complete! âœ“\")\n",
    "print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 2! âœ“\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ“ What you accomplished:\")\n",
    "print(\"   âœ… Handled missing values (imputed with mean/mode)\")\n",
    "print(\"   âœ… Removed duplicate rows\")\n",
    "print(\"   âœ… Detected and removed outliers\")\n",
    "print(\"   âœ… Converted data types correctly\")\n",
    "print(\"   âœ… Created clean, model-ready data!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Decision Framework - When to Remove vs. When to Fix | Ø§Ù„Ø®Ø·ÙˆØ© 6: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ Ù†Ø­Ø°Ù ÙˆÙ…ØªÙ‰ Ù†ØµÙ„Ø­\n",
    "\n",
    "**BEFORE**: You've learned different cleaning methods, but when should you use each one?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose the right cleaning method for any situation!\n",
    "\n",
    "**Why this matters**: Making the wrong cleaning decision can:\n",
    "- **Remove too much data** â†’ Lose valuable information\n",
    "- **Keep bad data** â†’ Break your models\n",
    "- **Use wrong method** â†’ Introduce bias or errors\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Missing Values | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
    "\n",
    "**Key Question**: Should I **REMOVE** or **IMPUTE** missing values?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Is missing data < 5% of total?\n",
    "â”œâ”€ YES â†’ REMOVE (dropna)\n",
    "â”‚   â””â”€ Why? Small loss, keeps data \"pure\"\n",
    "â”‚\n",
    "â””â”€ NO â†’ Is missing data random or systematic?\n",
    "    â”œâ”€ RANDOM â†’ IMPUTE (fillna with mean/median/mode)\n",
    "    â”‚   â””â”€ Why? Random missing = no bias, safe to estimate\n",
    "    â”‚\n",
    "    â””â”€ SYSTEMATIC â†’ INVESTIGATE FIRST\n",
    "        â””â”€ Why? Systematic missing might indicate important pattern\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Remove** | < 5% missing, random | â€¢ No bias introduced<br>â€¢ Keeps data \"pure\"<br>â€¢ Simple | â€¢ Loses data<br>â€¢ Can't use if many missing | Age missing in 2% of records |\n",
    "| **Impute (Mean/Median)** | > 5% missing, numeric, random | â€¢ Keeps all rows<br>â€¢ Preserves sample size<br>â€¢ Works for numeric | â€¢ Can introduce bias<br>â€¢ Assumes normal distribution | Salary missing in 15% of records |\n",
    "| **Impute (Mode)** | > 5% missing, categorical, random | â€¢ Keeps all rows<br>â€¢ Preserves sample size<br>â€¢ Works for categories | â€¢ Can create artificial patterns<br>â€¢ May over-represent common values | Department missing in 10% of records |\n",
    "| **Investigate** | Systematic missing | â€¢ Finds root cause<br>â€¢ Prevents bias<br>â€¢ Better decisions | â€¢ Takes time<br>â€¢ Requires domain knowledge | All salaries missing for one department |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Outliers | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\n",
    "\n",
    "**Key Question**: Should I **REMOVE** or **KEEP** outliers?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Is the outlier a data entry error?\n",
    "â”œâ”€ YES â†’ REMOVE\n",
    "â”‚   â””â”€ Example: Age = 150, Salary = 500000 (typo)\n",
    "â”‚\n",
    "â””â”€ NO â†’ Is the outlier a rare but valid event?\n",
    "    â”œâ”€ YES â†’ KEEP (but handle separately)\n",
    "    â”‚   â””â”€ Example: CEO salary in employee dataset\n",
    "    â”‚\n",
    "    â””â”€ NO â†’ Does it affect model performance?\n",
    "        â”œâ”€ YES â†’ REMOVE or TRANSFORM\n",
    "        â”‚   â””â”€ Example: Extreme values breaking linear regression\n",
    "        â”‚\n",
    "        â””â”€ NO â†’ KEEP\n",
    "            â””â”€ Example: Outlier in non-critical feature\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Remove** | Data entry errors, impossible values | â€¢ Removes noise<br>â€¢ Improves model accuracy<br>â€¢ Simple | â€¢ Loses information<br>â€¢ May remove valid rare events | Age = 150, Salary = 500000 |\n",
    "| **Cap/Clip** | Valid but extreme values | â€¢ Keeps data<br>â€¢ Reduces impact<br>â€¢ Preserves distribution | â€¢ Arbitrary threshold<br>â€¢ May hide important patterns | Cap salary at 99th percentile |\n",
    "| **Transform** | Skewed distributions | â€¢ Normalizes data<br>â€¢ Keeps all values<br>â€¢ Better for ML | â€¢ Changes interpretation<br>â€¢ More complex | Log transform for income |\n",
    "| **Keep** | Rare but valid events | â€¢ Preserves reality<br>â€¢ No information loss<br>â€¢ Important for analysis | â€¢ Can skew models<br>â€¢ May need special handling | CEO in employee dataset |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Duplicates | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
    "\n",
    "**Key Question**: Should I **REMOVE** all duplicates or **INVESTIGATE** first?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Are duplicates exact copies?\n",
    "â”œâ”€ YES â†’ REMOVE (keep first)\n",
    "â”‚   â””â”€ Why? No new information, wastes space\n",
    "â”‚\n",
    "â””â”€ NO â†’ Are duplicates near-duplicates (typos)?\n",
    "    â”œâ”€ YES â†’ FIX (merge or correct)\n",
    "    â”‚   â””â”€ Why? Same entity, different spelling\n",
    "    â”‚\n",
    "    â””â”€ NO â†’ Are duplicates valid (same person, different records)?\n",
    "        â””â”€ YES â†’ KEEP (but flag for analysis)\n",
    "            â””â”€ Why? Important information (e.g., repeat customers)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: E-commerce Dataset\n",
    "- **Missing values in \"price\"**: 20% missing\n",
    "  - **Decision**: IMPUTE with median (too much to remove, random missing)\n",
    "  - **Reason**: Random missing prices, median preserves distribution\n",
    "\n",
    "#### Example 2: Medical Dataset\n",
    "- **Outlier in \"age\"**: One patient age = 200\n",
    "  - **Decision**: REMOVE (impossible value)\n",
    "  - **Reason**: Data entry error, no one lives to 200\n",
    "\n",
    "#### Example 3: Customer Dataset\n",
    "- **Missing values in \"email\"**: 3% missing\n",
    "  - **Decision**: REMOVE (small percentage)\n",
    "  - **Reason**: Email is critical, can't impute, small loss acceptable\n",
    "\n",
    "#### Example 4: Sales Dataset\n",
    "- **Outlier in \"revenue\"**: One sale = $1,000,000 (normal range: $10-$1000)\n",
    "  - **Decision**: INVESTIGATE FIRST\n",
    "  - **Reason**: Could be valid (enterprise sale) or error (extra zero)\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Always investigate first** - Understand WHY data is missing/outlier/duplicate\n",
    "2. **Consider data loss** - Removing >10% of data is usually too much\n",
    "3. **Consider bias** - Systematic missing/outliers may indicate important patterns\n",
    "4. **Test both approaches** - Sometimes try removing AND imputing, compare results\n",
    "5. **Document decisions** - Write down WHY you chose each method (for reproducibility)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario**: You have a dataset with:\n",
    "- 8% missing values in \"income\" column\n",
    "- 2 outliers in \"age\" (ages 0 and 200)\n",
    "- 5 duplicate rows\n",
    "\n",
    "**Your task**: Decide what to do for each issue and explain why!\n",
    "\n",
    "**Answers**:\n",
    "1. **Income (8% missing)**: IMPUTE with median (too much to remove, likely random)\n",
    "2. **Age outliers**: REMOVE (impossible values - data entry errors)\n",
    "3. **Duplicates**: REMOVE (exact copies, no new information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.683569Z",
     "iopub.status.busy": "2026-01-17T16:18:53.683522Z",
     "iopub.status.idle": "2026-01-17T16:18:53.685829Z",
     "shell.execute_reply": "2026-01-17T16:18:53.685662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Practical Example: Decision-Making in Action\n",
      "Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ: Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø± ÙÙŠ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø©\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Scenario: Dataset with multiple issues\n",
      "   - 15% missing values in 'income' (too much to remove)\n",
      "   - 2% missing values in 'email' (can remove)\n",
      "   - 1 outlier: age = 200 (impossible, should remove)\n",
      "   - 3 duplicates (should remove)\n",
      "\n",
      "------------------------------------------------------------\n",
      "DECISION 1: Income missing (15%)\n",
      "------------------------------------------------------------\n",
      "   âŒ Can't remove: Would lose 15% of data (too much!)\n",
      "   âœ… Should impute: Use median (preserves distribution)\n",
      "   ğŸ“ Reason: Random missing, large percentage, numeric data\n",
      "\n",
      "------------------------------------------------------------\n",
      "DECISION 2: Email missing (2%)\n",
      "------------------------------------------------------------\n",
      "   âœ… Can remove: Only 2% loss (acceptable)\n",
      "   âŒ Can't impute: Email is unique, can't estimate\n",
      "   ğŸ“ Reason: Small percentage, critical field, can't fill\n",
      "\n",
      "------------------------------------------------------------\n",
      "DECISION 3: Age outlier (age = 200)\n",
      "------------------------------------------------------------\n",
      "   âœ… Should remove: Impossible value (data entry error)\n",
      "   âŒ Can't keep: Would break age-based models\n",
      "   ğŸ“ Reason: No human lives to 200, clearly an error\n",
      "\n",
      "------------------------------------------------------------\n",
      "DECISION 4: Duplicates (3 rows)\n",
      "------------------------------------------------------------\n",
      "   âœ… Should remove: Exact duplicates, no new information\n",
      "   ğŸ“ Reason: Wastes space, can bias models (same data counted twice)\n",
      "\n",
      "============================================================\n",
      "âœ… Decision Framework Applied Successfully!\n",
      "ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Practical Example: Comparing Different Cleaning Approaches\n",
    "# Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ: Ù…Ù‚Ø§Ø±Ù†Ø© Ø·Ø±Ù‚ Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Practical Example: Decision-Making in Action\")\n",
    "print(\"Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ: Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø± ÙÙŠ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a scenario with different data quality issues\n",
    "print(\"\\nğŸ“Š Scenario: Dataset with multiple issues\")\n",
    "print(\"   - 15% missing values in 'income' (too much to remove)\")\n",
    "print(\"   - 2% missing values in 'email' (can remove)\")\n",
    "print(\"   - 1 outlier: age = 200 (impossible, should remove)\")\n",
    "print(\"   - 3 duplicates (should remove)\")\n",
    "\n",
    "# Simulate the decision-making process\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 1: Income missing (15%)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   âŒ Can't remove: Would lose 15% of data (too much!)\")\n",
    "print(\"   âœ… Should impute: Use median (preserves distribution)\")\n",
    "print(\"   ğŸ“ Reason: Random missing, large percentage, numeric data\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 2: Email missing (2%)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   âœ… Can remove: Only 2% loss (acceptable)\")\n",
    "print(\"   âŒ Can't impute: Email is unique, can't estimate\")\n",
    "print(\"   ğŸ“ Reason: Small percentage, critical field, can't fill\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 3: Age outlier (age = 200)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   âœ… Should remove: Impossible value (data entry error)\")\n",
    "print(\"   âŒ Can't keep: Would break age-based models\")\n",
    "print(\"   ğŸ“ Reason: No human lives to 200, clearly an error\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 4: Duplicates (3 rows)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   âœ… Should remove: Exact duplicates, no new information\")\n",
    "print(\"   ğŸ“ Reason: Wastes space, can bias models (same data counted twice)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Decision Framework Applied Successfully!\")\n",
    "print(\"ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T16:18:53.686619Z",
     "iopub.status.busy": "2026-01-17T16:18:53.686553Z",
     "iopub.status.idle": "2026-01-17T16:18:53.688452Z",
     "shell.execute_reply": "2026-01-17T16:18:53.688268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Age converted to integer\n",
      "\n",
      "Data types after conversion:\n",
      "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\n",
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age              int64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert Age to int (optional - only if Age column exists and is float)\n",
    "\n",
    "# df_clean['Age'].round().astype(int)\n",
    "# - df_clean['Age']: Selects 'Age' column\n",
    "# - .round(): Rounds decimal values (e.g., 40.5 â†’ 41)\n",
    "# - .astype(int): Converts to integer type\n",
    "# Result: Converts float ages to integer ages\n",
    "# Note: This converts Age from float64 to int64 (removes decimal places)\n",
    "if 'Age' in df_clean.columns and df_clean['Age'].dtype == 'float64':\n",
    "    # Only convert if Age exists and is float (has NaN values were filled)\n",
    "    df_clean['Age'] = df_clean['Age'].round().astype(int)\n",
    "    print(\"\\nâœ… Age converted to integer\")\n",
    "else:\n",
    "    print(\"\\nâœ… Age column already has appropriate type\")\n",
    "\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(\"Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\")\n",
    "\n",
    "# df_clean.dtypes\n",
    "# - Shows data types after conversion\n",
    "# - Should show int64 for Age now (if conversion was performed)\n",
    "print(df_clean.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
