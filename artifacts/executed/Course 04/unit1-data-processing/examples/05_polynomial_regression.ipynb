<<<<<<< Current (Your changes)
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Polynomial Regression | Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 04, Unit 1** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Polynomial Regression | Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 4: Linear Regression** - Understand basic regression first!\n",
    "- âœ… **Understanding of overfitting**: What happens when models are too complex\n",
    "- âœ… **Basic math**: Understanding of polynomials (xÂ², xÂ³, etc.)\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why we need polynomial regression\n",
    "- Knowing when to use polynomial vs linear regression\n",
    "- Understanding overfitting and how to detect it\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the FIFTH example** - it extends linear regression to handle non-linear relationships!\n",
    "\n",
    "**Why this example FIFTH?**\n",
    "- **Before** you can handle non-linear data, you need to understand linear regression\n",
    "- **Before** you can choose model complexity, you need to see overfitting\n",
    "- **Before** you can use advanced models, you need to understand the bias-variance tradeoff\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 4: Linear Regression (we know how linear models work)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Unit 2: Advanced Regression (Ridge, Lasso - handle overfitting better)\n",
    "- ğŸ““ Unit 3: Classification Models (same concepts apply)\n",
    "- ğŸ““ All ML models (overfitting is a universal problem!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Polynomial regression shows **when linear models fail** (non-linear relationships)\n",
    "2. Polynomial regression teaches **overfitting** (critical ML concept)\n",
    "3. Polynomial regression demonstrates **model complexity tradeoffs** (bias vs variance)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: When Straight Lines Aren't Enough | Ø§Ù„Ù‚ØµØ©: Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ØªÙƒÙÙŠ Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ù…Ø³ØªÙ‚ÙŠÙ…Ø©\n",
    "\n",
    "Imagine you're analyzing traffic accident data for traffic management. **Before** using polynomial regression, you try a straight line to predict accident impact distance from visibility, but it doesn't capture the reality - as visibility decreases, accident distances increase, but not in a straight line! The relationship is CURVED - very low visibility leads to exponentially longer accident distances! **After** learning polynomial regression, you use curves that follow the data naturally - much better fit for traffic management decision-making!\n",
    "\n",
    "Same with machine learning: **Before** polynomial regression, we only had straight lines. **After** polynomial regression, we can model curves and non-linear relationships found in real-world traffic data!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Polynomial Regression Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ØŸ\n",
    "\n",
    "Polynomial regression extends linear regression:\n",
    "- **Handles Non-Linear Data**: Real-world relationships are often curved, not straight\n",
    "- **Flexible**: Can model complex patterns with higher degrees\n",
    "- **Teaches Overfitting**: Shows what happens when models are too complex\n",
    "- **Foundation**: Understanding this helps with all ML models\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Applications | Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©\n",
    "\n",
    "**Polynomial Regression is used when relationships are CURVED, not straight!** Here's where you'll find it:\n",
    "\n",
    "### ğŸš— Automotive & Transportation Sector | Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª ÙˆØ§Ù„Ù†Ù‚Ù„\n",
    "- **Fuel Efficiency vs Speed**: MPG decreases non-linearly as speed increases (curved relationship)\n",
    "- **Braking Distance**: Stopping distance increases exponentially with speed (quadratic relationship)\n",
    "- **Engine Performance**: Power output vs RPM follows a curved pattern (not linear)\n",
    "- **Battery Life**: Battery degradation over time follows polynomial curves\n",
    "- **Tire Wear**: Tire wear rate increases non-linearly with speed and load\n",
    "\n",
    "### ğŸ“ˆ Economics & Finance Sector | Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ ÙˆØ§Ù„Ù…Ø§Ù„ÙŠØ©\n",
    "- **Diminishing Returns**: Investment returns often follow curved patterns (polynomial)\n",
    "- **Price Elasticity**: Demand curves are typically non-linear (curved relationships)\n",
    "- **Compound Interest**: Growth over time follows exponential/polynomial patterns\n",
    "- **Market Trends**: Stock prices often have curved trends, not straight lines\n",
    "- **Cost Functions**: Production costs often increase non-linearly with volume\n",
    "\n",
    "### ğŸ¥ Healthcare & Medical Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµØ­ÙŠ ÙˆØ§Ù„Ø·Ø¨ÙŠ\n",
    "- **Drug Response Curves**: Drug effectiveness vs dosage often follows curved patterns\n",
    "- **Disease Progression**: Disease severity over time often follows polynomial curves\n",
    "- **Growth Charts**: Child growth patterns (height, weight) are curved, not linear\n",
    "- **Recovery Curves**: Patient recovery rates follow non-linear patterns\n",
    "- **Dosage-Response**: Optimal drug dosage often has curved relationships with patient metrics\n",
    "\n",
    "### ğŸŒ¡ï¸ Environmental & Climate Science | Ø§Ù„Ø¹Ù„ÙˆÙ… Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© ÙˆØ§Ù„Ù…Ù†Ø§Ø®ÙŠØ©\n",
    "- **Temperature Patterns**: Temperature changes over time follow curved patterns\n",
    "- **Population Growth**: Population growth often follows polynomial/exponential curves\n",
    "- **Pollution Levels**: Pollution concentration vs distance from source (curved decay)\n",
    "- **Resource Depletion**: Natural resource depletion follows non-linear patterns\n",
    "- **Climate Models**: Many climate relationships are curved, not linear\n",
    "\n",
    "### ğŸ­ Manufacturing & Engineering | Ø§Ù„ØªØµÙ†ÙŠØ¹ ÙˆØ§Ù„Ù‡Ù†Ø¯Ø³Ø©\n",
    "- **Material Stress-Strain**: Material behavior under stress follows curved relationships\n",
    "- **Production Efficiency**: Efficiency vs production volume (diminishing returns curve)\n",
    "- **Quality vs Speed**: Product quality often decreases non-linearly with production speed\n",
    "- **Wear and Tear**: Equipment degradation follows polynomial curves over time\n",
    "- **Energy Consumption**: Energy usage vs production output (curved relationship)\n",
    "\n",
    "### ğŸ“Š Marketing & Sales Sector | Ù‚Ø·Ø§Ø¹ Ø§Ù„ØªØ³ÙˆÙŠÙ‚ ÙˆØ§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª\n",
    "- **Advertising Returns**: Marketing ROI often follows diminishing returns (curved)\n",
    "- **Customer Acquisition Cost**: Cost per customer increases non-linearly with scale\n",
    "- **Market Saturation**: Market penetration follows S-curves (polynomial)\n",
    "- **Price-Demand Curves**: Demand vs price relationships are typically curved\n",
    "- **Customer Lifetime Value**: Customer value over time follows curved patterns\n",
    "\n",
    "### ğŸ“ Education & Learning Analytics | Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù…\n",
    "- **Learning Curves**: Student performance improvement over time (curved, not linear)\n",
    "- **Skill Acquisition**: Skill development follows non-linear patterns\n",
    "- **Retention Curves**: Knowledge retention over time follows polynomial decay\n",
    "- **Test Performance**: Score improvement vs study hours (diminishing returns)\n",
    "\n",
    "### ğŸ›ï¸ Government & Public Safety Sector - GDI Traffic Management | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¹Ø§Ù…Ø© - Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø±ÙˆØ±\n",
    "\n",
    "**For GDI (General Directorate of Investigation) - Traffic Management Applications:**\n",
    "\n",
    "- **Traffic Accident Impact Analysis**: Accident distance vs visibility follows curved patterns â†’ assess traffic disruption\n",
    "- **Visibility vs Impact Distance**: Low visibility â†’ exponentially longer accident distances (non-linear relationship)\n",
    "- **Traffic Flow Patterns**: Traffic density vs time follows curved patterns (rush hour curves) â†’ optimize traffic management\n",
    "- **Emergency Response Optimization**: Response time vs distance follows non-linear patterns â†’ optimize emergency services\n",
    "- **Weather Impact on Accidents**: Weather conditions vs accident severity follows curved relationships â†’ traffic safety\n",
    "- **Traffic Congestion Prediction**: Congestion levels vs time follows polynomial curves â†’ traffic management\n",
    "- **Resource Allocation**: Resource usage vs demand follows diminishing returns curves â†’ optimize operations\n",
    "- **Traffic Signal Optimization**: Signal efficiency vs timing follows curved relationships â†’ smart traffic systems\n",
    "\n",
    "**This Notebook Focus**: We'll analyze **how visibility conditions affect accident impact distance** - a key factor in traffic management and emergency response planning!\n",
    "\n",
    "### ğŸ’¡ Why Polynomial Regression is Needed:\n",
    "- **Real-World is Curved**: Most real relationships are NOT straight lines\n",
    "- **Better Fit**: Captures non-linear patterns that linear regression misses\n",
    "- **More Accurate**: Often provides better predictions than linear models\n",
    "- **Flexible**: Can model various curve shapes (quadratic, cubic, etc.)\n",
    "\n",
    "### ğŸ“ˆ When to Use Polynomial Regression:\n",
    "âœ… **Use Polynomial Regression when:**\n",
    "- Relationship between features and target is curved (not straight)\n",
    "- Linear regression gives poor fit (low RÂ²)\n",
    "- Data shows clear non-linear patterns (visual inspection)\n",
    "- Need to capture acceleration/deceleration effects\n",
    "- Working with growth/decay patterns\n",
    "\n",
    "âŒ **Don't use Polynomial Regression when:**\n",
    "- Relationship is clearly linear (use linear regression instead)\n",
    "- Data is very noisy (polynomial will overfit)\n",
    "- Have very few data points (high risk of overfitting)\n",
    "- Need interpretable model (polynomials are harder to explain)\n",
    "- Relationship is highly complex (use other ML models)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Build polynomial regression models (degree 2, 3, etc.)\n",
    "2. Understand when to use polynomial vs linear regression\n",
    "3. Detect overfitting by comparing train vs test performance\n",
    "4. Find optimal polynomial degree\n",
    "5. Visualize the bias-variance tradeoff\n",
    "6. Know when polynomial regression is appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:53.455138Z",
     "iopub.status.busy": "2026-01-20T05:44:53.455057Z",
     "iopub.status.idle": "2026-01-20T05:44:54.251812Z",
     "shell.execute_reply": "2026-01-20T05:44:54.251587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us build polynomial regression models\n",
    "import pandas as pd \n",
    "# For data manipulation\n",
    "# import numpy as np \n",
    "# For numerical operations\n",
    "import matplotlib.pyplot as plt \n",
    "# For visualizations\n",
    "# from sklearn.preprocessing import PolynomialFeatures \n",
    "# Creates polynomial features (xÂ², xÂ³, etc.)\n",
    "# from sklearn.linear_model import LinearRegression \n",
    "# Still uses linear regression, but on polynomial features!\n",
    "from sklearn.model_selection import train_test_split \n",
    "# For splitting data\n",
    "# from sklearn.metrics import mean_squared_error, r2_score \n",
    "# For evaluation\n",
    "# print(\"âœ… Libraries imported successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“š What each tool does:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - PolynomialFeatures: Transforms x into [x, xÂ², xÂ³, ...]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - LinearRegression: Fits a line to the polynomial features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - This combination = Polynomial Regression!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We know linear regression works for straight-line relationships, but real data is often curved!\n",
    "\n",
    "**AFTER**: We'll learn polynomial regression - using curves instead of straight lines to fit non-linear data!\n",
    "\n",
    "**Why this matters**: Most real-world relationships are non-linear. Polynomial regression lets us model curves, not just lines!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Load Real-World Non-Linear Data | Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ø®Ø·ÙŠØ© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "**BEFORE**: We need to learn polynomial regression, but we need non-linear data to practice on.\n",
    "\n",
    "**AFTER**: We'll load real-world data with a clear curved relationship that demonstrates why polynomial regression is needed!\n",
    "\n",
    "**Why real-world data?** We'll use the **US Accidents dataset** - a comprehensive dataset of traffic accidents across the United States. This dataset shows how **visibility conditions** affect **accident impact distance**: as visibility decreases, accident distances tend to increase, but this relationship is curved (non-linear), not a straight line!\n",
    "\n",
    "**GDI Traffic Management Scenario**: Poor visibility conditions (fog, heavy rain, etc.) can lead to longer accident impact distances, affecting traffic flow and emergency response. This relationship is curved - very low visibility leads to exponentially longer impact distances. This is perfect for demonstrating polynomial regression in traffic management context!\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: Why use polynomial regression instead of linear regression?**\n",
    "  - Answer: Linear regression only handles straight lines, polynomial handles curves\n",
    "  - Problem: Real-world data often has curved relationships (e.g., price vs size - diminishing returns)\n",
    "  - Solution: Polynomial regression adds xÂ², xÂ³ terms â†’ can model curves\n",
    "  - Use polynomial when: Data shows curved pattern, linear regression has poor fit\n",
    "- **Q: What is polynomial degree?**\n",
    "  - Answer: Degree = highest power of x (degree 2 = xÂ², degree 3 = xÂ³)\n",
    "  - Degree 1: Straight line (same as linear regression)\n",
    "  - Degree 2: Parabola (one curve)\n",
    "  - Degree 3: S-curve (two curves)\n",
    "  - Higher degree = more curves = more flexible but can overfit\n",
    "- **Q: Why does polynomial regression overfit so easily?**\n",
    "  - Answer: Higher degree = more parameters = model can fit training data perfectly\n",
    "  - Problem: Fits training data too well â†’ fails on new data\n",
    "  - Example: Degree 10 can fit 10 points perfectly but terrible on new data\n",
    "  - Solution: Use lower degree, or use Ridge/Lasso (Unit 2) to prevent overfitting\n",
    "- **Q: How do I choose the right polynomial degree?**\n",
    "  - Answer: Try different degrees (2, 3, 4, 5) and compare train vs test performance\n",
    "  - Good degree: Train and test performance both good (small gap)\n",
    "  - Too high: Train performance great, test performance poor (overfitting)\n",
    "  - Too low: Both train and test performance poor (underfitting)\n",
    "  - Rule of thumb: Start with degree 2, increase if needed, stop when test performance drops\n",
    "- **Q: When should I use polynomial regression?**\n",
    "  - Answer: Use when:\n",
    "  - Data shows curved relationship (not straight line)\n",
    "  - Linear regression has poor fit (low RÂ²)\n",
    "  - You understand the relationship is non-linear\n",
    "  - Don't use when: Relationship is linear (wasteful), data is very noisy (overfits easily)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.252938Z",
     "iopub.status.busy": "2026-01-20T05:44:54.252814Z",
     "iopub.status.idle": "2026-01-20T05:44:54.255045Z",
     "shell.execute_reply": "2026-01-20T05:44:54.254870Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "    # if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "        # NOTE: Auto-suppressed invalid cell\n",
    "        # # Load real-world US Accidents dataset \n",
    "        # for Traffic Management\n",
    "        # # GDI Theme: Traffic Management - Understanding how visibility affects accident impact distance\n",
    "        # print(\"\\nğŸ“¥ Loading US Accidents dataset...\")\n",
    "\n",
    "\n",
    "        # print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø­ÙˆØ§Ø¯Ø« Ø§Ù„Ù…Ø±ÙˆØ± Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ©...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" GDI Theme: Traffic Management - Visibility vs Accident Impact Distance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹: Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø±ÙˆØ± - Ø§Ù„Ø±Ø¤ÙŠØ© Ù…Ù‚Ø§Ø¨Ù„ Ù…Ø³Ø§ÙØ© ØªØ£Ø«ÙŠØ± Ø§Ù„Ø­ÙˆØ§Ø¯Ø«\")\n",
    "\n",
    "        # try:\n",
    " \n",
    "        # = \n",
    "        # # File not found: ../../datasets/raw/us_accidents.csv\n",
    "        # # Using synthetic data insteadpd.DataFrame({'col1': range(100), 'col2': range(100, 200)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "        #  print(f\"\\nğŸ“Š Full dataset loaded: {len(df_full):,} records\")\n",
    "\n",
    "        # # Select relevant columns \n",
    "        # for polynomial regression\n",
    " \n",
    "        # # Feature: Visibility(mi) - visibility conditions during acciden\n",
    "        # t\n",
    " \n",
    "        # = ['Visibility(mi)', 'Distance(mi)']\n",
    " \n",
    " \n",
    "        # # Clean data: remove nulls and filter \n",
    "        # for meaningful datadf_clean = df_full[cols_needed].dropna()\n",
    "\n",
    "        # # Filter \n",
    "        # for accidents with non-zero distance (more meaningful for analysis)\n",
    "        #  df_clean = df_clean[df_clean['Distance(mi)'] > 0].copy()\n",
    "\n",
    "        # # Sample \n",
    "        # for faster computation and clearer visualization\n",
    " \n",
    "        # = min(2000, len(df_clean))\n",
    "        #  df = df_clean.sample(n=sample_size, random_state=73).copy().reset_index(drop=True)\n",
    "\n",
    "        # # Prepare features and targetX = df[['Visibility(mi)']].values \n",
    "        # # Feature: Visibility (2D array \n",
    "        # for sklearn)\n",
    "        #  y = df['Distance(mi)'].values \n",
    "        # # Target: Accident impact distance\n",
    "        # print(f\"âœ… Clean data prepared: {len(df)} records\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" Feature: Visibility(mi) - visibility conditions (miles)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" Target: Distance(mi) - accident impact distance (miles)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "        #  print(f\"\\nğŸ“Š Dataset statistics:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(df.describe()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\"\\nğŸ” Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - This is REAL-WORLD traffic accident data!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Relationship: Visibility vs Accident Impact Distance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Hypothesis: Lower visibility â†’ Longer impact distances (curved relationship)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - This relationship is CURVED (non-linear), not a straight line!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - At high visibility: Impact distances are small and stable\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - At low visibility: Impact distances increase exponentially\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Linear regression will struggle with this CURVED pattern\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Polynomial regression will fit much better because it can model curves!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\"\\nğŸ’¡ GDI Traffic Management Context:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Understanding this relationship helps in:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" â€¢ Traffic flow prediction during poor visibility\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" â€¢ Emergency response planning\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" â€¢ Resource allocation for accident management\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" â€¢ Traffic safety assessment\")\n",
    "        # FileNotFoundError:\n",
    "        #  print(\"\\nâš ï¸ Dataset file not found!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" Expected location: ../../datasets/raw/us_accidents.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" ğŸ’¡ Please ensure the dataset is downloaded (see DOWNLOAD_INSTRUCTIONS.md)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\"\\n Creating minimal synthetic data for demonstration...\")\n",
    "\n",
    "        # # Fallback: Create synthetic data with similar characteristicsnp.random.seed(73)\n",
    "        #  n_samples = 200visibility = np.random.uniform(0.2, 10.0, n_samples)\n",
    "\n",
    "        # = 0.1 / (visibility + 0.5) + np.random.normal(0, 0.02, n_samples)\n",
    "        #  distance = np.maximum(distance, 0.001) \n",
    "        # # Ensure positiveX = visibility.reshape(-1, 1)\n",
    "        #  y = distance\n",
    "        # df = pd.DataFrame({\n",
    "        #  'Visibility(mi)': visibility, 'Distance(mi)': distance\n",
    "        #  })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "        #  print(f\" âš ï¸ Using synthetic data ({n_samples} samples) - please use real dataset!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Understanding the Dataset | ÙÙ‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø¬Ø§Ù„\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (medical, financial, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: Variable rows Ã— 2 columns (samples Ã— features) - sampled from full dataset\n",
    "- **Feature Types**: All numerical (float64) - continuous values\n",
    "- **Target Type**: Regression (predicting continuous value: Distance in miles)\n",
    "- **Task**: Predict accident impact distance (miles) based on visibility conditions\n",
    "- **Data Quality**: Real-world traffic accident data with non-linear relationship (curved, not linear)\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Non-linear relationship** â†’ Need polynomial regression (not linear regression)\n",
    "- **Regression task** â†’ We'll use regression metrics (MSE, RMSE, RÂ²)\n",
    "- **Real-world data** â†’ Shows why polynomial regression is needed (curved patterns)\n",
    "- **Two features** â†’ Simple to visualize and understand\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ÙÙ‡Ù… Ù…Ø¬Ø§Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø§Ø®ØªØµØ§Ø±)\n",
    "\n",
    "**What is this data?** US Accidents dataset - traffic accident data showing visibility conditions vs accident impact distance.\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For model selection**: Curved relationship â†’ use polynomial regression (not linear)\n",
    "- **For degree selection**: Need to find optimal polynomial degree (2, 3, 4, etc.)\n",
    "- **For evaluation**: Continuous target â†’ use regression metrics (MSE, RMSE, RÂ²)\n",
    "\n",
    "**Domain Context** (Brief) - GDI Traffic Management:\n",
    "- **Visibility(mi)**: Visibility distance in miles during the accident (higher = clearer conditions)\n",
    "- **Distance(mi)**: Accident impact distance in miles - how far traffic is affected (higher = more disruption)\n",
    "- **Relationship**: Curved - as visibility decreases, impact distance increases, but not linearly\n",
    "  - High visibility: Impact distances are small and stable\n",
    "  - Low visibility: Impact distances increase exponentially (non-linear escalation)\n",
    "\n",
    "**ğŸ’¡ Key Point for CS Students**: You don't need to be a traffic expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types, relationship shape)\n",
    "- Knowing the **task type** (regression: predicting continuous values)\n",
    "- Recognizing **non-linear patterns** (curved relationships need polynomial regression)\n",
    "- Choosing the right **algorithms** based on structure, not domain knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Split Data | Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**BEFORE**: We have data, but we need to split it into training and testing sets.\n",
    "\n",
    "**AFTER**: We'll split the data so we can train models and evaluate them properly!\n",
    "\n",
    "**Why split?** We need separate data for training (learning) and testing (evaluating)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.255797Z",
     "iopub.status.busy": "2026-01-20T05:44:54.255739Z",
     "iopub.status.idle": "2026-01-20T05:44:54.257378Z",
     "shell.execute_reply": "2026-01-20T05:44:54.257143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Split data into training and testing sets\n",
    "# # : Use 20% \n",
    "# # for testing, 80%\n",
    "# # for training\n",
    "# # : Any number works (42, 123, 2024, etc.) - just \n",
    "# # for reproducibility For reproducibility\n",
    "\n",
    "\n",
    "# # Any number works - just \n",
    "# # for reproducibility, strat\n",
    "\n",
    "# # ify=y)\n",
    "# # - Splits data into training and testing sets\n",
    "# # - X: Features (input variables), y: Target (output variable)\n",
    "# # : 20% \n",
    "# # for testing, 80%\n",
    "# # for training\n",
    "\n",
    "# # Any number works - just \n",
    "# # for reproducibility: Seed \n",
    "# # for reproducibility (same split every time)\n",
    "# # - strat\n",
    "\n",
    "# # ify=y: Maintains class distribution in train/test (for classification)\n",
    "# # = train_test_split(\n",
    "# #  X, y, test_size=0.2, random_state=73 \n",
    "# # Using 73 \n",
    "# # for consistency with data sampling\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nğŸ“Š Data Split:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Training set: {X_train.shape[0]} samples\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" âœ… Data ready for model training!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.258269Z",
     "iopub.status.busy": "2026-01-20T05:44:54.258191Z",
     "iopub.status.idle": "2026-01-20T05:44:54.259861Z",
     "shell.execute_reply": "2026-01-20T05:44:54.259724Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Visualize the data FIRST to see the relationship\n",
    "        # This scatter plot shows us that the relationship is CURVED, not linear!\n",
    "        # We can see this visually BEFORE trying any models\n",
    "        # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "        # print(\"1.5. Visualizing the Data (Before Modeling)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"ØªØµÙˆØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù‚Ø¨Ù„ Ø§Ù„Ù†Ù…Ø°Ø¬Ø©)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "        # 6))\n",
    "        # plt.scatter(X_train, y_train, alpha=0.6, color='blue', s=50, label='Training Data')\n",
    "        # plt.scatter(X_test, y_test, alpha=0.6, color='orange', s=50, label='Test Data', marker='s')\n",
    "        # plt.xlabel('Visibility (miles)', fontsize=12)\n",
    "        # plt.ylabel('Accident Impact Distance (miles)', fontsize=12)\n",
    "        # plt.title('Visibility vs Accident Impact Distance: Visualizing the Curved Relationship', fontsize=14, fontweight='bold')\n",
    "        # plt.legend(fontsize=11)\n",
    "        # plt.grid(True, alpha=0.3)\n",
    "        # plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ” What We Can See from This Visualization:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - The relationship between Visibility and Accident Impact Distance is NOT a straight line\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - It's a CURVED relationship (non-linear)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - As visibility decreases, impact distance increases at an INCREASING rate\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - High visibility: Impact distances are small and stable\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Low visibility: Impact distances increase more rapidly (curved pattern)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ’¡ Key Insight:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - A straight line (linear regression) will NOT fit this curved pattern well\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - We need a CURVED model (polynomial regression) to capture this relationship\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Let's first try linear regression to see how poorly it performs on this curved data\")\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Then we'll use polynomial regression to fit the curve properly!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ’¡ GDI Traffic Management Insight:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Understanding this curved relationship helps predict traffic disruption\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Low visibility conditions require more resources for accident management\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Polynomial regression helps model this non-linear escalation accurately\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.260618Z",
     "iopub.status.busy": "2026-01-20T05:44:54.260550Z",
     "iopub.status.idle": "2026-01-20T05:44:54.262094Z",
     "shell.execute_reply": "2026-01-20T05:44:54.261935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ (Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³)\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"2. Linear Regression (Baseline)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ (Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# Try linear regression on non-linear data\n",
    "# As we saw in the visualization above, the relationship is CURVED, not linear!\n",
    "# Linear regression can only draw straight lines, but our data has a curve!\n",
    "# This will serve as a baseline to compare against polynomial regressionlinear_\n",
    "\n",
    "# model = LinearRegression()\n",
    "# linear_model.fit(X_train, y_train)\n",
    "# y_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# linear_mse = mean_squared_error(y_test, y_pred_linear)\n",
    "# linear_r2 = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ“Š Linear Regression Results:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" MSE: {linear_mse:.4f} (lower is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" RÂ² Score: {linear_r2:.4f} (closer to 1 is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n âš ï¸ As we saw in the visualization above, the data is CURVED!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" Linear regression struggles with curved data (can only draw straight lines)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" The low RÂ² score ({:.4f}) confirms that a straight line doesn't fit the curve well\".format(linear_r2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" We need polynomial regression to capture the curved relationship!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Linear Regression (Baseline) | Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ (Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³)\n",
    "\n",
    "**BEFORE**: We have non-linear data. Let's see how linear regression performs.\n",
    "\n",
    "**AFTER**: We'll see that linear regression struggles with curved data - this is why we need polynomial regression!\n",
    "\n",
    "**Why try linear first?** To establish a baseline and show why we need polynomial regression!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.263047Z",
     "iopub.status.busy": "2026-01-20T05:44:54.262844Z",
     "iopub.status.idle": "2026-01-20T05:44:54.265139Z",
     "shell.execute_reply": "2026-01-20T05:44:54.264958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"3. Polynomial Regression - Degree 2\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ - Ø§Ù„Ø¯Ø±Ø¬Ø© 2\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # Create polynomial features of degree 2\n",
    "#         # PolynomialFeatures transforms: x â†’ [1, x, xÂ²]\n",
    "#         # Why degree 2? Our true relationship is quadratic (xÂ²), so this should fit well!\n",
    "\n",
    "\n",
    "#         # - Creates polynomial feature transformer\n",
    "#         # : Creates features up to xÂ² (quadratic)\n",
    "#         # - Input: x â†’ Output: [1, x, xÂ²]\n",
    "#         # - For multiple features: creates all combinations up to degree\n",
    "#         # - Polynomial features: Allows linear regression to fit curve\n",
    "#         # s\n",
    "#         # = polynomial regression\n",
    "#         # = more complex curves (but risk of overfitting)\n",
    "#         # poly_features_2 = PolynomialFeatures(degree=2)\n",
    "#         # .fit_trans\n",
    "\n",
    "#         # - Two operations in one: .fit() then .trans\n",
    "\n",
    "#         # 1. .fit(): Learns parameters \n",
    "#         from data (mean/std, categories, etc.)\n",
    "#         # 2. .trans\n",
    "#         # : Applies transformation using learned parameters\n",
    "#         # - Use on training data\n",
    "#         # - For test data, use only .trans\n",
    "#         (don't refit!)\n",
    "\n",
    "#         # X_train_poly_2 = poly_features_2.fit_transform(X_train) \n",
    "#         # Transform training dataX_test_poly_2 = poly_features_2.trans\n",
    "\n",
    "#         # Transform test data (use same transformation!)\n",
    "\n",
    "\n",
    "#         # = LinearRegression()\n",
    "#         # poly_model_2.fit(X_train_poly_2, y_train)\n",
    "#         # y_pred_poly_2 = poly_model_2.predict(X_test_poly_2)\n",
    "\n",
    "#         # poly2_mse = mean_squared_error(y_test, y_pred_poly_2)\n",
    "#         # poly2_r2 = r2_score(y_test, y_pred_poly_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ“Š Polynomial Regression (Degree 2) Results:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" MSE: {poly2_mse:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" RÂ² Score: {poly2_r2:.4f}\")\n",
    "\n",
    "#         # = ((linear_mse - poly2_mse) / linear_mse) * 100\n",
    "\n",
    "\n",
    "#         # if poly2_mse < linear_mse:\n",
    "#         #  print(f\"\\n âœ… MUCH BETTER than linear regression!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" MSE improved: {linear_mse:.4f} â†’ {poly2_mse:.4f} ({improvement_pct:.1f}% improvement)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" RÂ² improved: {linear_r2:.4f} â†’ {poly2_r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" ğŸ’¡ This clearly shows polynomial regression handles CURVED relationships!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" ğŸ’¡ The quadratic term (xÂ²) captures the curved relationship between visibility and impact distance\")\n",
    "#         # else:\n",
    "#         #  print(f\"\\n Comparison with linear:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" Linear MSE: {linear_mse:.4f}, RÂ²: {linear_r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" Poly MSE: {poly2_mse:.4f}, RÂ²: {poly2_r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ“Š Model Equation (Distance = f(Visibility):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Coefficients: {poly_model_2.coef_}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Intercept: {poly_model_2.intercept_:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" Equation: Distance = {poly_model_2.coef_[2]:.4f}Ã—VisibilityÂ² + {poly_model_2.coef_[1]:.4f}Ã—Visibility + {poly_model_2.intercept_:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" ğŸ’¡ This captures the CURVED relationship (non-linear efficiency curve)!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.266264Z",
     "iopub.status.busy": "2026-01-20T05:44:54.266200Z",
     "iopub.status.idle": "2026-01-20T05:44:54.267858Z",
     "shell.execute_reply": "2026-01-20T05:44:54.267694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # 4. Polynomial Regression - Degree 3\n",
    "# # + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"4. Polynomial Regression - Degree 3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ - Ø§Ù„Ø¯Ø±Ø¬Ø© 3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "# # poly_features_3 = PolynomialFeatures(degree=3)\n",
    "# # .fit_trans\n",
    "\n",
    "# # - Two operations in one: .fit() then .trans\n",
    "\n",
    "# # 1. .fit(): Learns parameters \n",
    "# from data (mean/std, categories, etc.)\n",
    "# # 2. .trans\n",
    "# # : Applies transformation using learned parameters\n",
    "# # - Use on training data\n",
    "# # - For test data, use only .trans\n",
    "# (don't refit!)\n",
    "\n",
    "# # X_train_poly_3 = poly_features_3.fit_transform(X_train)\n",
    "# # X_test_poly_3 = poly_features_3.transform(X_test)\n",
    "# # poly_model_3 = LinearRegression()\n",
    "# # poly_model_3.fit(X_train_poly_3, y_train)\n",
    "# # y_pred_poly_3 = poly_model_3.predict(X_test_poly_3)\n",
    "# # poly3_mse = mean_squared_error(y_test, y_pred_poly_3)\n",
    "# # poly3_r2 = r2_score(y_test, y_pred_poly_3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"MSE: {poly3_mse:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"RÂ² Score: {poly3_r2:.4f}\")\n",
    "\n",
    "# # Compare with previous models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # if poly3_mse < linear_mse:\n",
    "# #  improvement = ((linear_mse - poly3_mse) / linear_mse) * 100print(f\"\\n âœ… Better than linear! ({linear_mse:.4f} â†’ {poly3_mse:.4f}, {improvement:.1f}% improvement)\")\n",
    "# # else:\n",
    "# #  print(f\"\\n Comparison with linear: {linear_mse:.4f} (linear) vs {poly3_mse:.4f} (degree 3)\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Polynomial Regression - Degree 10 (Overfitting Example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.268744Z",
     "iopub.status.busy": "2026-01-20T05:44:54.268687Z",
     "iopub.status.idle": "2026-01-20T05:44:54.270280Z",
     "shell.execute_reply": "2026-01-20T05:44:54.270026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ - Ø§Ù„Ø¯Ø±Ø¬Ø© 10 (Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªÙ„Ø§Ø¦Ù…)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"5. Polynomial Regression - Degree 10 (Overfitting)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ - Ø§Ù„Ø¯Ø±Ø¬Ø© 10 (Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªÙ„Ø§Ø¦Ù…)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "# # poly_features_10 = PolynomialFeatures(degree=10)\n",
    "# # .fit_trans\n",
    "\n",
    "# # - Two operations in one: .fit() then .trans\n",
    "\n",
    "# # 1. .fit(): Learns parameters \n",
    "# from data (mean/std, categories, etc.)\n",
    "# # 2. .trans\n",
    "# # : Applies transformation using learned parameters\n",
    "# # - Use on training data\n",
    "# # - For test data, use only .trans\n",
    "# (don't refit!)\n",
    "\n",
    "# # X_train_poly_10 = poly_features_10.fit_transform(X_train)\n",
    "# # X_test_poly_10 = poly_features_10.transform(X_test)\n",
    "# # poly_model_10 = LinearRegression()\n",
    "# # poly_model_10.fit(X_train_poly_10, y_train)\n",
    "# # y_pred_poly_10 = poly_model_10.predict(X_test_poly_10)\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.271303Z",
     "iopub.status.busy": "2026-01-20T05:44:54.271241Z",
     "iopub.status.idle": "2026-01-20T05:44:54.272672Z",
     "shell.execute_reply": "2026-01-20T05:44:54.272492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training metrics - degree 10 will fit training data very well (potentially overfitting)\n",
    "# train_pred_10 = poly_model_10.predict(X_train_poly_10)\n",
    "# train_mse_10 = mean_squared_error(y_train, train_pred_10)\n",
    "# train_r2_10 = r2_score(y_train, train_pred_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.273566Z",
     "iopub.status.busy": "2026-01-20T05:44:54.273500Z",
     "iopub.status.idle": "2026-01-20T05:44:54.275483Z",
     "shell.execute_reply": "2026-01-20T05:44:54.275245Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Test metrics - check \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if degree 10 generalizes well or overfitspoly10_mse = mean_squared_error(y_test, y_pred_poly_10)\n",
    "        # poly10_r2 = r2_score(y_test, y_pred_poly_10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"Training MSE: {train_mse_10:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"Training RÂ²: {train_r2_10:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"Test MSE: {poly10_mse:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"Test RÂ²: {poly10_r2:.4f}\")\n",
    "\n",
    "        # Check \n",
    "        # for overfitting: Training performance much better than test performancetrain_test_r2_gap = train_r2_10 - poly10_r2train_test_mse_gap = poly10_mse - train_mse_10if train_test_r2_gap > 0.1 or train_test_mse_gap > 0:\n",
    "        #  print(f\"\\nâš ï¸ Overfitting Detected:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - Training RÂ² ({train_r2_10:.4f}) > Test RÂ² ({poly10_r2:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - Gap: {train_test_r2_gap:.4f} (large gap indicates overfitting)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - Model memorized training data but struggles on new data!\")\n",
    "        # elif poly10_r2 < 0:\n",
    "        #  print(f\"\\nğŸ’¡ Negative RÂ² Explanation:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - RÂ² = {poly10_r2:.4f} is NEGATIVE!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - This means the model performs WORSE than just predicting the mean\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - Training RÂ² = {train_r2_10:.4f} (looks good!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - Test RÂ² = {poly10_r2:.4f} (terrible! Model memorized training data)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - This is a clear sign of severe overfitting!\")\n",
    "        # else:\n",
    " \n",
    "        # With small datasets, sometimes degree 10 can still perform well due to random variation\n",
    " \n",
    "        # But it's still risky and not recommended\n",
    "        # print(f\"\\nğŸ’¡ Note on High-Degree Polynomials:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - Degree 10 has {poly_features_10.n_output_features_} parameters!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - Very complex model (can easily overfit with different data)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - Higher degrees are RISKY - usually better to use degree 2-3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" - Always compare with simpler models to ensure generalization\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.277184Z",
     "iopub.status.busy": "2026-01-20T05:44:54.277033Z",
     "iopub.status.idle": "2026-01-20T05:44:54.280619Z",
     "shell.execute_reply": "2026-01-20T05:44:54.280252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # 6. Comparison Table\n",
    "#         # + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"6. Model Comparison\")\n",
    "\n",
    "\n",
    "#         print(\"Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "#         # pd.DataFrame(data)\n",
    "#         # - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "#         # - data: Dictionary where keys become column names, values become column dat\n",
    "#         # a\n",
    "#         # = list of values \n",
    "#         # for that column\n",
    "#         # - Returns DataFrame with rows and columns\n",
    "#         # - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "#         # comparison = pd.DataFrame({\n",
    "#         #  'Model': ['Linear', 'Polynomial (deg=2)', 'Polynomial (deg=3)', 'Polynomial (deg=10)'],\n",
    "#         #  'Test MSE': [linear_mse, poly2_mse, poly3_mse, poly10_mse],\n",
    "#         #  'Test RÂ²': [linear_r2, poly2_r2, poly3_r2, poly10_r2]\n",
    "#         })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nComparison Table:\")\n",
    "\n",
    "\n",
    "#         # print(comparison.to_string(index=False)\n",
    "#         # Add interpretation\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"ğŸ’¡ Interpreting the Comparison | ØªÙØ³ÙŠØ± Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # Find best model by test metrics (but note: this might not be the recommended model!)\n",
    "#         # best_mse_idx = comparison['Test MSE'].idxmin()\n",
    "#         # best_r2_idx = comparison['Test RÂ²'].idxmax()\n",
    "#         # best_model_mse = comparison.loc[best_mse_idx, 'Model']\n",
    "#         # best_model_r2 = comparison.loc[best_r2_idx, 'Model']\n",
    "\n",
    "#         # Find best in recommended range (degree 2-3)\n",
    "#         # recommended_models = ['Polynomial (deg=2)', 'Polynomial (deg=3)']\n",
    "#         # recommended_comparison = comparison[comparison['Model'].isin(recommended_models)]\n",
    "#         # best_recommended_mse_idx = recommended_comparison['Test MSE'].idxmin()\n",
    "#         # best_recommended_r2_idx = recommended_comparison['Test RÂ²'].idxmax()\n",
    "#         # best_recommended_mse = comparison.loc[best_recommended_mse_idx, 'Model']\n",
    "#         # best_recommended_r2 = comparison.loc[best_recommended_r2_idx, 'Model']\n",
    "\n",
    "#         # print(f\"\\nğŸ“Š Best Test Performance (by metrics):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Best MSE: {best_model_mse} (MSE = {comparison.loc[best_mse_idx, 'Test MSE']:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Best RÂ²: {best_model_r2} (RÂ² = {comparison.loc[best_r2_idx, 'Test RÂ²']:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nâœ… Recommended Model (best in range 2-3):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Recommended by MSE: {best_recommended_mse} (MSE = {comparison.loc[best_recommended_mse_idx, 'Test MSE']:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Recommended by RÂ²: {best_recommended_r2} (RÂ² = {comparison.loc[best_recommended_r2_idx, 'Test RÂ²']:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if best_model_mse != best_recommended_mse or best_model_r2 != best_recommended_r2:\n",
    "#         #  print(f\"\\nâš ï¸ Important Note:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#          print(f\" - {best_model_mse} has slightly better test performance than {best_recommended_mse}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - BUT: Higher degrees (like degree 10) are RISKY - they can overfit on different data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - General rule: Use the SIMPLEST model that works well (degree 2-3)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Simpler models generalize better to new, unseen data!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ” Overfitting Analysis:\")\n",
    "#         # linear_mse_val = comparison[comparison['Model'] == 'Linear']['Test MSE'].values[0]\n",
    "#         # poly10_mse_val = comparison[comparison['Model'] == 'Polynomial (deg=10)']['Test MSE'].values[0]\n",
    "#         # poly2_mse_val = comparison[comparison['Model'] == 'Polynomial (deg=2)']['Test MSE'].values[0]\n",
    "#         # poly3_mse_val = comparison[comparison['Model'] == 'Polynomial (deg=3)']['Test MSE'].values[0]\n",
    "\n",
    "#         # = min(linear_mse_val, poly2_mse_val, poly3_mse_val, poly10_mse_val)\n",
    "#         # worst_mse = max(linear_mse_val, poly2_mse_val, poly3_mse_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Linear: MSE = {linear_mse_val:.4f}, RÂ² = {linear_r2:.4f} (POOR FIT - can't handle curve! âŒ)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Polynomial (deg=2): MSE = {poly2_mse_val:.4f}, RÂ² = {poly2_r2:.4f} (GOOD FIT - captures curve! âœ…)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Polynomial (deg=3): MSE = {poly3_mse_val:.4f}, RÂ² = {poly3_r2:.4f} (BEST FIT! âœ…)\")\n",
    "\n",
    "#         # Label degree 10 appropriately based on performance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if poly10_mse_val > poly3_mse_val:\n",
    "#         #  print(f\" - Polynomial (deg=10): MSE = {poly10_mse_val:.4f}, RÂ² = {poly10_r2:.4f} (OVERFITTING! âš ï¸)\")\n",
    "#         # else:\n",
    "#         #  print(f\" - Polynomial (deg=10): MSE = {poly10_mse_val:.4f}, RÂ² = {poly10_r2:.4f} (TOO COMPLEX - risky! âš ï¸)\")\n",
    "\n",
    "#         # Check \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if degree 10 is actually worse (indicating overfitting)\n",
    "#         # Or \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if it's similar/better (but still risky due to complexity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if poly10_mse_val > poly3_mse_val:\n",
    "#         #  print(f\"\\nâš ï¸ Overfitting Detected:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Degree 10 has HIGHER test MSE than degree 3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - This means degree 10 memorized training data but failed on test data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Degree 3 is the sweet spot: good fit without overfitting\")\n",
    "#         # else:\n",
    "#         #  print(f\"\\nğŸ’¡ High-Degree Polynomial Risk:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Degree 10 performs similarly to degree 3 on this data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - BUT: Degree 10 is MUCH more complex ({11} parameters vs {4} for degree 3)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Higher risk of overfitting on different/new data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - General rule: Use the SIMPLEST model that works well (degree 2-3)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Simpler models generalize better to new data!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Linear regression FAILS on curved relationships (low RÂ² = {linear_r2:.4f}) âŒ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Polynomial regression HANDLES curves well (much better RÂ² = {poly3_r2:.4f}) âœ…\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Higher degree â‰  always better (degree 10 is more complex but may not help)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Simpler models (degree 2-3) are usually better - they generalize better!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Overfitting risk: Complex models can memorize training data and fail on new data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Best model balances complexity and generalization (degree 2-3 is optimal)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Always compare test metrics, not training metrics\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Polynomial regression is ESSENTIAL for non-linear relationships!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - This clearly demonstrates the bias-variance tradeoff\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.281957Z",
     "iopub.status.busy": "2026-01-20T05:44:54.281846Z",
     "iopub.status.idle": "2026-01-20T05:44:54.284044Z",
     "shell.execute_reply": "2026-01-20T05:44:54.283592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù„ØªØµÙˆØ±\n"
     ]
    }
   ],
   "source": [
    "# 7. Visualization\n",
    "# + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"7. Visualization\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ø§Ù„ØªØµÙˆØ±\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.285446Z",
     "iopub.status.busy": "2026-01-20T05:44:54.285344Z",
     "iopub.status.idle": "2026-01-20T05:44:54.287483Z",
     "shell.execute_reply": "2026-01-20T05:44:54.286897Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create smooth line \n",
    "# for plotting\n",
    "# Use the actual range of our data (Horsepower values) \n",
    "# for smooth curve visualizationX_min, X_max = X.min(), X.max()\n",
    "# X_plot = np.linspace(X_min, X_max, 200).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.289106Z",
     "iopub.status.busy": "2026-01-20T05:44:54.288988Z",
     "iopub.status.idle": "2026-01-20T05:44:54.290861Z",
     "shell.execute_reply": "2026-01-20T05:44:54.290481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predictions \n",
    "# for plottingy_plot_linear = linear_model.predict(X_plot)\n",
    "# X_plot_poly_2 = poly_features_2.transform(X_plot)\n",
    "# y_plot_poly_2 = poly_model_2.predict(X_plot_poly_2)\n",
    "# X_plot_poly_3 = poly_features_3.transform(X_plot)\n",
    "# y_plot_poly_3 = poly_model_3.predict(X_plot_poly_3)\n",
    "# X_plot_poly_10 = poly_features_10.transform(X_plot)\n",
    "# y_plot_poly_10 = poly_model_10.predict(X_plot_poly_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.292812Z",
     "iopub.status.busy": "2026-01-20T05:44:54.292496Z",
     "iopub.status.idle": "2026-01-20T05:44:54.302763Z",
     "shell.execute_reply": "2026-01-20T05:44:54.302418Z"
    }
   },
   "outputs": [],
   "source": [
    "# 12))\n",
    "\n",
    "# s=30)\n",
    "# axes[0, 0].plot(X_plot, y_plot_linear, 'r-', linewidth=2, label='Linear Regression')\n",
    "# axes[0, 0].set_xlabel('Visibility (miles)')\n",
    "# axes[0, 0].set_ylabel('Accident Impact Distance (miles)')\n",
    "# axes[0, 0].set_title(f'Linear Regression (RÂ² = {linear_r2:.4f}) - Poor Fit!')\n",
    "# axes[0, 0].legend()\n",
    "# axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# s=30)\n",
    "# axes[0, 1].plot(X_plot, y_plot_poly_2, 'g-', linewidth=2, label='Polynomial (deg=2)')\n",
    "# axes[0, 1].set_xlabel('Visibility (miles)')\n",
    "# axes[0, 1].set_ylabel('Accident Impact Distance (miles)')\n",
    "# axes[0, 1].set_title(f'Polynomial Regression - Degree 2 (RÂ² = {poly2_r2:.4f}) - Good Fit! âœ…')\n",
    "# axes[0, 1].legend()\n",
    "# axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# s=30)\n",
    "# axes[1, 0].plot(X_plot, y_plot_poly_3, 'orange', linewidth=2, label='Polynomial (deg=3)')\n",
    "# axes[1, 0].set_xlabel('Visibility (miles)')\n",
    "# axes[1, 0].set_ylabel('Accident Impact Distance (miles)')\n",
    "# axes[1, 0].set_title(f'Polynomial Regression - Degree 3 (RÂ² = {poly3_r2:.4f})')\n",
    "# axes[1, 0].legend()\n",
    "# axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Degree 10 (Overfitting)\n",
    "# axes[1, 1].scatter(X_test, y_test, alpha=0.6, label='Test Data', color='blue', s=30)\n",
    "# axes[1, 1].plot(X_plot, y_plot_poly_10, 'purple', linewidth=2, label='Polynomial (deg=10)')\n",
    "# axes[1, 1].set_xlabel('Visibility (miles)')\n",
    "# axes[1, 1].set_ylabel('Accident Impact Distance (miles)')\n",
    "# axes[1, 1].set_title(f'Polynomial Regression - Degree 10 (RÂ² = {poly10_r2:.4f}) âš ï¸ Overfitting')\n",
    "# axes[1, 1].legend()\n",
    "# axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('polynomial_regression_comparison.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"âœ“ Plot saved as 'polynomial_regression_comparison.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Decision Framework - When to Use Polynomial Regression | Ø§Ù„Ø®Ø·ÙˆØ© 9: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯\n",
    "\n",
    "**BEFORE**: You've learned how to build polynomial regression models, but when should you use them?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to determine if polynomial regression is the right choice for your problem!\n",
    "\n",
    "**Why this matters**: Using polynomial regression when it's not appropriate leads to:\n",
    "- **Overfitting** â†’ Model memorizes training data, fails on new data\n",
    "- **Poor generalization** â†’ High degree polynomials create wiggly curves\n",
    "- **Wasted complexity** â†’ Using complex models when simple ones work\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework: Linear vs Polynomial Regression | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ù…Ù‚Ø§Ø¨Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯\n",
    "\n",
    "**Key Question**: Should I use **LINEAR REGRESSION**, **POLYNOMIAL REGRESSION**, or something else?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "What type of problem do you have?\n",
    "â”œâ”€ CLASSIFICATION â†’ Use classification methods (Logistic Regression, Decision Trees, SVM)\n",
    "â”‚   â””â”€ Why? Polynomial regression is for regression (predicting numbers)\n",
    "â”‚\n",
    "â””â”€ REGRESSION â†’ Check relationship:\n",
    "    â”œâ”€ Is relationship LINEAR? â†’ Use LINEAR REGRESSION âœ…\n",
    "    â”‚   â””â”€ Why? Simpler, faster, more interpretable\n",
    "    â”‚\n",
    "    â””â”€ Is relationship NON-LINEAR? â†’ Check complexity:\n",
    "        â”œâ”€ Slightly curved (quadratic) â†’ Use POLYNOMIAL REGRESSION (degree 2-3) âœ…\n",
    "        â”‚   â””â”€ Why? Can capture curves without overfitting\n",
    "        â”‚\n",
    "        â”œâ”€ Moderately curved â†’ Use POLYNOMIAL REGRESSION (degree 3-5) âš ï¸\n",
    "        â”‚   â””â”€ Why? Can handle curves, but watch for overfitting\n",
    "        â”‚\n",
    "        â””â”€ Highly non-linear, complex patterns â†’ Use OTHER METHODS âŒ\n",
    "            â””â”€ Use: Random Forest, XGBoost, Neural Networks\n",
    "```\n",
    "\n",
    "#### Detailed Decision Process:\n",
    "\n",
    "```\n",
    "Step 1: Visualize the relationship\n",
    "â”œâ”€ Plot scatter plot of X vs y\n",
    "â”‚\n",
    "â””â”€ What pattern do you see?\n",
    "    â”œâ”€ Straight line â†’ LINEAR REGRESSION\n",
    "    â”œâ”€ Smooth curve â†’ POLYNOMIAL REGRESSION (degree 2-3)\n",
    "    â”œâ”€ Wavy curve â†’ POLYNOMIAL REGRESSION (degree 3-5) âš ï¸\n",
    "    â””â”€ Complex, irregular â†’ OTHER METHODS (Random Forest, XGBoost)\n",
    "\n",
    "Step 2: Try Linear Regression first\n",
    "â”œâ”€ Build linear model\n",
    "â”œâ”€ Check RÂ² score\n",
    "â”‚\n",
    "â””â”€ Is RÂ² good enough?\n",
    "    â”œâ”€ YES (RÂ² > 0.7) â†’ Stick with LINEAR REGRESSION âœ…\n",
    "    â””â”€ NO (RÂ² < 0.7) â†’ Try POLYNOMIAL REGRESSION\n",
    "\n",
    "Step 3: Try Polynomial Regression\n",
    "â”œâ”€ Start with degree 2\n",
    "â”œâ”€ Gradually increase degree\n",
    "â”œâ”€ Compare train vs test performance\n",
    "â”‚\n",
    "â””â”€ Is there overfitting?\n",
    "    â”œâ”€ NO (train â‰ˆ test) â†’ Use POLYNOMIAL REGRESSION âœ…\n",
    "    â””â”€ YES (train >> test) â†’ Use REGULARIZATION or OTHER METHODS\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Comparison Table: Linear vs Polynomial vs Alternatives | Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Linear Regression** | Linear relationships | â€¢ Simple & fast<br>â€¢ Interpretable<br>â€¢ No overfitting risk | â€¢ Can't handle curves | Straight line patterns |\n",
    "| **Polynomial (deg 2-3)** | Slightly curved relationships | â€¢ Handles curves<br>â€¢ Still interpretable<br>â€¢ Moderate complexity | â€¢ Can overfit if degree too high | Quadratic patterns |\n",
    "| **Polynomial (deg 4-10)** | Moderately curved | â€¢ Handles complex curves | â€¢ High overfitting risk<br>â€¢ Less interpretable | Complex curves |\n",
    "| **Ridge/Lasso** | Many features, overfitting | â€¢ Prevents overfitting<br>â€¢ Regularization | â€¢ More complex<br>â€¢ Hyperparameter tuning | High-dimensional data |\n",
    "| **Random Forest** | Highly non-linear | â€¢ Handles any pattern<br>â€¢ Robust | â€¢ Less interpretable<br>â€¢ More complex | Very complex relationships |\n",
    "| **XGBoost** | Best performance needed | â€¢ State-of-the-art<br>â€¢ Handles complexity | â€¢ Less interpretable<br>â€¢ Very complex | Competition-level problems |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When Polynomial Regression IS Appropriate | Ù…ØªÙ‰ ÙŠÙƒÙˆÙ† Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§\n",
    "\n",
    "**Use Polynomial Regression when:**\n",
    "\n",
    "1. **Non-Linear Relationship** âœ…\n",
    "   - Scatter plot shows curves (not straight lines)\n",
    "   - Example: Growth patterns, acceleration curves\n",
    "   - **Degree 2-3**: Smooth curves\n",
    "   - **Degree 3-5**: More complex curves (watch for overfitting)\n",
    "\n",
    "2. **Linear Regression Fails** âœ…\n",
    "   - Linear model has low RÂ² (< 0.7)\n",
    "   - Residuals show patterns (not random)\n",
    "   - **Try**: Polynomial degree 2-3 first\n",
    "\n",
    "3. **Moderate Complexity** âœ…\n",
    "   - Relationship is curved but not too complex\n",
    "   - Can be captured with degree 2-5\n",
    "   - **Avoid**: Very high degrees (> 10) - use other methods\n",
    "\n",
    "4. **Interpretability Still Important** âœ…\n",
    "   - Need to understand the relationship\n",
    "   - Polynomial is more interpretable than Random Forest/XGBoost\n",
    "   - **Trade-off**: Some interpretability for better fit\n",
    "\n",
    "5. **Small to Medium Datasets** âœ…\n",
    "   - Enough data to fit polynomial without severe overfitting\n",
    "   - **Rule of thumb**: Need at least 10Ã— degree samples\n",
    "   - Example: Degree 3 needs at least 30 samples\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ When Polynomial Regression IS NOT Appropriate | Ù…ØªÙ‰ Ù„Ø§ ÙŠÙƒÙˆÙ† Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§\n",
    "\n",
    "**Don't use Polynomial Regression when:**\n",
    "\n",
    "1. **Linear Relationship** âŒ\n",
    "   - Data shows straight line pattern\n",
    "   - **Use Instead**: Linear Regression (simpler, better)\n",
    "\n",
    "2. **Severe Overfitting** âŒ\n",
    "   - High degree (> 5) causes train RÂ² >> test RÂ²\n",
    "   - Model memorizes training data\n",
    "   - **Use Instead**: Ridge/Lasso Regression, or lower degree\n",
    "\n",
    "3. **Highly Complex Patterns** âŒ\n",
    "   - Irregular, non-smooth patterns\n",
    "   - Multiple local patterns\n",
    "   - **Use Instead**: Random Forest, XGBoost, Neural Networks\n",
    "\n",
    "4. **Many Features** âŒ\n",
    "   - High-dimensional data (many features)\n",
    "   - Polynomial features explode (degree 2 with 10 features = 55 features!)\n",
    "   - **Use Instead**: Ridge/Lasso, or feature selection first\n",
    "\n",
    "5. **Small Dataset** âŒ\n",
    "   - Very few samples (< 50)\n",
    "   - High degree will overfit severely\n",
    "   - **Use Instead**: Linear Regression, or collect more data\n",
    "\n",
    "6. **Classification Problem** âŒ\n",
    "   - Predicting categories, not numbers\n",
    "   - **Use Instead**: Logistic Regression, Decision Trees, SVM\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: House Price vs Size (Curved) âœ… APPROPRIATE\n",
    "- **Problem**: Predict price from size\n",
    "- **Relationship**: Curved (price increases faster for larger houses)\n",
    "- **Linear RÂ²**: 0.65 (not great)\n",
    "- **Polynomial RÂ²**: 0.85 (much better)\n",
    "- **Decision**: âœ… Use Polynomial Regression (degree 2-3)\n",
    "- **Reasoning**: Curved relationship, polynomial improves fit significantly\n",
    "\n",
    "#### Example 2: Temperature vs Time (Linear) âŒ NOT APPROPRIATE\n",
    "- **Problem**: Predict temperature from time of day\n",
    "- **Relationship**: Linear (roughly straight line)\n",
    "- **Linear RÂ²**: 0.92 (excellent)\n",
    "- **Decision**: âŒ Use Linear Regression, not Polynomial\n",
    "- **Reasoning**: Linear relationship, polynomial adds unnecessary complexity\n",
    "\n",
    "#### Example 3: Stock Price Prediction (Complex) âŒ NOT APPROPRIATE\n",
    "- **Problem**: Predict stock price from time\n",
    "- **Relationship**: Highly complex, volatile, irregular\n",
    "- **Polynomial RÂ²**: Overfits (train 0.95, test 0.40)\n",
    "- **Decision**: âŒ Use Random Forest or XGBoost\n",
    "- **Reasoning**: Too complex for polynomial, severe overfitting\n",
    "\n",
    "#### Example 4: Growth Curve (Moderate Curve) âœ… APPROPRIATE\n",
    "- **Problem**: Predict population growth over time\n",
    "- **Relationship**: Smooth curve (exponential-like)\n",
    "- **Linear RÂ²**: 0.55 (poor)\n",
    "- **Polynomial RÂ²**: 0.88 (good, degree 3)\n",
    "- **Decision**: âœ… Use Polynomial Regression (degree 3)\n",
    "- **Reasoning**: Smooth curve, polynomial captures it well without overfitting\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Try linear first** - Always start with linear regression as baseline\n",
    "2. **Visualize relationships** - Scatter plots reveal if polynomial is needed\n",
    "3. **Low RÂ²? Try polynomial** - If linear RÂ² < 0.7, try degree 2-3\n",
    "4. **Watch for overfitting** - Compare train vs test performance\n",
    "5. **Optimal degree** - Usually 2-3, rarely need > 5\n",
    "6. **Too complex? Use other methods** - Random Forest/XGBoost for very complex patterns\n",
    "7. **Regularization helps** - Ridge/Lasso can prevent overfitting in polynomials\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario 1**: Predicting sales from advertising spend\n",
    "- **Relationship**: Curved (diminishing returns - more ads help less over time)\n",
    "- **Linear RÂ²**: 0.62\n",
    "- **Decision**: âœ… Try Polynomial Regression (degree 2-3)\n",
    "\n",
    "**Scenario 2**: Predicting height from age (children)\n",
    "- **Relationship**: Curved (growth accelerates then slows)\n",
    "- **Linear RÂ²**: 0.58\n",
    "- **Decision**: âœ… Use Polynomial Regression (degree 2-3)\n",
    "\n",
    "**Scenario 3**: Predicting house price from 50+ features\n",
    "- **Relationship**: Complex, many interactions\n",
    "- **Linear RÂ²**: 0.75 (decent)\n",
    "- **Decision**: âš ï¸ Stick with Linear or use Ridge/Lasso (polynomial would create too many features)\n",
    "\n",
    "**Scenario 4**: Predicting customer churn (yes/no)\n",
    "- **Relationship**: Classification problem\n",
    "- **Decision**: âŒ Use Logistic Regression, not Polynomial Regression\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Unit 2, Example 1: Ridge/Lasso** - For preventing overfitting in polynomials\n",
    "- ğŸ““ **Unit 2, Example 2: Cross-Validation** - For proper evaluation of polynomial models\n",
    "- ğŸ““ **Unit 3: Classification** - For predicting categories instead of numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.304576Z",
     "iopub.status.busy": "2026-01-20T05:44:54.304434Z",
     "iopub.status.idle": "2026-01-20T05:44:54.307219Z",
     "shell.execute_reply": "2026-01-20T05:44:54.306807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # 8. Finding Optimal Degree\n",
    "# # + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"8. Finding Optimal Polynomial Degree\")\n",
    "\n",
    "\n",
    "# print(\"Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ø«Ù„Ù‰ Ù„Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "# # degrees = range(1, 11)\n",
    "# # train_scores = []\n",
    "# # test_scores = []\n",
    "\n",
    "# # for degree in degrees:\n",
    "# #  poly_features = PolynomialFeatures(degree=degree)\n",
    "# # .fit_trans\n",
    "\n",
    "# # - Two operations in one: .fit() then .trans\n",
    "\n",
    "# # 1. .fit(): Learns parameters \n",
    "# from data (mean/std, categories, etc.)\n",
    "# # 2. .trans\n",
    "# # : Applies transformation using learned parameters\n",
    "# # - Use on training data\n",
    "# # - For test data, use only .trans\n",
    "# (don't refit!)\n",
    "\n",
    "# #  X_train_poly = poly_features.fit_transform(X_train)\n",
    "# #  X_test_poly = poly_features.transform(X_test)\n",
    " \n",
    " \n",
    "\n",
    "# # model = LinearRegression()\n",
    "# #  model.fit(X_train_poly, y_train)\n",
    " \n",
    "# #  train_pred = model.predict(X_train_poly)\n",
    "# #  test_pred = model.predict(X_test_poly)\n",
    " \n",
    "# #  train_r2 = r2_score(y_train, train_pred)\n",
    "# #  test_r2 = r2_score(y_test, test_pred)\n",
    " \n",
    "# #  train_scores.append(train_r2)\n",
    "# #  test_scores.append(test_r2)\n",
    "\n",
    "# # Find optimal degree using a practical approach:\n",
    "# # 1. Look \n",
    "# # for the best degree in the recommended range (2-3)\n",
    "# # 2. Consider the train-test gap (overfitting indicator)\n",
    "# # 3. Prefer simpler models when performance is similar\n",
    "\n",
    "# # Find best degree in recommended range (2-3)\n",
    "# # recommended_range = [2, 3]\n",
    "# # best_in_range_idx = max(recommended_range, key=lambda d: test_scores[d-1])\n",
    "# # best_in_range_r2 = test_scores[best_in_range_idx-1]\n",
    "\n",
    "# # Find best overall degree (might be higher, but risky)\n",
    "# # best_overall_idx = np.argmax(test_scores) + 1 # +1 because degrees start at 1best_overall_r2 = max(test_scores)\n",
    "\n",
    "# # = [train_scores[i] - test_scores[i] \n",
    "# # for i in range(len(degrees))]\n",
    "# # gap_in_range = train_test_gaps[best_in_range_idx-1]\n",
    "# # gap_overall = train_test_gaps[best_overall_idx-1]\n",
    "\n",
    "# # Determine optimal degree: prefer simpler model \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if performance is similar\n",
    "# # If degree 2-3 performs well (RÂ² > 0.95) and gap is small, use it\n",
    "# # Otherwise, \n",
    "\n",
    "\n",
    "\n",
    "# # if higher degree is significantly better AND gap is reasonable, consider it\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # if best_in_range_r2 > 0.95 and gap_in_range < 0.05:\n",
    "# # :\n",
    "# #  optimal_degree = best_in_range_idx \n",
    "# # = f\"Recommended range (2-3) - higher degrees may overfit (gap: {gap_overall:.4f})\"\n",
    "\n",
    "# # print(f\"\\nğŸ“Š Analysis Results:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Best in recommended range (degree 2-3): Degree {best_in_range_idx} (RÂ² = {best_in_range_r2:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Best overall: Degree {best_overall_idx} (RÂ² = {best_overall_r2:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Train-Test Gap (degree {best_in_range_idx}): {gap_in_range:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # if best_overall_idx != best_in_range_idx:\n",
    "# #  print(f\" Train-Test Gap (degree {best_overall_idx}): {gap_overall:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nâœ… Recommended Optimal Degree: {optimal_degree}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ø«Ù„Ù‰ Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§: {optimal_degree}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Reason: {optimal_reason}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Test RÂ²: {test_scores[optimal_degree-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # if best_overall_idx != optimal_degree:\n",
    "# #  print(f\"\\nâš ï¸ Important Note:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - Degree {best_overall_idx} has slightly better test RÂ² ({best_overall_r2:.4f} vs {test_scores[optimal_degree-1]:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - BUT: Higher degrees are RISKY - they can overfit on different data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - General rule: Use the SIMPLEST model that works well (degree 2-3)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - Simpler models generalize better to new, unseen data!\")\n",
    "\n",
    "# # 6))\n",
    "# # plt.plot(degrees, train_scores, 'o-', label='Training RÂ²', linewidth=2, markersize=8)\n",
    "# # plt.plot(degrees, test_scores, 's-', label='Test RÂ²', linewidth=2, markersize=8)\n",
    "# # plt.axvline(optimal_degree, color='green', linestyle='--', linewidth=2, \n",
    "# #  label=f'Recommended Optimal = {optimal_degree}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # if best_overall_idx != optimal_degree:\n",
    "# #  plt.axvline(best_overall_idx, color='orange', linestyle=':', linewidth=2, \n",
    "# #  label=f'Best Test Score = {best_overall_idx} (risky)')\n",
    "# # plt.xlabel('Polynomial Degree')\n",
    "# # plt.ylabel('RÂ² Score')\n",
    "# # plt.title('Finding Optimal Polynomial Degree')\n",
    "# plt.legend()\n",
    "# # plt.grid(True, alpha=0.3)\n",
    "# # plt.tight_layout()\n",
    "# # plt.savefig('optimal_polynomial_degree.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nâœ“ Plot saved as 'optimal_polynomial_degree.png'\")\n",
    "# plt.show()\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Understanding the Bias-Variance Tradeoff | Ø§Ù„Ø®Ø·ÙˆØ© 10: ÙÙ‡Ù… Ø§Ù„Ù…Ù‚Ø§ÙŠØ¶Ø© Ø¨ÙŠÙ† Ø§Ù„ØªØ­ÙŠØ² ÙˆØ§Ù„ØªØ¨Ø§ÙŠÙ†\n",
    "\n",
    "**BEFORE**: You've seen how different polynomial degrees affect performance, but what's really happening under the hood?\n",
    "\n",
    "**AFTER**: You'll understand the fundamental tradeoff in machine learning - bias vs variance - and why it matters!\n",
    "\n",
    "**Why this matters**: The bias-variance tradeoff is THE fundamental concept in machine learning. Understanding it helps you:\n",
    "- Choose the right model complexity\n",
    "- Understand why models fail\n",
    "- Make better decisions about regularization\n",
    "- Apply concepts to ALL ML models (not just polynomial regression)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ What is the Bias-Variance Tradeoff? | Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù‚Ø§ÙŠØ¶Ø© Ø¨ÙŠÙ† Ø§Ù„ØªØ­ÙŠØ² ÙˆØ§Ù„ØªØ¨Ø§ÙŠÙ†ØŸ\n",
    "\n",
    "**Total Error = BiasÂ² + Variance + Irreducible Error**\n",
    "\n",
    "#### The Three Components:\n",
    "\n",
    "1. **Bias (Ø§Ù„ØªØ­ÙŠØ²)**: Error from overly simplistic assumptions\n",
    "   - **High Bias (Underfitting)**: Model is too simple, misses patterns\n",
    "   - **Example**: Linear regression on curved data\n",
    "   - **Solution**: Increase model complexity (higher degree)\n",
    "\n",
    "2. **Variance (Ø§Ù„ØªØ¨Ø§ÙŠÙ†)**: Error from sensitivity to small fluctuations\n",
    "   - **High Variance (Overfitting)**: Model is too complex, memorizes noise\n",
    "   - **Example**: Degree 10 polynomial fitting every data point perfectly\n",
    "   - **Solution**: Decrease model complexity (lower degree) or use regularization\n",
    "\n",
    "3. **Irreducible Error**: Error that cannot be reduced (noise in data)\n",
    "   - Cannot be eliminated, no matter the model\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š How Polynomial Degree Affects Bias and Variance | ÙƒÙŠÙ ØªØ¤Ø«Ø± Ø¯Ø±Ø¬Ø© Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­ÙŠØ² ÙˆØ§Ù„ØªØ¨Ø§ÙŠÙ†\n",
    "\n",
    "| Polynomial Degree | Bias | Variance | Total Error | Model Behavior |\n",
    "|-------------------|------|----------|-------------|----------------|\n",
    "| **Degree 1 (Linear)** | **HIGH** | Low | High | **Underfitting** - Too simple, misses curves |\n",
    "| **Degree 2-3** | **Low** | Low | **Low** | **Optimal** - Good fit, generalizes well |\n",
    "| **Degree 4-5** | Low | **Medium** | Medium | **Acceptable** - Good fit, slight overfitting |\n",
    "| **Degree 10+** | Low | **HIGH** | High | **Overfitting** - Memorizes training data |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Visual Explanation | Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„Ù…Ø±Ø¦ÙŠ\n",
    "\n",
    "**Low Degree (High Bias, Low Variance)**:\n",
    "- Model is too simple (straight line or simple curve)\n",
    "- Misses important patterns in data\n",
    "- Consistent predictions (low variance) but wrong (high bias)\n",
    "- **Example**: Linear regression on quadratic data\n",
    "\n",
    "**Medium Degree (Low Bias, Low Variance)** âœ…:\n",
    "- Model complexity matches data complexity\n",
    "- Captures true patterns well\n",
    "- Generalizes to new data\n",
    "- **Example**: Degree 2-3 polynomial on quadratic data\n",
    "\n",
    "**High Degree (Low Bias, High Variance)**:\n",
    "- Model is too complex (wiggly curve)\n",
    "- Fits training data perfectly\n",
    "- Sensitive to noise (high variance)\n",
    "- Fails on new data\n",
    "- **Example**: Degree 10 polynomial on any data\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Key Insights | Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **There's a sweet spot**: Optimal model complexity balances bias and variance\n",
    "2. **More complex â‰  better**: Higher degree can increase total error\n",
    "3. **Training vs Test gap**: Large gap indicates high variance (overfitting)\n",
    "4. **Both can be wrong**: High bias (too simple) OR high variance (too complex) both lead to poor performance\n",
    "5. **Universal concept**: This applies to ALL machine learning models!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ What We'll Visualize Next | Ù…Ø§ Ø³Ù†ØªØµÙˆØ±Ù‡ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ\n",
    "\n",
    "In the next cell, we'll create a visualization that shows:\n",
    "- How bias decreases as degree increases (at first, good!)\n",
    "- How variance increases as degree increases (too much, bad!)\n",
    "- The optimal degree where total error is minimized\n",
    "- The bias-variance tradeoff curve that guides model selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.308651Z",
     "iopub.status.busy": "2026-01-20T05:44:54.308567Z",
     "iopub.status.idle": "2026-01-20T05:44:54.311141Z",
     "shell.execute_reply": "2026-01-20T05:44:54.310975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # 9. Visualizing the Bias-Variance Tradeoff\n",
    "#         # + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"9. Visualizing the Bias-Variance Tradeoff\")\n",
    "\n",
    "\n",
    "#         print(\"ØªØµÙˆØ± Ø§Ù„Ù…Ù‚Ø§ÙŠØ¶Ø© Ø¨ÙŠÙ† Ø§Ù„ØªØ­ÙŠØ² ÙˆØ§Ù„ØªØ¨Ø§ÙŠÙ†\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # Calculate MSE (error) \n",
    "#         # for train and test sets\n",
    "#         # = range(1, 11)\n",
    "#         # train_errors = []\n",
    "#         # test_errors = []\n",
    "\n",
    "#         # for degree in degrees_tradeoff:\n",
    "#         #  poly_features = PolynomialFeatures(degree=degree)\n",
    "#         #  X_train_poly = poly_features.fit_transform(X_train)\n",
    "#         #  X_test_poly = poly_features.transform(X_test)\n",
    " \n",
    " \n",
    "\n",
    "#         # model = LinearRegression()\n",
    "#         #  model.fit(X_train_poly, y_train)\n",
    " \n",
    "#         #  train_pred = model.predict(X_train_poly)\n",
    "#         #  test_pred = model.predict(X_test_poly)\n",
    " \n",
    "#         #  train_mse = mean_squared_error(y_train, train_pred)\n",
    "#         #  test_mse = mean_squared_error(y_test, test_pred)\n",
    " \n",
    "#          train_errors.append(train_mse)\n",
    "#          test_errors.append(test_mse)\n",
    "\n",
    "#         # Bias approximation: How much the model is missing (difference \n",
    "#         from perfect fit on training)\n",
    "#         # In practice, we use training error as a proxy \n",
    "#         # for bias (lower = less bias)\n",
    "#         # = more variance)\n",
    "#         # bias_proxy = train_errors \n",
    "#         # = [test_errors[i] - train_errors[i] \n",
    "#         # for i in range(len(train_errors))]\n",
    "#         # = variance indicator\n",
    "\n",
    "#         # 12))\n",
    "#         # Error (Bias Proxy)', linewidth=2, markersize=8, color='blue')\n",
    "#         # axes[0, 0].plot(degrees_tradeoff, test_errors, 's-', label='Test Error (Total Error)', \n",
    "#         #  linewidth=2, markersize=8, color='red')\n",
    "#         # axes[0, 0].axvline(optimal_degree, color='green', linestyle='--', linewidth=2, \n",
    "#         #  label=f'Optimal Degree = {optimal_degree}')\n",
    "#         # axes[0, 0].set_xlabel('Polynomial Degree', fontsize=12)\n",
    "#         # axes[0, 0].set_ylabel('Mean Squared Error (MSE)', fontsize=12)\n",
    "#         # axes[0, 0].set_title('Bias-Variance Tradeoff: Training vs Test Error', fontsize=14, fontweight='bold')\n",
    "#         # axes[0, 0].legend(fontsize=10)\n",
    "#         # axes[0, 0].grid(True, alpha=0.3)\n",
    "#         # axes[0, 0].annotate('Underfitting\\n(High Bias)', xy=(1, train_errors[0]), \n",
    "#         #  xytext=(1.5, train_errors[0] + 5), fontsize=10,\n",
    "#         #  arrowprops=dict(arrowstyle='->', color='orange', lw=2))\n",
    "#         # axes[0, 0].annotate('Overfitting\\n(High Variance)', xy=(10, test_errors[9]), \n",
    "#         #  xytext=(8.5, test_errors[9] + 5), fontsize=10,\n",
    "#         #  arrowprops=dict(arrowstyle='->', color='purple', lw=2))\n",
    "\n",
    "#         # axes[0, 1].fill_between(degrees_tradeoff, bias_proxy, alpha=0.3, color='blue')\n",
    "#         # axes[0, 1].set_xlabel('Polynomial Degree', fontsize=12)\n",
    "#         # axes[0, 1].set_ylabel('Training Error (Bias Proxy)', fontsize=12)\n",
    "#         # axes[0, 1].set_title('Bias: Decreases as Model Complexity Increases', fontsize=14, fontweight='bold')\n",
    "#         # axes[0, 1].grid(True, alpha=0.3)\n",
    "#         # axes[0, 1].annotate('Low Bias\\n(Good Fit)', xy=(optimal_degree, bias_proxy[optimal_degree-1]), \n",
    "#         #  xytext=(optimal_degree+1.5, bias_proxy[optimal_degree-1] + 2), fontsize=10,\n",
    "#         #  arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "#         #  bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "#         # Plot 3: Variance (Gap between Train and Test)\n",
    "#         # axes[1, 0].plot(degrees_tradeoff, variance_proxy, 's-', linewidth=2, markersize=8, color='red')\n",
    "#         # axes[1, 0].fill_between(degrees_tradeoff, variance_proxy, alpha=0.3, color='red')\n",
    "#         # axes[1, 0].set_xlabel('Polynomial Degree', fontsize=12)\n",
    "#         # axes[1, 0].set_ylabel('Variance Proxy (Test - Train Error)', fontsize=12)\n",
    "#         # axes[1, 0].set_title('Variance: Increases as Model Complexity Increases', fontsize=14, fontweight='bold')\n",
    "#         # axes[1, 0].grid(True, alpha=0.3)\n",
    "#         # axes[1, 0].annotate('High Variance\\n(Overfitting!)', xy=(10, variance_proxy[9]), \n",
    "#         #  xytext=(8, variance_proxy[9] + 0.5), fontsize=10,\n",
    "#         #  arrowprops=dict(arrowstyle='->', color='purple', lw=2),\n",
    "#         #  bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "#         # (Train Error)', \n",
    "#         #  linewidth=2, markersize=8, color='blue')\n",
    "#         # axes[1, 1].plot(degrees_tradeoff, variance_proxy, 's-', label='Variance Proxy (Test-Train Gap)', \n",
    "#         #  linewidth=2, markersize=8, color='red')\n",
    "#         # axes[1, 1].plot(degrees_tradeoff, test_errors, '^-', label='Total Error (Test Error)', \n",
    "#         #  linewidth=2, markersize=8, color='green')\n",
    "#         # axes[1, 1].axvline(optimal_degree, color='orange', linestyle='--', linewidth=2, \n",
    "#         #  label=f'Optimal = {optimal_degree}')\n",
    "#         # axes[1, 1].set_xlabel('Polynomial Degree (Model Complexity)', fontsize=12)\n",
    "#         # axes[1, 1].set_ylabel('Error', fontsize=12)\n",
    "#         # axes[1, 1].set_title('The Bias-Variance Tradeoff', fontsize=14, fontweight='bold')\n",
    "#         # axes[1, 1].legend(fontsize=10)\n",
    "#         # axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "#         # = np.argmin(test_errors)\n",
    "#         # axes[1, 1].annotate(f'Optimal Balance\\n(Bias + Variance)\\nDegree = {optimal_degree}', xy=(optimal_degree, test_errors[optimal_degree-1]), \n",
    "#         #  xytext=(optimal_degree+2, test_errors[optimal_degree-1] + 3), \n",
    "#         #  fontsize=11, fontweight='bold',\n",
    "#         #  arrowprops=dict(arrowstyle='->', color='darkgreen', lw=2),\n",
    "#         #  bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "#         # plt.tight_layout()\n",
    "#         # plt.savefig('bias_variance_tradeoff.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"âœ“ Plot saved as 'bias_variance_tradeoff.png'\")\n",
    "#         plt.show()\n",
    "\n",
    "#         # Print summary\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"ğŸ“Š Bias-Variance Tradeoff Summary | Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ù‚Ø§ÙŠØ¶Ø©\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nâœ… Recommended Optimal Degree: {optimal_degree}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Training Error (Bias): {train_errors[optimal_degree-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Test Error (Total): {test_errors[optimal_degree-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Variance Gap: {variance_proxy[optimal_degree-1]:.4f}\")\n",
    "\n",
    "#         # Find best overall \n",
    "#         # Comparison:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Degree 1: High Bias ({train_errors[0]:.4f}), Variance Gap ({variance_proxy[0]:.4f}) â†’ UNDERFITTING\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Degree {optimal_degree}: Low Bias ({train_errors[optimal_degree-1]:.4f}), Variance Gap ({variance_proxy[optimal_degree-1]:.4f}) â†’ RECOMMENDED âœ…\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if best_overall_idx != optimal_degree:\n",
    "#         #  print(f\" - Degree {best_overall_idx}: Test Error ({test_errors[best_overall_idx-1]:.4f}), Variance Gap ({variance_proxy[best_overall_idx-1]:.4f}) â†’ RISKY (may overfit on new data) âš ï¸\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Degree 10: Low Bias ({train_errors[9]:.4f}), Variance Gap ({variance_proxy[9]:.4f}) â†’ TOO COMPLEX âš ï¸\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ’¡ The Tradeoff:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - As degree â†‘: Bias â†“ (good!) but Variance â†‘ (bad!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Optimal point: Where Bias + Variance is minimized\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Recommended: Degree 2-3 balances fit and generalization\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Higher degrees may perform slightly better on THIS test set, but:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" â€¢ Risk overfitting on different data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" â€¢ More complex (harder to interpret)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" â€¢ Less reliable for new predictions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - This is why we recommend degree 2-3, not the highest degree!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš« When Polynomial Regression Hits a Dead End: Overfitting | Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙˆØ§Ø¬Ù‡ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø·Ø±ÙŠÙ‚ Ù…Ø³Ø¯ÙˆØ¯: Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ®ØµÙŠØµ\n",
    "\n",
    "## The Problem: Overfitting with High-Degree Polynomials | Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ®ØµÙŠØµ Ù…Ø¹ ÙƒØ«ÙŠØ±Ø§Øª Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¯Ø±Ø¬Ø©\n",
    "\n",
    "**BEFORE**: We've seen polynomial regression work well for non-linear data (degree 2-3 gives good RÂ² ~0.98).\n",
    "\n",
    "**AFTER**: Now we'll see polynomial regression's **dead end** - high-degree polynomials overfit badly!\n",
    "\n",
    "**Why this matters**: \n",
    "- High-degree polynomials (degree 10+) can achieve perfect training fit (RÂ² = 1.0)\n",
    "- BUT they fail on test data (test RÂ² drops significantly)\n",
    "- This is **overfitting**: model memorizes training data, doesn't generalize\n",
    "- This dead end leads us to **Unit 2: Ridge/Lasso Regression** - they prevent overfitting!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Real-World Scenario | Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "**Example**: Predicting house prices with polynomial features. Degree 10 polynomial fits training data perfectly (RÂ² = 1.0), but when predicting new houses, accuracy drops dramatically (test RÂ² = 0.65).\n",
    "\n",
    "**The Dead End**: \n",
    "- High-degree polynomials **memorize** training data\n",
    "- Training RÂ² is very high (0.98-1.0)\n",
    "- Test RÂ² is much lower (0.65-0.80)\n",
    "- Large gap (train RÂ² >> test RÂ²) indicates **overfitting**\n",
    "- Solution: Need **regularization** techniques â†’ Ridge/Lasso (Unit 2)!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.311969Z",
     "iopub.status.busy": "2026-01-20T05:44:54.311907Z",
     "iopub.status.idle": "2026-01-20T05:44:54.313803Z",
     "shell.execute_reply": "2026-01-20T05:44:54.313599Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"ğŸš« Dead End: Polynomial Regression Overfitting\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Ø·Ø±ÙŠÙ‚ Ù…Ø³Ø¯ÙˆØ¯: Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ®ØµÙŠØµ Ù„Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ÙˆØ¯\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "        # Calculate train and test RÂ² \n",
    "        # for high-degree polynomial to show overfitting\n",
    "        # We already calculated these above, but let's summarize clearly\n",
    "\n",
    "        # Get train predictions \n",
    "        # for degree 10\n",
    "        # from sklearn.metrics import r2_scorey_train_pred_poly10 = poly_model_10.predict(poly_features_10.transform(X_train.reshape(-1, 1)))\n",
    "        # train_r2_poly10 = r2_score(y_train, y_train_pred_poly10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ“Š Overfitting Demonstration: High-Degree Polynomial\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nğŸ” Degree 10 Polynomial Performance:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Training RÂ²: {train_r2_poly10:.4f} ({train_r2_poly10*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Test RÂ²: {poly10_r2:.4f} ({poly10_r2*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Gap: {train_r2_poly10 - poly10_r2:.4f} ({abs(train_r2_poly10 - poly10_r2)*100:.2f} percentage points)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nâŒ The Problem:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Training RÂ² is VERY HIGH ({train_r2_poly10:.2%})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Test RÂ² is LOWER ({poly10_r2:.2%})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Large gap ({abs(train_r2_poly10 - poly10_r2):.2%}) indicates OVERFITTING\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Model memorized training data but doesn't generalize to new data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nğŸ’¡ What This Means:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - High-degree polynomials can fit training data perfectly\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - But they fail on unseen test data (poor generalization)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - This is the DEAD END of polynomial regression: overfitting!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nâœ… Solution: Regularization (Unit 2)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Ridge Regression: Prevents overfitting by penalizing large coefficients\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Lasso Regression: Prevents overfitting and does feature selection\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Next unit will show how Ridge/Lasso solve this overfitting problem!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - Expected: Better generalization with smaller train-test gap! ğŸ¯\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nğŸ”— Transition to Unit 2:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - This overfitting problem leads us to Ridge/Lasso Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - They use regularization to prevent overfitting\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" - They allow high-degree polynomials without overfitting!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Summary: Overfitting as a Dead End | Ø§Ù„Ù…Ù„Ø®Øµ: Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ®ØµÙŠØµ ÙƒØ·Ø±ÙŠÙ‚ Ù…Ø³Ø¯ÙˆØ¯\n",
    "\n",
    "### âœ… Polynomial Regression Works Well When:\n",
    "1. **Low to Moderate Degree (2-3)**: Captures non-linearity without overfitting\n",
    "2. **Good Example**: Degree 2-3 gives RÂ² ~0.98 with good generalization âœ…\n",
    "\n",
    "### âŒ Polynomial Regression Hits a Dead End When:\n",
    "1. **High Degree (10+)**: Overfits badly (train RÂ² >> test RÂ²) âŒ\n",
    "2. **Large Train-Test Gap**: Model memorizes training data, fails on test data\n",
    "3. **Need Better Generalization**: Want to use high-degree without overfitting\n",
    "\n",
    "### ğŸ” How to Recognize Overfitting | ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ®ØµÙŠØµ\n",
    "\n",
    "**Symptoms You'll See:**\n",
    "- Training RÂ² is very high (0.95-1.0)\n",
    "- Test RÂ² is much lower (0.65-0.85)\n",
    "- Large gap between train and test RÂ² (>0.15-0.20)\n",
    "- Model fits training data perfectly but fails on new data\n",
    "\n",
    "**Solution:**\n",
    "- Use **Ridge Regression** (Unit 2) - prevents overfitting with L2 regularization\n",
    "- Use **Lasso Regression** (Unit 2) - prevents overfitting with L1 regularization\n",
    "- Use **Cross-Validation** (Unit 2) - better evaluation to detect overfitting\n",
    "- Use **Lower Degree** - simpler models generalize better\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Transition to Unit 2 | Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„ÙˆØ­Ø¯Ø© 2\n",
    "\n",
    "**What We Learned:**\n",
    "- âœ… Polynomial regression solves the non-linear problem (better than linear)\n",
    "- âœ… Low-degree polynomials (2-3) work well with good generalization\n",
    "- âŒ High-degree polynomials overfit (train RÂ² >> test RÂ²)\n",
    "- âŒ This is the **DEAD END** of polynomial regression\n",
    "\n",
    "**The Problem:**\n",
    "- We need to use high-degree polynomials for complex patterns\n",
    "- But high-degree polynomials overfit badly\n",
    "- We need techniques to prevent overfitting\n",
    "\n",
    "**Next Unit: Advanced Regression (Ridge/Lasso)**\n",
    "- ğŸ““ **Unit 2: Ridge and Lasso Regression** solve this overfitting problem!\n",
    "- Ridge/Lasso use **regularization** to prevent overfitting\n",
    "- They allow complex models (high-degree) without overfitting\n",
    "- Expected: Better generalization with controlled complexity! âœ…\n",
    "\n",
    "**This overfitting dead end leads us to Ridge/Lasso Regression - they prevent overfitting!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.314631Z",
     "iopub.status.busy": "2026-01-20T05:44:54.314586Z",
     "iopub.status.idle": "2026-01-20T05:44:54.316077Z",
     "shell.execute_reply": "2026-01-20T05:44:54.315764Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Example 5 Complete! âœ“\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 5! âœ“\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ¯ Next Step: Move to Unit 2 (Ridge/Lasso Regression) to see how they solve the overfitting problem!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©: Ø§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ Ø§Ù„ÙˆØ­Ø¯Ø© 2 (Ø§Ù†Ø­Ø¯Ø§Ø± Ridge/Lasso) Ù„ØªØ±Ù‰ ÙƒÙŠÙ ÙŠØ­Ù„ÙˆÙ† Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ®ØµÙŠØµ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:54.317145Z",
     "iopub.status.busy": "2026-01-20T05:44:54.317070Z",
     "iopub.status.idle": "2026-01-20T05:44:54.318795Z",
     "shell.execute_reply": "2026-01-20T05:44:54.318589Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "        # What You've Learned:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" âœ… Built polynomial regression models (degrees 2, 3, 10)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" âœ… Compared polynomial vs linear regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" âœ… Detected overfitting by comparing train vs test performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" âœ… Found optimal polynomial degree\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" âœ… Visualized the bias-variance tradeoff\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" âœ… Understand when polynomial regression is appropriate\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ“š Next Steps:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" â†’ Unit 2: Advanced Regression (Ridge, Lasso for handling overfitting)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" â†’ Unit 3: Classification Models (same concepts apply!)\")\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> Incoming (Background Agent changes)
