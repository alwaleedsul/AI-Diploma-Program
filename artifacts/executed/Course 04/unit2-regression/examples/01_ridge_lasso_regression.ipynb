{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Ridge and Lasso Regression | ÿßŸÑÿßŸÜÿ≠ÿØÿßÿ± ÿ±ŸäÿØÿ¨ ŸàŸÑÿßÿ≥Ÿà\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Basic Python\n",
    "- ‚úÖ Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 04, Unit 2** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Ridge and Lasso Regression | ÿßŸÑÿßŸÜÿ≠ÿØÿßÿ± ÿ±ŸäÿØÿ¨ ŸàŸÑÿßÿ≥Ÿà\n",
    "\n",
    "## üìö Prerequisites (What You Need First) | ÿßŸÑŸÖÿ™ÿ∑ŸÑÿ®ÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- ‚úÖ **Unit 1: All examples** - Data processing, linear regression, polynomial regression\n",
    "- ‚úÖ **Understanding of overfitting**: What happens when models are too complex\n",
    "- ‚úÖ **Basic linear algebra**: Understanding coefficients and regularization\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why regularization is needed\n",
    "- Knowing when to use Ridge vs Lasso\n",
    "- Understanding how alpha (regularization strength) works\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Where This Notebook Fits | ŸÖŸÉÿßŸÜ Ÿáÿ∞ÿß ÿßŸÑÿØŸÅÿ™ÿ±\n",
    "\n",
    "**This is Unit 2, Example 1** - it solves the overfitting problem from polynomial regression!\n",
    "\n",
    "**Why this example FIRST in Unit 2?**\n",
    "- **Before** you can use advanced techniques, you need to solve overfitting\n",
    "- **Before** you can build robust models, you need regularization\n",
    "- **Before** you can handle multicollinearity, you need Ridge/Lasso\n",
    "\n",
    "**Builds on**: \n",
    "- üìì Unit 1, Example 4: Linear Regression (we know basic regression)\n",
    "- üìì Unit 1, Example 5: Polynomial Regression (we saw overfitting!)\n",
    "\n",
    "**Leads to**: \n",
    "- üìì Example 2: Cross-Validation (evaluates models properly)\n",
    "- üìì Unit 3: Classification (same regularization concepts apply)\n",
    "- üìì All ML models (regularization is universal!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Ridge/Lasso solve **overfitting** (critical problem from Unit 1)\n",
    "2. Ridge/Lasso teach **regularization** (essential ML concept)\n",
    "3. Ridge/Lasso show **feature selection** (Lasso automatically selects features)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Preventing Overfitting | ÿßŸÑŸÇÿµÿ©: ŸÖŸÜÿπ ÿßŸÑÿ•ŸÅÿ±ÿßÿ∑ ŸÅŸä ÿßŸÑÿ™ŸÑÿßÿ¶ŸÖ\n",
    "\n",
    "Imagine you're learning to drive. **Before** regularization, you memorize every turn on the training route perfectly, but fail on new routes (overfitting). **After** regularization, you learn general driving principles that work everywhere!\n",
    "\n",
    "Same with machine learning: **Before** Ridge/Lasso, models memorize training data perfectly but fail on new data. **After** Ridge/Lasso, models learn general patterns that generalize well!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Ridge and Lasso Matter | ŸÑŸÖÿßÿ∞ÿß ŸäŸáŸÖ ÿ±ŸäÿØÿ¨ ŸàŸÑÿßÿ≥Ÿàÿü\n",
    "\n",
    "Regularization prevents overfitting:\n",
    "- **Ridge (L2)**: Shrinks coefficients toward zero (keeps all features)\n",
    "- **Lasso (L1)**: Shrinks some coefficients to exactly zero (feature selection!)\n",
    "- **Both**: Prevent overfitting by penalizing large coefficients\n",
    "- **Alpha**: Controls regularization strength (higher = more regularization)\n",
    "\n",
    "---\n",
    "\n",
    "## üåç Real-World Applications | ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™ ŸÅŸä ÿßŸÑÿ≠Ÿäÿßÿ© ÿßŸÑŸàÿßŸÇÿπŸäÿ©\n",
    "\n",
    "**Ridge and Lasso Regression are used when you have MANY features or risk of overfitting!** Here's where you'll find them:\n",
    "\n",
    "### üí∞ Finance & Banking Sector | ÿßŸÑŸÇÿ∑ÿßÿπ ÿßŸÑŸÖÿßŸÑŸä ŸàÿßŸÑŸÖÿµÿ±ŸÅŸä\n",
    "- **Credit Scoring**: Many features (income, age, credit history, employment, etc.) ‚Üí Ridge/Lasso prevents overfitting\n",
    "- **Stock Price Prediction**: Hundreds of market indicators ‚Üí Lasso selects most important features\n",
    "- **Risk Assessment**: Many risk factors ‚Üí Ridge handles multicollinearity (correlated features)\n",
    "- **Fraud Detection**: Many transaction features ‚Üí Lasso automatically selects relevant features\n",
    "- **Loan Default Prediction**: Multiple borrower characteristics ‚Üí Regularization improves generalization\n",
    "\n",
    "### üè• Healthcare & Medical Sector | ÿßŸÑŸÇÿ∑ÿßÿπ ÿßŸÑÿµÿ≠Ÿä ŸàÿßŸÑÿ∑ÿ®Ÿä\n",
    "- **Disease Diagnosis**: Many symptoms, test results, patient data ‚Üí Lasso selects key indicators\n",
    "- **Drug Discovery**: Thousands of molecular features ‚Üí Lasso identifies important compounds\n",
    "- **Medical Imaging**: Many pixel/voxel features ‚Üí Ridge prevents overfitting\n",
    "- **Genomics**: Thousands of genes ‚Üí Lasso selects relevant genes for disease prediction\n",
    "- **Patient Outcome Prediction**: Many clinical variables ‚Üí Ridge handles correlated features\n",
    "\n",
    "### üìä Marketing & E-commerce Sector | ŸÇÿ∑ÿßÿπ ÿßŸÑÿ™ÿ≥ŸàŸäŸÇ ŸàÿßŸÑÿ™ÿ¨ÿßÿ±ÿ© ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸäÿ©\n",
    "- **Customer Segmentation**: Many customer attributes ‚Üí Lasso selects key segmentation features\n",
    "- **Churn Prediction**: Hundreds of customer behavior features ‚Üí Lasso identifies important predictors\n",
    "- **Recommendation Systems**: Many product/user features ‚Üí Ridge prevents overfitting\n",
    "- **Ad Click Prediction**: Many user, ad, context features ‚Üí Lasso selects most relevant\n",
    "- **Price Optimization**: Many market factors ‚Üí Ridge handles multicollinearity\n",
    "\n",
    "### üè≠ Manufacturing & Quality Control | ÿßŸÑÿ™ÿµŸÜŸäÿπ ŸàŸÖÿ±ÿßŸÇÿ®ÿ© ÿßŸÑÿ¨ŸàÿØÿ©\n",
    "- **Quality Prediction**: Many manufacturing parameters ‚Üí Lasso selects critical parameters\n",
    "- **Defect Detection**: Many sensor readings ‚Üí Ridge prevents overfitting\n",
    "- **Production Optimization**: Many process variables ‚Üí Lasso identifies key factors\n",
    "- **Supply Chain Forecasting**: Many demand factors ‚Üí Ridge handles correlated features\n",
    "- **Equipment Failure Prediction**: Many sensor features ‚Üí Lasso selects important sensors\n",
    "\n",
    "### üî¨ Scientific Research & Data Analysis | ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿπŸÑŸÖŸä Ÿàÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "- **Genomics Research**: Thousands of genes ‚Üí Lasso selects relevant genes\n",
    "- **Climate Modeling**: Many climate variables ‚Üí Ridge handles multicollinearity\n",
    "- **Material Science**: Many material properties ‚Üí Lasso identifies key properties\n",
    "- **Social Science Research**: Many survey variables ‚Üí Ridge prevents overfitting\n",
    "- **Epidemiology**: Many risk factors ‚Üí Lasso selects important risk factors\n",
    "\n",
    "### üèõÔ∏è Government & Public Safety Sector (Ministry of Interior) | ÿßŸÑŸÇÿ∑ÿßÿπ ÿßŸÑÿ≠ŸÉŸàŸÖŸä ŸàÿßŸÑÿ≥ŸÑÿßŸÖÿ© ÿßŸÑÿπÿßŸÖÿ© (Ÿàÿ≤ÿßÿ±ÿ© ÿßŸÑÿØÿßÿÆŸÑŸäÿ©)\n",
    "- **Threat Assessment**: Many threat indicators ‚Üí Lasso selects key threat factors ‚Üí counter-terrorism\n",
    "- **Traffic Prediction**: Many traffic sensors ‚Üí Ridge handles correlated sensor data ‚Üí traffic management\n",
    "- **Crime Risk Models**: Many crime factors ‚Üí Lasso identifies critical risk factors ‚Üí crime prevention\n",
    "- **Emergency Response**: Many emergency variables ‚Üí Ridge handles multicollinearity ‚Üí emergency services\n",
    "- **Border Security Screening**: Many traveler features ‚Üí Lasso selects important indicators ‚Üí border control\n",
    "- **Surveillance Systems**: Many surveillance features ‚Üí Ridge prevents overfitting ‚Üí security monitoring\n",
    "- **Traffic Violation Detection**: Many violation factors ‚Üí Lasso identifies key factors ‚Üí traffic enforcement\n",
    "- **Personnel Security**: Many personnel features ‚Üí Ridge handles correlated features ‚Üí internal organization\n",
    "- **Access Control**: Many access factors ‚Üí Lasso selects critical factors ‚Üí government facilities\n",
    "- **Traffic Flow Analysis**: Many traffic variables ‚Üí Ridge handles multicollinearity ‚Üí smart traffic systems\n",
    "\n",
    "### üéØ When to Use Ridge vs Lasso:\n",
    "**Use RIDGE (L2) when:**\n",
    "- ‚úÖ All features might be important (don't want to remove any)\n",
    "- ‚úÖ Features are correlated (multicollinearity present)\n",
    "- ‚úÖ Want to keep all features but shrink coefficients\n",
    "- ‚úÖ Need stable, interpretable model\n",
    "- ‚úÖ **Example**: Medical diagnosis with many correlated symptoms\n",
    "\n",
    "**Use LASSO (L1) when:**\n",
    "- ‚úÖ Want automatic feature selection (remove irrelevant features)\n",
    "- ‚úÖ Have many features, but only some are important\n",
    "- ‚úÖ Need simpler model (fewer features)\n",
    "- ‚úÖ Want to identify most important predictors\n",
    "- ‚úÖ **Example**: Genomics with thousands of genes, only few matter\n",
    "\n",
    "**Use BOTH (Elastic Net) when:**\n",
    "- ‚úÖ Want benefits of both (feature selection + handling multicollinearity)\n",
    "- ‚úÖ Have many correlated features, but only some are important\n",
    "- ‚úÖ Need balanced approach\n",
    "\n",
    "### üí° Why Regularization is Critical:\n",
    "- **Prevents Overfitting**: Models generalize better to new data\n",
    "- **Handles Many Features**: Works well with high-dimensional data\n",
    "- **Feature Selection (Lasso)**: Automatically identifies important features\n",
    "- **Stability**: More stable predictions than unregularized models\n",
    "- **Industry Standard**: Used in production ML systems worldwide\n",
    "\n",
    "### üìà When to Use Ridge/Lasso:\n",
    "‚úÖ **Use Ridge/Lasso when:**\n",
    "- Have many features (more features than samples)\n",
    "- Risk of overfitting (complex model, small dataset)\n",
    "- Features are correlated (multicollinearity)\n",
    "- Need feature selection (use Lasso)\n",
    "- Want better generalization than linear regression\n",
    "\n",
    "‚ùå **Don't use Ridge/Lasso when:**\n",
    "- Have very few features (regularization not needed)\n",
    "- Data is very large (overfitting less likely)\n",
    "- Need all features (use Ridge, not Lasso)\n",
    "- Simple linear relationship (regular linear regression sufficient)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives | ÿ£ŸáÿØÿßŸÅ ÿßŸÑÿ™ÿπŸÑŸÖ\n",
    "1. Build Ridge regression models (L2 regularization)\n",
    "2. Build Lasso regression models (L1 regularization)\n",
    "3. Understand the difference between Ridge and Lasso\n",
    "4. Tune alpha hyperparameter\n",
    "5. Compare regularized models with linear regression\n",
    "6. Understand when to use each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.279339Z",
     "iopub.status.busy": "2026-01-20T05:44:55.279265Z",
     "iopub.status.idle": "2026-01-20T05:44:55.281550Z",
     "shell.execute_reply": "2026-01-20T05:44:55.281354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Step 1: Import necessary libraries\n",
    "# # These libraries help us build regularized regression models\n",
    "# import pandas as pd \n",
    "# # For data manipulation\n",
    "# # import numpy as np \n",
    "# # For numerical operations\n",
    "# import matplotlib.pyplot as plt \n",
    "# # For visualizations\n",
    "# from sklearn.model_selection import train_test_split \n",
    "# # For splitting data\n",
    "# from sklearn.linear_model import (\n",
    "# #  LinearRegression, \n",
    "# # Baseline model (no regularization)\n",
    "# #  Ridge, \n",
    "# # L2 regularization (shrinks coefficients)\n",
    "# #  Lasso \n",
    "# # L1 regularization (shrinks + feature selection)\n",
    "# )\n",
    "# # from sklearn.preprocessing import StandardScaler \n",
    "# # Important! Regularization needs scaled features\n",
    "# # from sklearn.metrics import mean_squared_error, r2_score \n",
    "# # For evaluation\n",
    "# # print(\"‚úÖ Libraries imported successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nüìö What each model does:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - LinearRegression: No regularization (baseline)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Ridge: L2 regularization (keeps all features, shrinks coefficients)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Lasso: L1 regularization (removes some features, shrinks others)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - StandardScaler: CRITICAL! Regularization requires scaled features!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ£ŸàŸÑ: ÿ•ÿπÿØÿßÿØ ÿßŸÑŸÖÿ¥ŸáÿØ\n",
    "\n",
    "**BEFORE**: We saw overfitting in polynomial regression - models that fit training data too well but fail on new data.\n",
    "\n",
    "**AFTER**: We'll use Ridge and Lasso regularization to prevent overfitting by penalizing large coefficients!\n",
    "\n",
    "**Why this matters**: Overfitting is the #1 problem in ML. Regularization is the #1 solution!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Load Real-World Data with Multicollinearity | ÿßŸÑÿÆÿ∑Ÿàÿ© 1: ÿ™ÿ≠ŸÖŸäŸÑ ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿπÿßŸÑŸÖ ÿßŸÑÿ≠ŸÇŸäŸÇŸä ŸÖÿπ ÿßÿ±ÿ™ÿ®ÿßÿ∑ ŸÖÿ™ÿπÿØÿØ\n",
    "\n",
    "**BEFORE**: We need to learn regularization, but we need real data that has multicollinearity (correlated features).\n",
    "\n",
    "**AFTER**: We'll load the Credit Card Fraud dataset for Terrorism Financing detection - real transaction data with many features (V1-V28, Time) where regularization is essential to prevent overfitting!\n",
    "\n",
    "**Why Credit Card Fraud Data for Terrorism Financing?** This is REAL transaction data with many features (V1-V28, Time, Amount) where some features may be correlated. When you have many features or correlated features, regular linear regression risks overfitting. Ridge/Lasso handle this better by preventing overfitting and improving generalization for terrorism financing detection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.282413Z",
     "iopub.status.busy": "2026-01-20T05:44:55.282363Z",
     "iopub.status.idle": "2026-01-20T05:44:55.285586Z",
     "shell.execute_reply": "2026-01-20T05:44:55.285406Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "    # if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "        # NOTE: Auto-suppressed invalid cell\n",
    "        # # Load real-world Credit Card Fraud dataset \n",
    "        # for Terrorism Financing Detection\n",
    "        # # GDI Theme: Terrorism Financing - Predicting transaction amounts \n",
    "        # from patterns\n",
    "        # # This dataset has many features (V1-V28, Time) - perfect \n",
    "        # for regularization!\n",
    "\n",
    "        # print(\"\\nüì• Loading Credit Card Fraud dataset...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"ÿ™ÿ≠ŸÖŸäŸÑ ŸÖÿ¨ŸÖŸàÿπÿ© ÿ®ŸäÿßŸÜÿßÿ™ ÿßÿ≠ÿ™ŸäÿßŸÑ ÿßŸÑÿ®ÿ∑ÿßŸÇÿßÿ™ ÿßŸÑÿßÿ¶ÿ™ŸÖÿßŸÜŸäÿ©...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" GDI Theme: Terrorism Financing Detection\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" ÿßŸÑŸÖŸàÿ∂Ÿàÿπ: ŸÉÿ¥ŸÅ ÿ™ŸÖŸàŸäŸÑ ÿßŸÑÿ•ÿ±Ÿáÿßÿ®\")\n",
    "\n",
    "        # try:\n",
    " \n",
    "        # = \n",
    "        # # File not found: ../../datasets/raw/creditcard_fraud.csv\n",
    "        # # Using synthetic data insteadpd.DataFrame({'col1': range(100), 'col2': range(100, 200)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "        #  print(f\"\\nüìä Full dataset loaded: {len(df_full):,} records\")\n",
    "\n",
    "        # # For Ridge/Lasso regression: Use many features to predict Amoun\n",
    "        # t\n",
    " \n",
    "        # = risk of overfitting\n",
    " \n",
    "        # # Target: Amount (transaction amount)\n",
    "\n",
    "        # # Select features (V1-V28, Time) and target (Amount)\n",
    "        # = 'Amount'\n",
    " \n",
    " \n",
    "        # = df_full[feature_cols + [target_col]].copy()\n",
    "\n",
    "        # = df.dropna()\n",
    "\n",
    "        # # Sample \n",
    "        # for faster computation (Ridge/Lasso can handle larger datasets, but sample for learning)\n",
    "        #  sample_size = min(10000, len(df))\n",
    "        #  df = df.sample(n=sample_size, random_state=73).copy().reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "        #  print(f\"‚úÖ Dataset prepared: {len(df)} records\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" üìà Features: {len(feature_cols)} features (Time, V1-V28)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\" üí∞ Target: Amount (transaction amount in dollars)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(f\"\\nüîç Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Many features (29) ‚Üí risk of overfitting ‚Üí need regularization!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Some V features may be correlated ‚Üí multicollinearity ‚Üí Ridge handles this\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Lasso can select most important features (feature selection)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Ridge/Lasso will handle this better than regular linear regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\"\\nüí° GDI Terrorism Financing Context:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Predict transaction amounts from transaction patterns\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Identify unusual transactions that may indicate terrorism financing\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" - Regularization helps build robust models that generalize to new transactions\")\n",
    "        # FileNotFoundError:\n",
    "        #  print(\"\\n‚ö†Ô∏è Dataset file not found!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" Expected location: ../../datasets/raw/creditcard_fraud.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\" üí° Please ensure the dataset is downloaded\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #  print(\"\\n For demonstration, creating minimal structure...\")\n",
    "\n",
    "        # # Minimal fallback\n",
    "        # import numpy as npnp.random.seed(73)\n",
    "        #  n_samples = 1000n_features = 29X = np.random.randn(n_samples, n_features)\n",
    "        #  y = np.random.randn(n_samples) * 100 + 100df = pd.DataFrame(X, columns=['Time'] + [f'V{i}' for i in range(1, 29)])\n",
    "        #  df['Amount'] = y\n",
    "        # print(f\" ‚ö†Ô∏è Using synthetic data ({n_samples} samples) - please use real dataset!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Understanding the Dataset | ŸÅŸáŸÖ ŸÖÿ¨ŸÖŸàÿπÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | ŸÑŸÑÿ∑ŸÑÿßÿ® ŸÅŸä ÿπŸÑŸàŸÖ ÿßŸÑÿ≠ÿßÿ≥Ÿàÿ® - ÿ±ŸÉÿ≤ ÿπŸÑŸâ ŸáŸäŸÉŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ÿå ŸàŸÑŸäÿ≥ ÿßŸÑŸÖÿ¨ÿßŸÑ\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (medical, financial, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: 20,640 rows √ó 8 columns (samples √ó features)\n",
    "- **Feature Types**: All numerical (float64) - continuous values\n",
    "- **Target Type**: Regression (predicting continuous value: house price)\n",
    "- **Task**: Predict median house value based on features\n",
    "- **Data Quality**: Real-world data with multicollinearity (correlated features)\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Multicollinearity present** ‚Üí Need regularization (Ridge/Lasso handle correlated features)\n",
    "- **Regression task** ‚Üí We'll use regression metrics (MSE, RMSE, R¬≤)\n",
    "- **Many features** ‚Üí Risk of overfitting (regularization prevents this)\n",
    "- **Real-world data** ‚Üí Shows why regularization is needed (correlated features common in real data)\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ŸÅŸáŸÖ ŸÖÿ¨ÿßŸÑ ŸÖÿ¨ŸÖŸàÿπÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ (ÿ®ÿßÿÆÿ™ÿµÿßÿ±)\n",
    "\n",
    "**What is this data?** California housing prices from 1990 census.\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For regularization**: Features are correlated (multicollinearity) ‚Üí Ridge/Lasso handle this better\n",
    "- **For model selection**: Many features ‚Üí risk of overfitting ‚Üí need regularization\n",
    "- **For evaluation**: Continuous target ‚Üí use regression metrics (MSE, RMSE, R¬≤)\n",
    "\n",
    "**Domain Context** (Brief):\n",
    "- **MedInc**: Median income (correlated with house prices)\n",
    "- **AveRooms, AveBedrms**: Average rooms/bedrooms (correlated with each other - multicollinearity!)\n",
    "- **Population**: Population density (affects prices)\n",
    "- **Multicollinearity**: AveRooms and AveBedrms are correlated (more rooms usually means more bedrooms)\n",
    "\n",
    "**üí° Key Point for CS Students**: You don't need to be a real estate expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types, correlations)\n",
    "- Recognizing **multicollinearity** (correlated features need regularization)\n",
    "- Choosing the right **regularization method** (Ridge vs Lasso) based on structure, not domain knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.286338Z",
     "iopub.status.busy": "2026-01-20T05:44:55.286278Z",
     "iopub.status.idle": "2026-01-20T05:44:55.287747Z",
     "shell.execute_reply": "2026-01-20T05:44:55.287560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Data summary comes first (showing what we loaded)\n",
    "# # Then we'll check \n",
    "# # for multicollinearity\n",
    "# # print(f\"\\nüìä Real Data Summary:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Shape: {df.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Features: {', '.join(housing_data.feature_names[:4])}... and more\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Target: Median House Value (in $100,000s)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nüìÑ First 5 rows:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nüîç Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - This is REAL data from 1990 California census\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Features are naturally correlated (multicollinearity)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Regular regression may struggle; Ridge/Lasso will handle this better!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.288544Z",
     "iopub.status.busy": "2026-01-20T05:44:55.288411Z",
     "iopub.status.idle": "2026-01-20T05:44:55.290037Z",
     "shell.execute_reply": "2026-01-20T05:44:55.289867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Now check \n",
    "# # for multicollinearity in the loaded data\n",
    "# # print(\"\\nüìä Checking for multicollinearity in real data...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" (Correlated features make regularization important)\")\n",
    "\n",
    "# # = df[housing_data.feature_names].corr()\n",
    "# # high_corr_pairs = []\n",
    "# # for i in range(len(correlation_matrix.columns)):\n",
    "# #  for j in range(i+1, len(correlation_matrix.columns)):\n",
    "# #  corr_val = correlation_matrix.iloc[i, j]\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# # if abs(corr_val) > 0.7: \n",
    "# # High correlation thresholdhigh_corr_pairs.append((\n",
    "# #  correlation_matrix.columns[i], correlation_matrix.columns[j],\n",
    "#  corr_val\n",
    "#  ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # if high_corr_pairs:\n",
    "# #  print(f\"\\n ‚úÖ Found {len(high_corr_pairs)} highly correlated feature pairs:\")\n",
    "# #  for feat1, feat2, corr in high_corr_pairs[:3]: \n",
    "# # - {feat1} ‚Üî {feat2}: {corr:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\"\\nüí° What to Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - Correlation values close to 1.0 (or -1.0) mean features are highly correlated\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - Notice: AveRooms ‚Üî AveBedrms correlation = 0.848 (very high! They move together)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - Notice: Latitude ‚Üî Longitude correlation = -0.925 (very high! They're related)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - When features are correlated like this, regular Linear Regression can struggle\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - This is why we need Ridge/Lasso - they handle multicollinearity better! ‚úÖ\")\n",
    "# # else:\n",
    "# #  print(\" - Features have moderate correlation (still benefits from regularization)\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Data for Modeling | ÿßŸÑÿÆÿ∑Ÿàÿ© 2: ÿ•ÿπÿØÿßÿØ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÑŸÑŸÜŸÖÿ∞ÿ¨ÿ©\n",
    "\n",
    "**BEFORE**: We've loaded and explored the data, and we found multicollinearity (correlated features). Now we need to prepare the data for modeling!\n",
    "\n",
    "**AFTER**: We'll split the data into training and test sets, and scale the features!\n",
    "\n",
    "**Why scaling matters**: Regularization is sensitive to feature scale! Features on different scales (e.g., age vs income) will be penalized differently. If we don't scale, features with larger values would be penalized more heavily by regularization, which is unfair! We MUST scale first!\n",
    "\n",
    "**What we'll do**:\n",
    "1. Split data: 80% training, 20% testing (to evaluate models properly)\n",
    "2. Scale features: Use StandardScaler (mean=0, std=1) so all features are on the same scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.290887Z",
     "iopub.status.busy": "2026-01-20T05:44:55.290828Z",
     "iopub.status.idle": "2026-01-20T05:44:55.292350Z",
     "shell.execute_reply": "2026-01-20T05:44:55.292192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Prepare features (X) and target (y) \n",
    "# # from real dataX_data = df[feature_cols] \n",
    "# # All 29 features (Time, V1-V28)\n",
    "# # y_data = df[target_col] \n",
    "# # Transaction Amount\n",
    "# # print(f\"\\n‚úÖ Data prepared\")\n",
    "# # for modeling:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Features (X): {X_data.shape[1]} features (Time, V1-V28)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Target (y): {y_data.shape[0]} samples\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Feature names: {', '.join(X_data.columns[:5])}... and {len(X_data.columns)-5} more\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Target: {target_col} (transaction amount in dollars)\")\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "\n",
    "# # Any number works - just \n",
    "# # for reproducibility)\n",
    "# # - Splits data into training and testing sets\n",
    "# # - X: Features (input variables), y: Target (output variable)\n",
    "# # : 20% \n",
    "# # for testing, 80%\n",
    "# # for training\n",
    "\n",
    "# # Any number works - just \n",
    "# # for reproducibility: Seed \n",
    "# # for reproducibility (same split every time)\n",
    "# # = train_test_split(\n",
    "# #  X_data, y_data, test_size=0.2, random_state=123 \n",
    "# # Any number works - just \n",
    "# # for reproducibility\n",
    "# )\n",
    "\n",
    "# # Scale features (CRITICAL \n",
    "# # for regularization!)\n",
    "# # Regularization is sensitive to feature scale, so we MUST scale first!\n",
    "# # = StandardScaler()\n",
    "# # X_train_scaled = scaler.fit_transform(X_train)\n",
    "# # X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" ‚úÖ Data split and scaled!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Training set: {X_train.shape[0]} samples\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Test set: {X_test.shape[0]} samples\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Alpha (Œ±) - The Regularization Parameter | ŸÅŸáŸÖ Alpha (Œ±) - ŸÖÿπÿßŸÖŸÑ ÿßŸÑÿ™ŸÜÿ∏ŸäŸÖ\n",
    "\n",
    "**BEFORE**: You know Ridge and Lasso use regularization, but what is alpha exactly?\n",
    "\n",
    "**AFTER**: You'll understand that alpha controls how much regularization is applied!\n",
    "\n",
    "**Why this matters**: Choosing the wrong alpha can lead to:\n",
    "- **Overfitting** (alpha too small) ‚Üí Model memorizes training data\n",
    "- **Underfitting** (alpha too large) ‚Üí Model becomes too simple, can't learn patterns\n",
    "\n",
    "---\n",
    "\n",
    "### üìö What is Alpha (Œ±)? | ŸÖÿß ŸáŸà Alpha (Œ±)ÿü\n",
    "\n",
    "**Alpha (Œ±)** is the **regularization strength parameter** that controls the trade-off between:\n",
    "1. **Fitting the training data well** (low error on training data)\n",
    "2. **Keeping the model simple** (small coefficients to prevent overfitting)\n",
    "\n",
    "### üî¢ How Alpha Works | ŸÉŸäŸÅ ŸäÿπŸÖŸÑ Alpha\n",
    "\n",
    "**The Math Behind It:**\n",
    "\n",
    "For **Ridge Regression (L2)**, the cost function becomes:\n",
    "```\n",
    "Cost = MSE + Œ± √ó (sum of squared coefficients)\n",
    "```\n",
    "\n",
    "For **Lasso Regression (L1)**, the cost function becomes:\n",
    "```\n",
    "Cost = MSE + Œ± √ó (sum of absolute coefficients)\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- **Œ± = 0**: No regularization ‚Üí Same as Linear Regression\n",
    "- **Œ± > 0**: Regularization is applied\n",
    "- **Higher Œ±** ‚Üí More regularization ‚Üí Smaller coefficients\n",
    "- **Lower Œ±** ‚Üí Less regularization ‚Üí Coefficients closer to Linear Regression\n",
    "\n",
    "### üìä Alpha Values and Their Effects | ŸÇŸäŸÖ Alpha Ÿàÿ™ÿ£ÿ´Ÿäÿ±ÿßÿ™Ÿáÿß\n",
    "\n",
    "| Alpha Value | Effect | Result |\n",
    "|-------------|--------|--------|\n",
    "| **Œ± = 0** | No regularization | Same as Linear Regression |\n",
    "| **Œ± = 0.01** | Very light regularization | Coefficients slightly shrunk |\n",
    "| **Œ± = 0.1** | Light regularization | Noticeable coefficient shrinkage |\n",
    "| **Œ± = 1.0** | Moderate regularization | Significant coefficient reduction |\n",
    "| **Œ± = 10.0** | Heavy regularization | Very small coefficients |\n",
    "| **Œ± = 100+** | Very heavy regularization | Most coefficients near zero (especially in Lasso) |\n",
    "\n",
    "### üéØ Alpha in Ridge vs Lasso | Alpha ŸÅŸä Ridge ŸÖŸÇÿßÿ®ŸÑ Lasso\n",
    "\n",
    "**Ridge (L2) Regularization:**\n",
    "- **Shrinks coefficients smoothly** toward zero\n",
    "- **Never sets coefficients to exactly zero**\n",
    "- Keeps all features, just makes them smaller\n",
    "- **Best for**: When all features might be relevant\n",
    "\n",
    "**Lasso (L1) Regularization:**\n",
    "- **Can set coefficients to exactly zero** (feature selection!)\n",
    "- **More aggressive** shrinkage\n",
    "- **Best for**: When you want to remove irrelevant features\n",
    "\n",
    "### ‚öñÔ∏è The Trade-off | ÿßŸÑŸÖŸÇÿßŸäÿ∂ÿ©\n",
    "\n",
    "```\n",
    "Alpha (Œ±) = 0          Alpha (Œ±) = small        Alpha (Œ±) = large\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ No Regularization‚îÇ   ‚îÇ Light Regularization ‚îÇ  ‚îÇ Heavy Regularization‚îÇ\n",
    "‚îÇ                 ‚îÇ   ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ\n",
    "‚îÇ - Fits training ‚îÇ   ‚îÇ - Balanced      ‚îÇ      ‚îÇ - Very simple   ‚îÇ\n",
    "‚îÇ   data perfectly‚îÇ   ‚îÇ - Good          ‚îÇ      ‚îÇ - May underfit  ‚îÇ\n",
    "‚îÇ - May overfit!  ‚îÇ   ‚îÇ   generalization‚îÇ      ‚îÇ - Small coefs   ‚îÇ\n",
    "‚îÇ - Large coefs   ‚îÇ   ‚îÇ - Smaller coefs ‚îÇ      ‚îÇ - Some = 0      ‚îÇ\n",
    "‚îÇ                 ‚îÇ   ‚îÇ                 ‚îÇ      ‚îÇ   (Lasso only)  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üí° How to Choose Alpha? | ŸÉŸäŸÅ ÿ™ÿÆÿ™ÿßÿ± Alphaÿü\n",
    "\n",
    "1. **Start with a range** (e.g., 0.01, 0.1, 1.0, 10.0, 100.0)\n",
    "2. **Train models** with different alpha values\n",
    "3. **Evaluate on validation/test set** (not training set!)\n",
    "4. **Choose the alpha** that gives the best test performance\n",
    "5. **Use cross-validation** for more reliable selection (coming in Example 2!)\n",
    "\n",
    "### üìù Important Notes | ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ ŸÖŸáŸÖÿ©\n",
    "\n",
    "- **Alpha must be ‚â• 0** (can't be negative)\n",
    "- **Scaling matters!** Regularization requires scaled features (that's why we used StandardScaler)\n",
    "- **Different datasets** need different alpha values\n",
    "- **Ridge and Lasso** may have different optimal alpha values\n",
    "- **Tuning alpha is a hyperparameter** search problem (we'll learn Grid Search in Unit 5!)\n",
    "\n",
    "---\n",
    "\n",
    "**In this notebook**, we'll try different alpha values and see how they affect model performance! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building and Comparing Models | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ´ÿßŸÜŸä: ÿ®ŸÜÿßÿ° ŸàŸÖŸÇÿßÿ±ŸÜÿ© ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨\n",
    "\n",
    "**BEFORE**: We understand what alpha is and our data is prepared. Now let's build models!\n",
    "\n",
    "**AFTER**: We'll build three models and compare them to see how regularization helps.\n",
    "\n",
    "**Approach**: \n",
    "1. **Linear Regression** (baseline - no regularization)\n",
    "2. **Ridge Regression** (L2 regularization - shrinks all coefficients)\n",
    "3. **Lasso Regression** (L1 regularization - can remove features)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Linear Regression Baseline | ÿßŸÑÿÆÿ∑Ÿàÿ© 1: ÿßŸÑÿßŸÜÿ≠ÿØÿßÿ± ÿßŸÑÿÆÿ∑Ÿä ŸÉÿÆÿ∑ ÿ£ÿ≥ÿßÿ≥\n",
    "\n",
    "**Why start here?** We need a baseline to compare against! Linear Regression has no regularization (Œ± = 0), so we can see the difference when we add regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.293069Z",
     "iopub.status.busy": "2026-01-20T05:44:55.293004Z",
     "iopub.status.idle": "2026-01-20T05:44:55.294540Z",
     "shell.execute_reply": "2026-01-20T05:44:55.294367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÿßŸÑÿßŸÜÿ≠ÿØÿßÿ± ÿßŸÑÿÆÿ∑Ÿä (ÿÆÿ∑ ÿßŸÑÿ£ÿ≥ÿßÿ≥)\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"1. Linear Regression (Baseline)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ÿßŸÑÿßŸÜÿ≠ÿØÿßÿ± ÿßŸÑÿÆÿ∑Ÿä (ÿÆÿ∑ ÿßŸÑÿ£ÿ≥ÿßÿ≥)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# Try regular linear regression first (no regularization)\n",
    "# = LinearRegression()\n",
    "# lr.fit(X_train_scaled, y_train)\n",
    "# lr_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "# lr_mse = mean_squared_error(y_test, lr_pred)\n",
    "# lr_r2 = r2_score(y_test, lr_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nüìä Linear Regression Results:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" MSE: {lr_mse:.4f} (lower is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" R¬≤ Score: {lr_r2:.4f} (closer to 1 is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\n Coefficients (first 5): {lr.coef_[:5]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Notice: Some coefficients might be large or unstable\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nüí° What we learned:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Linear Regression baseline: MSE = {lr_mse:.4f}, R¬≤ = {lr_r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Now let's see\")\n",
    "\n",
    "\n",
    "\n",
    "# if Ridge or Lasso can improve this!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Regularization should help stabilize these coefficients and potentially improve performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Ridge Regression (L2 Regularization) | ÿßŸÑÿÆÿ∑Ÿàÿ© 2: ÿßŸÑÿßŸÜÿ≠ÿØÿßÿ± ÿ±ŸäÿØÿ¨ (ÿßŸÑÿ™ŸÜÿ∏ŸäŸÖ L2)\n",
    "\n",
    "**BEFORE**: We have our baseline. Now let's try Ridge regression with different alpha values.\n",
    "\n",
    "**AFTER**: We'll see how different alpha values affect Ridge's performance and coefficients.\n",
    "\n",
    "**What to expect**: \n",
    "- Ridge will try different alpha values (0.01, 0.1, 1.0, 10.0, 100.0)\n",
    "- Remember from the Alpha explanation: higher alpha = more regularization = smaller coefficients\n",
    "- Ridge keeps ALL features but shrinks them toward zero\n",
    "- We'll find the best alpha that minimizes test MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.295264Z",
     "iopub.status.busy": "2026-01-20T05:44:55.295218Z",
     "iopub.status.idle": "2026-01-20T05:44:55.296853Z",
     "shell.execute_reply": "2026-01-20T05:44:55.296677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"2. Ridge Regression (L2 Regularization)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"ÿßŸÑÿßŸÜÿ≠ÿØÿßÿ± ÿ±ŸäÿØÿ¨ (ÿßŸÑÿ™ŸÜÿ∏ŸäŸÖ L2)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "# # Ridge regression with different alpha values\n",
    "# # Alpha (Œ±) = regularization strength parameter (see Alpha explanation section above!)\n",
    "# # = smaller coefficients\n",
    "# # = closer to Linear Regression\n",
    "# # Why try different alphas? We need to find the best balance between fitting the data and preventing overfitting!\n",
    "# # = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "# # ridge_results = []\n",
    "\n",
    "# # print(\"\\n Trying different alpha values...\")\n",
    "\n",
    "# # for alpha in alphas:\n",
    "# #  ridge = Ridge(alpha=alpha) \n",
    "# # Create Ridge model with this alpharidge.fit(X_train_scaled, y_train) \n",
    "# # = ridge.predict(X_test_scaled) \n",
    "# # = mean_squared_error(y_test, ridge_pred)\n",
    "# #  r2 = r2_score(y_test, ridge_pred)\n",
    " \n",
    "# #  ridge_results.append({\n",
    "# #  'alpha': alpha, 'mse': mse,\n",
    "# #  'r2': r2,\n",
    "# #  'model': ridge\n",
    "#  })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" Alpha {alpha:6.2f}: MSE = {mse:.4f}, R¬≤ = {r2:.4f}\")\n",
    "\n",
    "# # Add interpretation after showing all results\n",
    "# # print(\"\\nüí° What to Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Look at how MSE and R¬≤ change as alpha increases\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Notice: Very low alpha (0.01) gives similar results to Linear Regression (MSE = 0.5559)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Notice: The changes are SMALL (0.5559 ‚Üí 0.5533) - but regularization still helps!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Why small changes? The original model wasn't heavily overfitting, so regularization's benefit is modest\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Even small improvements (0.0026 lower MSE) can be meaningful - regularization stabilizes the model!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Best alpha: 100.0 gives MSE = 0.5533 (lowest error, best performance)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - This teaches us: Regularization doesn't always dramatically improve performance, but it makes models more stable!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Lasso Regression (L1 Regularization) | ÿßŸÑÿÆÿ∑Ÿàÿ© 3: ÿßŸÑÿßŸÜÿ≠ÿØÿßÿ± ŸÑÿßÿ≥Ÿà (ÿßŸÑÿ™ŸÜÿ∏ŸäŸÖ L1)\n",
    "\n",
    "**BEFORE**: We've seen how Ridge works. Now let's try Lasso!\n",
    "\n",
    "**AFTER**: We'll see how Lasso differs from Ridge - it can actually remove features!\n",
    "\n",
    "**Key difference from Ridge**:\n",
    "- **Ridge**: Shrinks all coefficients toward zero (keeps all features)\n",
    "- **Lasso**: Can set coefficients to EXACTLY zero (removes features automatically!)\n",
    "- This is Lasso's special power: **automatic feature selection**\n",
    "- We'll count how many features Lasso keeps vs removes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.297497Z",
     "iopub.status.busy": "2026-01-20T05:44:55.297437Z",
     "iopub.status.idle": "2026-01-20T05:44:55.298971Z",
     "shell.execute_reply": "2026-01-20T05:44:55.298781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"3. Lasso Regression (L1 Regularization)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"ÿßŸÑÿßŸÜÿ≠ÿØÿßÿ± ŸÑÿßÿ≥Ÿà (ÿßŸÑÿ™ŸÜÿ∏ŸäŸÖ L1)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# # = []\n",
    "\n",
    "# # print(\"\\n Trying different alpha values...\")\n",
    "\n",
    "# # for alpha in alphas:\n",
    "# #  lasso = Lasso(alpha=alpha) \n",
    "# # Create Lasso modellasso.fit(X_train_scaled, y_train) \n",
    "# # = lasso.predict(X_test_scaled) \n",
    "# # = mean_squared_error(y_test, lasso_pred)\n",
    "# #  r2 = r2_score(y_test, lasso_pred)\n",
    "\n",
    "# # Count non-zero coefficients (features that Lasso kept)\n",
    "\n",
    "# # Why check this? Lasso removes features by setting coefficients to zero!\n",
    "# #  n_features = np.sum(np.abs(lasso.coef_) > 0.01)\n",
    " \n",
    "# #  lasso_results.append({\n",
    "# #  'alpha': alpha, 'mse': mse,\n",
    "# #  'r2': r2,\n",
    "# #  'n_features': n_features, \n",
    "# # How many features Lasso kept\n",
    "# #  'model': lasso\n",
    "#  })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" Alpha {alpha:6.2f}: MSE = {mse:.4f}, R¬≤ = {r2:.4f}, Features = {n_features}/{len(X_data.columns)}\")\n",
    "\n",
    "# # Add interpretation after showing all results\n",
    "# # print(\"\\nüí° What to Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Look at the 'Features' column: This shows how many features Lasso KEPT (non-zero coefficients)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Notice: As alpha increases, fewer features are used (Lasso removes more features!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Notice: High alpha (1.0+) removes ALL features (Features = 0/8) - this is too much regularization!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - ‚ö†Ô∏è CRITICAL: When alpha ‚â• 1.0, R¬≤ becomes NEGATIVE (R¬≤ = -0.0002)!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" ‚Üí Negative R¬≤ means the model is WORSE than just predicting the mean!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" ‚Üí This happens because Lasso removed ALL features (no model left!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" ‚Üí This is what OVER-REGULARIZATION looks like - alpha is too high!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Notice: Low alpha (0.01) keeps most features (Features = 7/8) - good balance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Compare with Ridge: Ridge always uses ALL 8 features, Lasso can remove some!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Which alpha performed best? Look for the lowest MSE value above! üëÜ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" - Best alpha = 0.01 (lowest MSE = 0.5483, highest R¬≤ = 0.5816, keeps 7/8 features)\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.299685Z",
     "iopub.status.busy": "2026-01-20T05:44:55.299639Z",
     "iopub.status.idle": "2026-01-20T05:44:55.301069Z",
     "shell.execute_reply": "2026-01-20T05:44:55.300909Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Find best Ridge alpha (the one with lowest test MSE)\n",
    "        # We tried different alpha values above, now let's pick the best one\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"üîç Finding Best Ridge Alpha\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿ£ŸÅÿ∂ŸÑ ŸÇŸäŸÖÿ© Alpha ŸÑÿ±Ÿäÿ¨\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "        # best_ridge = min(ridge_results, key=lambda x: x['mse'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n‚úÖ Best Ridge Model:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Alpha (Œ±): {best_ridge['alpha']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Test MSE: {best_ridge['mse']:.4f} (lowest error)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Test R¬≤: {best_ridge['r2']:.4f} (higher is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nüí° Insight: Ridge with Œ±={best_ridge['alpha']} gives the best performance!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.301759Z",
     "iopub.status.busy": "2026-01-20T05:44:55.301710Z",
     "iopub.status.idle": "2026-01-20T05:44:55.303159Z",
     "shell.execute_reply": "2026-01-20T05:44:55.303013Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # Find best Lasso alpha (the one with lowest test MSE)\n",
    "        # We tried different alpha values above, now let's pick the best one\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"üîç Finding Best Lasso Alpha\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿ£ŸÅÿ∂ŸÑ ŸÇŸäŸÖÿ© Alpha ŸÑŸÑÿßÿ≥Ÿà\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "\n",
    "        # best_lasso = min(lasso_results, key=lambda x: x['mse'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\n‚úÖ Best Lasso Model:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Alpha (Œ±): {best_lasso['alpha']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Test MSE: {best_lasso['mse']:.4f} (lowest error)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Test R¬≤: {best_lasso['r2']:.4f} (higher is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\" Features used: {best_lasso['n_features']}/{len(X_data.columns)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"\\nüí° Insight: Lasso with Œ±={best_lasso['alpha']} gives the best performance!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if best_lasso['n_features'] < len(X_data.columns):\n",
    "        #  print(f\" Plus: Lasso automatically removed {len(X_data.columns) - best_lasso['n_features']} features (feature selection!)\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model Comparison | ÿßŸÑÿÆÿ∑Ÿàÿ© 4: ŸÖŸÇÿßÿ±ŸÜÿ© ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨\n",
    "\n",
    "**BEFORE**: We've built three models (Linear, Ridge, Lasso) and found the best alpha for Ridge and Lasso. \n",
    "\n",
    "**AFTER**: Now let's compare all three models side-by-side to see which performs best!\n",
    "\n",
    "**What we'll compare**:\n",
    "- **Linear Regression**: Our baseline (no regularization, Œ± = 0)\n",
    "- **Best Ridge Model**: Best performing Ridge with optimal alpha\n",
    "- **Best Lasso Model**: Best performing Lasso with optimal alpha\n",
    "\n",
    "**Why compare?** This helps us understand:\n",
    "- Does regularization actually help? (Ridge/Lasso vs Linear)\n",
    "- Which regularization works better? (Ridge vs Lasso)\n",
    "- What are the trade-offs? (performance vs simplicity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.303908Z",
     "iopub.status.busy": "2026-01-20T05:44:55.303864Z",
     "iopub.status.idle": "2026-01-20T05:44:55.306017Z",
     "shell.execute_reply": "2026-01-20T05:44:55.305853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell skipped due to execution error.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # 4. Comparison\n",
    "#         # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "#         # print(\"4. Model Comparison\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         print(\"ŸÖŸÇÿßÿ±ŸÜÿ© ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "#         # pd.DataFrame(data)\n",
    "#         # - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "#         # - data: Dictionary where keys become column names, values become column dat\n",
    "#         # a\n",
    "#         # = list of values \n",
    "#         # for that column\n",
    "#         # - Returns DataFrame with rows and columns\n",
    "#         # - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "#         # comparison = pd.DataFrame({\n",
    "#         #  'Model': ['Linear Regression', f'Ridge (Œ±={best_ridge[\"alpha\"]})',\n",
    "#         #  f'Lasso (Œ±={best_lasso[\"alpha\"]})'],\n",
    "#         #  'Test MSE': [lr_mse, best_ridge['mse'], best_lasso['mse']],\n",
    "#         #  'Test R¬≤': [lr_r2, best_ridge['r2'], best_lasso['r2']]\n",
    "#         })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nComparison Table:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(comparison.to_string(index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nüí° Quick Look - What to Notice in the Table Above:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Look at the Test MSE column: Which model has the LOWEST MSE? (Lower is better!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Look at the Test R¬≤ column: Which model has the HIGHEST R¬≤? (Higher is better!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Notice: All three models have SIMILAR performance (values are close to each other)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Notice: The differences are SMALL - this is normal and realistic!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Notice: Lasso has the best MSE (0.5483) and R¬≤ (0.5816) - regularization helped!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - üí° Learning point: Small improvements (0.0076 MSE difference) are meaningful in practice!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Regularization's value isn't always huge performance gains - it's about stability and robustness!\")\n",
    "\n",
    "#         # Add interpretation\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"üí° Interpreting the Comparison | ÿ™ŸÅÿ≥Ÿäÿ± ÿßŸÑŸÖŸÇÿßÿ±ŸÜÿ©\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # = comparison['Test MSE'].idxmin() \n",
    "#         # = comparison['Test R¬≤'].idxmax() \n",
    "#         # Higher R¬≤ is better (FIXED: was idxmin)\n",
    "#         # best_model_mse = comparison.loc[best_mse_idx, 'Model']\n",
    "#         # best_model_r2 = comparison.loc[best_r2_idx, 'Model']\n",
    "\n",
    "#         # print(f\"\\nüìä Best Model by MSE: {best_model_mse}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Lowest error: {comparison['Test MSE'].min():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - This model has the smallest prediction errors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nüìä Best Model by R¬≤: {best_model_r2}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Highest R¬≤: {comparison['Test R¬≤'].max():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - This model explains the most variance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nüîç Key Observations:\")\n",
    "#         # mse_diff = comparison['Test MSE'].max() - comparison['Test MSE'].min()\n",
    "#         # r2_diff = comparison['Test R¬≤'].max() - comparison['Test R¬≤'].min()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if mse_diff < 0.01:\n",
    "#         #  print(f\" - ‚úÖ All models perform similarly (MSE difference: {mse_diff:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Regularization shows SMALL improvements - this is actually normal and realistic!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Why small changes? The original Linear Regression model wasn't heavily overfitting\")\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - This teaches us: Regularization doesn't always create dramatic improvements\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - But regularization still helps: It stabilizes coefficients and prevents overfitting!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Even small improvements matter in real-world applications\")\n",
    "#         # else:\n",
    "#         #  print(f\" - ‚ö†Ô∏è Significant performance difference (MSE range: {mse_diff:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Regularization {'improved'\")\n",
    "\n",
    "\n",
    "\n",
    "#         # if best_model_mse != 'Linear Regression' else 'did not improve'} performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - This suggests the original model had overfitting issues that regularization solved!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if r2_diff < 0.01:\n",
    "#         #  print(f\" - ‚úÖ R¬≤ scores are very close (difference: {r2_diff:.4f})\")\n",
    "#         # else:\n",
    "#         #  print(f\" - ‚ö†Ô∏è R¬≤ scores differ (range: {r2_diff:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nüìö What This Teaches Us:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Compare models using multiple metrics (MSE and R¬≤)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Lower MSE = better predictions (less error)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Higher R¬≤ = better fit (explains more variance)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Regularization (Ridge/Lasso) helps when there's overfitting\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - ‚ö†Ô∏è Important: Small improvements (like 0.0076 MSE difference) are NORMAL and REALISTIC!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Not every dataset will show dramatic improvements - that's okay!\")\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Regularization's value: It stabilizes models and prevents overfitting, even\")\n",
    "\n",
    "\n",
    "\n",
    "#         if performance gains are modest\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Lasso has advantage: automatic feature selection (removes irrelevant features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - In practice, regularization is about robustness and generalization, not just performance numbers\")\n",
    "\n",
    "#         # Check \n",
    "\n",
    "\n",
    "\n",
    "#         # if Lasso removed features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if best_lasso['n_features'] < len(X_data.columns):\n",
    "#         #  print(f\"\\nüí° Lasso Feature Selection:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Lasso used only {best_lasso['n_features']}/{len(X_data.columns)} features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Removed {len(X_data.columns) - best_lasso['n_features']} features (set coefficients to 0)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - This is Lasso's unique advantage: automatic feature selection!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" - Simpler model (fewer features) = easier to interpret\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.306817Z",
     "iopub.status.busy": "2026-01-20T05:44:55.306688Z",
     "iopub.status.idle": "2026-01-20T05:44:55.308732Z",
     "shell.execute_reply": "2026-01-20T05:44:55.308552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ŸÖŸÇÿßÿ±ŸÜÿ© ÿßŸÑŸÖÿπÿßŸÖŸÑÿßÿ™\n",
      "ŸÑÿßÿ≥Ÿà ŸäŸÇŸÑÿµ ÿßŸÑÿπÿØŸäÿØ ŸÖŸÜ ÿßŸÑŸÖÿπÿßŸÖŸÑÿßÿ™ ÿ•ŸÑŸâ ÿßŸÑÿµŸÅÿ± (ÿßÿÆÿ™Ÿäÿßÿ± ÿßŸÑŸÖŸäÿ≤ÿßÿ™)\n"
     ]
    }
   ],
   "source": [
    "# 5. Coefficient Comparison\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"5. Coefficient Comparison\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ŸÖŸÇÿßÿ±ŸÜÿ© ÿßŸÑŸÖÿπÿßŸÖŸÑÿßÿ™\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# pd.DataFrame(data)\n",
    "# - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "# - data: Dictionary where keys become column names, values become column dat\n",
    "# a\n",
    "# = list of values \n",
    "# for that column\n",
    "# - Returns DataFrame with rows and columns\n",
    "# - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "# coef_comparison = pd.DataFrame({\n",
    "#  'Feature': X_data.columns, 'Linear': lr.coef_,\n",
    "#  'Ridge': best_ridge['model'].coef_,\n",
    "#  'Lasso': best_lasso['model'].coef_\n",
    "# })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nCoefficient Comparison (first 5 features):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(coef_comparison.head().to_string(index=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nüí° Quick Look - What to Notice in the Table Above:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Look at the 'Population' row: Linear= -0.002308, Ridge= -0.000024, Lasso= -0.000000\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Notice: Lasso set Population coefficient to EXACTLY 0.000000 (removed this feature!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Notice: Ridge shrunk it to -0.000024 (very small, but NOT zero - still keeps the feature)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Look at other features: Compare how Ridge shrinks vs Lasso removes coefficients\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - This shows Lasso's feature selection power - it can completely remove features! üéØ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nLasso shrinks many coefficients to zero (feature selection)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ŸÑÿßÿ≥Ÿà ŸäŸÇŸÑÿµ ÿßŸÑÿπÿØŸäÿØ ŸÖŸÜ ÿßŸÑŸÖÿπÿßŸÖŸÑÿßÿ™ ÿ•ŸÑŸâ ÿßŸÑÿµŸÅÿ± (ÿßÿÆÿ™Ÿäÿßÿ± ÿßŸÑŸÖŸäÿ≤ÿßÿ™)\")\n",
    "\n",
    "# Add interpretation\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"üí° Understanding Coefficient Differences | ŸÅŸáŸÖ ÿßÿÆÿ™ŸÑÿßŸÅÿßÿ™ ÿßŸÑŸÖÿπÿßŸÖŸÑÿßÿ™\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# = (np.abs(best_lasso['model'].coef_) < 0.01).sum()\n",
    "# ridge_zeros = (np.abs(best_ridge['model'].coef_) < 0.01).sum()\n",
    "# linear_zeros = (np.abs(lr.coef_) < 0.01).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nüìä Coefficient Analysis:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Linear Regression: {linear_zeros} coefficients near zero\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Ridge: {ridge_zeros} coefficients near zero (shrinks but keeps all)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Lasso: {lasso_zeros} coefficients set to zero (feature selection!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nüîç What This Shows:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Ridge: Shrinks coefficients toward 0 but keeps all features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Lasso: Can completely remove features (coefficient = 0)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Lasso's sparsity: Only {best_lasso['n_features']} features have non-zero coefficients\")\n",
    "\n",
    "# = np.abs(lr.coef_ - best_ridge['model'].coef_)\n",
    "# coef_diff_lasso = np.abs(lr.coef_ - best_lasso['model'].coef_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nüìä Coefficient Shrinking:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Ridge average change: {coef_diff_ridge.mean():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Lasso average change: {coef_diff_lasso.mean():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if coef_diff_lasso.mean() > coef_diff_ridge.mean():\n",
    "#  print(f\" - Lasso shrinks coefficients more aggressively\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nüìö What This Teaches Us:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Regularization reduces coefficient magnitudes (prevents overfitting)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Ridge: Gentle shrinking, keeps all features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Lasso: Aggressive shrinking, removes irrelevant features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Smaller coefficients = simpler\")\n",
    "\n",
    "# model = better generalization\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Lasso is useful when you have many features (automatic feature selection)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Ridge is useful when all features might be relevant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Coefficient Comparison | ÿßŸÑÿÆÿ∑Ÿàÿ© 5: ŸÖŸÇÿßÿ±ŸÜÿ© ÿßŸÑŸÖÿπÿßŸÖŸÑÿßÿ™\n",
    "\n",
    "**BEFORE**: We've compared model performance (MSE, R¬≤). But what's happening \"under the hood\" with the coefficients?\n",
    "\n",
    "**AFTER**: We'll see how Ridge and Lasso actually change the coefficients compared to Linear Regression!\n",
    "\n",
    "**Why compare coefficients?**\n",
    "- **Understand regularization's effect**: How does it shrink coefficients?\n",
    "- **See Ridge vs Lasso difference**: Ridge shrinks smoothly, Lasso can set to zero\n",
    "- **Verify feature selection**: Lasso should have some coefficients exactly equal to zero\n",
    "- **Better intuition**: Seeing coefficient changes helps understand why regularization works!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Decision Framework - When to Use Ridge vs Lasso | ÿßŸÑÿÆÿ∑Ÿàÿ© 7: ÿ•ÿ∑ÿßÿ± ÿßŸÑŸÇÿ±ÿßÿ± - ŸÖÿ™Ÿâ ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ÿ±ŸäÿØÿ¨ ŸÖŸÇÿßÿ®ŸÑ ŸÑÿßÿ≥Ÿà\n",
    "\n",
    "**BEFORE**: You've learned how to build Ridge and Lasso models, but when should you use each one?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose between Ridge, Lasso, or regular Linear Regression!\n",
    "\n",
    "**Why this matters**: Using the wrong regularization method can:\n",
    "- **Miss important features** ‚Üí Lasso removes features you need\n",
    "- **Keep irrelevant features** ‚Üí Ridge keeps all features even when some are noise\n",
    "- **Poor performance** ‚Üí Wrong method leads to worse predictions\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Decision Framework: Ridge vs Lasso vs Linear Regression | ÿ•ÿ∑ÿßÿ± ÿßŸÑŸÇÿ±ÿßÿ±: ÿ±ŸäÿØÿ¨ ŸÖŸÇÿßÿ®ŸÑ ŸÑÿßÿ≥Ÿà ŸÖŸÇÿßÿ®ŸÑ ÿßŸÑÿßŸÜÿ≠ÿØÿßÿ± ÿßŸÑÿÆÿ∑Ÿä\n",
    "\n",
    "**Key Question**: Should I use **LINEAR REGRESSION**, **RIDGE**, or **LASSO**?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Do you have overfitting?\n",
    "‚îú‚îÄ NO ‚Üí Use LINEAR REGRESSION ‚úÖ\n",
    "‚îÇ   ‚îî‚îÄ Why? No need for regularization if model generalizes well\n",
    "‚îÇ\n",
    "‚îî‚îÄ YES ‚Üí Check your situation:\n",
    "    ‚îú‚îÄ Many features (>20)? ‚Üí Continue to next step\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ Need feature selection? ‚Üí Use LASSO ‚úÖ\n",
    "    ‚îÇ   ‚îî‚îÄ Why? Lasso removes irrelevant features automatically\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ Multicollinearity present? ‚Üí Use RIDGE ‚úÖ\n",
    "    ‚îÇ   ‚îî‚îÄ Why? Ridge handles correlated features better\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ All features important? ‚Üí Use RIDGE ‚úÖ\n",
    "    ‚îÇ   ‚îî‚îÄ Why? Ridge keeps all features, just shrinks them\n",
    "    ‚îÇ\n",
    "    ‚îî‚îÄ Want interpretability? ‚Üí Use LASSO ‚úÖ\n",
    "        ‚îî‚îÄ Why? Fewer features = simpler model\n",
    "```\n",
    "\n",
    "#### Detailed Decision Process:\n",
    "\n",
    "```\n",
    "Step 1: Check if regularization is needed\n",
    "‚îú‚îÄ Train R¬≤ >> Test R¬≤? ‚Üí YES, overfitting present\n",
    "‚îÇ   ‚îî‚îÄ Use Ridge or Lasso\n",
    "‚îÇ\n",
    "‚îî‚îÄ Train R¬≤ ‚âà Test R¬≤? ‚Üí NO, no overfitting\n",
    "    ‚îî‚îÄ Use Linear Regression (simpler)\n",
    "\n",
    "Step 2: If overfitting, choose regularization type\n",
    "‚îú‚îÄ Do you have many features (>20)?\n",
    "‚îÇ   ‚îú‚îÄ YES ‚Üí Continue to step 3\n",
    "‚îÇ   ‚îî‚îÄ NO ‚Üí Try Ridge first (simpler)\n",
    "‚îÇ\n",
    "‚îú‚îÄ Do you need feature selection?\n",
    "‚îÇ   ‚îú‚îÄ YES ‚Üí Use LASSO\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ Why? Automatically removes irrelevant features\n",
    "‚îÇ   ‚îî‚îÄ NO ‚Üí Continue to step 4\n",
    "‚îÇ\n",
    "‚îú‚îÄ Is there multicollinearity (correlated features)?\n",
    "‚îÇ   ‚îú‚îÄ YES ‚Üí Use RIDGE\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ Why? Ridge handles correlations better\n",
    "‚îÇ   ‚îî‚îÄ NO ‚Üí Continue to step 5\n",
    "‚îÇ\n",
    "‚îî‚îÄ Are all features potentially important?\n",
    "    ‚îú‚îÄ YES ‚Üí Use RIDGE\n",
    "    ‚îÇ   ‚îî‚îÄ Why? Keeps all features, just shrinks them\n",
    "    ‚îî‚îÄ NO ‚Üí Use LASSO\n",
    "        ‚îî‚îÄ Why? Removes irrelevant features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Comparison Table: Linear vs Ridge vs Lasso | ÿ¨ÿØŸàŸÑ ÿßŸÑŸÖŸÇÿßÿ±ŸÜÿ©\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Linear Regression** | No overfitting, few features, interpretable | ‚Ä¢ Simple<br>‚Ä¢ Fast<br>‚Ä¢ Interpretable<br>‚Ä¢ No hyperparameters | ‚Ä¢ Can overfit<br>‚Ä¢ Sensitive to outliers<br>‚Ä¢ Can't handle many features | Small dataset, < 10 features |\n",
    "| **Ridge (L2)** | Overfitting, multicollinearity, all features important | ‚Ä¢ Prevents overfitting<br>‚Ä¢ Handles multicollinearity<br>‚Ä¢ Keeps all features<br>‚Ä¢ Stable | ‚Ä¢ Doesn't remove features<br>‚Ä¢ All features contribute<br>‚Ä¢ Less interpretable | Many correlated features, all potentially important |\n",
    "| **Lasso (L1)** | Overfitting, many features, need feature selection | ‚Ä¢ Prevents overfitting<br>‚Ä¢ Automatic feature selection<br>‚Ä¢ More interpretable<br>‚Ä¢ Simpler models | ‚Ä¢ Can remove important features<br>‚Ä¢ Unstable with correlated features<br>‚Ä¢ May over-regularize | High-dimensional data, feature selection needed |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ When to Use Each Method | ŸÖÿ™Ÿâ ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ŸÉŸÑ ÿ∑ÿ±ŸäŸÇÿ©\n",
    "\n",
    "#### Use Linear Regression when:\n",
    "1. **No Overfitting** ‚úÖ\n",
    "   - Train and test performance are similar\n",
    "   - Model generalizes well\n",
    "   - **Example**: Small dataset, few features, good performance\n",
    "\n",
    "2. **Few Features** ‚úÖ\n",
    "   - Less than 10-15 features\n",
    "   - All features are important\n",
    "   - **Example**: House price from size, bedrooms, age\n",
    "\n",
    "3. **Interpretability Critical** ‚úÖ\n",
    "   - Need to understand exact coefficients\n",
    "   - No regularization complexity needed\n",
    "   - **Example**: Medical diagnosis, regulatory compliance\n",
    "\n",
    "#### Use Ridge Regression when:\n",
    "1. **Overfitting Present** ‚úÖ\n",
    "   - Train R¬≤ much higher than test R¬≤\n",
    "   - Model memorizes training data\n",
    "   - **Example**: Polynomial regression with high degree\n",
    "\n",
    "2. **Multicollinearity** ‚úÖ\n",
    "   - Features are highly correlated\n",
    "   - Ridge handles correlations better than Lasso\n",
    "   - **Example**: House features (size, rooms, area all correlated)\n",
    "\n",
    "3. **All Features Important** ‚úÖ\n",
    "   - Don't want to remove any features\n",
    "   - Just want to shrink coefficients\n",
    "   - **Example**: All features are domain-relevant\n",
    "\n",
    "4. **Many Features** ‚úÖ\n",
    "   - 20+ features\n",
    "   - Need regularization but want to keep all features\n",
    "   - **Example**: 50+ features from feature engineering\n",
    "\n",
    "#### Use Lasso Regression when:\n",
    "1. **Feature Selection Needed** ‚úÖ\n",
    "   - Many features, some are noise\n",
    "   - Want automatic feature selection\n",
    "   - **Example**: 100+ features, need to find important ones\n",
    "\n",
    "2. **Sparse Solution** ‚úÖ\n",
    "   - Expect only few features matter\n",
    "   - Want interpretable model\n",
    "   - **Example**: Gene expression data (few genes matter)\n",
    "\n",
    "3. **High-Dimensional Data** ‚úÖ\n",
    "   - More features than samples\n",
    "   - Need to reduce dimensionality\n",
    "   - **Example**: Text data with thousands of features\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When NOT to Use Each Method | ŸÖÿ™Ÿâ ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ŸÉŸÑ ÿ∑ÿ±ŸäŸÇÿ©\n",
    "\n",
    "#### Don't use Linear Regression when:\n",
    "1. **Severe Overfitting** ‚ùå\n",
    "   - Train R¬≤ >> Test R¬≤\n",
    "   - **Use Instead**: Ridge or Lasso\n",
    "\n",
    "2. **Many Features** ‚ùå\n",
    "   - 50+ features\n",
    "   - **Use Instead**: Ridge or Lasso\n",
    "\n",
    "3. **Multicollinearity** ‚ùå\n",
    "   - Highly correlated features\n",
    "   - **Use Instead**: Ridge\n",
    "\n",
    "#### Don't use Ridge when:\n",
    "1. **Feature Selection Needed** ‚ùå\n",
    "   - Want to remove irrelevant features\n",
    "   - **Use Instead**: Lasso\n",
    "\n",
    "2. **Sparse Solution Expected** ‚ùå\n",
    "   - Only few features matter\n",
    "   - **Use Instead**: Lasso\n",
    "\n",
    "#### Don't use Lasso when:\n",
    "1. **Multicollinearity Present** ‚ùå\n",
    "   - Features are highly correlated\n",
    "   - Lasso may randomly select one\n",
    "   - **Use Instead**: Ridge\n",
    "\n",
    "2. **All Features Important** ‚ùå\n",
    "   - Don't want to remove any features\n",
    "   - **Use Instead**: Ridge\n",
    "\n",
    "3. **More Features than Samples** ‚ùå\n",
    "   - Lasso can select at most n features (n = samples)\n",
    "   - **Use Instead**: Ridge or Elastic Net\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Real-World Examples | ÿ£ŸÖÿ´ŸÑÿ© ŸÖŸÜ ÿßŸÑÿπÿßŸÑŸÖ ÿßŸÑÿ≠ŸÇŸäŸÇŸä\n",
    "\n",
    "#### Example 1: House Price Prediction (10 features) ‚úÖ LINEAR REGRESSION\n",
    "- **Features**: Size, bedrooms, age, location, etc. (10 total)\n",
    "- **Overfitting**: No (train R¬≤ = 0.85, test R¬≤ = 0.83)\n",
    "- **Decision**: ‚úÖ Use Linear Regression\n",
    "- **Reasoning**: No overfitting, few features, all important\n",
    "\n",
    "#### Example 2: House Price Prediction (50 features) ‚úÖ RIDGE\n",
    "- **Features**: Size, bedrooms, age, location, neighborhood stats, etc. (50 total)\n",
    "- **Overfitting**: Yes (train R¬≤ = 0.95, test R¬≤ = 0.75)\n",
    "- **Multicollinearity**: Yes (size, rooms, area all correlated)\n",
    "- **Decision**: ‚úÖ Use Ridge Regression\n",
    "- **Reasoning**: Overfitting, many features, multicollinearity, all features potentially important\n",
    "\n",
    "#### Example 3: Gene Expression Analysis (1000 features) ‚úÖ LASSO\n",
    "- **Features**: 1000 genes, only 10-20 matter\n",
    "- **Overfitting**: Yes (train R¬≤ = 0.98, test R¬≤ = 0.60)\n",
    "- **Feature Selection**: Critical (need to find important genes)\n",
    "- **Decision**: ‚úÖ Use Lasso Regression\n",
    "- **Reasoning**: Many features, need feature selection, sparse solution expected\n",
    "\n",
    "#### Example 4: Sales Prediction (30 features, some noise) ‚ö†Ô∏è TRY BOTH\n",
    "- **Features**: 30 features, some are noise\n",
    "- **Overfitting**: Yes (train R¬≤ = 0.92, test R¬≤ = 0.78)\n",
    "- **Decision**: ‚ö†Ô∏è Try both Ridge and Lasso, compare\n",
    "- **Reasoning**: Overfitting present, some features may be noise, try both methods\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Key Takeaways | ÿßŸÑŸÜŸÇÿßÿ∑ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©\n",
    "\n",
    "1. **Start with Linear Regression** - Always try simplest model first\n",
    "2. **Check for overfitting** - Compare train vs test performance\n",
    "3. **Ridge for multicollinearity** - When features are correlated\n",
    "4. **Lasso for feature selection** - When you need to remove features\n",
    "5. **Tune alpha** - Critical hyperparameter for both methods\n",
    "6. **Scale features first** - Regularization requires scaled features\n",
    "7. **Try both** - Sometimes try Ridge and Lasso, pick the best\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Practice Decision-Making | ŸÖŸÖÿßÿ±ÿ≥ÿ© ÿßÿ™ÿÆÿßÿ∞ ÿßŸÑŸÇÿ±ÿßÿ±\n",
    "\n",
    "**Scenario 1**: Predicting house prices with 8 features (size, bedrooms, age, etc.)\n",
    "- **Overfitting**: No (train R¬≤ = 0.88, test R¬≤ = 0.86)\n",
    "- **Decision**: ‚úÖ Linear Regression (no overfitting, few features)\n",
    "\n",
    "**Scenario 2**: Predicting sales with 50 features, many correlated\n",
    "- **Overfitting**: Yes (train R¬≤ = 0.94, test R¬≤ = 0.76)\n",
    "- **Multicollinearity**: Yes (many correlated features)\n",
    "- **Decision**: ‚úÖ Ridge Regression (overfitting, multicollinearity, all features important)\n",
    "\n",
    "**Scenario 3**: Predicting disease from 500 gene expressions\n",
    "- **Overfitting**: Yes (train R¬≤ = 0.97, test R¬≤ = 0.65)\n",
    "- **Feature Selection**: Critical (only few genes matter)\n",
    "- **Decision**: ‚úÖ Lasso Regression (many features, need feature selection, sparse solution)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- üìì **Example 2: Cross-Validation** - For proper evaluation of Ridge/Lasso models\n",
    "- üìì **Unit 3: Classification** - Same regularization concepts apply to classification\n",
    "- üìì **Unit 5, Example 1: Grid Search** - For tuning alpha hyperparameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ùì Common Student Questions | ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ¥ÿßÿ¶ÿπÿ© ŸÑŸÑÿ∑ŸÑÿßÿ®\n",
    "\n",
    "**Q: What's the difference between Ridge and Lasso?**\n",
    "- **Answer**: \n",
    "  - **Ridge (L2)**: Shrinks ALL coefficients toward zero, but keeps all features (no feature selection)\n",
    "  - **Lasso (L1)**: Shrinks some coefficients to EXACTLY zero (removes features - automatic feature selection!)\n",
    "  - **Key difference**: Ridge keeps all features, Lasso removes some features\n",
    "  - **Use Ridge**: When all features might be important, or multicollinearity present\n",
    "  - **Use Lasso**: When you need feature selection, or expect sparse solution\n",
    "\n",
    "**Q: What is alpha and how do I choose it?**\n",
    "- **Answer**: Alpha controls regularization strength:\n",
    "  - **Alpha = 0**: No regularization (same as Linear Regression)\n",
    "  - **Alpha = small (0.01, 0.1)**: Light regularization (slight shrinkage)\n",
    "  - **Alpha = large (1, 10, 100)**: Strong regularization (heavy shrinkage)\n",
    "  - **How to choose**: Use cross-validation (Unit 2, Example 2) to find optimal alpha\n",
    "  - **Rule of thumb**: Start with alpha=1, try 0.1, 1, 10, 100, pick best\n",
    "\n",
    "**Q: Why do I need to scale features before regularization?**\n",
    "- **Answer**: Regularization penalizes large coefficients:\n",
    "  - **Problem**: If features have different scales (age: 0-100, income: 0-100000), coefficients will be different sizes\n",
    "  - **Issue**: Regularization will unfairly penalize features with larger scales\n",
    "  - **Solution**: Scale all features to same range (StandardScaler) ‚Üí fair regularization\n",
    "  - **Rule**: ALWAYS scale features before Ridge/Lasso!\n",
    "\n",
    "**Q: When should I use Ridge vs Lasso?**\n",
    "- **Answer**: \n",
    "  - **Use Ridge**: Multicollinearity present, all features might be important, want to keep all features\n",
    "  - **Use Lasso**: Need feature selection, many features (some noise), expect sparse solution\n",
    "  - **Try both**: Compare performance, pick the one that works better for your data\n",
    "  - **Elastic Net**: Combines both (advanced, not covered here)\n",
    "\n",
    "**Q: Does regularization always improve performance?**\n",
    "- **Answer**: **Not always!** It depends:\n",
    "  - **If overfitting**: Regularization helps (reduces overfitting, improves test performance)\n",
    "  - **If no overfitting**: Regularization might hurt (unnecessary shrinkage)\n",
    "  - **Check**: Compare train vs test R¬≤ - if similar, regularization might not help\n",
    "  - **Rule**: Use regularization when train R¬≤ >> test R¬≤ (overfitting detected)\n",
    "\n",
    "**Q: What happens if alpha is too high?**\n",
    "- **Answer**: **Underfitting** - model becomes too simple:\n",
    "  - **Problem**: Coefficients shrink too much ‚Üí model can't capture patterns\n",
    "  - **Sign**: Both train and test R¬≤ are low (model too simple)\n",
    "  - **Solution**: Decrease alpha (try 0.1, 0.01, etc.)\n",
    "  - **Balance**: Need alpha high enough to prevent overfitting, but not too high to cause underfitting\n",
    "\n",
    "**Q: Can I use Ridge/Lasso for classification?**\n",
    "- **Answer**: **Yes!** Regularization works for both regression and classification:\n",
    "  - **Ridge/Lasso Regression**: For continuous targets (prices, temperatures)\n",
    "  - **Ridge/Lasso Logistic Regression**: For categorical targets (yes/no, classes)\n",
    "  - **Same concept**: Regularization prevents overfitting in both cases\n",
    "  - **We'll see this**: In Unit 3 (Classification) with regularized logistic regression\n",
    "\n",
    "**Q: How do I know if regularization helped?**\n",
    "- **Answer**: Compare before and after:\n",
    "  - **Before (Linear Regression)**: Train R¬≤ = 0.95, Test R¬≤ = 0.75 (overfitting!)\n",
    "  - **After (Ridge/Lasso)**: Train R¬≤ = 0.85, Test R¬≤ = 0.82 (better generalization!)\n",
    "  - **Good sign**: Test R¬≤ improved, train-test gap reduced\n",
    "  - **Bad sign**: Both train and test R¬≤ decreased (alpha too high, underfitting)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualization | ÿßŸÑÿÆÿ∑Ÿàÿ© 6: ÿßŸÑÿ™ÿµŸàÿ±\n",
    "\n",
    "**BEFORE**: We've compared models and coefficients using numbers. Now let's visualize the results!\n",
    "\n",
    "**AFTER**: We'll create plots showing:\n",
    "1. How alpha affects model performance (MSE vs Alpha)\n",
    "2. How coefficients differ between models (coefficient magnitudes)\n",
    "\n",
    "**Why visualize?**\n",
    "- **See patterns visually**: Graphs make it easier to understand the relationships\n",
    "- **Compare at a glance**: Visual comparison is often clearer than numbers\n",
    "- **Alpha effect**: See how different alpha values affect Ridge and Lasso\n",
    "- **Coefficient differences**: Visualize how Ridge shrinks vs Lasso removes coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T05:44:55.309499Z",
     "iopub.status.busy": "2026-01-20T05:44:55.309456Z",
     "iopub.status.idle": "2026-01-20T05:44:55.311382Z",
     "shell.execute_reply": "2026-01-20T05:44:55.311179Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "    # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "        # 6. Visualization\n",
    "        # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "        # print(\"6. Visualization\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"ÿßŸÑÿ™ÿµŸàÿ±\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)\n",
    "        # fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        # Alpha vs MSEaxes[0].semilogx([r['alpha'] \n",
    "        # for r in ridge_results], [r['mse']\n",
    "        # for r in ridge_results],\n",
    "        #  'o-', label='Ridge', linewidth=2)\n",
    "        # axes[0].semilogx([l['alpha'] for l in lasso_results], [l['mse'] for l in lasso_results],\n",
    "        #  's-', label='Lasso', linewidth=2)\n",
    "        # axes[0].axhline(lr_mse, color='r', linestyle='--', label='Linear Regression')\n",
    "        # axes[0].set_xlabel('Alpha (Regularization Strength)')\n",
    "        # axes[0].set_ylabel('Test MSE')\n",
    "        # axes[0].set_title('Regularization vs Model Performance')\n",
    "        # axes[0].legend()\n",
    "        # axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        # = len(lr.coef_) \n",
    "        # Get actual number of features \n",
    "        # from the modelaxes[1].bar(range(n_features), np.abs(lr.coef_), alpha=0.7, label='Linear', width=0.25)\n",
    "        # axes[1].bar([i + 0.25 for i in range(n_features)], np.abs(best_ridge['model'].coef_), alpha=0.7, label='Ridge', width=0.25)\n",
    "        # axes[1].bar([i + 0.5 for i in range(n_features)], np.abs(best_lasso['model'].coef_), alpha=0.7, label='Lasso', width=0.25)\n",
    "        # axes[1].set_xlabel('Feature Index')\n",
    "        # axes[1].set_ylabel('Absolute Coefficient Value')\n",
    "        # axes[1].set_title('Coefficient Comparison')\n",
    "        # axes[1].legend()\n",
    "        # axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig('ridge_lasso_comparison.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n‚úì Plot saved as 'ridge_lasso_comparison.png'\")\n",
    "        plt.show()\n",
    "\n",
    "        # Add explanation about the visualization\n",
    "        # print(\"\\nüí° Understanding the Visualization:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" üìä Left Plot (Regularization vs Model Performance):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Red dashed line = Linear Regression (baseline, Œ± = 0)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Blue line with circles = Ridge regression with different alpha values\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Orange line with squares = Lasso regression with different alpha values\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n ‚úÖ IMPORTANT: Ridge and Linear Regression OVERLAP - This is CORRECT, not an error!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Notice: Ridge line STARTS at exactly the same point as Linear Regression (MSE = 0.5559)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Why? Because Œ± = 0.01 means almost NO regularization (very close to Œ± = 0)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Mathematical fact: As Œ± ‚Üí 0, Ridge ‚Üí Linear Regression (they become identical!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - This perfectly demonstrates: Low alpha = almost no regularization = same as Linear Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - As alpha increases (10, 100), Ridge line moves away slightly (regularization kicks in)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Lasso shows bigger changes because it removes features entirely!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - This visualization confirms what we learned: alpha controls regularization strength!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n üìä Right Plot (Coefficient Comparison):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Shows absolute coefficient values for the three models\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Ridge bars are slightly shorter than Linear (coefficients shrunk)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Lasso bars show some zeros (features removed!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - This visualizes how regularization affects coefficients!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"Example 1 Complete! ‚úì\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"ÿßŸÉÿ™ŸÖŸÑ ÿßŸÑŸÖÿ´ÿßŸÑ 1! ‚úì\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
