{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff and Learning Curves\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand bias and variance in machine learning\n",
    "- Analyze bias-variance tradeoff through learning curves\n",
    "- Identify and handle overfitting/underfitting\n",
    "- Select optimal model complexity using validation sets\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of regression and model evaluation\n",
    "- âœ… Python 3.8+ installed\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 04, Unit 2**:\n",
    "- Analyzing bias-variance tradeoff through learning curves\n",
    "- Identifying and handling overfitting/underfitting\n",
    "- Selecting optimal model complexity using validation sets\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Bias-Variance Tradeoff\n",
    "\n",
    "**Bias**: Error from overly simplistic assumptions (underfitting)\n",
    "**Variance**: Error from sensitivity to small fluctuations (overfitting)\n",
    "**Tradeoff**: Finding the right balance between model complexity and generalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:25.223201Z",
     "iopub.status.busy": "2026-01-15T20:04:25.223116Z",
     "iopub.status.idle": "2026-01-15T20:04:26.010322Z",
     "shell.execute_reply": "2026-01-15T20:04:26.010111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:26.011218Z",
     "iopub.status.busy": "2026-01-15T20:04:26.011126Z",
     "iopub.status.idle": "2026-01-15T20:04:26.012698Z",
     "shell.execute_reply": "2026-01-15T20:04:26.012520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: X=(100, 1), y=(100,)\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = np.sin(X).ravel() + 0.3 * np.random.randn(100)\n",
    "\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Learning Curves - Analyzing Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:26.013461Z",
     "iopub.status.busy": "2026-01-15T20:04:26.013405Z",
     "iopub.status.idle": "2026-01-15T20:04:27.678742Z",
     "shell.execute_reply": "2026-01-15T20:04:27.678501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 1:\n",
      "  Final Train MSE: 0.4825\n",
      "  Final Val MSE: 0.8638\n",
      "  Gap (overfitting indicator): 0.3812\n",
      "Degree 3:\n",
      "  Final Train MSE: 0.2482\n",
      "  Final Val MSE: 11.9686\n",
      "  Gap (overfitting indicator): 11.7204\n",
      "Degree 10:\n",
      "  Final Train MSE: 0.0634\n",
      "  Final Val MSE: 455.6392\n",
      "  Gap (overfitting indicator): 455.5758\n"
     ]
    }
   ],
   "source": [
    "# Create learning curves for different model complexities\n",
    "degrees = [1, 3, 10]\n",
    "models = {}\n",
    "\n",
    "for degree in degrees:\n",
    "    model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)), ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, cv=5, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    models[degree] = {\n",
    "        'train_sizes': train_sizes,\n",
    "        'train_mean': -train_scores.mean(axis=1), 'train_std': train_scores.std(axis=1),\n",
    "        'val_mean': -val_scores.mean(axis=1), 'val_std': val_scores.std(axis=1)\n",
    "    }\n",
    "    \n",
    "    print(f\"Degree {degree}:\")\n",
    "    print(f\"  Final Train MSE: {models[degree]['train_mean'][-1]:.4f}\")\n",
    "    print(f\"  Final Val MSE: {models[degree]['val_mean'][-1]:.4f}\")\n",
    "    print(f\"  Gap (overfitting indicator): {models[degree]['val_mean'][-1] - models[degree]['train_mean'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Identifying Overfitting and Underfitting\n",
    "\n",
    "Let's compare model performance to identify optimal complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:04:27.679735Z",
     "iopub.status.busy": "2026-01-15T20:04:27.679673Z",
     "iopub.status.idle": "2026-01-15T20:04:27.682143Z",
     "shell.execute_reply": "2026-01-15T20:04:27.681961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Bias-Variance Analysis:\n",
      "============================================================\n",
      "\n",
      "Degree 1 (Linear - Underfitting):\n",
      "  Characteristic: High bias, low variance\n",
      "  Train MSE: 0.4825\n",
      "  Val MSE: 0.8638\n",
      "  Gap: 0.3812 (small gap, but both high = underfitting)\n",
      "\n",
      "Degree 3 (Balanced):\n",
      "  Characteristic: Balanced bias-variance\n",
      "  Train MSE: 0.2482\n",
      "  Val MSE: 11.9686\n",
      "  Gap: 11.7204 (reasonable gap)\n",
      "\n",
      "Degree 10 (Overfitting - High Variance):\n",
      "  Characteristic: Low bias, high variance\n",
      "  Train MSE: 0.0634\n",
      "  Val MSE: 455.6392\n",
      "  Gap: 455.5758 (large gap = overfitting)\n"
     ]
    }
   ],
   "source": [
    "# Analyze bias-variance tradeoff\n",
    "print(\"=\" * 60)\n",
    "print(\"Bias-Variance Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Degree 1 (Underfitting - High Bias)\n",
    "print(\"\\nDegree 1 (Linear - Underfitting):\")\n",
    "print(\"  Characteristic: High bias, low variance\")\n",
    "print(\"  Train MSE: {:.4f}\".format(models[1]['train_mean'][-1]))\n",
    "print(\"  Val MSE: {:.4f}\".format(models[1]['val_mean'][-1]))\n",
    "print(\"  Gap: {:.4f} (small gap, but both high = underfitting)\".format(\n",
    "    models[1]['val_mean'][-1] - models[1]['train_mean'][-1]))\n",
    "\n",
    "# Degree 3 (Balanced)\n",
    "print(\"\\nDegree 3 (Balanced):\")\n",
    "print(\"  Characteristic: Balanced bias-variance\")\n",
    "print(\"  Train MSE: {:.4f}\".format(models[3]['train_mean'][-1]))\n",
    "print(\"  Val MSE: {:.4f}\".format(models[3]['val_mean'][-1]))\n",
    "print(\"  Gap: {:.4f} (reasonable gap)\".format(\n",
    "    models[3]['val_mean'][-1] - models[3]['train_mean'][-1]))\n",
    "\n",
    "# Degree 10 (Overfitting - High Variance)\n",
    "print(\"\\nDegree 10 (Overfitting - High Variance):\")\n",
    "print(\"  Characteristic: Low bias, high variance\")\n",
    "print(\"  Train MSE: {:.4f}\".format(models[10]['train_mean'][-1]))\n",
    "print(\"  Val MSE: {:.4f}\".format(models[10]['val_mean'][-1]))\n",
    "print(\"  Gap: {:.4f} (large gap = overfitting)\".format(\n",
    "    models[10]['val_mean'][-1] - models[10]['train_mean'][-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Bias**: Model too simple, cannot capture patterns (underfitting)\n",
    "2. **Variance**: Model too complex, memorizes noise (overfitting)\n",
    "3. **Learning Curves**: Show train/validation performance vs training set size\n",
    "4. **Optimal Complexity**: Balance where validation error is minimized\n",
    "\n",
    "### How to Use:\n",
    "- **Large gap between train/val**: Overfitting â†’ reduce complexity\n",
    "- **Both train/val high**: Underfitting â†’ increase complexity\n",
    "- **Both train/val similar and low**: Good model\n",
    "\n",
    "**Reference:** Course 04, Unit 2: \"Analyzing bias-variance tradeoff through learning curves\" and \"Identifying and handling overfitting/underfitting\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}