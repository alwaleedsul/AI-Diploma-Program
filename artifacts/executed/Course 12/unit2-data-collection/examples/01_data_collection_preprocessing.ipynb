{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Preprocessing for Graduation Project\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Collect and acquire datasets for graduation project\n",
    "- Perform comprehensive data cleaning and preprocessing\n",
    "- Implement feature engineering techniques\n",
    "- Validate data quality and create train/validation/test splits\n",
    "- Document data collection and preprocessing procedures\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Unit 1: Project proposal completed\n",
    "- ‚úÖ Understanding of data science pipelines\n",
    "- ‚úÖ Python, Pandas, NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 12, Unit 2**:\n",
    "- Collecting and acquiring datasets\n",
    "- Performing data cleaning and preprocessing using Python libraries\n",
    "- Implementing feature engineering techniques\n",
    "- Validating data quality and preparing train/validation/test splits\n",
    "- Creating data exploration notebooks with visualizations\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Data Collection and Preparation** is critical for project success. Quality data leads to quality models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T00:06:51.349574Z",
     "iopub.status.busy": "2026-01-17T00:06:51.349458Z",
     "iopub.status.idle": "2026-01-17T00:06:52.211162Z",
     "shell.execute_reply": "2026-01-17T00:06:52.210943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported!\n",
      "Ready for data collection and preprocessing!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n",
    "print(\"Ready for data collection and preprocessing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection Strategies\n",
    "\n",
    "Explore different data sources and collection methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T00:06:52.212129Z",
     "iopub.status.busy": "2026-01-17T00:06:52.212031Z",
     "iopub.status.idle": "2026-01-17T00:06:52.214244Z",
     "shell.execute_reply": "2026-01-17T00:06:52.214066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Data Collection Strategies\n",
      "============================================================\n",
      "\n",
      "Public Datasets:\n",
      "  Sources: Kaggle Datasets, UCI Machine Learning Repository, Google Dataset Search...\n",
      "  Advantages: Ready to use, often cleaned, documented\n",
      "  Considerations: Check licensing, ensure relevance to your problem\n",
      "\n",
      "APIs:\n",
      "  Sources: Twitter API (for social media analysis), Reddit API, News APIs...\n",
      "  Advantages: Real-time data, structured format\n",
      "  Considerations: API limits, authentication, rate limiting\n",
      "\n",
      "Web Scraping:\n",
      "  Sources: BeautifulSoup + requests, Scrapy framework, Selenium for dynamic content...\n",
      "  Advantages: Access to large amounts of data\n",
      "  Considerations: Legal and ethical considerations, robots.txt, ToS\n",
      "\n",
      "Custom Collection:\n",
      "  Sources: Surveys, Experiments, Sensors/IoT devices...\n",
      "  Advantages: Tailored to your specific needs\n",
      "  Considerations: Time-consuming, requires IRB approval for human subjects\n",
      "\n",
      "‚úÖ Choose data sources aligned with your project goals and ethical guidelines!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Data Collection Strategies\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_sources = {\n",
    "    \"Public Datasets\": {\n",
    "        \"sources\": [\n",
    "            \"Kaggle Datasets\", \"UCI Machine Learning Repository\",\n",
    "            \"Google Dataset Search\",\n",
    "            \"Papers with Code Datasets\",\n",
    "            \"Hugging Face Datasets\"\n",
    "        ],\n",
    "        \"advantages\": \"Ready to use, often cleaned, documented\",\n",
    "        \"considerations\": \"Check licensing, ensure relevance to your problem\"\n",
    "    },\n",
    "    \"APIs\": {\n",
    "        \"sources\": [\n",
    "            \"Twitter API (for social media analysis)\",\n",
    "            \"Reddit API\",\n",
    "            \"News APIs\",\n",
    "            \"Government data APIs\"\n",
    "        ],\n",
    "        \"advantages\": \"Real-time data, structured format\",\n",
    "        \"considerations\": \"API limits, authentication, rate limiting\"\n",
    "    },\n",
    "    \"Web Scraping\": {\n",
    "        \"sources\": [\n",
    "            \"BeautifulSoup + requests\",\n",
    "            \"Scrapy framework\",\n",
    "            \"Selenium for dynamic content\"\n",
    "        ],\n",
    "        \"advantages\": \"Access to large amounts of data\",\n",
    "        \"considerations\": \"Legal and ethical considerations, robots.txt, ToS\"\n",
    "    },\n",
    "    \"Custom Collection\": {\n",
    "        \"sources\": [\n",
    "            \"Surveys\",\n",
    "            \"Experiments\",\n",
    "            \"Sensors/IoT devices\"\n",
    "        ],\n",
    "        \"advantages\": \"Tailored to your specific needs\",\n",
    "        \"considerations\": \"Time-consuming, requires IRB approval for human subjects\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for source_type, details in data_sources.items():\n",
    "    print(f\"\\n{source_type}:\")\n",
    "    print(f\"  Sources: {', '.join(details['sources'][:3])}...\")\n",
    "    print(f\"  Advantages: {details['advantages']}\")\n",
    "    print(f\"  Considerations: {details['considerations']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Choose data sources aligned with your project goals and ethical guidelines!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Cleaning and Preprocessing Pipeline\n",
    "\n",
    "Comprehensive data cleaning workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T00:06:52.215187Z",
     "iopub.status.busy": "2026-01-17T00:06:52.215121Z",
     "iopub.status.idle": "2026-01-17T00:06:52.221217Z",
     "shell.execute_reply": "2026-01-17T00:06:52.221035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Data Cleaning Pipeline\n",
      "============================================================\n",
      "\n",
      "1. Initial Data Inspection:\n",
      "   Shape: (1000, 4)\n",
      "   Missing values:\n",
      "feature1     0\n",
      "feature2     0\n",
      "feature3    43\n",
      "target       0\n",
      "dtype: int64\n",
      "\n",
      "2. Handling Missing Values:\n",
      "   After dropping: (957, 4)\n",
      "   After filling: Missing values = 0\n",
      "\n",
      "3. Encoding Categorical Variables:\n",
      "   Original categories: ['C' 'A' 'B']\n",
      "   Encoded values: [2 0 1]\n",
      "\n",
      "4. Feature Scaling:\n",
      "   Features scaled: ['feature1', 'feature3']\n",
      "   Mean after scaling: [0.0, 0.0]\n",
      "   Std after scaling: [1.0005, 1.0005]\n",
      "\n",
      "‚úÖ Data cleaning pipeline complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7n/l2c2z2x57871xg4f_0drsv1m0000gn/T/ipykernel_63790/2953942063.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_filled['feature3'].fillna(data_filled['feature3'].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data for demonstration\n",
    "np.random.seed(42)\n",
    "sample_data = pd.DataFrame({\n",
    "    'feature1': np.random.randn(1000), 'feature2': np.random.choice(['A', 'B', 'C'], 1000),\n",
    "    'feature3': np.random.randn(1000) + np.random.choice([0, np.nan], 1000, p=[0.95, 0.05]),\n",
    "    'target': np.random.choice([0, 1], 1000)\n",
    "})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Data Cleaning Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Initial Data Inspection:\")\n",
    "print(f\"   Shape: {sample_data.shape}\")\n",
    "print(f\"   Missing values:\\n{sample_data.isnull().sum()}\")\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\n2. Handling Missing Values:\")\n",
    "# Option 1: Drop rows with missing values\n",
    "data_dropped = sample_data.dropna()\n",
    "print(f\"   After dropping: {data_dropped.shape}\")\n",
    "\n",
    "# Option 2: Fill missing values (mean for numeric, mode for categorical)\n",
    "data_filled = sample_data.copy()\n",
    "data_filled['feature3'].fillna(data_filled['feature3'].mean(), inplace=True)\n",
    "print(f\"   After filling: Missing values = {data_filled.isnull().sum().sum()}\")\n",
    "\n",
    "# Handle categorical variables\n",
    "print(\"\\n3. Encoding Categorical Variables:\")\n",
    "le = LabelEncoder()\n",
    "data_encoded = data_filled.copy()\n",
    "data_encoded['feature2_encoded'] = le.fit_transform(data_encoded['feature2'])\n",
    "print(f\"   Original categories: {data_filled['feature2'].unique()}\")\n",
    "print(f\"   Encoded values: {data_encoded['feature2_encoded'].unique()}\")\n",
    "\n",
    "# Feature scaling\n",
    "print(\"\\n4. Feature Scaling:\")\n",
    "scaler = StandardScaler()\n",
    "numeric_features = ['feature1', 'feature3']\n",
    "data_scaled = data_encoded.copy()\n",
    "data_scaled[numeric_features] = scaler.fit_transform(data_scaled[numeric_features])\n",
    "print(f\"   Features scaled: {numeric_features}\")\n",
    "print(f\"   Mean after scaling: {data_scaled[numeric_features].mean().round(4).tolist()}\")\n",
    "print(f\"   Std after scaling: {data_scaled[numeric_features].std().round(4).tolist()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data cleaning pipeline complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train/Validation/Test Split\n",
    "\n",
    "Proper data splitting for model development and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-17T00:06:52.222035Z",
     "iopub.status.busy": "2026-01-17T00:06:52.221981Z",
     "iopub.status.idle": "2026-01-17T00:06:52.225569Z",
     "shell.execute_reply": "2026-01-17T00:06:52.225386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Data Splitting Strategy\n",
      "============================================================\n",
      "Total samples: 1000\n",
      "Training set: 600 (60.0%)\n",
      "Validation set: 200 (20.0%)\n",
      "Test set: 200 (20.0%)\n",
      "\n",
      "‚úÖ Data split complete!\n",
      "üìù Use validation set for hyperparameter tuning\n",
      "üìù Use test set ONLY for final evaluation\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "X = data_scaled[['feature1', 'feature3', 'feature2_encoded']]\n",
    "y = data_scaled['target']\n",
    "\n",
    "# Split: 60% train, 20% validation, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Data Splitting Strategy\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Training set: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Data split complete!\")\n",
    "print(\"üìù Use validation set for hyperparameter tuning\")\n",
    "print(\"üìù Use test set ONLY for final evaluation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Steps in Data Collection & Preparation:\n",
    "1. **Data Collection**: Identify and acquire relevant datasets\n",
    "2. **Data Inspection**: Understand data structure, types, distributions\n",
    "3. **Data Cleaning**: Handle missing values, outliers, inconsistencies\n",
    "4. **Feature Engineering**: Create meaningful features from raw data\n",
    "5. **Data Splitting**: Train/Validation/Test sets (60/20/20 or 70/15/15)\n",
    "6. **Documentation**: Document all preprocessing steps for reproducibility\n",
    "\n",
    "### Best Practices:\n",
    "- Always check data quality before modeling\n",
    "- Document all preprocessing steps\n",
    "- Save cleaned datasets for reproducibility\n",
    "- Use stratified splitting for imbalanced data\n",
    "- Validate data representativeness\n",
    "\n",
    "**Reference:** Course 12, Unit 2: \"Data Collection and Preparation\" - All practical activities covered\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
