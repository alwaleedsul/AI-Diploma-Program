{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron, MLP, and Deep Learning Framework Setup\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand deep learning fundamentals compared to traditional ML\n",
    "- Implement basic perceptron from scratch\n",
    "- Build Multi-Layer Perceptron (MLP) models\n",
    "- Set up TensorFlow and PyTorch environments\n",
    "- Compare TensorFlow and PyTorch approaches\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of basic machine learning concepts\n",
    "- âœ… Python 3.8+ installed\n",
    "- âœ… Basic NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 08, Unit 1**:\n",
    "- Deep learning fundamentals compared to traditional ML\n",
    "- Setting up TensorFlow and PyTorch\n",
    "- Implementing basic perceptron and MLP\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 1 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Deep Learning** is a subset of machine learning that uses neural networks with multiple layers to learn hierarchical representations of data. Unlike traditional ML, deep learning can automatically discover features from raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:05:16.214247Z",
     "iopub.status.busy": "2026-01-15T12:05:16.214096Z",
     "iopub.status.idle": "2026-01-15T12:05:19.723624Z",
     "shell.execute_reply": "2026-01-15T12:05:19.723420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  TensorFlow not available. Install with: pip install tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PyTorch 2.9.1 imported successfully!\n",
      "âœ… NumPy and Matplotlib ready!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try importing TensorFlow\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    HAS_TF = True\n",
    "    print(f\"âœ… TensorFlow {tf.__version__} imported successfully!\")\n",
    "except ImportError:\n",
    "    HAS_TF = False\n",
    "    print(\"âš ï¸  TensorFlow not available. Install with: pip install tensorflow\")\n",
    "\n",
    "# Try importing PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    HAS_TORCH = True\n",
    "    print(f\"âœ… PyTorch {torch.__version__} imported successfully!\")\n",
    "except ImportError:\n",
    "    HAS_TORCH = False\n",
    "    print(\"âš ï¸  PyTorch not available. Install with: pip install torch\")\n",
    "\n",
    "print(\"âœ… NumPy and Matplotlib ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Deep Learning vs Traditional ML\n",
    "\n",
    "Let's compare the fundamental differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:05:19.724635Z",
     "iopub.status.busy": "2026-01-15T12:05:19.724541Z",
     "iopub.status.idle": "2026-01-15T12:05:19.726709Z",
     "shell.execute_reply": "2026-01-15T12:05:19.726532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Deep Learning vs Traditional ML\n",
      "============================================================\n",
      "\n",
      "Feature Engineering:\n",
      "  Traditional ML: Manual feature extraction required\n",
      "  Deep Learning: Automatic feature learning from raw data\n",
      "\n",
      "Data Requirements:\n",
      "  Traditional ML: Works well with small to medium datasets\n",
      "  Deep Learning: Requires large datasets for best performance\n",
      "\n",
      "Model Complexity:\n",
      "  Traditional ML: Simpler, more interpretable models\n",
      "  Deep Learning: Complex, hierarchical representations\n",
      "\n",
      "Performance:\n",
      "  Traditional ML: Good for structured data\n",
      "  Deep Learning: Excels with unstructured data (images, text, audio)\n",
      "\n",
      "Training Time:\n",
      "  Traditional ML: Faster training\n",
      "  Deep Learning: Longer training, benefits from GPUs\n",
      "\n",
      "âœ… Key Insight: Deep learning automatically learns features, making it powerful for complex patterns!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Deep Learning vs Traditional ML\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison = {\n",
    "    \"Feature Engineering\": {\n",
    "        \"Traditional ML\": \"Manual feature extraction required\", \"Deep Learning\": \"Automatic feature learning from raw data\"\n",
    "    },\n",
    "    \"Data Requirements\": {\n",
    "        \"Traditional ML\": \"Works well with small to medium datasets\",\n",
    "        \"Deep Learning\": \"Requires large datasets for best performance\"\n",
    "    },\n",
    "    \"Model Complexity\": {\n",
    "        \"Traditional ML\": \"Simpler, more interpretable models\",\n",
    "        \"Deep Learning\": \"Complex, hierarchical representations\"\n",
    "    },\n",
    "    \"Performance\": {\n",
    "        \"Traditional ML\": \"Good for structured data\",\n",
    "        \"Deep Learning\": \"Excels with unstructured data (images, text, audio)\"\n",
    "    },\n",
    "    \"Training Time\": {\n",
    "        \"Traditional ML\": \"Faster training\",\n",
    "        \"Deep Learning\": \"Longer training, benefits from GPUs\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for aspect, details in comparison.items():\n",
    "    print(f\"\\n{aspect}:\")\n",
    "    print(f\"  Traditional ML: {details['Traditional ML']}\")\n",
    "    print(f\"  Deep Learning: {details['Deep Learning']}\")\n",
    "\n",
    "print(\"\\nâœ… Key Insight: Deep learning automatically learns features, making it powerful for complex patterns!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Basic Perceptron Implementation\n",
    "\n",
    "A perceptron is the simplest neural network - a single neuron with weights and bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:05:19.727553Z",
     "iopub.status.busy": "2026-01-15T12:05:19.727489Z",
     "iopub.status.idle": "2026-01-15T12:05:19.731762Z",
     "shell.execute_reply": "2026-01-15T12:05:19.731599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Perceptron: Learning AND Gate\n",
      "============================================================\n",
      "\n",
      "Predictions:\n",
      "Input [0,0]: 0 (expected 0)\n",
      "Input [0,1]: 0 (expected 0)\n",
      "Input [1,0]: 0 (expected 0)\n",
      "Input [1,1]: 1 (expected 1)\n",
      "\n",
      "Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"Simple Perceptron implementation from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, n_iterations=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the perceptron\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for _ in range(self.n_iterations):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                # Forward pass\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_predicted = self.activate(linear_output)\n",
    "                \n",
    "                # Update weights and bias\n",
    "                update = self.learning_rate * (y[idx] - y_predicted)\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "    \n",
    "    def activate(self, x):\n",
    "        \"\"\"Step activation function\"\"\"\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self.activate(linear_output)\n",
    "\n",
    "# Example: Simple AND gate\n",
    "print(\"=\" * 60)\n",
    "print(\"Perceptron: Learning AND Gate\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# AND gate truth table\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Train perceptron\n",
    "perceptron = Perceptron(learning_rate=0.1, n_iterations=100)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Test\n",
    "predictions = perceptron.predict(X)\n",
    "print(\"\\nPredictions:\")\n",
    "print(f\"Input [0,0]: {predictions[0]} (expected 0)\")\n",
    "print(f\"Input [0,1]: {predictions[1]} (expected 0)\")\n",
    "print(f\"Input [1,0]: {predictions[2]} (expected 0)\")\n",
    "print(f\"Input [1,1]: {predictions[3]} (expected 1)\")\n",
    "print(f\"\\nAccuracy: {np.mean(predictions == y) * 100:.0f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multi-Layer Perceptron (MLP) with TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:05:19.732535Z",
     "iopub.status.busy": "2026-01-15T12:05:19.732473Z",
     "iopub.status.idle": "2026-01-15T12:05:19.734631Z",
     "shell.execute_reply": "2026-01-15T12:05:19.734367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MLP with TensorFlow (Installation Required)\n",
      "============================================================\n",
      "\n",
      "    To use TensorFlow for MLP:\n",
      "    \n",
      "    1. Install TensorFlow:\n",
      "       pip install tensorflow\n",
      "    \n",
      "    2. Basic MLP structure:\n",
      "       model = tf.keras.Sequential([\n",
      "           tf.keras.layers.Dense(64, activation='relu', input_shape=(n_features,)),\n",
      "           tf.keras.layers.Dense(32, activation='relu'),\n",
      "           tf.keras.layers.Dense(1, activation='sigmoid')  # for binary classification\n",
      "       ])\n",
      "       \n",
      "    3. Compile and train:\n",
      "       model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
      "       model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if HAS_TF:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MLP with TensorFlow/Keras\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Generate sample data\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Build MLP model\n",
    "    model_tf = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model_tf.compile(\n",
    "        optimizer='adam', loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model_tf.summary()\n",
    "    \n",
    "    # Train model (just a few epochs for demonstration)\n",
    "    print(\"\\nTraining model (5 epochs)...\")\n",
    "    history = model_tf.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model_tf.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "    print(\"âœ… TensorFlow MLP created successfully!\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MLP with TensorFlow (Installation Required)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "    To use TensorFlow for MLP:\n",
    "    \n",
    "    1. Install TensorFlow:\n",
    "       pip install tensorflow\n",
    "    \n",
    "    2. Basic MLP structure:\n",
    "       model = tf.keras.Sequential([\n",
    "           tf.keras.layers.Dense(64, activation='relu', input_shape=(n_features,)),\n",
    "           tf.keras.layers.Dense(32, activation='relu'),\n",
    "           tf.keras.layers.Dense(1, activation='sigmoid')  # for binary classification\n",
    "       ])\n",
    "       \n",
    "    3. Compile and train:\n",
    "       model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "       model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Layer Perceptron (MLP) with PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:05:19.735435Z",
     "iopub.status.busy": "2026-01-15T12:05:19.735360Z",
     "iopub.status.idle": "2026-01-15T12:05:21.894727Z",
     "shell.execute_reply": "2026-01-15T12:05:21.894488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MLP with PyTorch\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Training model (5 epochs)...\n",
      "\n",
      "Test Accuracy: 0.6600\n",
      "âœ… PyTorch MLP created successfully!\n"
     ]
    }
   ],
   "source": [
    "if HAS_TORCH:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MLP with PyTorch\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    # Generate sample data\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "    \n",
    "    # Define MLP model\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MLP, self).__init__()\n",
    "            self.fc1 = nn.Linear(20, 64)\n",
    "            self.fc2 = nn.Linear(64, 32)\n",
    "            self.fc3 = nn.Linear(32, 1)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.relu(self.fc2(x))\n",
    "            x = self.sigmoid(self.fc3(x))\n",
    "            return x\n",
    "    \n",
    "    # Create model\n",
    "    model_torch = MLP()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model_torch.parameters(), lr=0.001)\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model_torch)\n",
    "    \n",
    "    # Train model (just a few epochs for demonstration)\n",
    "    print(\"\\nTraining model (5 epochs)...\")\n",
    "    model_torch.train()\n",
    "    for epoch in range(5):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_torch(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    model_torch.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model_torch(X_test_tensor)\n",
    "        test_preds = (test_outputs > 0.5).float()\n",
    "        test_acc = (test_preds == y_test_tensor).float().mean()\n",
    "        print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    print(\"âœ… PyTorch MLP created successfully!\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MLP with PyTorch (Installation Required)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "    To use PyTorch for MLP:\n",
    "    \n",
    "    1. Install PyTorch:\n",
    "       pip install torch\n",
    "    \n",
    "    2. Basic MLP structure:\n",
    "       class MLP(nn.Module):\n",
    "           def __init__(self):\n",
    "               super().__init__()\n",
    "               self.fc1 = nn.Linear(n_features, 64)\n",
    "               self.fc2 = nn.Linear(64, 32)\n",
    "               self.fc3 = nn.Linear(32, 1)\n",
    "           \n",
    "           def forward(self, x):\n",
    "               x = torch.relu(self.fc1(x))\n",
    "               x = torch.relu(self.fc2(x))\n",
    "               x = torch.sigmoid(self.fc3(x))\n",
    "               return x\n",
    "       \n",
    "    3. Train with optimizer:\n",
    "       optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "       for epoch in range(epochs):\n",
    "           optimizer.zero_grad()\n",
    "           outputs = model(X_train)\n",
    "           loss = criterion(outputs, y_train)\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Deep Learning vs Traditional ML**: Automatic feature learning, requires more data, better for unstructured data\n",
    "2. **Perceptron**: Single neuron, can learn simple linear patterns (e.g., AND gate)\n",
    "3. **MLP (Multi-Layer Perceptron)**: Multiple layers of neurons, can learn complex non-linear patterns\n",
    "4. **TensorFlow/Keras**: High-level API, easier to use, great for rapid prototyping\n",
    "5. **PyTorch**: More flexible, imperative style, better for research and custom architectures\n",
    "\n",
    "### Framework Comparison:\n",
    "- **TensorFlow**: Industry standard, production-ready, extensive ecosystem\n",
    "- **PyTorch**: Research-friendly, dynamic computation graphs, intuitive API\n",
    "\n",
    "### When to Use:\n",
    "- **TensorFlow**: Production deployment, large-scale systems, when you need TF Serving\n",
    "- **PyTorch**: Research, experimentation, when you need dynamic graphs\n",
    "\n",
    "**Reference:** Course 08, Unit 1: \"Deep learning fundamentals compared to traditional ML\", \"Setting up TensorFlow and PyTorch\", and \"Implementing basic perceptron and MLP\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}