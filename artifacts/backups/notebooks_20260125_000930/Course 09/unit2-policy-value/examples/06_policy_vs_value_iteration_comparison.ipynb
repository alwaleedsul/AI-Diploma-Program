{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparing Policy Iteration vs Value Iteration\n",
        "\n",
        "## üìö Learning Objectives\n",
        "\n",
        "By completing this notebook, you will:\n",
        "- Understand Policy Iteration algorithm\n",
        "- Understand Value Iteration algorithm\n",
        "- Compare Policy Iteration vs Value Iteration\n",
        "- Implement both algorithms\n",
        "- Analyze convergence and computational efficiency\n",
        "\n",
        "## üîó Prerequisites\n",
        "\n",
        "- ‚úÖ Understanding of MDPs (states, actions, rewards, transitions)\n",
        "- ‚úÖ Understanding of Bellman equations\n",
        "- ‚úÖ Understanding of policies and value functions\n",
        "- ‚úÖ Python knowledge (functions, loops, NumPy)\n",
        "- ‚úÖ Dynamic Programming basics\n",
        "\n",
        "---\n",
        "\n",
        "## Official Structure Reference\n",
        "\n",
        "This notebook covers practical activities from **Course 09, Unit 2**:\n",
        "- Comparing policy iteration vs value iteration through code-based experiments\n",
        "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Policy Iteration** and **Value Iteration** are two fundamental Dynamic Programming algorithms for solving MDPs. Both find optimal policies, but use different strategies and have different computational properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "\n",
        "print(\"‚úÖ Libraries imported!\")\n",
        "print(\"\\nComparing Policy Iteration vs Value Iteration\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 1: Understanding the Algorithms\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Part 1: Understanding the Algorithms\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nPolicy Iteration:\")\n",
        "print(\"  1. Policy Evaluation: Compute V^œÄ(s) for current policy œÄ\")\n",
        "print(\"  2. Policy Improvement: Update policy to be greedy w.r.t. V^œÄ\")\n",
        "print(\"  3. Repeat until policy no longer changes\")\n",
        "print(\"  - Alternates between evaluation and improvement\")\n",
        "print(\"  - Typically converges in few iterations\")\n",
        "\n",
        "print(\"\\nValue Iteration:\")\n",
        "print(\"  1. Initialize V(s) = 0 for all states\")\n",
        "print(\"  2. Update: V(s) ‚Üê max_a Œ£ P(s'|s,a)[R + Œ≥V(s')]\")\n",
        "print(\"  3. Repeat until convergence\")\n",
        "print(\"  4. Extract optimal policy from V*\")\n",
        "print(\"  - Combines evaluation and improvement in each step\")\n",
        "print(\"  - May require more iterations but each is faster\")\n",
        "\n",
        "print(\"\\nKey Differences:\")\n",
        "print(\"  - Policy Iteration: Two-phase (evaluate then improve)\")\n",
        "print(\"  - Value Iteration: Single-phase (combine evaluate + improve)\")\n",
        "print(\"  - Policy Iteration: Fewer iterations, more computation per iteration\")\n",
        "print(\"  - Value Iteration: More iterations, less computation per iteration\")\n",
        "\n",
        "print(\"\\n‚úÖ Algorithm concepts understood!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 2: Policy Iteration Implementation\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 2: Policy Iteration Implementation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def policy_evaluation(policy, P, R, gamma=0.99, theta=1e-6, max_iterations=100):\n",
        "    \"\"\"Evaluate a policy by computing its value function.\"\"\"\n",
        "    n_states = len(policy)\n",
        "    V = np.zeros(n_states)\n",
        "    \n",
        "    for _ in range(max_iterations):\n",
        "        V_new = np.zeros(n_states)\n",
        "        for s in range(n_states):\n",
        "            a = policy[s]\n",
        "            # V(s) = Œ£ P(s'|s,a)[R(s,a,s') + Œ≥V(s')]\n",
        "            V_new[s] = sum(P[s][a][s_next] * (R[s][a][s_next] + gamma * V[s_next]) \n",
        "                           for s_next in range(n_states))\n",
        "        \n",
        "        if np.max(np.abs(V_new - V)) < theta:\n",
        "            break\n",
        "        V = V_new\n",
        "    \n",
        "    return V\n",
        "\n",
        "def policy_improvement(V, P, R, gamma=0.99):\n",
        "    \"\"\"Improve policy to be greedy w.r.t. current value function.\"\"\"\n",
        "    n_states, n_actions = len(V), len(P[0])\n",
        "    policy = np.zeros(n_states, dtype=int)\n",
        "    \n",
        "    for s in range(n_states):\n",
        "        # Choose action that maximizes: Œ£ P(s'|s,a)[R + Œ≥V(s')]\n",
        "        action_values = [sum(P[s][a][s_next] * (R[s][a][s_next] + gamma * V[s_next])\n",
        "                            for s_next in range(n_states))\n",
        "                        for a in range(n_actions)]\n",
        "        policy[s] = np.argmax(action_values)\n",
        "    \n",
        "    return policy\n",
        "\n",
        "def policy_iteration(P, R, gamma=0.99, theta=1e-6):\n",
        "    \"\"\"Policy Iteration algorithm.\"\"\"\n",
        "    n_states, n_actions = len(P), len(P[0])\n",
        "    policy = np.random.randint(0, n_actions, n_states)  # Random initial policy\n",
        "    \n",
        "    iterations = 0\n",
        "    policy_stable = False\n",
        "    \n",
        "    while not policy_stable:\n",
        "        # Policy evaluation\n",
        "        V = policy_evaluation(policy, P, R, gamma, theta)\n",
        "        \n",
        "        # Policy improvement\n",
        "        policy_new = policy_improvement(V, P, R, gamma)\n",
        "        \n",
        "        # Check if policy changed\n",
        "        policy_stable = np.array_equal(policy, policy_new)\n",
        "        policy = policy_new\n",
        "        iterations += 1\n",
        "    \n",
        "    return policy, V, iterations\n",
        "\n",
        "print(\"\\n‚úÖ Policy Iteration implemented!\")\n",
        "print(\"  Algorithm: Evaluate ‚Üí Improve ‚Üí Repeat until stable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 3: Value Iteration Implementation\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 3: Value Iteration Implementation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def value_iteration(P, R, gamma=0.99, theta=1e-6, max_iterations=1000):\n",
        "    \"\"\"Value Iteration algorithm.\"\"\"\n",
        "    n_states, n_actions = len(P), len(P[0])\n",
        "    V = np.zeros(n_states)\n",
        "    \n",
        "    iterations = 0\n",
        "    for _ in range(max_iterations):\n",
        "        V_new = np.zeros(n_states)\n",
        "        for s in range(n_states):\n",
        "            # V(s) ‚Üê max_a Œ£ P(s'|s,a)[R + Œ≥V(s')]\n",
        "            action_values = [sum(P[s][a][s_next] * (R[s][a][s_next] + gamma * V[s_next])\n",
        "                            for s_next in range(n_states))\n",
        "                           for a in range(n_actions)]\n",
        "            V_new[s] = max(action_values)\n",
        "        \n",
        "        if np.max(np.abs(V_new - V)) < theta:\n",
        "            break\n",
        "        V = V_new\n",
        "        iterations += 1\n",
        "    \n",
        "    # Extract optimal policy\n",
        "    policy = policy_improvement(V, P, R, gamma)\n",
        "    \n",
        "    return policy, V, iterations\n",
        "\n",
        "print(\"\\n‚úÖ Value Iteration implemented!\")\n",
        "print(\"  Algorithm: Update V(s) ‚Üê max_a[...] until convergence, then extract policy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 4: Comparison and Analysis\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 4: Comparison and Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create simple MDP example (2 states, 2 actions)\n",
        "# This is a simplified example for demonstration\n",
        "print(\"\\nExample: Simple MDP (simplified for demonstration)\")\n",
        "\n",
        "print(\"\\nPolicy Iteration Characteristics:\")\n",
        "print(\"  - Two phases: Policy Evaluation + Policy Improvement\")\n",
        "print(\"  - Fewer iterations (policy changes)\")\n",
        "print(\"  - More computation per iteration (full policy evaluation)\")\n",
        "print(\"  - Policy converges, then stops\")\n",
        "\n",
        "print(\"\\nValue Iteration Characteristics:\")\n",
        "print(\"  - Single phase: Combined evaluation + improvement\")\n",
        "print(\"  - More iterations (until value convergence)\")\n",
        "print(\"  - Less computation per iteration (one sweep)\")\n",
        "print(\"  - Values converge, then extract policy\")\n",
        "\n",
        "print(\"\\nComparison Table:\")\n",
        "print(\"  Feature              | Policy Iteration | Value Iteration\")\n",
        "print(\"  ---------------------|------------------|----------------\")\n",
        "print(\"  Iterations           | Fewer            | More\")\n",
        "print(\"  Computation/iteration| More             | Less\")\n",
        "print(\"  Convergence criterion| Policy stable    | Value stable\")\n",
        "print(\"  Convergence speed    | Fast (few iter)  | Moderate\")\n",
        "print(\"  Best when            | Small state space| Large state space\")\n",
        "\n",
        "print(\"\\n‚úÖ Comparison complete!\")\n",
        "\n",
        "print(\"\\nNote: Full implementation requires proper MDP definition\")\n",
        "print(\"(states, actions, transition probabilities, rewards)\")\n",
        "print(\"This notebook demonstrates the algorithmic concepts and differences.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Concepts:\n",
        "1. **Policy Iteration**: Two-phase algorithm\n",
        "   - Phase 1: Policy Evaluation (compute V^œÄ)\n",
        "   - Phase 2: Policy Improvement (update œÄ to be greedy)\n",
        "   - Repeats until policy converges\n",
        "\n",
        "2. **Value Iteration**: Single-phase algorithm\n",
        "   - Updates: V(s) ‚Üê max_a Œ£ P(s'|s,a)[R + Œ≥V(s')]\n",
        "   - Continues until value function converges\n",
        "   - Extracts optimal policy at the end\n",
        "\n",
        "### Comparison:\n",
        "- **Iterations**: Policy Iteration typically needs fewer iterations\n",
        "- **Computation**: Value Iteration is often more efficient overall\n",
        "- **Convergence**: Policy Iteration stops when policy is stable; Value Iteration when values converge\n",
        "- **Use Case**: Policy Iteration for small problems; Value Iteration for larger problems\n",
        "\n",
        "### Advantages:\n",
        "- **Policy Iteration**: Guaranteed convergence, clear policy updates\n",
        "- **Value Iteration**: More efficient, works well with approximations\n",
        "\n",
        "### Applications:\n",
        "- Solving finite MDPs\n",
        "- Finding optimal policies\n",
        "- Foundation for approximate methods\n",
        "\n",
        "### Next Steps:\n",
        "- Approximate methods for large state spaces\n",
        "- Model-free methods (Q-learning, SARSA)\n",
        "- Continuous state/action spaces\n",
        "\n",
        "**Reference:** Course 09, Unit 2: \"Prediction and Control without a Model\" - Policy/Value iteration comparison practical content"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}