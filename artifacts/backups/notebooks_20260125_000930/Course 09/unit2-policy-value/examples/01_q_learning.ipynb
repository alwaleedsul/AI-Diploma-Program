{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Q Learning\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 09, Unit 2** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Q Learning\n",
    "\n",
    "## ğŸ“š Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "\n",
    "This notebook demonstrates key concepts through hands-on examples.\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the core concepts\n",
    "- See practical implementations\n",
    "- Be ready for exercises\n",
    "\n",
    "## ğŸ”— Prerequisites | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "- âœ… Python 3.8+ installed\n",
    "- âœ… Required libraries (see `requirements.txt`)\n",
    "\n",
    "- âœ… **numpy** library: `pip install numpy`\n",
    "- âœ… Basic Python knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Code Example | Ù…Ø«Ø§Ù„ Ø§Ù„ÙƒÙˆØ¯\n",
    "\n",
    "Run the code below to see the demonstration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualization:eward for transition.\"\"\"\n",
    "        if next\n",
    "state == self.goal_state:\n",
    "            return 10.0  # Goal reward\n",
    "        return -0.1  # Small negative reward for each step\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"Get next state after action.\"\"\"\n",
    "        if action == 0:  # left\n",
    "            return max(0, state - 1)\n",
    "        else:  # right\n",
    "            return min(self.states - 1, state + 1)\n",
    "\n",
    "# Initialize Q-table\n",
    "# ØªÙ‡ÙŠØ¦Ø© Ø¬Ø¯ÙˆÙ„ Q\n",
    "env = SimpleGridWorld()\n",
    "Q = np.zeros((env.states, env.actions))\n",
    "\n",
    "print(\"\\nInitial Q-table:\")\n",
    "print(\"Ø¬Ø¯ÙˆÙ„ Q Ø§Ù„Ø£ÙˆÙ„ÙŠ:\")\n",
    "print(Q)\n",
    "\n",
    "# Visualization: Q-Table Heatmap (After Training)\n",
    "# ØªØµÙˆØ±: Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ø¬Ø¯ÙˆÙ„ Q (Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    im = plt.imshow(Q, cmap='YlOrRd', aspect='auto', interpolation='nearest')\n",
    "    plt.colorbar(im, label='Q-Value | Ù‚ÙŠÙ…Ø© Q')\n",
    "    plt.xlabel('Actions | Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('States | Ø§Ù„Ø­Ø§Ù„Ø§Øª', fontsize=12, fontweight='bold')\n",
    "    plt.title('Trained Q-Table Heatmap | Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ø¬Ø¯ÙˆÙ„ Q Ø§Ù„Ù…Ø¯Ø±Ø¨', fontsize=14, pad=20, fontweight='bold')\n",
    "    plt.xticks([0, 1], ['Left | ÙŠØ³Ø§Ø±', 'Right | ÙŠÙ…ÙŠÙ†'])\n",
    "    plt.yticks(range(env.states), [f'State {i} | Ø§Ù„Ø­Ø§Ù„Ø© {i}' for i in range(env.states)])\n",
    "    \n",
    "    # Add text annotations with Q-values\n",
    "    for i in range(env.states):\n",
    "        for j in range(env.actions):\n",
    "            text = plt.text(j, i, f'{Q[i, j]:.2f}', \n",
    "                           ha='center', va='center', \n",
    "                           color='white' if Q[i, j] > 0.5 else 'black',\n",
    "                           fontweight='bold', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\nâœ… Q-table heatmap visualization displayed\")\n",
    "    print(\"ØªÙ… Ø¹Ø±Ø¶ Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ø¬Ø¯ÙˆÙ„ Q Ø¨Ù†Ø¬Ø§Ø­\")\n",
    "except ImportError:\n",
    "    print(\"\\nâš ï¸  Install matplotlib for Q-table visualization:\")\n",
    "    print(\"   pip install matplotlib\")\n",
    "    print(\"Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª matplotlib Ù„ØªØµÙˆØ± Ø¬Ø¯ÙˆÙ„ Q\")\n",
    "\n",
    "# Q-Learning parameters\n",
    "# Ù…Ø¹Ø§Ù…Ù„Ø§Øª Q-Learning\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Q-Learning Training\")\n",
    "print(\"ØªØ¯Ø±ÙŠØ¨ Q-Learning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training loop\n",
    "# Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
    "num\n",
    "episodes = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = 0  # Start state\n",
    "    \n",
    "    while state != env.goal_state:\n",
    "        # Epsilon-greedy action selection\n",
    "        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… epsilon-greedy\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.randint(env.actions)  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q[state])  # Exploit\n",
    "        \n",
    "        # Take action\n",
    "        next\n",
    "state = env.get_next_state(state, action)\n",
    "        reward = env.get_reward(state, action, next_state)\n",
    "        \n",
    "        # Q-learning update\n",
    "        # ØªØ­Ø¯ÙŠØ« Q-learning\n",
    "        Q[state, action] = Q[state, action] + alpha * (\n",
    "            reward + gamma * np.max(Q[next_state]) - Q[state, action]\n",
    "        )\n",
    "        \n",
    "        state = next\n",
    "state\n",
    "\n",
    "print(\"\\nTrained Q-table:\")\n",
    "print(\"Ø¬Ø¯ÙˆÙ„ Q Ø§Ù„Ù…Ø¯Ø±Ø¨:\")\n",
    "print(Q)\n",
    "\n",
    "# Extract policy\n",
    "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³ÙŠØ§Ø³Ø©\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Extracted Policy\")\n",
    "print(\"Ø§Ù„Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "policy = {}\n",
    "for state in range(env.states):\n",
    "    best\n",
    "action = np.argmax(Q[state])\n",
    "    action\n",
    "name = \"left\" if best\n",
    "action == 0 else \"right\"\n",
    "    policy[state] = action\n",
    "name\n",
    "    print(f\"State {state}: {action_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example completed successfully!\")\n",
    "print(\"ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize Q-table as heatmap (after training)\n",
    "# ØªØµÙˆØ± Ø¬Ø¯ÙˆÙ„ Q ÙƒØ®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© (Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    im = plt.imshow(Q, cmap='YlOrRd', aspect='auto', interpolation='nearest')\n",
    "    plt.colorbar(im, label='Q-Value | Ù‚ÙŠÙ…Ø© Q', pad=0.02)\n",
    "    plt.xlabel('Actions | Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª (0=Left/ÙŠØ³Ø§Ø±, 1=Right/ÙŠÙ…ÙŠÙ†)', \n",
    "               fontsize=13, fontweight='bold')\n",
    "    plt.ylabel('States | Ø§Ù„Ø­Ø§Ù„Ø§Øª', fontsize=13, fontweight='bold')\n",
    "    plt.title('Trained Q-Table Heatmap | Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ø¬Ø¯ÙˆÙ„ Q Ø§Ù„Ù…Ø¯Ø±Ø¨', fontsize=15, pad=20, fontweight='bold')\n",
    "    plt.xticks([0, 1], ['Left | ÙŠØ³Ø§Ø±', 'Right | ÙŠÙ…ÙŠÙ†'], fontsize=11)\n",
    "    plt.yticks(range(env.states), [f'State {i} | Ø§Ù„Ø­Ø§Ù„Ø© {i}' for i in range(env.states)], fontsize=11)\n",
    "    \n",
    "    # Add Q-value annotations\n",
    "    for i in range(env.states):\n",
    "        for j in range(env.actions):\n",
    "            text = plt.text(j, i, f'{Q[i, j]:.2f}', \n",
    "                           ha='center', va='center', \n",
    "                           color='white' if Q[i, j] > np.max(Q) * 0.5 else 'black', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\nâœ… Q-table heatmap visualization displayed!\")\n",
    "    print(\"ØªÙ… Ø¹Ø±Ø¶ Ø®Ø±ÙŠØ·Ø© Ø­Ø±Ø§Ø±ÙŠØ© Ù„Ø¬Ø¯ÙˆÙ„ Q Ø¨Ù†Ø¬Ø§Ø­!\")\n",
    "except ImportError:\n",
    "    print(\"\\nğŸ“¦ To see Q-table visualization, install:\")\n",
    "    print(\"   pip install matplotlib\")\n",
    "    print(\"\\nÙ…Ù„Ø§Ø­Ø¸Ø©: Ù„Ø±Ø¤ÙŠØ© Ø§Ù„ØªØµÙˆØ±ØŒ Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª:\")\n",
    "    print(\"   pip install matplotlib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Summary | Ø§Ù„Ù…Ù„Ø®Øµ\n",
    "\n",
    "Great job completing this example!\n",
    "\n",
    "**What you learned:**\n",
    "- Core concepts demonstrated in the code\n",
    "- Practical implementation details\n",
    "\n",
    "**Next steps:**\n",
    "- Complete the exercises in `exercises/` folder\n",
    "- Review the quiz materials\n",
    "- Proceed to the next example\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Tip:** If you see errors, make sure:\n",
    "- All libraries are installed: `pip install -r requirements.txt`\n",
    "- You're using Python 3.8+\n",
    "- Cells are executed in order\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
