{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation: Monitoring Learning Curves, Rewards, and Stability\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Monitor learning curves during Deep RL training\n",
    "- Track rewards and evaluate model performance\n",
    "- Assess training stability\n",
    "- Visualize training progress\n",
    "- Identify convergence and performance issues\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of Deep RL algorithms (DQN, Actor-Critic)\n",
    "- âœ… Understanding of training loops\n",
    "- âœ… Python knowledge (matplotlib, numpy)\n",
    "- âœ… Experience with OpenAI Gym\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 3**:\n",
    "- Training and evaluation: monitoring learning curves, rewards, and stability to evaluate model performance\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 3 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Monitoring and evaluation** are crucial for Deep RL training. Learning curves, reward tracking, and stability metrics help assess training progress and identify issues early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import gym\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nTraining and Evaluation: Monitoring Learning Curves\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Monitoring Learning Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Monitoring Learning Curves\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class TrainingMonitor:\n",
    "    \"\"\"Monitor training progress for Deep RL.\"\"\"\n",
    "    \n",
    "    def __init__(self, window\n",
    "size =100):\n",
    "        self.episode\n",
    "rewards = []\n",
    "        self.episode\n",
    "lengths = []\n",
    "        self.window\n",
    "size = window\n",
    "size\n",
    "        self.reward\n",
    "window = deque(maxlen= window\n",
    "size)\n",
    "    \n",
    "    def record_episode(self, reward, length):\n",
    "        \"\"\"Record an episode's results.\"\"\"\n",
    "        self.episode_rewards.append(reward)\n",
    "        self.episode_lengths.append(length)\n",
    "        self.reward_window.append(reward)\n",
    "    \n",
    "    def get_average_reward(self):\n",
    "        \"\"\"Get average reward over window.\"\"\"\n",
    "        return np.mean(self.reward_window) if self.reward_window else 0.0\n",
    "    \n",
    "    def plot_learning_curve(self):\n",
    "        \"\"\"Plot learning curves.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Plot 1: Episode Rewards\n",
    "        axes[0].plot(self.episode_rewards, alpha=0.3, label='Raw rewards', color='blue')\n",
    "        if len(self.episode_rewards) >= self.window_size:\n",
    "            smoothed = [np.mean(self.episode_rewards[max(0, i-self.window_size):i+1]) \n",
    "                       for i in range(len(self.episode_rewards))]\n",
    "            axes[0].plot(smoothed, label=f'Smoothed (window={self.window_size})', \n",
    "                        color='red', linewidth=2)\n",
    "        axes[0].set_xlabel('Episode')\n",
    "        axes[0].set_ylabel('Reward')\n",
    "        axes[0].set_title('Learning Curve: Episode Rewards')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Episode Lengths\n",
    "        axes[1].plot(self.episode_lengths, alpha=0.3, label='Episode lengths', color='green')\n",
    "        if len(self.episode_lengths) >= self.window_size:\n",
    "            smoothed\n",
    "lengths = [np.mean(self.episode_lengths[max(0, i-self.window_size):i+1]) \n",
    "                               for i in range(len(self.episode_lengths))]\n",
    "            axes[1].plot(smoothed_lengths, label=f'Smoothed (window={self.window_size})', \n",
    "                        color='orange', linewidth=2)\n",
    "        axes[1].set_xlabel('Episode')\n",
    "        axes[1].set_ylabel('Episode Length')\n",
    "        axes[1].set_title('Learning Curve: Episode Lengths')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Simulate training data\n",
    "monitor = TrainingMonitor(window\n",
    "size =50)\n",
    "\n",
    "# Simulate training progress (improving over time)\n",
    "np.random.seed(42)\n",
    "for episode in range(200):\n",
    "    # Simulate improving performance\n",
    "    base\n",
    "        reward = 20 + episode * 0.5 + np.random.normal(0, 5)\n",
    "    reward = max(0, base_reward)\n",
    "    length = int(50 + episode * 0.3 + np.random.normal(0, 10))\n",
    "    length = max(1, length)\n",
    "    monitor.record_episode(reward, length)\n",
    "\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "print(f\"  Total episodes: {len(monitor.episode_rewards)}\")\n",
    "print(f\"  Average reward (final 50 episodes): {monitor.get_average_reward():.2f}\")\n",
    "print(f\"  Best episode reward: {max(monitor.episode_rewards):.2f}\")\n",
    "print(f\"  Average episode length: {np.mean(monitor.episode_lengths):.2f}\")\n",
    "\n",
    "monitor.plot_learning_curve()\n",
    "\n",
    "print(\"\\nâœ… Learning curves monitored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Evaluating Stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Evaluating Stability\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def evaluate_stability(rewards, window\n",
    "size =100):\n",
    "    \"\"\"Evaluate training stability metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Calculate standard deviation over windows\n",
    "    if len(rewards) >= window\n",
    "size:\n",
    "        windows = [rewards[i:i+window_size] for i in range(0, len(rewards)-window_size+1, window_size//2)]\n",
    "        stds = [np.std(window) for window in windows]\n",
    "        metrics['average_std'] = np.mean(stds)\n",
    "        metrics['max_std'] = np.max(stds)\n",
    "    \n",
    "    # Calculate reward variance\n",
    "    metrics['overall_variance'] = np.var(rewards)\n",
    "    metrics['overall_std'] = np.std(rewards)\n",
    "    \n",
    "    # Check for convergence (stability in later episodes)\n",
    "    if len(rewards) >= 2 * window_size:\n",
    "        early\n",
    "rewards = rewards[:window_size]\n",
    "        late\n",
    "rewards = rewards[-window_size:]\n",
    "        metrics['early_mean'] = np.mean(early_rewards)\n",
    "        metrics['late_mean'] = np.mean(late_rewards)\n",
    "        metrics['improvement'] = metrics['late_mean'] - metrics['early_mean']\n",
    "        metrics['early_std'] = np.std(early_rewards)\n",
    "        metrics['late_std'] = np.std(late_rewards)\n",
    "        metrics['stability_improvement'] = metrics['early_std'] - metrics['late_std']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "stability\n",
    "metrics = evaluate\n",
    "stability(monitor.episode_rewards)\n",
    "\n",
    "print(\"\\nStability Metrics:\")\n",
    "for key, value in stability_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Visualize stability\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "if len(monitor.episode_rewards) >= 100:\n",
    "    window\n",
    "std = [np.std(monitor.episode_rewards[max(0, i-50):i+1]) \n",
    "                  for i in range(len(monitor.episode_rewards))]\n",
    "    plt.plot(window_std, label='Rolling Std Dev', color='purple')\n",
    "    plt.axhline(y= stability\n",
    "metrics.get('overall_std', 0), color='r', \n",
    "               linestyle='--', label=f'Overall Std: {stability_metrics.get(\"overall_std\", 0):.2f}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Standard Deviation')\n",
    "    plt.title('Training Stability: Reward Variance')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if len(monitor.episode_rewards) >= 200:\n",
    "    early = monitor.episode_rewards[:100]\n",
    "    late = monitor.episode_rewards[-100:]\n",
    "    plt.hist(early, bins=20, alpha=0.7, label='Early episodes', density=True)\n",
    "    plt.hist(late, bins=20, alpha=0.7, label='Late episodes', density=True)\n",
    "    plt.xlabel('Reward')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Reward Distribution: Early vs Late')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Stability evaluated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Metrics:\n",
    "1. **Learning Curves**: Episode rewards and lengths over time\n",
    "2. **Smoothed Curves**: Moving averages to reduce noise\n",
    "3. **Stability Metrics**: Variance, standard deviation, convergence\n",
    "4. **Performance Tracking**: Best rewards, average performance\n",
    "\n",
    "### Best Practices:\n",
    "- Monitor rewards, episode lengths, and loss (if applicable)\n",
    "- Use smoothing to identify trends\n",
    "- Track stability metrics (variance reduction over time)\n",
    "- Compare early vs late performance\n",
    "- Set up early stopping based on convergence\n",
    "\n",
    "### Evaluation Checklist:\n",
    "- âœ… Learning curves showing improvement\n",
    "- âœ… Stable/declining variance over time\n",
    "- âœ… Convergence to good performance\n",
    "- âœ… Consistent behavior in late training\n",
    "\n",
    "### Applications:\n",
    "- All Deep RL algorithms (DQN, A2C, PPO, etc.)\n",
    "- Hyperparameter tuning\n",
    "- Algorithm comparison\n",
    "- Debugging training issues\n",
    "\n",
    "**Reference:** Course 09, Unit 3: \"Deep Reinforcement Learning\" - Training and evaluation practical contenttt"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
