{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Strategies: Epsilon-Greedy\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the exploration-exploitation dilemma\n",
    "- Implement Epsilon-Greedy exploration strategy\n",
    "- Visualize the impact of different epsilon values\n",
    "- Compare exploration vs exploitation behavior\n",
    "- Apply Epsilon-Greedy to RL problems\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… OpenAI Gym setup (previous notebook)\n",
    "- âœ… Understanding of RL basics (actions, rewards, policies)\n",
    "- âœ… Python knowledge (functions, loops, matplotlib)\n",
    "- âœ… NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 09, Unit 1**:\n",
    "- Exploration strategies: programming Epsilon-Greedy strategy and visualizing its impact\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 1 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Exploration-Exploitation Dilemma**: RL agents must balance exploring new actions (to discover better rewards) with exploiting known good actions (to maximize rewards). **Epsilon-Greedy** is a simple but effective exploration strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nExploration Strategies: Epsilon-Greedy\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Understanding Epsilon-Greedy Strategy\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Understanding Epsilon-Greedy Strategy\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nEpsilon-Greedy Algorithm:\")\n",
    "print(\"  1. With probability Îµ (epsilon): Explore (choose random action)\")\n",
    "print(\"  2. With probability 1-Îµ: Exploit (choose best known action)\")\n",
    "\n",
    "print(\"\\nKey Parameters:\")\n",
    "print(\"  - Îµ (epsilon): Exploration rate (0.0 to 1.0)\")\n",
    "print(\"  - Îµ = 1.0: Pure exploration (always random)\")\n",
    "print(\"  - Îµ = 0.0: Pure exploitation (always greedy)\")\n",
    "print(\"  - Îµ = 0.1: 10% exploration, 90% exploitation (common choice)\")\n",
    "\n",
    "print(\"\\nEpsilon Decay:\")\n",
    "print(\"  - Start with high Îµ (more exploration)\")\n",
    "print(\"  - Gradually decrease Îµ (more exploitation as agent learns)\")\n",
    "print(\"  - Common: Îµ = 1.0 â†’ 0.01 over training\")\n",
    "\n",
    "print(\"\\nâœ… Epsilon-Greedy strategy understood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: Implementing Epsilon-Greedy\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Implementing Epsilon-Greedy\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def epsilon_greedy_action(q_values, epsilon, n_actions):\n",
    "    \"\"\"\n",
    "    Choose action using epsilon-greedy strategy.\n",
    "    \n",
    "    Args:\n",
    "        q_values: Array of Q-values for each action\n",
    "        epsilon: Exploration rate (0.0 to 1.0)\n",
    "        n_actions: Number of possible actions\n",
    "    \n",
    "    Returns:\n",
    "        Selected action (integer)\n",
    "    \"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        # Explore: choose random action\n",
    "        return np.random.randint(n_actions)\n",
    "    else:\n",
    "        # Exploit: choose best action (greedy)\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "# Example: Simple bandit problem\n",
    "n_actions = 4\n",
    "q_values = np.array([0.1, 0.3, 0.5, 0.2])  # True Q-values\n",
    "epsilon = 0.3\n",
    "\n",
    "print(f\"\\nExample Q-values: {q_values}\")\n",
    "print(f\"Epsilon: {epsilon}\")\n",
    "print(f\"\\nRunning epsilon-greedy for 20 steps:\")\n",
    "\n",
    "action_counts = np.zeros(n_actions)\n",
    "for i in range(20):\n",
    "    action = epsilon_greedy_action(q_values, epsilon, n_actions)\n",
    "    action_counts[action] += 1\n",
    "    print(f\"  Step {i+1}: Action {action} ({'explore' if np.random.random() < epsilon else 'exploit'})\")\n",
    "\n",
    "print(f\"\\nAction selection counts:\")\n",
    "for i in range(n_actions):\n",
    "    print(f\"  Action {i}: {int(action_counts[i])} times ({action_counts[i]/20*100:.1f}%)\")\n",
    "\n",
    "best_action = np.argmax(q_values)\n",
    "print(f\"\\nBest action (highest Q-value): {best_action}\")\n",
    "print(f\"Best action selected: {action_counts[best_action]} times ({action_counts[best_action]/20*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ… Epsilon-Greedy implementation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3: Visualizing Impact of Different Epsilon Values\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: Visualizing Impact of Different Epsilon Values\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test different epsilon values\n",
    "epsilon_values = [0.0, 0.1, 0.3, 0.5, 1.0]\n",
    "n_steps = 1000\n",
    "n_actions = 4\n",
    "q_values = np.array([0.1, 0.3, 0.5, 0.2])\n",
    "best_action = np.argmax(q_values)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    action_counts = np.zeros(n_actions)\n",
    "    for _ in range(n_steps):\n",
    "        action = epsilon_greedy_action(q_values, epsilon, n_actions)\n",
    "        action_counts[action] += 1\n",
    "    results[epsilon] = action_counts\n",
    "n_steps\n",
    "\n",
    "print(\"\\nAction selection percentages for different epsilon values:\")\n",
    "print(\"Action | Îµ=0.0 | Îµ=0.1 | Îµ=0.3 | Îµ=0.5 | Îµ=1.0\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(n_actions):\n",
    "    percentages = [f\"{results[eps][i]*100:.1f}%\" for eps in epsilon_values]\n",
    "    marker = \" â­\" if i == best_action else \"\"\n",
    "    print(f\"  {i}{marker:3s} | {' | '.join(percentages)}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(epsilon_values))\n",
    "width = 0.2\n",
    "\n",
    "for i in range(n_actions):\n",
    "    percentages = [results[eps][i] * 100 for eps in epsilon_values]\n",
    "    ax.bar(x + i*width, percentages, width, label=f'Action {i}{\" (best)\" if i == best_action else \"\"}')\n",
    "\n",
    "ax.set_xlabel('Epsilon Value', fontsize=12)\n",
    "ax.set_ylabel('Selection Percentage (%)', fontsize=12)\n",
    "ax.set_title('Epsilon-Greedy: Action Selection by Epsilon Value', fontsize=14)\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels([str(eps) for eps in epsilon_values])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Impact visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4: Epsilon Decay Strategy\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 4: Epsilon Decay Strategy\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def epsilon_decay(initial_epsilon, final_epsilon, decay_rate, episode):\n",
    "    \"\"\"\n",
    "    Calculate epsilon with exponential decay.\n",
    "    \n",
    "    Args:\n",
    "        initial_epsilon: Starting epsilon value\n",
    "        final_epsilon: Minimum epsilon value\n",
    "        decay_rate: Decay rate (0.0 to 1.0)\n",
    "        episode: Current episode number\n",
    "    \n",
    "    Returns:\n",
    "        Decayed epsilon value\n",
    "    \"\"\"\n",
    "    return max(final_epsilon, initial_epsilon * (decay_rate ** episode))\n",
    "\n",
    "# Simulate epsilon decay over training\n",
    "initial_epsilon = 1.0\n",
    "final_epsilon = 0.01\n",
    "decay_rate = 0.995\n",
    "n_episodes = 500\n",
    "\n",
    "epsilon_history = []\n",
    "for episode in range(n_episodes):\n",
    "    epsilon = epsilon_decay(initial_epsilon, final_epsilon, decay_rate, episode)\n",
    "    epsilon_history.append(epsilon)\n",
    "\n",
    "# Visualize epsilon decay\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilon_history, linewidth=2)\n",
    "plt.xlabel('Episode', fontsize=12)\n",
    "plt.ylabel('Epsilon Value', fontsize=12)\n",
    "plt.title('Epsilon Decay Over Training', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=final_epsilon, color='r', linestyle='--', label=f'Final Îµ={final_epsilon}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEpsilon Decay:\")\n",
    "print(f\"  Initial Îµ: {initial_epsilon}\")\n",
    "print(f\"  Final Îµ: {final_epsilon}\")\n",
    "print(f\"  Decay rate: {decay_rate}\")\n",
    "print(f\"  Episodes: {n_episodes}\")\n",
    "print(f\"  Îµ at episode 100: {epsilon_history[100]:.4f}\")\n",
    "print(f\"  Îµ at episode 250: {epsilon_history[250]:.4f}\")\n",
    "print(f\"  Îµ at episode 500: {epsilon_history[499]:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Epsilon decay strategy complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Exploration-Exploitation Dilemma**: Balance between trying new actions vs. using known good actions\n",
    "2. **Epsilon-Greedy**: Simple strategy that explores with probability Îµ, exploits otherwise\n",
    "3. **Epsilon Decay**: Gradually reduce exploration as agent learns (start high, end low)\n",
    "4. **Trade-off**: High Îµ = more exploration but slower convergence; Low Îµ = faster convergence but may miss optimal actions\n",
    "\n",
    "### Implementation:\n",
    "- **Epsilon-Greedy Function**: Choose random action with probability Îµ, otherwise greedy\n",
    "- **Epsilon Decay**: Exponential or linear decay from initial to final epsilon\n",
    "- **Common Values**: Îµ_start = 1.0, Îµ_end = 0.01, decay = 0.995\n",
    "\n",
    "### Best Practices:\n",
    "- Start with high epsilon (1.0) for exploration\n",
    "- Gradually decrease epsilon during training\n",
    "- Use epsilon decay schedules (exponential or linear)\n",
    "- Monitor exploration vs exploitation balance\n",
    "\n",
    "### Applications:\n",
    "- Q-learning and SARSA algorithms\n",
    "- Multi-armed bandit problems\n",
    "- Any RL algorithm needing exploration\n",
    "\n",
    "**Reference:** Course 09, Unit 1: \"Introduction to Reinforcement Learning\" - Exploration strategies practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}