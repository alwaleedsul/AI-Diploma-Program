{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating Text Generation Quality Using Metrics (BLEU, Perplexity)\n",
        "\n",
        "## ðŸ“š Learning Objectives\n",
        "\n",
        "By completing this notebook, you will:\n",
        "- Evaluate text with BLEU score\n",
        "- Calculate perplexity\n",
        "- Assess text quality\n",
        "- Compare generation methods\n",
        "- Interpret evaluation results\n",
        "\n",
        "## ðŸ”— Prerequisites\n",
        "\n",
        "- âœ… Understanding of text evaluation\n",
        "- âœ… Understanding of language models\n",
        "- âœ… Metric calculation knowledge\n",
        "\n",
        "---\n",
        "\n",
        "## Official Structure Reference\n",
        "\n",
        "This notebook covers practical activities from **Course 10, Unit 2**:\n",
        "- Evaluating text generation quality using metrics like BLEU and perplexity\n",
        "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Text generation evaluation** uses metrics like BLEU and perplexity to assess quality, fluency, and coherence of generated text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import numpy as np\n",
        "\n",
        "print(\"âœ… Libraries imported!\")\n",
        "print(\"\\nEvaluating Text Generation Quality\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nBLEU Score:\")\n",
        "print(\"  - N-gram precision\")\n",
        "print(\"  - Range: 0 to 1\")\n",
        "print(\"  - Higher is better\")\n",
        "print(\"  - Compares to reference\")\n",
        "\n",
        "print(\"\\nPerplexity:\")\n",
        "print(\"  - Language model quality\")\n",
        "print(\"  - Lower is better\")\n",
        "print(\"  - Measures uncertainty\")\n",
        "print(\"  - Cross-entropy based\")\n",
        "\n",
        "print(\"\\nOther Metrics:\")\n",
        "print(\"  - ROUGE (for summarization)\")\n",
        "print(\"  - METEOR\")\n",
        "print(\"  - Human evaluation\")\n",
        "print(\"  - Semantic similarity\")\n",
        "\n",
        "print(\"\\nâœ… Text evaluation concepts understood!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
