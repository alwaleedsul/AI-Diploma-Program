{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sequence-to-Sequence Models with Attention for Machine Translation\n",
        "\n",
        "## üìö Learning Objectives\n",
        "\n",
        "By completing this notebook, you will:\n",
        "- Understand sequence-to-sequence (seq2seq) architectures\n",
        "- Implement attention mechanisms for seq2seq models\n",
        "- Build a machine translation model using seq2seq with attention\n",
        "- Apply encoder-decoder architectures for sequence tasks\n",
        "\n",
        "## üîó Prerequisites\n",
        "\n",
        "- ‚úÖ Unit 4: RNNs and LSTMs completed\n",
        "- ‚úÖ Understanding of encoder-decoder architectures\n",
        "- ‚úÖ Python, TensorFlow/Keras knowledge\n",
        "\n",
        "---\n",
        "\n",
        "## Official Structure Reference\n",
        "\n",
        "This notebook covers practical activities from **Course 07, Unit 4**:\n",
        "- Implementing machine translation model using seq2seq with attention mechanisms\n",
        "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 4 Practical Content\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Sequence-to-Sequence (seq2seq)** models use encoder-decoder architectures to map sequences to sequences. **Attention mechanisms** help models focus on relevant parts of the input when generating output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Try importing TensorFlow/Keras\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras.models import Model\n",
        "    from tensorflow.keras.layers import LSTM, Dense, Embedding, Input, Attention\n",
        "    HAS_TF = True\n",
        "    print(\"‚úÖ TensorFlow/Keras available!\")\n",
        "except ImportError:\n",
        "    HAS_TF = False\n",
        "    print(\"‚ö†Ô∏è  TensorFlow not available. Install with: pip install tensorflow\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding Seq2Seq Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Sequence-to-Sequence (Seq2Seq) Architecture\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nKey Components:\")\n",
        "print(\"  1. Encoder: Processes input sequence\")\n",
        "print(\"  2. Decoder: Generates output sequence\")\n",
        "print(\"  3. Attention: Connects encoder and decoder\")\n",
        "print(\"  4. Context Vector: Summary of input sequence\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Seq2Seq with Attention:\")\n",
        "print(\"-\" * 60)\n",
        "print(\"  Encoder: Input sequence ‚Üí Hidden states\")\n",
        "print(\"  Attention: Hidden states ‚Üí Attention weights\")\n",
        "print(\"  Decoder: Attention weights + Previous output ‚Üí Next token\")\n",
        "print(\"  Output: Generated sequence\")\n",
        "\n",
        "print(\"\\n‚úÖ Attention allows model to focus on relevant input parts!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Implementing Seq2Seq with Attention for Translation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if HAS_TF:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Seq2Seq with Attention Implementation\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(\"\\nImplementation Structure:\")\n",
        "    print(\"\"\"\n",
        "    # Simplified Seq2Seq with Attention\n",
        "    \n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_len,))\n",
        "    encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "    encoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "    \n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(max_len,))\n",
        "    decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "    \n",
        "    # Attention\n",
        "    attention = Attention()\n",
        "    context_vector = attention([decoder_outputs, encoder_outputs])\n",
        "    \n",
        "    # Output\n",
        "    decoder_concat = tf.concat([decoder_outputs, context_vector], axis=-1)\n",
        "    decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "    output = decoder_dense(decoder_concat)\n",
        "    \n",
        "    # Model\n",
        "    model = Model([encoder_inputs, decoder_inputs], output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "    \"\"\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Model Architecture:\")\n",
        "    print(\"  - Encoder: LSTM processes source sequence\")\n",
        "    print(\"  - Decoder: LSTM generates target sequence\")\n",
        "    print(\"  - Attention: Focuses on relevant encoder states\")\n",
        "    print(\"  - Output: Probability distribution over target vocabulary\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Applications:\")\n",
        "    print(\"  - Machine Translation (EN‚ÜíFR, EN‚ÜíAR, etc.)\")\n",
        "    print(\"  - Text Summarization\")\n",
        "    print(\"  - Question Answering\")\n",
        "    print(\"  - Chatbot dialogue generation\")\n",
        "else:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Seq2Seq with Attention (Installation Required)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\"\"\n",
        "    To implement seq2seq with attention:\n",
        "    \n",
        "    1. Install TensorFlow:\n",
        "       pip install tensorflow\n",
        "    \n",
        "    2. Build encoder-decoder architecture:\n",
        "       - Encoder: LSTM/GRU to encode input\n",
        "       - Decoder: LSTM/GRU to decode output\n",
        "       - Attention: Connect encoder and decoder\n",
        "    \n",
        "    3. Train on parallel corpora:\n",
        "       - Input sequences (source language)\n",
        "       - Target sequences (target language)\n",
        "    \n",
        "    4. Use attention to focus on relevant input parts\n",
        "    \"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Concepts:\n",
        "1. **Seq2Seq Architecture**: Encoder-decoder for sequence tasks\n",
        "   - **Encoder**: Processes input sequence into hidden states\n",
        "   - **Decoder**: Generates output sequence from hidden states\n",
        "   - **Attention**: Allows decoder to focus on relevant encoder states\n",
        "\n",
        "2. **Attention Mechanism**: \n",
        "   - Computes attention weights for each encoder state\n",
        "   - Creates context vector from weighted encoder states\n",
        "   - Improves translation quality for long sequences\n",
        "\n",
        "3. **Machine Translation**:\n",
        "   - Input: Source language sequence\n",
        "   - Output: Target language sequence\n",
        "   - Train on parallel corpora (aligned sentence pairs)\n",
        "\n",
        "### Applications:\n",
        "- Machine Translation (Google Translate, DeepL)\n",
        "- Text Summarization\n",
        "- Question Answering\n",
        "- Dialogue Systems\n",
        "\n",
        "**Reference:** Course 07, Unit 4: \"Deep Learning for NLP\" - Seq2seq with attention practical content\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
