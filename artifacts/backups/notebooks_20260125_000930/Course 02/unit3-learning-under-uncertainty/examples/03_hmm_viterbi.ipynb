{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models (HMMs) and Viterbi Algorithm\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand Hidden Markov Models (HMMs) structure and components\n",
    "- Implement HMMs for sequence prediction\n",
    "- Implement Viterbi algorithm for sequence decoding\n",
    "- Apply HMMs to practical problems (speech recognition, POS tagging)\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of probability and Markov chains\n",
    "- âœ… Python 3.8+ installed\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 02, Unit 3**:\n",
    "- Working with Hidden Markov Models (HMMs) for sequence prediction\n",
    "- Implementing Viterbi algorithm for sequence decoding\n",
    "- Applying HMMs to practical problems (speech recognition, POS tagging)\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 3 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Hidden Markov Models\n",
    "\n",
    "**Hidden Markov Models (HMMs)** are statistical models for sequences where:\n",
    "- **Hidden states**: Unobserved states (e.g., weather: Sunny, Rainy)\n",
    "- **Observations**: Observed outputs (e.g., activities: Walk, Shop, Clean)\n",
    "- **Transitions**: Probabilities between hidden states\n",
    "- **Emissions**: Probabilities of observations given states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"Ready to work with HMMs and Viterbi algorithm!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple HMM Implementation\n",
    "\n",
    "Let's implement a simple HMM for weather prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHMM:\n",
    "    \"\"\"Simple Hidden Markov Model implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, states, observations, transition_probs, emission_probs, initial_probs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - states: List of hidden states\n",
    "        - observations: List of possible observations\n",
    "        - transition_probs: Dict of transition probabilities P(state_i | state_j)\n",
    "        - emission_probs: Dict of emission probabilities P(obs | state)\n",
    "        - initial_probs: Initial state probabilities\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.observations = observations\n",
    "        self.transition_probs = transition_probs\n",
    "        self.emission_probs = emission_probs\n",
    "        self.initial_probs = initial_probs\n",
    "    \n",
    "    def forward(self, obs_sequence):\n",
    "        \"\"\"Forward algorithm: Compute probability of observation sequence\"\"\"\n",
    "        T = len(obs_sequence)\n",
    "        N = len(self.states)\n",
    "        \n",
    "        # Initialize alpha (forward probabilities)\n",
    "        alpha = np.zeros((T, N))\n",
    "        \n",
    "        # Initialization\n",
    "        for i, state in enumerate(self.states):\n",
    "            alpha[0, i] = self.initial_probs[state] * self.emission_probs[state][obs_sequence[0]]\n",
    "        \n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for j, state_j in enumerate(self.states):\n",
    "                alpha[t, j] = sum(\n",
    "                    alpha[t-1, i] * self.transition_probs[state_j][self.states[i]] \n",
    "                    for i in range(N)\n",
    "                ) * self.emission_probs[state_j][obs_sequence[t]]\n",
    "        \n",
    "        # Termination\n",
    "        return alpha, sum(alpha[T-1, :])\n",
    "\n",
    "# Example: Weather HMM\n",
    "states = ['Sunny', 'Rainy']\n",
    "observations = ['Walk', 'Shop', 'Clean']\n",
    "\n",
    "# Transition probabilities: P(next_state | current_state)\n",
    "transition_probs = {\n",
    "    'Sunny': {'Sunny': 0.7, 'Rainy': 0.3},\n",
    "    'Rainy': {'Sunny': 0.4, 'Rainy': 0.6}\n",
    "}\n",
    "\n",
    "# Emission probabilities: P(observation | state)\n",
    "emission_probs = {\n",
    "    'Sunny': {'Walk': 0.6, 'Shop': 0.3, 'Clean': 0.1},\n",
    "    'Rainy': {'Walk': 0.1, 'Shop': 0.4, 'Clean': 0.5}\n",
    "}\n",
    "\n",
    "# Initial probabilities\n",
    "initial_probs = {'Sunny': 0.6, 'Rainy': 0.4}\n",
    "\n",
    "# Create HMM\n",
    "hmm = SimpleHMM(states, observations, transition_probs, emission_probs, initial_probs)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Hidden Markov Model: Weather Prediction\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"States: {states}\")\n",
    "print(f\"Observations: {observations}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Viterbi Algorithm for Sequence Decoding\n",
    "\n",
    "The Viterbi algorithm finds the most likely sequence of hidden states given observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(hmm, obs_sequence):\n",
    "    \"\"\"\n",
    "    Viterbi algorithm: Find most likely sequence of hidden states\n",
    "    \n",
    "    Returns:\n",
    "    - best_path: Most likely state sequence\n",
    "    - best_prob: Probability of best path\n",
    "    \"\"\"\n",
    "    T = len(obs_sequence)\n",
    "    N = len(hmm.states)\n",
    "    \n",
    "    # Initialize viterbi and backpointer tables\n",
    "    viterbi_table = np.zeros((T, N))\n",
    "    backpointer = np.zeros((T, N), dtype=int)\n",
    "    \n",
    "    # Initialization\n",
    "    for i, state in enumerate(hmm.states):\n",
    "        viterbi_table[0, i] = hmm.initial_probs[state] * hmm.emission_probs[state][obs_sequence[0]]\n",
    "        backpointer[0, i] = 0\n",
    "    \n",
    "    # Recursion\n",
    "    for t in range(1, T):\n",
    "        for j, state_j in enumerate(hmm.states):\n",
    "            # Find best previous state\n",
    "            probs = [\n",
    "                viterbi_table[t-1, i] * hmm.transition_probs[state_j][hmm.states[i]]\n",
    "                for i in range(N)\n",
    "            ]\n",
    "            best_prev = np.argmax(probs)\n",
    "            viterbi_table[t, j] = probs[best_prev] * hmm.emission_probs[state_j][obs_sequence[t]]\n",
    "            backpointer[t, j] = best_prev\n",
    "    \n",
    "    # Termination: Find best final state\n",
    "    best_final = np.argmax(viterbi_table[T-1, :])\n",
    "    best_prob = viterbi_table[T-1, best_final]\n",
    "    \n",
    "    # Backtrack to find best path\n",
    "    best_path = [hmm.states[best_final]]\n",
    "    for t in range(T-1, 0, -1):\n",
    "        best_final = backpointer[t, best_final]\n",
    "        best_path.insert(0, hmm.states[best_final])\n",
    "    \n",
    "    return best_path, best_prob\n",
    "\n",
    "# Example: Decode observation sequence\n",
    "obs_seq = ['Walk', 'Shop', 'Clean']\n",
    "print(\"=\" * 60)\n",
    "print(f\"Observation Sequence: {obs_seq}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_states, prob = viterbi(hmm, obs_seq)\n",
    "print(f\"Most likely state sequence: {best_states}\")\n",
    "print(f\"Probability: {prob:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Application - Part-of-Speech Tagging\n",
    "\n",
    "Let's apply HMMs to POS tagging (simplified example).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: POS Tagging HMM\n",
    "pos_states = ['Noun', 'Verb', 'Det']\n",
    "pos_words = ['the', 'cat', 'runs']\n",
    "\n",
    "# Transition: P(POS_i | POS_j)\n",
    "pos_transitions = {\n",
    "    'Noun': {'Noun': 0.1, 'Verb': 0.3, 'Det': 0.6},\n",
    "    'Verb': {'Noun': 0.5, 'Verb': 0.2, 'Det': 0.3},\n",
    "    'Det': {'Noun': 0.7, 'Verb': 0.2, 'Det': 0.1}\n",
    "}\n",
    "\n",
    "# Emission: P(word | POS)\n",
    "pos_emissions = {\n",
    "    'Noun': {'the': 0.1, 'cat': 0.7, 'runs': 0.2},\n",
    "    'Verb': {'the': 0.05, 'cat': 0.1, 'runs': 0.85},\n",
    "    'Det': {'the': 0.8, 'cat': 0.15, 'runs': 0.05}\n",
    "}\n",
    "\n",
    "pos_initial = {'Noun': 0.3, 'Verb': 0.3, 'Det': 0.4}\n",
    "\n",
    "pos_hmm = SimpleHMM(pos_states, pos_words, pos_transitions, pos_emissions, pos_initial)\n",
    "\n",
    "# Tag sentence: \"the cat runs\"\n",
    "sentence = ['the', 'cat', 'runs']\n",
    "pos_sequence, pos_prob = viterbi(pos_hmm, sentence)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POS Tagging Example:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Tagged: {list(zip(sentence, pos_sequence))}\")\n",
    "print(f\"Probability: {pos_prob:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **HMM Components**: States, observations, transitions, emissions\n",
    "2. **Forward Algorithm**: Compute probability of observation sequence\n",
    "3. **Viterbi Algorithm**: Find most likely hidden state sequence\n",
    "4. **Applications**: Speech recognition, POS tagging, sequence prediction\n",
    "\n",
    "### Applications:\n",
    "- Natural language processing (POS tagging, NER)\n",
    "- Speech recognition\n",
    "- Bioinformatics (gene prediction)\n",
    "- Time series prediction\n",
    "\n",
    "**Reference:** Course 02, Unit 3: \"Working with Hidden Markov Models (HMMs)\" and \"Implementing Viterbi algorithm for sequence decoding\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
