{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation and Optimization\n",
        "\n",
        "## ðŸ“š Learning Objectives\n",
        "\n",
        "By completing this notebook, you will:\n",
        "- Conduct experiments and collect performance metrics\n",
        "- Compare results with baseline or standard models\n",
        "- Analyze failure cases and identify model weaknesses\n",
        "- Visualize results using graphs, confusion matrices, and heat maps\n",
        "- Iteratively improve model parameters or retrain with improved data\n",
        "\n",
        "## ðŸ”— Prerequisites\n",
        "\n",
        "- âœ… Unit 3: Model development completed\n",
        "- âœ… Trained models ready for evaluation\n",
        "- âœ… Understanding of evaluation metrics\n",
        "\n",
        "---\n",
        "\n",
        "## Official Structure Reference\n",
        "\n",
        "This notebook covers practical activities from **Course 12, Unit 4**:\n",
        "- Conducting experiments and collecting performance metrics\n",
        "- Comparing results with baseline or standard models\n",
        "- Analyzing failure cases and identifying weaknesses in the model\n",
        "- Visualizing results using graphs, confusion matrices, or heat maps\n",
        "- Iteratively improving model parameters or retraining with improved data\n",
        "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 4 Practical Content\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Model Evaluation and Optimization** involves comprehensive analysis of model performance, identifying areas for improvement, and iteratively refining the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, \n",
        "                            roc_curve, auc, precision_recall_curve)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"âœ… Libraries imported!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate data and split\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training: {X_train.shape}, Test: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Model Comparison - Multiple Algorithms\n",
        "\n",
        "Compare different algorithms to select the best approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train multiple models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM': SVC(probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Model Comparison\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "    results[name] = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred),\n",
        "        'recall': recall_score(y_test, y_pred),\n",
        "        'f1': f1_score(y_test, y_pred),\n",
        "        'predictions': y_pred,\n",
        "        'probabilities': y_proba\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Accuracy:  {results[name]['accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {results[name]['precision']:.4f}\")\n",
        "    print(f\"  Recall:    {results[name]['recall']:.4f}\")\n",
        "    print(f\"  F1-Score:  {results[name]['f1']:.4f}\")\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(results, key=lambda x: results[x]['f1'])\n",
        "print(f\"\\nâœ… Best Model (F1-Score): {best_model_name} ({results[best_model_name]['f1']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Confusion Matrix and ROC Curve Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize confusion matrix for best model\n",
        "best_predictions = results[best_model_name]['predictions']\n",
        "cm = confusion_matrix(y_test, best_predictions)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"Confusion Matrix: {best_model_name}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n{cm}\")\n",
        "print(f\"\\nTrue Negatives: {cm[0,0]}, False Positives: {cm[0,1]}\")\n",
        "print(f\"False Negatives: {cm[1,0]}, True Positives: {cm[1,1]}\")\n",
        "\n",
        "# ROC Curve (if probabilities available)\n",
        "if results[best_model_name]['probabilities'] is not None:\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, results[best_model_name]['probabilities'])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
        "    \n",
        "    print(\"\\nâœ… Use matplotlib to visualize:\")\n",
        "    print(\"  - Confusion matrix heatmap\")\n",
        "    print(\"  - ROC curves for all models\")\n",
        "    print(\"  - Precision-Recall curves\")\n",
        "    print(\"  - Feature importance plots\")\n",
        "else:\n",
        "    print(\"\\nNote: Some models don't provide probability estimates\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Failure Case Analysis\n",
        "\n",
        "Analyze where and why the model fails to improve performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze failure cases\n",
        "best_model = models[best_model_name]\n",
        "predictions = results[best_model_name]['predictions']\n",
        "\n",
        "# Find misclassified samples\n",
        "misclassified_mask = predictions != y_test\n",
        "misclassified_indices = np.where(misclassified_mask)[0]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Failure Case Analysis\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total test samples: {len(y_test)}\")\n",
        "print(f\"Correctly classified: {np.sum(predictions == y_test)}\")\n",
        "print(f\"Misclassified: {len(misclassified_indices)} ({len(misclassified_indices)/len(y_test)*100:.2f}%)\")\n",
        "\n",
        "# Analyze false positives and false negatives\n",
        "false_positives = np.where((predictions == 1) & (y_test == 0))[0]\n",
        "false_negatives = np.where((predictions == 0) & (y_test == 1))[0]\n",
        "\n",
        "print(f\"\\nFalse Positives (Type I errors): {len(false_positives)}\")\n",
        "print(f\"False Negatives (Type II errors): {len(false_negatives)}\")\n",
        "\n",
        "# Feature analysis for failure cases (if probabilities available)\n",
        "if results[best_model_name]['probabilities'] is not None:\n",
        "    probs = results[best_model_name]['probabilities']\n",
        "    fp_probs = probs[false_positives] if len(false_positives) > 0 else []\n",
        "    fn_probs = probs[false_negatives] if len(false_negatives) > 0 else []\n",
        "    \n",
        "    if len(fp_probs) > 0:\n",
        "        print(f\"\\nFalse Positive Analysis:\")\n",
        "        print(f\"  Average confidence: {np.mean(fp_probs):.4f}\")\n",
        "        print(f\"  These samples were predicted as positive but are actually negative\")\n",
        "    \n",
        "    if len(fn_probs) > 0:\n",
        "        print(f\"\\nFalse Negative Analysis:\")\n",
        "        print(f\"  Average confidence: {np.mean(fn_probs):.4f}\")\n",
        "        print(f\"  These samples were predicted as negative but are actually positive\")\n",
        "\n",
        "print(\"\\nâœ… Use failure case analysis to:\")\n",
        "print(\"  - Identify data quality issues\")\n",
        "print(\"  - Improve feature engineering\")\n",
        "print(\"  - Adjust class weights or thresholds\")\n",
        "print(\"  - Collect more training data for difficult cases\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Evaluation Steps:\n",
        "1. **Baseline Comparison**: Compare with simple baseline models\n",
        "2. **Multiple Algorithms**: Try different algorithms to find best fit\n",
        "3. **Comprehensive Metrics**: Use accuracy, precision, recall, F1, ROC-AUC\n",
        "4. **Visualization**: Confusion matrices, ROC curves, precision-recall curves\n",
        "5. **Failure Analysis**: Understand where and why model fails\n",
        "6. **Iterative Improvement**: Refine based on analysis\n",
        "\n",
        "### Optimization Strategies:\n",
        "- **Data**: Collect more data, improve data quality, feature engineering\n",
        "- **Model**: Try different architectures, ensemble methods\n",
        "- **Hyperparameters**: Optimize learning rate, regularization, etc.\n",
        "- **Thresholds**: Adjust decision thresholds for precision/recall trade-off\n",
        "\n",
        "**Reference:** Course 12, Unit 4: \"Evaluation and Optimization\" - All practical activities covered\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}