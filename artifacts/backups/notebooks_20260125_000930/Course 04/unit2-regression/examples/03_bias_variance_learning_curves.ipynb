{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff and Learning Curves\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand bias and variance in machine learning\n",
    "- Analyze bias-variance tradeoff through learning curves\n",
    "- Identify and handle overfitting/underfitting\n",
    "- Select optimal model complexity using validation sets\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of regression and model evaluation\n",
    "- âœ… Python 3.8+ installed\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 04, Unit 2**:\n",
    "- Analyzing bias-variance tradeoff through learning curves\n",
    "- Identifying and handling overfitting/underfitting\n",
    "- Selecting optimal model complexity using validation sets\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Bias-Variance Tradeoff\n",
    "\n",
    "**Bias**: Error from overly simplistic assumptions (underfitting)\n",
    "**Variance**: Error from sensitivity to small fluctuations (overfitting)\n",
    "**Tradeoff**: Finding the right balance between model complexity and generalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection \n",
    "import learning_curve, validation_curve\n",
    "from sklearn.linear_model \n",
    "import LinearRegression\n",
    "from sklearn.preprocessing \n",
    "import PolynomialFeatures\n",
    "from sklearn.pipeline \n",
    "import Pipeline\n",
    "from sklearn.metrics \n",
    "import mean_squared_error_\n",
    "print(\"âœ… Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data_np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)_y =  np.sin(X).ravel() + 0.3 * np.random.randn(100)\n",
    "y = np.sin(X).ravel() + 0.3 * np.random.randn(100)\n",
    "\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Learning Curves - Analyzing Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning curves for different model complexities_degrees = [1, 3, 10]\n",
    "models = {}\n",
    "\n",
    "for degree in degrees:\n",
    "    model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)), ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    train_sizes, train_scores, val_scores = learning_curve(model, X, y, cv=5, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    models[degree] = {\n",
    "        'train_sizes': train_sizes,\n",
    "        'train_mean': -train_scores.mean(axis=1), 'train_std': train_scores.std(axis=1),\n",
    "        'val_mean': -val_scores.mean(axis=1), 'val_std': val_scores.std(axis=1)\n",
    "    }\n",
    "    \n",
    "    print(f\"Degree {degree}:\")\n",
    "    print(f\"  Final Train MSE: {models[degree]['train_mean'][-1]:.4f}\")\n",
    "    print(f\"  Final Val MSE: {models[degree]['val_mean'][-1]:.4f}\")\n",
    "    print(f\"  Gap (overfitting indicator): {models[degree]['val_mean'][-1] - models[degree]['train_mean'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Identifying Overfitting and Underfitting\n",
    "\n",
    "Let's compare model performance to identify optimal complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze bias-variance tradeoff_\n",
    "print(\"=\" * 60)\n",
    "print(\"Bias-Variance Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Degree 1 (Underfitting - High Bias)\n",
    "print(\"\\nDegree 1 (Linear - Underfitting):\")\n",
    "print(\"  Characteristic: High bias, low variance\")\n",
    "print(\"  Train MSE: {:.4f}\".format(models[1]['train_mean'][-1]))\n",
    "print(\"  Val MSE: {:.4f}\".format(models[1]['val_mean'][-1]))\n",
    "print(\"  Gap: {:.4f} (small gap, but both high = underfitting)\".format(models[1]['val_mean'][-1] - models[1]['train_mean'][-1]))\n",
    "\n",
    "# Degree 3 (Balanced)\n",
    "print(\"\\nDegree 3 (Balanced):\")\n",
    "print(\"  Characteristic: Balanced bias-variance\")\n",
    "print(\"  Train MSE: {:.4f}\".format(models[3]['train_mean'][-1]))\n",
    "print(\"  Val MSE: {:.4f}\".format(models[3]['val_mean'][-1]))\n",
    "print(\"  Gap: {:.4f} (reasonable gap)\".format(models[3]['val_mean'][-1] - models[3]['train_mean'][-1]))\n",
    "\n",
    "# Degree 10 (Overfitting - High Variance)\n",
    "print(\"\\nDegree 10 (Overfitting - High Variance):\")\n",
    "print(\"  Characteristic: Low bias, high variance\")\n",
    "print(\"  Train MSE: {:.4f}\".format(models[10]['train_mean'][-1]))\n",
    "print(\"  Val MSE: {:.4f}\".format(models[10]['val_mean'][-1]))\n",
    "print(\"  Gap: {:.4f} (large gap = overfitting)\".format(models[10]['val_mean'][-1] - models[10]['train_mean'][-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Bias**: Model too simple, cannot capture patterns (underfitting)\n",
    "2. **Variance**: Model too complex, memorizes noise (overfitting)\n",
    "3. **Learning Curves**: Show train/validation performance vs training set size\n",
    "4. **Optimal Complexity**: Balance where validation error is minimized\n",
    "\n",
    "### How to Use:\n",
    "- **Large gap between train/val**: Overfitting â†’ reduce complexity\n",
    "- **Both train/val high**: Underfitting â†’ increase complexity\n",
    "- **Both train/val similar and low**: Good model\n",
    "\n",
    "**Reference:** Course 04, Unit 2: \"Analyzing bias-variance tradeoff through learning curves\" and \"Identifying and handling overfitting/underfitting\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}