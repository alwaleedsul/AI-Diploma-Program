{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. LIME Explanations | ØªÙØ³ÙŠØ±Ø§Øª LIME\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 06, Unit 4** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. LIME Explanations | ØªÙØ³ÙŠØ±Ø§Øª LIME\n",
    "\n",
    "## ðŸš¨ THE PROBLEM: We Need Faster, Simpler Explanations | Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: Ù†Ø­ØªØ§Ø¬ ØªÙØ³ÙŠØ±Ø§Øª Ø£Ø³Ø±Ø¹ ÙˆØ£Ø¨Ø³Ø·\n",
    "\n",
    "**Remember the limitation from the previous notebook?**\n",
    "\n",
    "We learned SHAP for explaining model predictions. But we discovered:\n",
    "\n",
    "**What if we need a simpler, faster explanation method that works with any model?**\n",
    "\n",
    "**The Problem**: Sometimes we need:\n",
    "- âŒ **Faster explanations** (SHAP can be slow for large datasets)\n",
    "- âŒ **Simpler methods** (easier to understand and implement)\n",
    "- âŒ **Model-agnostic approaches** (work with any black-box model)\n",
    "- âŒ **Local explanations** (explain individual predictions quickly)\n",
    "\n",
    "**We've learned:**\n",
    "- âœ… How to use SHAP for model explanations (Notebook 1)\n",
    "- âœ… How to generate local and global explanations\n",
    "- âœ… Feature importance concepts\n",
    "\n",
    "**But we haven't learned:**\n",
    "- âŒ How to use **LIME** for fast local explanations\n",
    "- âŒ How to provide **simpler explanations** for non-technical users\n",
    "- âŒ How to use **model-agnostic** explanation methods\n",
    "- âŒ How to **compare different explanation methods**\n",
    "\n",
    "**We need LIME (Local Interpretable Model-agnostic Explanations)** to:\n",
    "1. Provide fast local explanations\n",
    "2. Work with any black-box model\n",
    "3. Offer simpler, more intuitive explanations\n",
    "4. Enable real-time explainability\n",
    "\n",
    "**This notebook solves that problem** by teaching you LIME for fast, model-agnostic explanations!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: SHAP Explanations** - Understanding explainability basics\n",
    "- âœ… **Basic Python knowledge**: Functions, data manipulation, ML concepts\n",
    "- âœ… **Basic ML knowledge**: Model training, predictions\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why we need alternative explanation methods\n",
    "- Knowing how LIME differs from SHAP\n",
    "- Understanding local interpretability concepts\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the SECOND example in Unit 4** - it teaches you faster, simpler explanations!\n",
    "\n",
    "**Why this example SECOND?**\n",
    "- **Before** you can use LIME effectively, you need to understand SHAP (Example 1)\n",
    "- **Before** you can compare methods, you need multiple approaches\n",
    "- **Before** you can use counterfactuals, you need local explanations\n",
    "\n",
    "**Builds on**: \n",
    "- ðŸ““ Example 1: SHAP Explanations (we learned SHAP, now we learn LIME!)\n",
    "\n",
    "**Leads to**: \n",
    "- ðŸ““ Example 3: Counterfactual Analysis (what-if explanations)\n",
    "- ðŸ““ Example 4: Accountability Frameworks (accountability structures)\n",
    "- ðŸ““ Example 5: Human-in-the-Loop (HITL approaches)\n",
    "- ðŸ““ Example 6: Transparency Tools (transparency frameworks)\n",
    "\n",
    "**Why this order?**\n",
    "1. LIME provides **alternative approach** (complements SHAP)\n",
    "2. LIME enables **faster explanations** (critical for real-time systems)\n",
    "3. LIME shows **model-agnostic** methods (works with any model)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Fast and Simple Explanations | Ø§Ù„Ù‚ØµØ©: ØªÙØ³ÙŠØ±Ø§Øª Ø³Ø±ÙŠØ¹Ø© ÙˆØ¨Ø³ÙŠØ·Ø©\n",
    "\n",
    "Imagine you're a loan officer using AI to approve loans. **Before** LIME, you'd use SHAP but it might be too slow for real-time decisions. **After** using LIME, you get fast, simple explanations that show which factors matter most - quick and understandable!\n",
    "\n",
    "Same with AI: **Before** we have SHAP but it may be slow, now we learn LIME - fast local explanations using simple linear models! **After** LIME, we can explain predictions quickly for any model!\n",
    "\n",
    "---\n",
    "\n",
    "## Why LIME Explanations Matter | Ù„Ù…Ø§Ø°Ø§ ØªÙ‡Ù… ØªÙØ³ÙŠØ±Ø§Øª LIMEØŸ\n",
    "\n",
    "LIME explanations are essential for ethical AI:\n",
    "- **Speed**: Fast explanations for real-time systems\n",
    "- **Simplicity**: Easy to understand for non-technical users\n",
    "- **Model-Agnostic**: Works with any black-box model\n",
    "- **Local Focus**: Explains individual predictions clearly\n",
    "- **Flexibility**: Works with tabular, text, and image data\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Understand LIME and how it differs from SHAP\n",
    "2. Learn how to generate LIME explanations\n",
    "3. Apply LIME to tabular data\n",
    "4. Understand local interpretability concepts\n",
    "5. Compare LIME vs SHAP approaches\n",
    "6. Use LIME for real-time explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:59:11.792919Z",
     "iopub.status.busy": "2025-12-26T16:59:11.792775Z",
     "iopub.status.idle": "2025-12-26T16:59:13.760364Z",
     "shell.execute_reply": "2025-12-26T16:59:13.760117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: LIME library not available. Using simplified LIME implementation.\n",
      "================================================================================\n",
      "Unit 4 - Example 2: LIME Explanations\n",
      "================================================================================\n",
      "\n",
      "Generating dataset...\n",
      "Dataset shape: (1000, 5)\n",
      "\n",
      "Training Random Forest model...\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy: 0.9167\n",
      "\n",
      "Generating LIME explanations...\n",
      "Generated 10 LIME explanations\n",
      "\n",
      "================================================================================\n",
      "Creating Visualizations...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: lime_explanation.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: lime_comparison.png\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. LIME provides local, interpretable explanations\n",
      "2. LIME approximates complex models with simple linear models locally\n",
      "3. LIME works for any black-box model\n",
      "4. LIME explanations are instance-specific\n",
      "5. LIME helps understand individual predictions\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unit 4: Interpretability, Transparency, and Accountability\n",
    "Example 2: LIME Explanations\n",
    "This example demonstrates LIME (Local Interpretable Model-agnostic Explanations):\n",
    "- LIME for tabular data\n",
    "- LIME for text data (simplified)\n",
    "- Local interpretability\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Try to import LIME, use simplified version if not available\n",
    "try:\n",
    "    import lime\n",
    "    import lime.lime_tabular\n",
    "    LIME_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIME_AVAILABLE = False\n",
    "    print(\"Note: LIME library not available. Using simplified LIME implementation.\")\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "# ============================================================================\n",
    "# SIMPLIFIED LIME IMPLEMENTATION\n",
    "# ============================================================================\n",
    "def lime_explanation_simple(model, X_sample, X_train, feature_names, n\n",
    "samples =5000):\n",
    "    \"\"\"\n",
    "    Simplified LIME implementation using local linear approximation\n",
    "    \"\"\"\n",
    "    # Generate perturbed samples around the instance\n",
    "    np.random.seed(42)\n",
    "    perturbations = np.random.normal(0, 0.1, (n_samples, X_sample.shape[1]))\n",
    "    X\n",
    "perturbed = X\n",
    "sample + perturbations\n",
    "    # Get predictions for perturbed samples\n",
    "    y\n",
    "perturbed = model.predict_proba(X_perturbed)[:, 1]\n",
    "    # Calculate distances (weights)\n",
    "    distances = np.exp(-np.sum((X_perturbed - X_sample) ** 2, axis=1))\n",
    "    # Fit linear model to approximate local behavior\n",
    "    linear\n",
    "model = Ridge(alpha=0.1)\n",
    "    linear_model.fit(X_perturbed, y_perturbed, sample\n",
    "weight =distances)\n",
    "    # Get feature importance (coefficients)\n",
    "    feature\n",
    "importance = linear\n",
    "model.coef_\n",
    "    return feature_importance, linear_model\n",
    "# ============================================================================\n",
    "# GENERATE DATASET\n",
    "# ============================================================================\n",
    "def generate_dataset(n\n",
    "samples =1000):\n",
    "    \"\"\"\n",
    "    Generate synthetic dataset for LIME demonstration\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    # Features\n",
    "    age = np.random.randint(25, 70, n_samples)\n",
    "    income = np.random.normal(60000, 25000, n_samples)\n",
    "    credit\n",
    "history = np.random.choice([0, 1, 2, 3], n_samples)  # 0=bad, 3=excellent\n",
    "    loan\n",
    "amount = np.random.uniform(10000, 200000, n_samples)\n",
    "    # Target (loan approval)\n",
    "    approval\n",
    "prob = (credit_history\n",
    "3 * 0.5 +\n",
    "                     (income\n",
    "100000) * 0.3 +\n",
    "                     (1 - loan_amount\n",
    "200000) * 0.15 +\n",
    "                     (age\n",
    "70) * 0.05 +\n",
    "                     np.random.normal(0, 0.05, n_samples))\n",
    "    approval = (approval_prob > 0.5).astype(int)\n",
    "    df = pd.DataFrame({\n",
    "        'age': age, 'income': income,\n",
    "        'credit_history': credit_history,\n",
    "        'loan_amount': loan_amount,\n",
    "        'approved': approval\n",
    "    })\n",
    "    return df\n",
    "# ============================================================================\n",
    "# LIME EXPLANATIONS\n",
    "# ============================================================================\n",
    "def explain_with_lime(model, X_sample, X_train, feature_names, use\n",
    "library =True):\n",
    "    \"\"\"\n",
    "    Generate LIME explanations for a single instance\n",
    "    \"\"\"\n",
    "    if LIME_AVAILABLE and use_library:\n",
    "        # Use actual LIME library\n",
    "        explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "            X_train, feature\n",
    "names = feature\n",
    "names, mode='classification'\n",
    "        )\n",
    "        explanation = explainer.explain_instance(\n",
    "            X_sample[0], model.predict_proba, num\n",
    "features =len(feature_names)\n",
    "        )\n",
    "        return explanation, explainer\n",
    "    else:\n",
    "        # Use simplified implementation\n",
    "        feature_importance, linear\n",
    "model = lime\n",
    "explanation\n",
    "simple(\n",
    "            model, X_sample, X_train, feature_names\n",
    "        )\n",
    "        return feature_importance, linear_model\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "def plot_lime_explanation(feature_importance, feature_names, X_sample, sample\n",
    "idx =0):\n",
    "    \"\"\"\n",
    "    Plot LIME explanation for a single instance\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    # Sort by absolute importance\n",
    "    indices = np.argsort(np.abs(feature_importance))[::-1]\n",
    "    colors = ['red' if feature_importance[i] < 0 else 'green' for i in indices]\n",
    "    bars = ax.barh(range(len(feature_names)), [feature_importance[i] for i in indices],\n",
    "                   color=colors, alpha=0.7)\n",
    "    # Add value labels\n",
    "    for i, (bar, idx) in enumerate(zip(bars, indices)):\n",
    "        height = bar.get_height()\n",
    "        width = bar.get_width()\n",
    "        label = f'{feature_names[idx]}\\n={X_sample[0, idx]:.2f}'\n",
    "        ax.text(width/2 if width > 0 else width/2, i, label,\n",
    "               ha='center' if width > 0 else 'right', va='center', fontsize=9)\n",
    "    ax.set_yticks(range(len(feature_names)))\n",
    "    ax.set_yticklabels([feature_names[i] for i in indices])\n",
    "    ax.set_xlabel('LIME Feature Importance', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'LIME Explanation (Sample {sample_idx})', fontsize=12, fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unit4-transparency-accountability', dpi=300, bbox\n",
    "inches ='tight')\n",
    "    print(\"âœ… Saved: lime_explanation.png\")\n",
    "    plt.close()\n",
    "def plot_lime_comparison(explanations, feature_names, n\n",
    "samples =5):\n",
    "    \"\"\"\n",
    "    Plot LIME explanations for multiple samples\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, n_samples, figsize=(20, 6))\n",
    "    for idx, (importance, X_sample) in enumerate(explanations[:n_samples]):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "        # Sort by absolute importance\n",
    "        indices = np.argsort(np.abs(importance))[::-1]\n",
    "        top\n",
    "k = min(4, len(indices))\n",
    "        top\n",
    "indices = indices[:top_k]\n",
    "        top\n",
    "importance = importance[top_indices]\n",
    "        top\n",
    "names = [feature_names[i] for i in top_indices]\n",
    "        colors = ['red' if imp < 0 else 'green' for imp in top_importance]\n",
    "        axes[idx].barh(range(len(top_names)), top_importance, color=colors, alpha=0.7)\n",
    "        axes[idx].set_yticks(range(len(top_names)))\n",
    "        axes[idx].set_yticklabels(top_names, fontsize=8)\n",
    "        axes[idx].set_title(f'Sample {idx}', fontsize=10, fontweight='bold')\n",
    "        axes[idx].axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "        axes[idx].grid(axis='x', alpha=0.3)\n",
    "    plt.suptitle('LIME Explanations for Multiple Samples', fontsize=12, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unit4-transparency-accountability', dpi=300, bbox\n",
    "inches ='tight')\n",
    "    print(\"âœ… Saved: lime_comparison.png\")\n",
    "    plt.close()\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if_\n",
    "name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"Unit 4 - Example 2: LIME Explanations\")\n",
    "    print(\"=\"*80)\n",
    "    # Generate dataset\n",
    "    print(\"\\nGenerating dataset...\")\n",
    "    df = generate\n",
    "dataset(n\n",
    "samples =1000)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    # Prepare data\n",
    "    feature\n",
    "names = ['age', 'income', 'credit_history', 'loan_amount']\n",
    "    X = df[feature_names].values\n",
    "    y = df['approved'].values\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y\n",
    "test = train\n",
    "test\n",
    "split(\n",
    "        X, y, test\n",
    "size =0.3, random\n",
    "state =42, stratify=y\n",
    "    )\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train\n",
    "scaled = scaler.fit_transform(X_train)\n",
    "    X_test\n",
    "scaled = scaler.transform(X_test)\n",
    "    # Train model\n",
    "    print(\"\\nTraining Random Forest model...\")\n",
    "    model = RandomForestClassifier(n\n",
    "estimators =100, random\n",
    "state =42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train\n",
    "acc = accuracy\n",
    "score(y_train, model.predict(X_train_scaled))\n",
    "    test\n",
    "acc = accuracy\n",
    "score(y_test, model.predict(X_test_scaled))\n",
    "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    # Generate LIME explanations for multiple samples\n",
    "    print(\"\\nGenerating LIME explanations...\")\n",
    "    explanations = []\n",
    "    for i in range(min(10, len(X_test_scaled))):\n",
    "        X\n",
    "sample = X\n",
    "test\n",
    "scaled[i:i+1]\n",
    "        explanation, _ = explain\n",
    "with\n",
    "lime(\n",
    "            model, X_sample, X_train_scaled, feature_names, use\n",
    "library =LIME_AVAILABLE\n",
    "        )\n",
    "        if isinstance(explanation, np.ndarray):\n",
    "            explanations.append((explanation, X_test[i:i+1]))\n",
    "        else:\n",
    "            # If using LIME library, extract feature importance\n",
    "            exp\n",
    "list = explanation.as_list()\n",
    "            importance = np.zeros(len(feature_names))\n",
    "            for feature_name, value in exp_list:\n",
    "                idx = feature\n",
    "names.index(feature_name)\n",
    "                importance[idx] = value\n",
    "            explanations.append((importance, X_test[i:i+1]))\n",
    "    print(f\"Generated {len(explanations)} LIME explanations\")\n",
    "    # Create visualizations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Creating Visualizations...\")\n",
    "    print(\"=\"*80)\n",
    "    plot_lime_explanation(explanations[0][0], feature_names, explanations[0][1], sample\n",
    "idx =0)\n",
    "    plot_lime_comparison(explanations, feature_names, n\n",
    "samples =5)\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nKey Takeaways:\")\n",
    "    print(\"1. LIME provides local, interpretable explanations\")\n",
    "    print(\"2. LIME approximates complex models with simple linear models locally\")\n",
    "    print(\"3. LIME works for any black-box model\")\n",
    "    print(\"4. LIME explanations are instance-specific\")\n",
    "    print(\"5. LIME helps understand individual predictions\")\n",
    "    print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸš« When LIME Explanations Hit a Limitation | Ø¹Ù†Ø¯Ù…Ø§ ØªØµÙ„ ØªÙØ³ÙŠØ±Ø§Øª LIME Ø¥Ù„Ù‰ Ø­Ø¯\n",
    "\n",
    "### The Limitation We Discovered\n",
    "\n",
    "We've learned LIME for fast, local explanations. **But there's still a challenge:**\n",
    "\n",
    "**What if we need to understand \"what if\" scenarios - how would predictions change if features were different?**\n",
    "\n",
    "LIME works well when:\n",
    "- âœ… We need fast local explanations\n",
    "- âœ… We want to understand current predictions\n",
    "- âœ… We can use model-agnostic methods\n",
    "\n",
    "**But we also need:**\n",
    "- âŒ **Counterfactual explanations** (what-if scenarios)\n",
    "- âŒ **Actionable insights** (what to change to get different outcomes)\n",
    "- âŒ **Alternative scenarios** (how predictions would change)\n",
    "- âŒ **Causal reasoning** (understanding cause-effect relationships)\n",
    "\n",
    "### Why This Is a Problem\n",
    "\n",
    "When we only have feature importance:\n",
    "- We know what features matter, but not what to change\n",
    "- We can't explore alternative scenarios\n",
    "- We can't understand \"what if\" questions\n",
    "- We can't provide actionable recommendations\n",
    "\n",
    "### The Solution: Counterfactual Analysis\n",
    "\n",
    "We need **counterfactual analysis** to:\n",
    "1. Understand \"what if\" scenarios\n",
    "2. Provide actionable insights\n",
    "3. Explore alternative outcomes\n",
    "4. Enable causal reasoning\n",
    "\n",
    "**This is exactly what we'll learn in the next notebook: Counterfactual Analysis!**\n",
    "\n",
    "---\n",
    "\n",
    "## âž¡ï¸ Next Steps | Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©\n",
    "\n",
    "**You've completed this notebook!** Now you understand:\n",
    "- âœ… How to use SHAP for explanations (Notebook 1)\n",
    "- âœ… How to use LIME for fast explanations (This notebook!)\n",
    "- âœ… **The limitation**: We need \"what if\" scenarios!\n",
    "\n",
    "**Next notebook**: `03_counterfactual_analysis.ipynb`\n",
    "- Learn about counterfactual explanations\n",
    "- Understand \"what if\" scenarios\n",
    "- Explore alternative outcomes\n",
    "- Provide actionable insights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
