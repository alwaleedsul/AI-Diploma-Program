{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Connecting Probability Theory to ML Model Implementations\n",
        "\n",
        "## üìö Learning Objectives\n",
        "\n",
        "By completing this notebook, you will:\n",
        "- Connect probability theory to ML model implementations\n",
        "- Understand how probability distributions are used in ML\n",
        "- Apply probability concepts to model training and evaluation\n",
        "- See probability theory in action in real ML scenarios\n",
        "\n",
        "## üîó Prerequisites\n",
        "\n",
        "- ‚úÖ Understanding of probability distributions\n",
        "- ‚úÖ Understanding of machine learning concepts\n",
        "- ‚úÖ Python, NumPy, scikit-learn knowledge\n",
        "\n",
        "---\n",
        "\n",
        "## Official Structure Reference\n",
        "\n",
        "This notebook covers practical activities from **Course 03, Unit 5**:\n",
        "- Connecting probability theory to ML model implementations\n",
        "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 5 Practical Content\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Probability theory** is fundamental to machine learning. ML models make probabilistic predictions, loss functions are based on probability, and uncertainty quantification relies on probability distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Libraries imported!\")\n",
        "print(\"\\nConnecting Probability Theory to ML Model Implementations\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 1: Probability in Classification (Logistic Regression)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Part 1: Probability in Classification (Logistic Regression)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Generate data\n",
        "np.random.seed(42)\n",
        "n_samples = 200\n",
        "X = np.random.randn(n_samples, 2)\n",
        "# Create binary classification problem\n",
        "y = ((X[:, 0] + X[:, 1]) > 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get probability predictions (this is probability theory in action!)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"\\nLogistic Regression Model:\")\n",
        "print(f\"  Training samples: {len(X_train)}\")\n",
        "print(f\"  Test samples: {len(X_test)}\")\n",
        "print(f\"\\nSample predictions:\")\n",
        "for i in range(5):\n",
        "    print(f\"  Sample {i+1}: True class={y_test[i]}, Predicted prob={y_proba[i]:.4f}, Predicted class={y_pred[i]}\")\n",
        "\n",
        "# Cross-entropy loss (based on probability)\n",
        "loss = log_loss(y_test, y_proba)\n",
        "print(f\"\\nCross-entropy loss (negative log-likelihood): {loss:.4f}\")\n",
        "\n",
        "# Visualize probability predictions\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "scatter = plt.scatter(X_test[:, 0], X_test[:, 1], c=y_proba, cmap='RdYlBu', s=50, alpha=0.7)\n",
        "plt.colorbar(scatter, label='Predicted Probability')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Probability Predictions (Logistic Regression)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(y_proba[y_test == 0], bins=20, alpha=0.7, label='Class 0', density=True)\n",
        "plt.hist(y_proba[y_test == 1], bins=20, alpha=0.7, label='Class 1', density=True)\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Probability Distribution by True Class')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Probability in classification demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 2: Bayesian Inference in ML (Naive Bayes)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 2: Bayesian Inference in ML (Naive Bayes)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Naive Bayes uses Bayes' theorem: P(class|features) ‚àù P(features|class) * P(class)\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Get probability predictions using Bayes' theorem\n",
        "y_proba_nb = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(f\"\\nNaive Bayes Model (Bayesian Classification):\")\n",
        "print(f\"  Class priors: {nb_model.class_prior_}\")\n",
        "print(f\"  Uses Bayes' theorem for classification\")\n",
        "print(f\"\\nSample predictions:\")\n",
        "for i in range(5):\n",
        "    print(f\"  Sample {i+1}: True class={y_test[i]}, Bayesian prob={y_proba_nb[i]:.4f}\")\n",
        "\n",
        "# Compare with logistic regression\n",
        "loss_nb = log_loss(y_test, y_proba_nb)\n",
        "print(f\"\\nCross-entropy loss (Naive Bayes): {loss_nb:.4f}\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "scatter = plt.scatter(X_test[:, 0], X_test[:, 1], c=y_proba_nb, cmap='RdYlBu', s=50, alpha=0.7)\n",
        "plt.colorbar(scatter, label='Bayesian Probability')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Bayesian Probability Predictions (Naive Bayes)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_proba, y_proba_nb, alpha=0.6, s=50)\n",
        "plt.plot([0, 1], [0, 1], 'r--', linewidth=2)\n",
        "plt.xlabel('Logistic Regression Probability')\n",
        "plt.ylabel('Naive Bayes Probability')\n",
        "plt.title('Probability Comparison')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Bayesian inference in ML demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 3: Probability Distributions in Loss Functions\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 3: Probability Distributions in Loss Functions\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Many ML loss functions are derived from probability distributions\n",
        "print(\"\\nLoss Functions and Probability Distributions:\")\n",
        "print(\"  1. Mean Squared Error (MSE) ‚Üî Gaussian distribution\")\n",
        "print(\"  2. Cross-entropy loss ‚Üî Multinomial distribution\")\n",
        "print(\"  3. Negative log-likelihood ‚Üî Maximum likelihood estimation\")\n",
        "\n",
        "# Demonstrate: MSE and Gaussian distribution\n",
        "y_true_reg = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "y_pred_reg = np.array([1.2, 1.8, 3.1, 3.9, 5.2])\n",
        "\n",
        "mse = np.mean((y_true_reg - y_pred_reg)**2)\n",
        "print(f\"\\nRegression Example:\")\n",
        "print(f\"  True values: {y_true_reg}\")\n",
        "print(f\"  Predictions: {y_pred_reg}\")\n",
        "print(f\"  MSE: {mse:.4f}\")\n",
        "\n",
        "# MSE assumes Gaussian noise: y ~ N(≈∑, œÉ¬≤)\n",
        "# Maximizing likelihood = minimizing MSE\n",
        "sigma = np.sqrt(mse)\n",
        "print(f\"  Estimated œÉ (std of residuals): {sigma:.4f}\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "residuals = y_true_reg - y_pred_reg\n",
        "plt.scatter(y_pred_reg, residuals, s=100, alpha=0.7)\n",
        "plt.axhline(0, color='r', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals (Gaussian noise assumption)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "x = np.linspace(-3, 3, 100)\n",
        "plt.hist(residuals, bins=10, density=True, alpha=0.7, label='Residuals')\n",
        "plt.plot(x, stats.norm.pdf(x, 0, sigma), 'r-', linewidth=2, label=f'Gaussian(0, {sigma:.2f}¬≤)')\n",
        "plt.xlabel('Residual Value')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Residual Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Probability distributions in loss functions demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Connections:\n",
        "1. **Classification**: Models output probabilities (logistic regression, Naive Bayes)\n",
        "2. **Loss Functions**: Derived from probability distributions (MSE ‚Üî Gaussian, cross-entropy ‚Üî multinomial)\n",
        "3. **Bayesian ML**: Uses Bayes' theorem for inference (Naive Bayes, Bayesian neural networks)\n",
        "4. **Uncertainty**: Probability distributions quantify prediction uncertainty\n",
        "5. **Maximum Likelihood**: Training often maximizes likelihood (equivalent to minimizing negative log-likelihood)\n",
        "\n",
        "### Probability in ML:\n",
        "- **Predictions**: Probabilistic outputs, not just point estimates\n",
        "- **Training**: Maximum likelihood estimation\n",
        "- **Evaluation**: Likelihood-based metrics (log-likelihood, cross-entropy)\n",
        "- **Uncertainty**: Probability distributions represent uncertainty\n",
        "\n",
        "### Applications:\n",
        "- Probabilistic predictions\n",
        "- Uncertainty quantification\n",
        "- Bayesian inference\n",
        "- Model comparison\n",
        "- Decision making under uncertainty\n",
        "\n",
        "**Reference:** Course 03, Unit 5: \"Probability and Statistical Inference\" - Connecting probability to ML practical content"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}