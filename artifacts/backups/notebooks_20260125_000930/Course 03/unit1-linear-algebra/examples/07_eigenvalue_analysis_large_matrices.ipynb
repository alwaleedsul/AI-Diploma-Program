{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue Analysis on Large-Dimensional Matrices\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Solve eigenvalue problems programmatically for large matrices\n",
    "- Apply eigenvalue analysis on large-dimensional matrices\n",
    "- Understand the importance of eigenvalues in ML\n",
    "- Apply eigenvalue decomposition to real ML problems\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of eigenvalues and eigenvectors\n",
    "- âœ… Understanding of matrix operations\n",
    "- âœ… Python and NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 03, Unit 1**:\n",
    "- Solving eigenvalue problems programmatically and applying eigenvalue analysis on large-dimensional matrices\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 1 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Eigenvalue analysis** is crucial for understanding matrix properties and is fundamental in machine learning for dimensionality reduction, principal component analysis, and understanding model behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nEigenvalue Analysis on Large-Dimensional Matrices\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Computing Eigenvalues for Large Matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Computing Eigenvalues for Large Matrices\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For large matrices, use efficient algorithms\n",
    "# NumPy provides optimized eigenvalue computation\n",
    "\n",
    "# Create a large symmetric matrix (common in ML)\n",
    "np.random.seed(42)\n",
    "n = 100  # Large dimension\n",
    "A = np.random.randn(n, n)\n",
    "A_symmetric = (A + A.T)\n",
    "2  # Make symmetric (real eigenvalues)\n",
    "\n",
    "print(f\"\\nComputing eigenvalues for {n}x{n} matrix...\")\n",
    "\n",
    "# Full eigenvalue decomposition\n",
    "eigenvalues_full, eigenvectors_full = np.linalg.eig(A_symmetric)\n",
    "print(f\"Eigenvalues computed: {len(eigenvalues_full)}\")\n",
    "print(f\"First 5 eigenvalues: {eigenvalues_full[:5]}\")\n",
    "\n",
    "# For symmetric matrices, use specialized function\n",
    "eigenvalues_sym, eigenvectors_sym = eigh(A_symmetric)\n",
    "print(f\"\\nUsing eigh (more efficient for symmetric):\")\n",
    "print(f\"First 5 eigenvalues: {eigenvalues_sym[:5]}\")\n",
    "\n",
    "# Verify: Av = Î»v\n",
    "idx = 0\n",
    "v = eigenvectors_sym[:, idx]\n",
    "lambda_val = eigenvalues_sym[idx]\n",
    "Av = A_symmetric @ v\n",
    "lambda_v = lambda_val * v\n",
    "\n",
    "print(f\"\\nVerification (Av = Î»v):\")\n",
    "print(f\"Max difference: {np.max(np.abs(Av - lambda_v)):.2e}\")\n",
    "\n",
    "print(\"\\nâœ… Eigenvalues computed for large matrix!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: Eigenvalue Analysis for Dimensionality Reduction\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Eigenvalue Analysis for Dimensionality Reduction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Eigenvalues are fundamental for PCA (Principal Component Analysis)\n",
    "# PCA uses eigenvalue decomposition of the covariance matrix\n",
    "\n",
    "# Create sample high-dimensional data\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 200, 50\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Add some structure (correlation)\n",
    "X[:, :5] = X[:, 0:1] + 0.1 * np.random.randn(n_samples, 5)\n",
    "\n",
    "print(f\"\\nOriginal data shape: {X.shape}\")\n",
    "\n",
    "# Center the data\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# Compute covariance matrix\n",
    "cov_matrix = np.cov(X_centered.T)\n",
    "print(f\"\\nCovariance matrix shape: {cov_matrix.shape}\")\n",
    "\n",
    "# Eigenvalue decomposition of covariance matrix\n",
    "eigenvalues, eigenvectors = eigh(cov_matrix)\n",
    "\n",
    "# Sort by eigenvalue magnitude (descending)\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "print(f\"\\nTop 5 eigenvalues:\")\n",
    "print(eigenvalues[:5])\n",
    "\n",
    "# Variance explained by each component\n",
    "variance_explained = eigenvalues\n",
    "eigenvalues.sum()\n",
    "print(f\"\\nVariance explained by top 5 components:\")\n",
    "print(variance_explained[:5])\n",
    "\n",
    "# Cumulative variance\n",
    "cumulative_variance = np.cumsum(variance_explained)\n",
    "print(f\"\\nCumulative variance explained:\")\n",
    "print(cumulative_variance[:5])\n",
    "\n",
    "print(\"\\nâœ… Eigenvalue analysis completed for dimensionality reduction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3: Application to ML Problems\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: Application to ML Problems\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example: Using PCA (which uses eigenvalue decomposition internally)\n",
    "print(\"\\nPCA for Dimensionality Reduction:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(f\"\\nReduced data shape: {X_pca.shape}\")\n",
    "print(f\"Variance explained by 10 components: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Visualize eigenvalues\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(eigenvalues[:10], 'bo-')\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('Eigenvalues (Top 10)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cumulative_variance[:20], 'ro-')\n",
    "plt.axhline(y=0.95, color='g', linestyle='--', label='95% variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.title('Cumulative Variance Explained')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Number of components to explain 95% variance\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"\\nNumber of components to explain 95% variance: {n_components_95}\")\n",
    "\n",
    "print(\"\\nâœ… Eigenvalue analysis applied to ML dimensionality reduction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Eigenvalue Decomposition**: Breaking down matrices into eigenvalues and eigenvectors\n",
    "2. **Symmetric Matrices**: Have real eigenvalues and orthogonal eigenvectors\n",
    "3. **Numerical Methods**: Use `eigh()` for symmetric matrices (more efficient than `eig()`)\n",
    "4. **PCA and Eigenvalues**: Principal Component Analysis uses eigenvalue decomposition\n",
    "5. **Dimensionality Reduction**: Eigenvalues determine importance of components\n",
    "\n",
    "### Best Practices:\n",
    "- Use `scipy.linalg.eigh()` for symmetric matrices (faster, more stable)\n",
    "- Verify decomposition: Av = Î»v\n",
    "- Sort eigenvalues by magnitude for dimensionality reduction\n",
    "- Use cumulative variance to select number of components\n",
    "\n",
    "### Applications:\n",
    "- Principal Component Analysis (PCA)\n",
    "- Dimensionality reduction\n",
    "- Data compression\n",
    "- Feature extraction\n",
    "- Understanding data structure\n",
    "\n",
    "**Reference:** Course 03, Unit 1: \"Linear Algebra for Machine Learning\" - Eigenvalue analysis on large-dimensional matrices practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}