{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experimenting with ML Model Parameters\n",
        "\n",
        "## ðŸ“š Learning Objectives\n",
        "\n",
        "By completing this notebook, you will:\n",
        "- Experiment with changes in ML model parameters\n",
        "- Observe changes in model fit to training data\n",
        "- Understand the impact of parameter changes on model behavior\n",
        "- Connect linear algebra concepts to ML model optimization\n",
        "\n",
        "## ðŸ”— Prerequisites\n",
        "\n",
        "- âœ… Understanding of linear algebra and vectors\n",
        "- âœ… Basic understanding of machine learning models\n",
        "- âœ… Python and NumPy knowledge\n",
        "\n",
        "---\n",
        "\n",
        "## Official Structure Reference\n",
        "\n",
        "This notebook covers practical activities from **Course 03, Unit 1**:\n",
        "- Experimenting with changes in ML model parameters and observing changes in model fit\n",
        "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 1 Practical Content\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Model parameters** can be viewed as vectors in parameter space. Changes to these parameters directly affect how well the model fits the training data. Understanding this relationship is fundamental to machine learning optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "print(\"âœ… Libraries imported!\")\n",
        "print(\"\\nExperimenting with ML Model Parameters\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 1: Linear Regression Parameter Experiments\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Part 1: Linear Regression Parameter Experiments\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create sample dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "X = np.random.randn(n_samples, 2)\n",
        "y = 2 * X[:, 0] + 3 * X[:, 1] + 0.5 * np.random.randn(n_samples)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTraining samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples: {X_test.shape[0]}\")\n",
        "\n",
        "# Train baseline model\n",
        "model_baseline = LinearRegression()\n",
        "model_baseline.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nBaseline Model Parameters (weights):\")\n",
        "print(f\"Coefficients: {model_baseline.coef_}\")\n",
        "print(f\"Intercept: {model_baseline.intercept_:.4f}\")\n",
        "\n",
        "# Evaluate\n",
        "y_pred_baseline = model_baseline.predict(X_test)\n",
        "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
        "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
        "\n",
        "print(f\"\\nBaseline Performance:\")\n",
        "print(f\"MSE: {mse_baseline:.4f}\")\n",
        "print(f\"RÂ²: {r2_baseline:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment: Modify parameters manually\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Experiment: Modifying Parameters Manually\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "baseline_params = np.concatenate([model_baseline.coef_, [model_baseline.intercept_]])\n",
        "\n",
        "# Create modified models with different parameter vectors\n",
        "params_scaled_05 = baseline_params * 0.5\n",
        "params_scaled_15 = baseline_params * 1.5\n",
        "params_noise = baseline_params + 0.1 * np.random.randn(len(baseline_params))\n",
        "\n",
        "def evaluate_params(X, y, params):\n",
        "    \"\"\"Evaluate model with given parameters\"\"\"\n",
        "    coef = params[:-1]\n",
        "    intercept = params[-1]\n",
        "    y_pred = X @ coef + intercept\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    return mse, r2\n",
        "\n",
        "print(\"\\nParameter Modifications:\")\n",
        "for name, params in [(\"Baseline\", baseline_params),\n",
        "                      (\"Scaled 0.5x\", params_scaled_05),\n",
        "                      (\"Scaled 1.5x\", params_scaled_15),\n",
        "                      (\"With noise\", params_noise)]:\n",
        "    mse, r2 = evaluate_params(X_test, y_test, params)\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Parameters: {params}\")\n",
        "    print(f\"  MSE: {mse:.4f}, RÂ²: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 2: Regularization Parameter Experiments\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 2: Regularization Parameter Experiments\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Experiment with different regularization strengths\n",
        "alpha_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "\n",
        "ridge_results = []\n",
        "for alpha in alpha_values:\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(X_train, y_train)\n",
        "    ridge_pred = ridge.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, ridge_pred)\n",
        "    r2 = r2_score(y_test, ridge_pred)\n",
        "    param_norm = np.linalg.norm(ridge.coef_)\n",
        "    ridge_results.append({'alpha': alpha, 'mse': mse, 'r2': r2, 'param_norm': param_norm})\n",
        "\n",
        "print(\"\\nRegularization Effects on Parameters:\")\n",
        "for r in ridge_results:\n",
        "    print(f\"  Alpha {r['alpha']:6.3f}: MSE={r['mse']:.4f}, RÂ²={r['r2']:.4f}, ||Î¸||={r['param_norm']:.4f}\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "alphas = [r['alpha'] for r in ridge_results]\n",
        "mses = [r['mse'] for r in ridge_results]\n",
        "plt.semilogx(alphas, mses, 'b-o', label='Ridge')\n",
        "plt.axhline(y=mse_baseline, color='g', linestyle='--', label='Baseline')\n",
        "plt.xlabel('Regularization Strength (Î±)')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('MSE vs Regularization Strength')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "norms = [r['param_norm'] for r in ridge_results]\n",
        "plt.semilogx(alphas, norms, 'r-s', label='Ridge')\n",
        "plt.xlabel('Regularization Strength (Î±)')\n",
        "plt.ylabel('Parameter Norm ||Î¸||')\n",
        "plt.title('Parameter Norm vs Regularization')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Regularization parameter effects visualized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Concepts:\n",
        "1. **Parameters as Vectors**: Model parameters can be viewed as vectors in parameter space\n",
        "2. **Parameter Space**: Different parameter values create different model fits\n",
        "3. **Regularization**: Constrains parameter values to prevent overfitting\n",
        "4. **Optimization**: Finding optimal parameters minimizes the loss\n",
        "\n",
        "### Best Practices:\n",
        "- Visualize parameter space to understand model behavior\n",
        "- Experiment with different parameter values to see their effects\n",
        "- Use regularization to constrain parameter space\n",
        "- Monitor both training and test performance\n",
        "\n",
        "### Applications:\n",
        "- Model optimization\n",
        "- Hyperparameter tuning\n",
        "- Understanding model behavior\n",
        "- Regularization selection\n",
        "\n",
        "**Reference:** Course 03, Unit 1: \"Linear Algebra for Machine Learning\" - Experimenting with ML model parameters practical content"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}