{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Function Approximation on Real-World ML Models\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Apply function approximation on real-world ML models\n",
    "- Understand power series and function approximations\n",
    "- Use Taylor series for function approximation\n",
    "- Apply approximations to optimize ML models\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of derivatives and calculus\n",
    "- âœ… Understanding of power series\n",
    "- âœ… Basic understanding of ML models\n",
    "- âœ… Python and NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 03, Unit 2**:\n",
    "- Applying function approximation on real-world ML models\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 2 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Function approximation** allows us to approximate complex functions using simpler forms (like power series), which is essential for optimization and understanding model behavior in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nApplying Function Approximation on Real-World ML Models\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Taylor Series Approximation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Part 1: Taylor Series Approximation\")\n",
    "print(\"=\" * 60)\n",
    "# Taylor series: f(x) â‰ˆ f(a) + f'(a)(x-a) + f''(a)(x-a)Â²/2! + ...\n",
    "def exp_taylor(x, a=0, n_terms=5):\n",
    "    \"\"\"Taylor series approximation of exp(x) around point a\"\"\"\n",
    "    result = 0\n",
    "    x_minus_a = x - a\n",
    "    for n in range(n_terms):\n",
    "        # f^(n)(a) = exp(a) for all n\n",
    "        result += (np.exp(a) * (x_minus_a)**n) / math.factorial(n)\n",
    "    return result\n",
    "\n",
    "def sin_taylor(x, a=0, n_terms=5):\n",
    "    \"\"\"Taylor series approximation of sin(x) around point a\"\"\"\n",
    "    result = 0\n",
    "    x_minus_a = x - a\n",
    "    for n in range(n_terms):\n",
    "        # Derivatives of sin cycle: sin, cos, -sin, -cos\n",
    "        derivatives = [np.sin(a), np.cos(a), -np.sin(a), -np.cos(a)]\n",
    "        result += (derivatives[n % 4] * (x_minus_a)**n) / math.factorial(n)\n",
    "    return result\n",
    "\n",
    "# Visualize approximations\n",
    "x = np.linspace(-2, 2, 100)\n",
    "plt.figure(figsize=(14, 5))\n",
    "# Exponential approximation\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, np.exp(x), 'b-', label='exp(x)', linewidth=2)\n",
    "for n in [1, 2, 3, 5, 10]:\n",
    "    y_approx = [exp_taylor(xi, a=0, n_terms=n) for xi in x]\n",
    "    plt.plot(x, y_approx, '--', label=f'Taylor (n={n})', alpha=0.7)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Taylor Series Approximation of exp(x)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-1, 8)\n",
    "# Sine approximation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, np.sin(x), 'r-', label='sin(x)', linewidth=2)\n",
    "for n in [1, 3, 5, 7, 9]:\n",
    "    y_approx = [sin_taylor(xi, a=0, n_terms=n) for xi in x]\n",
    "    plt.plot(x, y_approx, '--', label=f'Taylor (n={n})', alpha=0.7)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Taylor Series Approximation of sin(x)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nâœ… Taylor series approximations visualized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Function Approximation in ML Loss Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 2: Function Approximation in ML Loss Functions\")\n",
    "print(\"=\" * 60)\n",
    "# Example: Approximate loss function around optimal point\n",
    "# This helps understand optimization behavior\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 1)\n",
    "y = 2 * X.flatten() + 1 + 0.5 * np.random.randn(100)\n",
    "# Fit linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "w_opt = model.coef_[0]\n",
    "b_opt = model.intercept_\n",
    "print(f\"\\nOptimal parameters: w={w_opt:.4f}, b={b_opt:.4f}\")\n",
    "# Define loss function\n",
    "def loss_function(w, b):\n",
    "    y_pred = w * X.flatten() + b\n",
    "    return mean_squared_error(y, y_pred)\n",
    "# Create grid around optimal point\n",
    "w_range = np.linspace(w_opt - 1, w_opt + 1, 50)\n",
    "b_range = np.linspace(b_opt - 1, b_opt + 1, 50)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "# Compute loss surface\n",
    "loss_surface = np.zeros_like(W)\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        loss_surface[i, j] = loss_function(W[i, j], B[i, j])\n",
    "# Quadratic approximation (Taylor series up to 2nd order)\n",
    "# L(w, b) â‰ˆ L(w_opt, b_opt) + gradient^T * [w-w_opt, b-b_opt] + 0.5 * [w-w_opt, b-b_opt]^T * Hessian * [w-w_opt, b-b_opt]\n",
    "# Compute gradient numerically\n",
    "def compute_gradient(w, b, h=1e-5):\n",
    "    grad_w = (loss_function(w + h, b) - loss_function(w - h, b)) / (2 * h)\n",
    "    grad_b = (loss_function(w, b + h) - loss_function(w, b - h)) / (2 * h)\n",
    "    return grad_w, grad_b\n",
    "# Compute Hessian numerically\n",
    "def compute_hessian(w, b, h=1e-3):\n",
    "    gw_wplus, gb_wplus = compute_gradient(w + h, b, h)\n",
    "    gw_wminus, gb_wminus = compute_gradient(w - h, b, h)\n",
    "    grad_w_w = (gw_wplus - gw_wminus) / (2 * h)\n",
    "    grad_b_w = (gb_wplus - gb_wminus) / (2 * h)\n",
    "    gw_bplus, gb_bplus = compute_gradient(w, b + h, h)\n",
    "    gw_bminus, gb_bminus = compute_gradient(w, b - h, h)\n",
    "    grad_w_b = (gw_bplus - gw_bminus) / (2 * h)\n",
    "    grad_b_b = (gb_bplus - gb_bminus) / (2 * h)\n",
    "    return np.array([[grad_w_w, grad_w_b], [grad_b_w, grad_b_b]])\n",
    "grad_w, grad_b = compute_gradient(w_opt, b_opt)\n",
    "hessian = compute_hessian(w_opt, b_opt)\n",
    "loss_opt = loss_function(w_opt, b_opt)\n",
    "print(f\"\\nOptimal loss: {loss_opt:.4f}\")\n",
    "print(f\"Gradient at optimum: [{grad_w:.6f}, {grad_b:.6f}]\")\n",
    "print(f\"Hessian at optimum:\\n{hessian}\")\n",
    "# Quadratic approximation\n",
    "def quadratic_approximation(w, b):\n",
    "    dw = w - w_opt\n",
    "    db = b - b_opt\n",
    "    delta = np.array([dw, db])\n",
    "    return loss_opt + np.array([grad_w, grad_b]) @ delta + 0.5 * delta @ hessian @ delta\n",
    "# Visualize\n",
    "loss_approx = np.zeros_like(W)\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        loss_approx[i, j] = quadratic_approximation(W[i, j], B[i, j])\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "surf1 = ax1.plot_surface(W, B, loss_surface, cmap='viridis', alpha=0.8)\n",
    "ax1.scatter([w_opt], [b_opt], [loss_opt], c='r', s=100, marker='*')\n",
    "ax1.set_xlabel('w')\n",
    "ax1.set_ylabel('b')\n",
    "ax1.set_zlabel('Loss')\n",
    "ax1.set_title('True Loss Surface')\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "surf2 = ax2.plot_surface(W, B, loss_approx, cmap='plasma', alpha=0.8)\n",
    "ax2.scatter([w_opt], [b_opt], [loss_opt], c='r', s=100, marker='*')\n",
    "ax2.set_xlabel('w')\n",
    "ax2.set_ylabel('b')\n",
    "ax2.set_zlabel('Loss')\n",
    "ax2.set_title('Quadratic Approximation (Taylor)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nâœ… Function approximation applied to ML loss function!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Linear Approximation for Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Part 3: Linear Approximation for Optimization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Linear approximation is used in gradient descent\n",
    "# f(x) â‰ˆ f(x0) + f'(x0)(x - x0)\n",
    "# For optimization: x_new = x_old - learning_rate * f'(x_old)\n",
    "\n",
    "def gradient_descent_with_approximation(f, df, x0, learning_rate=0.1, n_iter=50):\n",
    "    \"\"\"Gradient descent using linear approximation\"\"\"\n",
    "    x = x0\n",
    "    history = [x]\n",
    "    values = [f(x)]\n",
    "    for i in range(n_iter):\n",
    "        x = x - learning_rate * df(x)\n",
    "        history.append(x)\n",
    "        values.append(f(x))\n",
    "    return x, history, values\n",
    "\n",
    "# Example: Minimize f(x) = (x - 3)^2\n",
    "def f_example(x):\n",
    "    return (x - 3)**2\n",
    "\n",
    "def df_example(x):\n",
    "    return 2 * (x - 3)\n",
    "\n",
    "x0 = 0.0\n",
    "x_opt, history, values = gradient_descent_with_approximation(f_example, df_example, x0, learning_rate=0.1, n_iter=20)\n",
    "\n",
    "print(f\"\\nStarting point: x0 = {x0}\")\n",
    "print(f\"Optimal point: x* = {x_opt:.4f} (true optimum: 3.0)\")\n",
    "print(f\"Final function value: f(x*) = {f_example(x_opt):.6f}\")\n",
    "\n",
    "# Visualize\n",
    "x_range = np.linspace(-1, 5, 100)\n",
    "y_range = [f_example(x) for x in x_range]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_range, y_range, 'b-', label='f(x) = (x-3)Â²', linewidth=2)\n",
    "plt.plot(history, values, 'ro-', label='Gradient Descent', markersize=8)\n",
    "plt.axvline(x=3, color='g', linestyle='--', label='True Optimum', alpha=0.7)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent Using Linear Approximation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show linear approximations at each step\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_range, y_range, 'b-', label='f(x)', linewidth=2, alpha=0.5)\n",
    "for i, x_i in enumerate(history[:5]):\n",
    " # Linear approximation: f(x) â‰ˆ f(x_i) + f'(x_i)(x - x_i)\n",
    " y_i = f_example(x_i)\n",
    " df_i = df_example(x_i)\n",
    " approx = [y_i + df_i * (x - x_i) for x in x_range]\n",
    " plt.plot(x_range, approx, '--', alpha=0.7, label=f'Approx at x={x_i:.2f}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Linear Approximations at Each Step')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-1, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Linear approximation used for gradient descent optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Taylor Series**: Approximate functions using power series\n",
    "2. **Linear Approximation**: f(x) â‰ˆ f(a) + f'(a)(x-a)\n",
    "3. **Quadratic Approximation**: Includes second-order terms (Hessian)\n",
    "4. **Optimization**: Use approximations to find optimal points\n",
    "5. **Gradient Descent**: Uses linear approximation to minimize functions\n",
    "\n",
    "### Best Practices:\n",
    "- Use Taylor series for local approximations\n",
    "- Verify approximations near the expansion point\n",
    "- Higher-order terms improve accuracy\n",
    "- Linear approximation sufficient for gradient descent\n",
    "\n",
    "### Applications:\n",
    "- Function optimization\n",
    "- Gradient descent algorithms\n",
    "- Understanding loss surfaces\n",
    "- Model behavior analysis\n",
    "\n",
    "**Reference:** Course 03, Unit 2: \"Calculus for Machine Learning\" - Function approximation practical content"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
