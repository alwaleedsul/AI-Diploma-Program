{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Bias Detection in Machine Learning Models | ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤ ŸÅŸä ŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Detect bias in datasets and model outputs\n",
    "- Use metrics and visualizations to assess fairness\n",
    "- Identify sources of discriminatory impact\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Basic Python\n",
    "- ‚úÖ Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 06, Unit 2** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Bias Detection in Machine Learning Models | ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤ ŸÅŸä ŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä\n",
    "\n",
    "## üö® THE PROBLEM: AI Systems Are Making Unfair Decisions | ÿßŸÑŸÖÿ¥ŸÉŸÑÿ©: ÿ£ŸÜÿ∏ŸÖÿ© ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿ™ÿ™ÿÆÿ∞ ŸÇÿ±ÿßÿ±ÿßÿ™ ÿ∫Ÿäÿ± ÿπÿßÿØŸÑÿ©\n",
    "\n",
    "**Remember the COMPAS case study from Unit 1?**\n",
    "\n",
    "We discovered that COMPAS showed:\n",
    "- Higher false positive rates for Black defendants\n",
    "- Higher false negative rates for White defendants\n",
    "- Racial bias in risk predictions\n",
    "\n",
    "**The Problem**: We understand ethics foundations, but **AI systems show bias. How do we detect it?**\n",
    "\n",
    "**We've learned:**\n",
    "- ‚úÖ How to identify ethical issues (frameworks from Unit 1)\n",
    "- ‚úÖ How bias manifests in real systems (COMPAS case study)\n",
    "- ‚úÖ That bias is a serious ethical problem\n",
    "\n",
    "**But we haven't learned:**\n",
    "- ‚ùå How to **detect** bias in AI systems\n",
    "- ‚ùå How to **measure** bias quantitatively\n",
    "- ‚ùå How to **identify** where bias exists\n",
    "\n",
    "**We need bias detection tools** to:\n",
    "1. Find where bias exists in our models\n",
    "2. Measure how unfair our systems are\n",
    "3. Identify which groups are being discriminated against\n",
    "\n",
    "**This notebook solves that problem** by teaching you bias detection metrics and tools!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites (What You Need First) | ÿßŸÑŸÖÿ™ÿ∑ŸÑÿ®ÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- ‚úÖ **Unit 1: Foundations of AI Ethics** - You need to understand ethical frameworks and case studies!\n",
    "- ‚úÖ **Basic Python knowledge**: Functions, dictionaries, data manipulation\n",
    "- ‚úÖ **Basic ML knowledge**: Classification, train/test split, confusion matrices\n",
    "- ‚úÖ **Understanding of bias**: What is algorithmic bias? (from Unit 1 case studies)\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why bias detection matters\n",
    "- Knowing which metrics to use for bias detection\n",
    "- Interpreting bias detection results\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Where This Notebook Fits | ŸÖŸÉÿßŸÜ Ÿáÿ∞ÿß ÿßŸÑÿØŸÅÿ™ÿ±\n",
    "\n",
    "**This is the FIRST example in Unit 2** - it addresses the problem from Unit 1!\n",
    "\n",
    "**Why this example FIRST?**\n",
    "- **Before** you can mitigate bias, you need to detect it (addresses the problem!)\n",
    "- **Before** you can ensure fairness, you need to measure it\n",
    "- **Before** you can fix problems, you need to identify them\n",
    "\n",
    "**Builds on**: \n",
    "- üìì Unit 1: Foundations (we identified bias as a problem - now we learn to detect it!)\n",
    "\n",
    "**Leads to**: \n",
    "- üìì Example 2: Bias Mitigation (once we detect bias, we learn to fix it!)\n",
    "- üìì Example 3: Fair Representation (ensuring fair representation in data)\n",
    "- üìì Example 4: Bias Case Studies (analyzing real bias cases)\n",
    "- üìì Example 5: Fair AI Development (building fair AI systems)\n",
    "\n",
    "**Why this order?**\n",
    "1. Detection **addresses the problem** from Unit 1 (we need to detect bias!)\n",
    "2. Detection provides **measurement tools** (needed before mitigation)\n",
    "3. Detection teaches **what bias looks like** (critical for understanding)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Finding the Problem Before Fixing It | ÿßŸÑŸÇÿµÿ©: ÿ•Ÿäÿ¨ÿßÿØ ÿßŸÑŸÖÿ¥ŸÉŸÑÿ© ŸÇÿ®ŸÑ ÿ•ÿµŸÑÿßÿ≠Ÿáÿß\n",
    "\n",
    "Imagine you're a doctor diagnosing a patient. **Before** you can treat an illness, you need to diagnose it - run tests, check symptoms, identify the problem. **After** diagnosis, you can prescribe the right treatment!\n",
    "\n",
    "Same with AI bias: **Before** we can fix bias, we need to detect it - measure fairness metrics, check for disparities, identify where bias exists. **After** detection, we can apply the right mitigation strategies!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Bias Detection Matters | ŸÑŸÖÿßÿ∞ÿß ŸäŸáŸÖ ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤ÿü\n",
    "\n",
    "Bias detection is essential for ethical AI:\n",
    "- **Identify Problems**: Find where bias exists in your models\n",
    "- **Measure Fairness**: Quantify how fair (or unfair) your system is\n",
    "- **Track Progress**: Monitor if bias mitigation efforts are working\n",
    "- **Ensure Compliance**: Meet fairness requirements and regulations\n",
    "- **Build Trust**: Demonstrate commitment to fairness\n",
    "\n",
    "## Learning Objectives | ÿ£ŸáÿØÿßŸÅ ÿßŸÑÿ™ÿπŸÑŸÖ\n",
    "1. Understand different types of bias in ML models\n",
    "2. Learn fairness metrics (demographic parity, equalized odds)\n",
    "3. Detect bias using statistical measures\n",
    "4. Visualize bias in model predictions\n",
    "5. Interpret bias detection results\n",
    "6. Understand when to use different fairness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Inputs & üì§ Outputs | ÿßŸÑŸÖÿØÿÆŸÑÿßÿ™ ŸàÿßŸÑŸÖÿÆÿ±ÿ¨ÿßÿ™\n",
    "\n",
    "**Inputs:** What we use in this notebook\n",
    "\n",
    "- Libraries and concepts as introduced in this notebook; see prerequisites and code comments.\n",
    "\n",
    "**Outputs:** What you'll see when you run the cells\n",
    "\n",
    "- Printed results, figures, and summaries as shown when you run the cells.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:58:40.012599Z",
     "iopub.status.busy": "2025-12-26T16:58:40.012260Z",
     "iopub.status.idle": "2025-12-26T16:58:41.045360Z",
     "shell.execute_reply": "2025-12-26T16:58:41.045113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Libraries imported successfully!\n",
      "\n",
      "üìö What each library does:\n",
      "   - matplotlib/seaborn: Create visualizations (bias charts, heatmaps)\n",
      "   - numpy: Numerical operations (arrays, calculations)\n",
      "   - pandas: Data manipulation (DataFrames, analysis)\n",
      "   - sklearn: Machine learning (models, metrics, data splitting)\n",
      "   - os: File operations (saving images)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us detect bias in machine learning models\n",
    "\n",
    "import matplotlib.pyplot as plt  # For creating visualizations: Charts, graphs, bias visualizations\n",
    "import numpy as np  # For numerical operations: Arrays, calculations, random number generation\n",
    "import pandas as pd  # For data manipulation: DataFrames, data analysis\n",
    "from sklearn.model_selection import train_test_split  # For splitting data: Separate training and testing sets\n",
    "from sklearn.ensemble import RandomForestClassifier  # For ML model: Classification algorithm\n",
    "from sklearn.metrics import confusion_matrix, classification_report  # For model evaluation: Performance metrics\n",
    "import seaborn as sns  # For statistical visualizations: Heatmaps, advanced plots\n",
    "import os  # For file operations: Saving images\n",
    "\n",
    "# Configure matplotlib settings: Set default figure size and font size for better visualizations\n",
    "plt.rcParams['font.size'] = 10  # Font size: Make text readable (10pt is good for most displays)\n",
    "plt.rcParams['figure.figsize'] = (14, 8)  # Figure size: 14 inches wide, 8 inches tall (good for detailed charts)\n",
    "\n",
    "print(\" Libraries imported successfully!\")\n",
    "print(\"\\nüìö What each library does:\")\n",
    "print(\"   - matplotlib/seaborn: Create visualizations (bias charts, heatmaps)\")\n",
    "print(\"   - numpy: Numerical operations (arrays, calculations)\")\n",
    "print(\"   - pandas: Data manipulation (DataFrames, analysis)\")\n",
    "print(\"   - sklearn: Machine learning (models, metrics, data splitting)\")\n",
    "print(\"   - os: File operations (saving images)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:58:41.057626Z",
     "iopub.status.busy": "2025-12-26T16:58:41.057514Z",
     "iopub.status.idle": "2025-12-26T16:58:41.062903Z",
     "shell.execute_reply": "2025-12-26T16:58:41.062730Z"
    }
   },
   "source": [
    "Step 2: Generate synthetic data with intentional bias\n",
    "This creates a dataset where we know bias exists, so we can practice detecting it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Detecting Bias with Fairness Metrics | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ´ÿßŸÑÿ´: ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÖŸÇÿßŸäŸäÿ≥ ÿßŸÑÿπÿØÿßŸÑÿ©\n",
    "\n",
    "### üìö Prerequisites (What You Need First)\n",
    "-  **Biased data generated** (from Part 2) - Understanding the dataset with known bias\n",
    "-  **Understanding of fairness** - Knowing what fairness means\n",
    "\n",
    "### üîó Relationship: What This Builds On\n",
    "This is where we actually detect the bias we created!\n",
    "- Builds on: Biased dataset, understanding of fairness metrics\n",
    "- Shows: How to measure and detect bias\n",
    "\n",
    "### üìñ The Story\n",
    "**Before detection**: We have biased data but don't know how to measure it.\n",
    "**After detection**: We can quantify bias using fairness metrics and see exactly where it exists!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Calculate Fairness Metrics | ÿßŸÑÿÆÿ∑Ÿàÿ© 3: ÿ≠ÿ≥ÿßÿ® ŸÖŸÇÿßŸäŸäÿ≥ ÿßŸÑÿπÿØÿßŸÑÿ©\n",
    "\n",
    "**BEFORE**: We have data but don't know how to measure bias.\n",
    "\n",
    "**AFTER**: We'll calculate fairness metrics (demographic parity, equalized odds) to detect bias!\n",
    "\n",
    "**Why fairness metrics?** They provide:\n",
    "- Quantitative measures of bias\n",
    "- Standard ways to compare groups\n",
    "- Clear thresholds for what's \"fair\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:58:41.063799Z",
     "iopub.status.busy": "2025-12-26T16:58:41.063741Z",
     "iopub.status.idle": "2025-12-26T16:58:41.065528Z",
     "shell.execute_reply": "2025-12-26T16:58:41.065350Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_analyze_bias(df):\n",
    "    \"\"\"Train a model and analyze for bias using fairness metrics.\"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualizing Bias | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿÆÿßŸÖÿ≥: ÿ™ÿµŸàÿ± ÿßŸÑÿ™ÿ≠Ÿäÿ≤\n",
    "\n",
    "### üìö Prerequisites (What You Need First)\n",
    "- ‚úÖ **Bias detection results** (from Part 4) - Having fairness metrics calculated\n",
    "- ‚úÖ **Visualization libraries** (from Part 1) - Understanding matplotlib/seaborn\n",
    "\n",
    "### üîó Relationship: What This Builds On\n",
    "This visualizes the bias we detected!\n",
    "- Builds on: Bias detection results, visualization skills\n",
    "- Shows: Visual representation of bias metrics\n",
    "\n",
    "### üìñ The Story\n",
    "**Before visualization**: We have numbers but can't easily see the bias.\n",
    "**After visualization**: We can see bias clearly in charts and graphs!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Visualize Bias Detection Results | ÿßŸÑÿÆÿ∑Ÿàÿ© 5: ÿ™ÿµŸàÿ± ŸÜÿ™ÿßÿ¶ÿ¨ ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤\n",
    "\n",
    "**BEFORE**: We have bias metrics but no visual representation.\n",
    "\n",
    "**AFTER**: We'll create charts showing demographic parity, equalized odds, and confusion matrices!\n",
    "\n",
    "**Why visualize?** Visual representation helps us:\n",
    "- See bias patterns clearly\n",
    "- Compare groups easily\n",
    "- Communicate findings to others\n",
    "- Identify which groups are most affected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:58:41.066466Z",
     "iopub.status.busy": "2025-12-26T16:58:41.066408Z",
     "iopub.status.idle": "2025-12-26T16:58:41.653587Z",
     "shell.execute_reply": "2025-12-26T16:58:41.653364Z"
    }
   },
   "source": [
    "Step 5: Create visualizations of bias detection results\n",
    "This helps us see bias patterns clearly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary: What We Learned | ÿßŸÑŸÖŸÑÿÆÿµ: ŸÖÿß ÿ™ÿπŸÑŸÖŸÜÿßŸá\n",
    "\n",
    "**BEFORE this notebook**: We knew bias exists but didn't know how to detect it in our models.\n",
    "\n",
    "**AFTER this notebook**: We can:\n",
    "- ‚úÖ Generate synthetic data with known bias for practice\n",
    "- ‚úÖ Calculate fairness metrics (demographic parity, equalized odds)\n",
    "- ‚úÖ Train ML models and detect bias in their predictions\n",
    "- ‚úÖ Visualize bias using charts and confusion matrices\n",
    "- ‚úÖ Interpret bias detection results\n",
    "- ‚úÖ Understand when to use different fairness metrics\n",
    "\n",
    "### Key Takeaways | ÿßŸÑÿßÿ≥ÿ™ŸÜÿ™ÿßÿ¨ÿßÿ™ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ©\n",
    "\n",
    "1. **Multiple Metrics Matter**: Different fairness metrics reveal different types of bias\n",
    "2. **Demographic Parity vs. Equalized Odds**: They measure different aspects of fairness\n",
    "3. **Bias Detection is Essential**: Must test for bias before and after model deployment\n",
    "4. **Visualization Helps**: Charts make bias patterns easier to understand\n",
    "5. **Systematic Approach**: Following a structured process ensures comprehensive bias detection\n",
    "\n",
    "---\n",
    "\n",
    "## üö´ When Bias Detection Hits a Limitation | ÿπŸÜÿØŸÖÿß ŸäÿµŸÑ ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤ ÿ•ŸÑŸâ ÿ≠ÿØ\n",
    "\n",
    "### The Limitation We Discovered\n",
    "\n",
    "We've learned how to detect bias in AI systems using metrics like demographic parity and equalized odds. **But there's a critical problem:**\n",
    "\n",
    "**We detected bias, but how do we fix it?**\n",
    "\n",
    "Bias detection tells us:\n",
    "- ‚úÖ Where bias exists\n",
    "- ‚úÖ How severe the bias is\n",
    "- ‚úÖ Which groups are affected\n",
    "\n",
    "**But it doesn't tell us:**\n",
    "- ‚ùå How to **remove** the bias\n",
    "- ‚ùå How to **mitigate** unfairness\n",
    "- ‚ùå How to **fix** the model\n",
    "\n",
    "### Why This Is a Problem\n",
    "\n",
    "When we detect bias but can't fix it:\n",
    "- We know there's a problem but can't solve it\n",
    "- Unfair systems continue to operate\n",
    "- Affected groups continue to be disadvantaged\n",
    "- Detection alone isn't enough\n",
    "\n",
    "### The Solution: Bias Mitigation\n",
    "\n",
    "We need **bias mitigation techniques** to:\n",
    "1. Remove bias from models\n",
    "2. Make systems fairer across groups\n",
    "3. Fix the problems we detected\n",
    "\n",
    "**This is exactly what we'll learn in the next notebook: Bias Mitigation!**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Steps | ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿ™ÿßŸÑŸäÿ©\n",
    "\n",
    "**You've completed this notebook!** Now you understand:\n",
    "- ‚úÖ How to detect bias using fairness metrics\n",
    "- ‚úÖ How to visualize bias patterns\n",
    "- ‚úÖ **The limitation**: Detection isn't enough - we need to fix it!\n",
    "\n",
    "**Next notebook**: `02_bias_mitigation.ipynb`\n",
    "- Learn how to mitigate bias using pre-processing, in-processing, and post-processing\n",
    "- Fix the bias problems you detected\n",
    "- Make models fairer across groups\n",
    "\n",
    "**Congratulations!** üéâ You've learned how to detect bias in machine learning models systematically!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Bias in Data | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ´ÿßŸÜŸä: ŸÅŸáŸÖ ÿßŸÑÿ™ÿ≠Ÿäÿ≤ ŸÅŸä ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\n",
    "\n",
    "### üìö Prerequisites (What You Need First)\n",
    "-  **Library imports** (from Part 1) - Understanding data manipulation and ML tools\n",
    "-  **Understanding of bias** (from Unit 1) - Knowing what bias is\n",
    "\n",
    "### üîó Relationship: What This Builds On\n",
    "This creates data with intentional bias so we can practice detecting it!\n",
    "- Builds on: Data manipulation skills, understanding of bias\n",
    "- Shows: How bias manifests in data\n",
    "\n",
    "### üìñ The Story\n",
    "**Before biased data**: We need data with known bias to practice detection.\n",
    "**After biased data**: We have a dataset where we know bias exists, so we can test our detection methods!\n",
    "## Part 4: Training Model and Detecting Bias | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ±ÿßÿ®ÿπ: ÿ™ÿØÿ±Ÿäÿ® ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸàÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤\n",
    "\n",
    "### üìö Prerequisites (What You Need First)\n",
    "-  **Fairness metrics** (from Part 3) - Understanding how to calculate bias\n",
    "-  **Biased data** (from Part 2) - Having data to analyze\n",
    "\n",
    "### üîó Relationship: What This Builds On\n",
    "This trains a model and uses our fairness metrics to detect bias!\n",
    "- Builds on: Fairness metric functions, biased dataset\n",
    "- Shows: How to detect bias in a trained model\n",
    "\n",
    "### üìñ The Story\n",
    "**Before training**: We have data and metrics but no model to analyze.\n",
    "**After training**: We have a trained model and can detect bias in its predictions!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Train Model and Detect Bias | ÿßŸÑÿÆÿ∑Ÿàÿ© 4: ÿ™ÿØÿ±Ÿäÿ® ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸàÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑÿ™ÿ≠Ÿäÿ≤\n",
    "\n",
    "**BEFORE**: We have data and fairness metrics but haven't trained a model yet.\n",
    "\n",
    "**AFTER**: We'll train a model and use our fairness metrics to detect bias in its predictions!\n",
    "\n",
    "**Why train a model?** Real-world bias detection happens on trained models, not just data!\n",
    "\n",
    "---\n",
    "\n",
    "# Step 4: Train a machine learning model and detect bias in its predictions\n",
    "# This shows how bias manifests in model predictions\n",
    "\n",
    "# BEFORE: We have data and metrics but no model predictions\n",
    "# AFTER: We'll have a trained model and bias detection results\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ü§ñ TRAINING MODEL AND DETECTING BIAS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWe'll:\")\n",
    "print(\"  1. Train a Random Forest classifier on our biased data\")\n",
    "print(\"  2. Make predictions on test data\")\n",
    "print(\"  3. Calculate fairness metrics on predictions\")\n",
    "print(\"  4. Detect bias in the model's behavior\\n\")\n",
    "\n",
    "def train_and_analyze_bias(df):\n",
    "    \"\"\"\n",
    "    Train a model and analyze for bias using fairness metrics.\n",
    "    \n",
    "    HOW IT WORKS:\n",
    "    1. Prepare features and target variable\n",
    "    2. Split data into training and testing sets\n",
    "    3. Train Random Forest classifier\n",
    "    4. Make predictions on test set\n",
    "    5. Calculate demographic parity and equalized odds\n",
    "    6. Return results for visualization\n",
    "    \n",
    "    ‚è∞ WHEN to use: After having data and fairness metric functions - detect bias in model\n",
    "    üí° WHY use: Shows how bias in data translates to bias in model predictions\n",
    "    \"\"\"\n",
    "    # Prepare features: Select columns to use for prediction\n",
    "    X = df[['age', 'experience_years', 'education_level', 'skill_score']]  # Features: Input variables for the model\n",
    "    y = df['hired']  # Target: What we want to predict (hired or not)\n",
    "    \n",
    "    # Split data: Separate into training and testing sets\n",
    "    # Why split? Training set teaches the model, test set evaluates it\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y  # Split: 70% train, 30% test, stratified to maintain class balance\n",
    "    )\n",
    "    \n",
    "    # Train model: Create and train Random Forest classifier\n",
    "    # Why Random Forest? Good for classification, handles non-linear relationships\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)  # Model: 100 trees, fixed random seed\n",
    "    model.fit(X_train, y_train)  # Train: Learn patterns from training data\n",
    "    print(\" Model trained successfully!\")  # Success message: Confirm training complete\n",
    "    \n",
    "    # Make predictions: Use model to predict on test data\n",
    "    y_pred = model.predict(X_test)  # Predictions: Model's predictions for test set\n",
    "    \n",
    "    # Add predictions to test data: Combine predictions with test data for analysis\n",
    "    test_df = X_test.copy()  # Copy: Create copy of test features\n",
    "    test_df['hired'] = y_test.values  # Actual: Add actual outcomes\n",
    "    test_df['predicted'] = y_pred  # Predicted: Add model predictions\n",
    "    test_df['group'] = df.loc[X_test.index, 'group'].values  # Group: Add group labels for fairness analysis\n",
    "    \n",
    "    # Calculate bias metrics: Use our fairness functions to detect bias\n",
    "    parity_rates, parity_disparity = calculate_demographic_parity(test_df)  # Demographic parity: Overall prediction balance\n",
    "    equalized_odds, tpr_disparity, fpr_disparity = calculate_equalized_odds(test_df)  # Equalized odds: Accuracy balance\n",
    "    \n",
    "    return test_df, model, parity_rates, parity_disparity, equalized_odds, tpr_disparity, fpr_disparity  # Return: All results for analysis\n",
    "\n",
    "# Train model and detect bias\n",
    "print(\"Training model and analyzing for bias...\")\n",
    "test_df, model, parity_rates, parity_disparity, equalized_odds, tpr_disparity, fpr_disparity = train_and_analyze_bias(df)\n",
    "\n",
    "# Print initial results\n",
    "print(\"\\nüìä BIAS DETECTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. Demographic Parity\")\n",
    "print(\"-\" * 60)\n",
    "for group, rate in parity_rates.items():\n",
    "    print(f\"  {group}: {rate:.3f} ({rate*100:.1f}%)\")\n",
    "print(f\"\\n  Disparity: {parity_disparity:.3f}\")\n",
    "if parity_disparity > 0.1:\n",
    "    print(\"  ‚ö†Ô∏è  HIGH DISPARITY - Potential bias detected!\")\n",
    "else:\n",
    "    print(\"   Low disparity - Fair from demographic parity perspective\")\n",
    "\n",
    "print(\"\\n2. Equalized Odds\")\n",
    "print(\"-\" * 60)\n",
    "for group, metrics in equalized_odds.items():\n",
    "    print(f\"  {group}:\")\n",
    "    print(f\"    TPR: {metrics['TPR']:.3f}\")\n",
    "    print(f\"    FPR: {metrics['FPR']:.3f}\")\n",
    "print(f\"\\n  TPR Disparity: {tpr_disparity:.3f}\")\n",
    "print(f\"  FPR Disparity: {fpr_disparity:.3f}\")\n",
    "if tpr_disparity > 0.1 or fpr_disparity > 0.1:\n",
    "    print(\"  ‚ö†Ô∏è  HIGH DISPARITY - Bias in equalized odds!\")\n",
    "else:\n",
    "    print(\"   Low disparity - Fair from equalized odds perspective\")\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "def visualize_demographic_parity(parity_rates, disparity):\n",
    "    \"\"\"Visualize demographic parity\"\"\"\n",
    "    groups = list(parity_rates.keys())\n",
    "    rates = list(parity_rates.values())\n",
    "    colors = ['#3498db', '#e74c3c']\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax.bar(groups, rates, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    # Add value labels\n",
    "    for bar, rate in zip(bars, rates):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{rate:.3f}\\n({rate*100:.1f}%)',\n",
    "               ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    # Add disparity line\n",
    "    ax.axhline(y=max(rates), color='red', linestyle='--', alpha=0.5, label='Max')\n",
    "    ax.axhline(y=min(rates), color='blue', linestyle='--', alpha=0.5, label='Min')\n",
    "    ax.set_ylabel('Positive Prediction Rate', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Demographic Parity Analysis\\n'\n",
    "                f''\n",
    "                f'Disparity: {disparity:.3f}',\n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_ylim(0, max(rates) * 1.2)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('demographic_parity.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(\" Saved: demographic_parity.png\")\n",
    "    plt.close()\n",
    "def visualize_equalized_odds(equalized_odds, tpr_disparity, fpr_disparity):\n",
    "    \"\"\"Visualize equalized odds metrics\"\"\"\n",
    "    groups = list(equalized_odds.keys())\n",
    "    tprs = [equalized_odds[g]['TPR'] for g in groups]\n",
    "    fprs = [equalized_odds[g]['FPR'] for g in groups]\n",
    "    x = np.arange(len(groups))\n",
    "    width = 0.35\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars1 = ax.bar(x - width/2, tprs, width, label='True Positive Rate (TPR)',\n",
    "                   color='#2ecc71', alpha=0.8, edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, fprs, width, label='False Positive Rate (FPR)',\n",
    "                   color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    ax.set_xlabel('Group', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Rate', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Equalized Odds Analysis\\n'\n",
    "                f''\n",
    "                f'TPR Disparity: {tpr_disparity:.3f} | FPR Disparity: {fpr_disparity:.3f}',\n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(groups)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_ylim(0, max(max(tprs), max(fprs)) * 1.2)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('equalized_odds.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(\" Saved: equalized_odds.png\")\n",
    "    plt.close()\n",
    "def visualize_confusion_matrices(test_df):\n",
    "    \"\"\"Visualize confusion matrices by group\"\"\"\n",
    "    groups = test_df['group'].unique()\n",
    "    fig, axes = plt.subplots(1, len(groups), figsize=(14, 5))\n",
    "    for idx, group in enumerate(groups):\n",
    "        group_data = test_df[test_df['group'] == group]\n",
    "        cm = confusion_matrix(group_data['hired'], group_data['predicted'])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                   cbar_kws={'label': 'Count'})\n",
    "        axes[idx].set_title(f'{group}\\nConfusion Matrix',\n",
    "                           fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Predicted', fontsize=10)\n",
    "        axes[idx].set_ylabel('Actual', fontsize=10)\n",
    "        axes[idx].set_xticklabels(['Not Hired', 'Hired'])\n",
    "        axes[idx].set_yticklabels(['Not Hired', 'Hired'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrices_by_group.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(\" Saved: confusion_matrices_by_group.png\")\n",
    "    plt.close()\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"Unit 2 - Example 1: Bias Detection in ML Models\")\n",
    "    print(\"\")\n",
    "    print(\"=\"*80)\n",
    "    # Generate data\n",
    "    print(\"\\nüìä Generating synthetic data with bias...\")\n",
    "    print(\"\")\n",
    "    df = generate_biased_data(n_samples=2000)\n",
    "    # Show data summary\n",
    "    print(\"\\nüìã Data Summary\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Groups: {df[\"group'].value_counts().to_dict()}')\n",
    "    print(f\"\\nHiring rates by group:\")\n",
    "    for group in df['group'].unique():\n",
    "        rate = df[df['group'] == group]['hired'].mean()\n",
    "        print(f\"  {group}: {rate:.3f} ({rate*100:.1f}%)\")\n",
    "    # Train model and analyze\n",
    "    print(\"\\nüîç Training model and analyzing bias...\")\n",
    "    print(\"\")\n",
    "    test_df, model, parity_rates, parity_disparity, equalized_odds, tpr_disparity, fpr_disparity = train_and_analyze_bias(df)\n",
    "    # Print results\n",
    "    print(\"\\nüìä BIAS DETECTION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n1. Demographic Parity\")\n",
    "    print(\"-\" * 60)\n",
    "    for group, rate in parity_rates.items():\n",
    "        print(f\"  {group}: {rate:.3f} ({rate*100:.1f}%)\")\n",
    "    print(f\"\\n  Disparity\")\n",
    "    if parity_disparity > 0.1:\n",
    "        print(\"  ‚ö†Ô∏è  HIGH DISPARITY - Potential bias detected!\")\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"   Low disparity - Fair from demographic parity perspective\")\n",
    "        print(\"\")\n",
    "    print(\"\\n2. Equalized Odds\")\n",
    "    print(\"-\" * 60)\n",
    "    for group, metrics in equalized_odds.items():\n",
    "        print(f\"  {group}:\")\n",
    "        print(f\"    TPR: {metrics[\"TPR']:.3f}')\n",
    "        print(f\"    FPR: {metrics[\"FPR']:.3f}')\n",
    "    print(f\"\\n  TPR Disparity\")\n",
    "    print(f\"FPR Disparity\")\n",
    "    if tpr_disparity > 0.1 or fpr_disparity > 0.1:\n",
    "        print(\"  ‚ö†Ô∏è  HIGH DISPARITY - Bias in equalized odds!\")\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"   Low disparity - Fair from equalized odds perspective\")\n",
    "        print(\"\")\n",
    "    # Create visualizations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Creating Visualizations\")\n",
    "    print(\"=\"*80)\n",
    "    visualize_demographic_parity(parity_rates, parity_disparity)\n",
    "    visualize_equalized_odds(equalized_odds, tpr_disparity, fpr_disparity)\n",
    "    visualize_confusion_matrices(test_df)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" Example completed successfully!\")\n",
    "    print(\"\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nKey Takeaways\")\n",
    "    print(\"1. Multiple fairness metrics can reveal different types of bias\")\n",
    "    print(\"\")\n",
    "    print(\"2. Demographic parity and equalized odds measure different aspects\")\n",
    "    print(\"\")\n",
    "    print(\"3. It\"s important to test for bias before and after model deployment\")\n",
    "    print(\"\")\n",
    "    print(\"=\"*80 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}