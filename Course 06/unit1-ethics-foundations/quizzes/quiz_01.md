# Quiz 1: Foundations of AI Ethics
## اختبار 1: أسس أخلاقيات الذكاء الاصطناعي

**Time Limit:** 30 minutes | **Marks:** 30 points

---

## Part 1: Multiple Choice (10 points)

### Question 1 (2 points)
What is utilitarianism in ethical frameworks?
- A) Focus on individual rights and duties
- B) Maximize overall happiness/utility for the greatest number
- C) Focus on character and virtues
- D) Follow rules regardless of consequences

---

### Question 2 (2 points)
What is deontology?
- A) Focus on consequences of actions
- B) Focus on duties and rules that should be followed
- C) Focus on character traits
- D) Focus on maximizing utility

---

### Question 3 (2 points)
What was the main ethical issue with the COMPAS system?
- A) It was too slow
- B) It showed racial bias in risk assessment
- C) It was too expensive
- D) It was not accurate

---

### Question 4 (2 points)
What is virtue ethics?
- A) Focus on rules and duties
- B) Focus on consequences
- C) Focus on character and moral virtues
- D) Focus on individual rights

---

### Question 5 (2 points)
Why is ethical decision-making important in AI?
- A) It makes models faster
- B) It ensures AI systems are fair, just, and respect human values
- C) It reduces costs
- D) It improves accuracy

---

## Part 2: Short Answer (10 points)

### Question 6 (5 points)
Explain the three main ethical frameworks and how they apply to AI.

**Three Main Frameworks:**

1. **Utilitarianism**: 
   - Maximize overall happiness/utility
   - In AI: Design systems that benefit the most people
   - Example: Autonomous vehicles minimizing total harm

2. **Deontology**:
   - Follow moral rules and duties regardless of consequences
   - In AI: Respect privacy, autonomy, human dignity
   - Example: Never use AI for surveillance without consent

3. **Virtue Ethics**:
   - Focus on character and moral virtues
   - In AI: Develop systems that embody virtues like fairness, honesty
   - Example: Transparent AI systems that build trust

---

### Question 7 (5 points)
Describe a real-world case study of ethical issues in AI (e.g., COMPAS). What were the problems and lessons learned?

**COMPAS Case Study:**

**Problem:**
- COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) used AI to predict recidivism risk
- Showed racial bias: Black defendants were more likely to be flagged as high-risk
- Used in sentencing decisions, affecting people's lives

**Lessons Learned:**
1. **Bias in Training Data**: Historical data reflects societal biases
2. **Transparency**: Need explainable AI for critical decisions
3. **Fairness Metrics**: Must evaluate models for fairness, not just accuracy
4. **Human Oversight**: Critical decisions need human judgment
5. **Regular Auditing**: Continuously monitor for bias and discrimination

---

## Part 3: Case Analysis (10 points)

### Question 8 (5 points)
You are designing an AI hiring system. What ethical considerations should you address?

**Key Ethical Considerations:**

1. **Bias and Fairness**:
   - Ensure no discrimination based on protected characteristics
   - Test for demographic parity and equalized odds
   - Use diverse training data

2. **Transparency**:
   - Explainable decisions (why was candidate rejected?)
   - Clear criteria and processes
   - Allow candidates to understand decisions

3. **Privacy**:
   - Protect candidate data
   - Limit data collection to what's necessary
   - Secure data storage

4. **Accountability**:
   - Human oversight for final decisions
   - Clear responsibility for system outcomes
   - Appeal process for candidates

5. **Validity**:
   - Ensure system actually predicts job performance
   - Regular validation and updates
   - Avoid proxy discrimination

---

### Question 9 (5 points)
What are the key principles that should guide ethical AI development?

**Key Principles:**

1. **Fairness**: Treat all individuals and groups equitably
2. **Transparency**: Systems should be understandable and explainable
3. **Accountability**: Clear responsibility for AI decisions and outcomes
4. **Privacy**: Protect individual data and privacy rights
5. **Safety**: Ensure AI systems are safe and reliable
6. **Human Autonomy**: Preserve human decision-making and control
7. **Beneficence**: AI should benefit humanity and society
8. **Non-maleficence**: Do no harm, minimize risks
9. **Justice**: Distribute benefits and burdens fairly
10. **Respect for Human Dignity**: Maintain human worth and rights

---

## Answer Key

**Part 1:**
1. B) Maximize overall happiness/utility for the greatest number
2. B) Focus on duties and rules that should be followed
3. B) It showed racial bias in risk assessment
4. C) Focus on character and moral virtues
5. B) It ensures AI systems are fair, just, and respect human values

**Part 2:**
6. All three frameworks explained with AI applications - 5 points
7. Case study with problems and lessons - 5 points

**Part 3:**
8. Multiple ethical considerations addressed - 5 points
9. Key principles clearly explained - 5 points

**Total: 30 points**

---

## Grading Rubric

- **90-100% (27-30 points):** Excellent understanding
- **80-89% (24-26 points):** Good understanding
- **70-79% (21-23 points):** Satisfactory
- **60-69% (18-20 points):** Needs improvement
- **Below 60% (<18 points):** Requires additional study

