{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05. Human-in-the-Loop (HITL) Approaches | ŸÜŸáÿ¨ ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜ ŸÅŸä ÿßŸÑÿ≠ŸÑŸÇÿ©\n",
        "\n",
        "## üìö Learning Objectives\n",
        "\n",
        "By completing this notebook, you will:\n",
        "- Understand the key concepts of this topic\n",
        "- Apply the topic using Python code examples\n",
        "- Practice with small, realistic datasets or scenarios\n",
        "\n",
        "## üîó Prerequisites\n",
        "\n",
        "- ‚úÖ Basic Python\n",
        "- ‚úÖ Basic NumPy/Pandas (when applicable)\n",
        "\n",
        "---\n",
        "\n",
        "## Official Structure Reference\n",
        "\n",
        "This notebook supports **Course 06, Unit 4** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05. Human-in-the-Loop (HITL) Approaches | ŸÜŸáÿ¨ ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜ ŸÅŸä ÿßŸÑÿ≠ŸÑŸÇÿ©\n",
        "\n",
        "## üö® THE PROBLEM: We Need Human Oversight | ÿßŸÑŸÖÿ¥ŸÉŸÑÿ©: ŸÜÿ≠ÿ™ÿßÿ¨ ÿßŸÑÿ•ÿ¥ÿ±ÿßŸÅ ÿßŸÑÿ®ÿ¥ÿ±Ÿä\n",
        "\n",
        "**Remember the limitation from the previous notebook?**\n",
        "\n",
        "We learned accountability frameworks for defining responsibilities. But we discovered:\n",
        "\n",
        "**How do we incorporate human judgment into AI decision-making?**\n",
        "\n",
        "**The Problem**: Ethical AI systems also need:\n",
        "- ‚ùå **Human oversight** (human judgment for critical decisions)\n",
        "- ‚ùå **Human-in-the-loop** (HITL) approaches\n",
        "- ‚ùå **Human review** for uncertain cases\n",
        "- ‚ùå **Human validation** of AI decisions\n",
        "\n",
        "**We've learned:**\n",
        "- ‚úÖ How to use SHAP, LIME, and counterfactuals (Notebooks 1-3)\n",
        "- ‚úÖ How to establish accountability frameworks (Notebook 4)\n",
        "- ‚úÖ Explanation methods and accountability\n",
        "\n",
        "**But we haven't learned:**\n",
        "- ‚ùå How to **incorporate human judgment** into AI decisions\n",
        "- ‚ùå How to implement **human-in-the-loop** approaches\n",
        "- ‚ùå How to enable **human review** for uncertain cases\n",
        "- ‚ùå How to **combine AI with human judgment**\n",
        "\n",
        "**We need human-in-the-loop approaches** to:\n",
        "1. Incorporate human judgment into AI decisions\n",
        "2. Enable human review for uncertain cases\n",
        "3. Provide human oversight for critical decisions\n",
        "4. Combine AI efficiency with human judgment\n",
        "\n",
        "**This notebook solves that problem** by teaching you human-in-the-loop approaches for ethical AI!\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Prerequisites (What You Need First) | ÿßŸÑŸÖÿ™ÿ∑ŸÑÿ®ÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©\n",
        "\n",
        "**BEFORE starting this notebook**, you should have completed:\n",
        "- ‚úÖ **Example 1: SHAP Explanations** - Understanding explainability\n",
        "- ‚úÖ **Example 2: LIME Explanations** - Understanding local explanations\n",
        "- ‚úÖ **Example 3: Counterfactual Analysis** - Understanding \"what if\" scenarios\n",
        "- ‚úÖ **Example 4: Accountability Frameworks** - Understanding accountability\n",
        "- ‚úÖ **Basic Python knowledge**: Functions, data manipulation\n",
        "\n",
        "**If you haven't completed these**, you might struggle with:\n",
        "- Understanding why human oversight matters\n",
        "- Knowing how to implement HITL approaches\n",
        "- Understanding human-AI collaboration\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Where This Notebook Fits | ŸÖŸÉÿßŸÜ Ÿáÿ∞ÿß ÿßŸÑÿØŸÅÿ™ÿ±\n",
        "\n",
        "**This is the FIFTH example in Unit 4** - it teaches you human oversight!\n",
        "\n",
        "**Why this example FIFTH?**\n",
        "- **Before** you can implement HITL, you need explainability (Examples 1-3)\n",
        "- **Before** you can implement HITL, you need accountability (Example 4)\n",
        "- **Before** you can build transparent systems, you need human oversight\n",
        "\n",
        "**Builds on**: \n",
        "- üìì Example 1: SHAP Explanations (explainability)\n",
        "- üìì Example 2: LIME Explanations (local explanations)\n",
        "- üìì Example 3: Counterfactual Analysis (\"what if\" scenarios)\n",
        "- üìì Example 4: Accountability Frameworks (accountability)\n",
        "\n",
        "**Leads to**: \n",
        "- üìì Example 6: Transparency Tools (transparency frameworks)\n",
        "\n",
        "**Why this order?**\n",
        "1. HITL provides **human oversight** (needed for ethical AI)\n",
        "2. HITL teaches **human-AI collaboration** (critical for trust)\n",
        "3. HITL shows **uncertainty handling** (essential for transparency)\n",
        "\n",
        "---\n",
        "\n",
        "## The Story: Combining AI with Human Judgment | ÿßŸÑŸÇÿµÿ©: ÿßŸÑÿ¨ŸÖÿπ ÿ®ŸäŸÜ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ŸàÿßŸÑÿ≠ŸÉŸÖ ÿßŸÑÿ®ÿ¥ÿ±Ÿä\n",
        "\n",
        "Imagine you're a doctor using AI to diagnose patients. **Before** HITL, AI would make all decisions automatically (risky!). **After** implementing HITL, uncertain cases get human review - combining AI efficiency with human judgment!\n",
        "\n",
        "Same with AI: **Before** we have automated decisions but no human oversight, now we learn HITL - incorporate human judgment for uncertain cases! **After** HITL, we have AI systems with human oversight!\n",
        "\n",
        "---\n",
        "\n",
        "## Why HITL Approaches Matter | ŸÑŸÖÿßÿ∞ÿß ÿ™ŸáŸÖ ŸÜŸáÿ¨ ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜ ŸÅŸä ÿßŸÑÿ≠ŸÑŸÇÿ©ÿü\n",
        "\n",
        "Human-in-the-loop approaches are essential for ethical AI:\n",
        "- **Human Judgment**: Incorporate human expertise into AI decisions\n",
        "- **Oversight**: Provide human oversight for critical decisions\n",
        "- **Trust**: Build user confidence through human involvement\n",
        "- **Safety**: Reduce risks through human validation\n",
        "- **Ethics**: Ensure ethical decisions through human judgment\n",
        "\n",
        "## Learning Objectives | ÿ£ŸáÿØÿßŸÅ ÿßŸÑÿ™ÿπŸÑŸÖ\n",
        "1. Understand human-in-the-loop approaches\n",
        "2. Learn how to identify uncertain cases\n",
        "3. Implement human review mechanisms\n",
        "4. Combine AI predictions with human judgment\n",
        "5. Evaluate HITL effectiveness\n",
        "6. Design HITL workflows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-26T16:59:22.813090Z",
          "iopub.status.busy": "2025-12-26T16:59:22.812888Z",
          "iopub.status.idle": "2025-12-26T16:59:24.239268Z",
          "shell.execute_reply": "2025-12-26T16:59:24.239050Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Unit 4 - Example 5: Human-in-the-Loop Approaches\n",
            "================================================================================\n",
            "\n",
            "Automated Model:\n",
            "  Accuracy: 0.9267\n",
            "  Demographic Parity Difference: 0.0887\n",
            "\n",
            "HITL Model:\n",
            "  Accuracy: 0.9333\n",
            "  Demographic Parity Difference: 0.0868\n",
            "  Human Reviews: 10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Saved: hitl_comparison.png\n",
            "\n",
            "‚úÖ Example completed!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Unit 4: Interpretability, Transparency, and Accountability\n",
        "Example 5: Human-in-the-Loop (HITL) Approaches\n",
        "This example demonstrates human-in-the-loop approaches for AI fairness evaluation.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from fairlearn.metrics import demographic_parity_difference\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "sns.set_style(\"whitegrid\")\n",
        "def generate_dataset(n_samples=1000):\n",
        "    np.random.seed(42)\n",
        "    sensitive = np.random.choice([0, 1], n_samples, p=[0.6, 0.4])\n",
        "    X1 = np.random.normal(0, 1, n_samples)\n",
        "    X2 = np.random.normal(0, 1, n_samples)\n",
        "    y = (0.4 * X1 + 0.3 * X2 + np.random.normal(0, 0.1, n_samples) > 0).astype(int)\n",
        "    return pd.DataFrame({'feature1': X1, 'feature2': X2, 'sensitive': sensitive, 'target': y})\n",
        "def hitl_fairness_evaluation(model, X_test, y_test, sensitive_test, uncertainty_threshold=0.1):\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "    uncertainty = np.abs(y_pred_proba - 0.5)\n",
        "    uncertain_mask = uncertainty < uncertainty_threshold\n",
        "    human_reviewed = y_pred.copy()\n",
        "    if uncertain_mask.sum() > 0:\n",
        "        human_predictions = y_test[uncertain_mask].copy()\n",
        "        human_reviewed[uncertain_mask] = human_predictions\n",
        "    return {\n",
        "        'automated': y_pred,\n",
        "        'hitl': human_reviewed,\n",
        "        'uncertain_count': uncertain_mask.sum(),\n",
        "        'uncertain_mask': uncertain_mask\n",
        "    }\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*80)\n",
        "    print(\"Unit 4 - Example 5: Human-in-the-Loop Approaches\")\n",
        "    print(\"=\"*80)\n",
        "    df = generate_dataset()\n",
        "    X = df[['feature1', 'feature2']].values\n",
        "    y = df['target'].values\n",
        "    sensitive = df['sensitive'].values\n",
        "    X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(\n",
        "        X, y, sensitive, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    results = hitl_fairness_evaluation(model, X_test_scaled, y_test, sensitive_test)\n",
        "    auto_acc = accuracy_score(y_test, results['automated'])\n",
        "    hitl_acc = accuracy_score(y_test, results['hitl'])\n",
        "    auto_dp = abs(demographic_parity_difference(y_test, results['automated'], sensitive_features=sensitive_test))\n",
        "    hitl_dp = abs(demographic_parity_difference(y_test, results['hitl'], sensitive_features=sensitive_test))\n",
        "    print(f\"\\nAutomated Model:\")\n",
        "    print(f\"  Accuracy: {auto_acc:.4f}\")\n",
        "    print(f\"  Demographic Parity Difference: {auto_dp:.4f}\")\n",
        "    print(f\"\\nHITL Model:\")\n",
        "    print(f\"  Accuracy: {hitl_acc:.4f}\")\n",
        "    print(f\"  Demographic Parity Difference: {hitl_dp:.4f}\")\n",
        "    print(f\"  Human Reviews: {results['uncertain_count']}\")\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    axes[0].bar(['Automated', 'HITL'], [auto_acc, hitl_acc], color=['#e74c3c', '#2ecc71'])\n",
        "    axes[0].set_title('Accuracy Comparison', fontweight='bold')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[1].bar(['Automated', 'HITL'], [auto_dp, hitl_dp], color=['#e74c3c', '#2ecc71'])\n",
        "    axes[1].set_title('Fairness Comparison', fontweight='bold')\n",
        "    axes[1].set_ylabel('Demographic Parity Difference')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('unit4-transparency-accountability', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\n‚úÖ Saved: hitl_comparison.png\")\n",
        "    plt.close()\n",
        "    print(\"\\n‚úÖ Example completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üö´ When HITL Approaches Hit a Limitation | ÿπŸÜÿØŸÖÿß ÿ™ÿµŸÑ ŸÜŸáÿ¨ ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜ ŸÅŸä ÿßŸÑÿ≠ŸÑŸÇÿ© ÿ•ŸÑŸâ ÿ≠ÿØ\n",
        "\n",
        "### The Limitation We Discovered\n",
        "\n",
        "We've learned human-in-the-loop approaches for incorporating human judgment. **But there's still a challenge:**\n",
        "\n",
        "**How do we integrate all transparency tools into a comprehensive framework?**\n",
        "\n",
        "HITL approaches work well when:\n",
        "- ‚úÖ We can identify uncertain cases\n",
        "- ‚úÖ We can incorporate human judgment\n",
        "- ‚úÖ We have human oversight mechanisms\n",
        "\n",
        "**But comprehensive transparency also needs:**\n",
        "- ‚ùå **Integrated transparency tools** (combine all explanation methods)\n",
        "- ‚ùå **Transparency frameworks** (systematic approach to transparency)\n",
        "- ‚ùå **Tool comparison** (choose the right tool for each situation)\n",
        "- ‚ùå **Complete transparency solutions** (end-to-end transparency)\n",
        "\n",
        "### Why This Is a Problem\n",
        "\n",
        "When we have individual tools but no framework:\n",
        "- We may not know which tool to use when\n",
        "- We may not integrate tools effectively\n",
        "- We may not have a systematic transparency approach\n",
        "- We may miss important transparency aspects\n",
        "\n",
        "### The Solution: Transparency Tools and Frameworks\n",
        "\n",
        "We need **transparency tools and frameworks** to:\n",
        "1. Compare and integrate different explanation methods\n",
        "2. Create comprehensive transparency frameworks\n",
        "3. Choose the right tool for each situation\n",
        "4. Build complete transparency solutions\n",
        "\n",
        "**This is exactly what we'll learn in the next notebook: Transparency Tools!**\n",
        "\n",
        "---\n",
        "\n",
        "## ‚û°Ô∏è Next Steps | ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿ™ÿßŸÑŸäÿ©\n",
        "\n",
        "**You've completed this notebook!** Now you understand:\n",
        "- ‚úÖ How to use SHAP, LIME, and counterfactuals (Notebooks 1-3)\n",
        "- ‚úÖ How to establish accountability (Notebook 4)\n",
        "- ‚úÖ How to implement HITL (This notebook!)\n",
        "- ‚úÖ **The limitation**: We need comprehensive transparency frameworks!\n",
        "\n",
        "**Next notebook**: `06_transparency_tools.ipynb`\n",
        "- Learn about transparency tools comparison\n",
        "- Understand transparency frameworks\n",
        "- Integrate all explanation methods\n",
        "- Build comprehensive transparency solutions\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "course2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}