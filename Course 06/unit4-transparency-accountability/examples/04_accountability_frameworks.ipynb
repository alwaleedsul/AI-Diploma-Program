{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Accountability Frameworks | ÿ£ÿ∑ÿ± ÿßŸÑŸÖÿ≥ÿßÿ°ŸÑÿ©\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Basic Python\n",
    "- ‚úÖ Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 06, Unit 4** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Accountability Frameworks | ÿ£ÿ∑ÿ± ÿßŸÑŸÖÿ≥ÿßÿ°ŸÑÿ©\n",
    "\n",
    "## üö® THE PROBLEM: We Need Accountability for AI Decisions | ÿßŸÑŸÖÿ¥ŸÉŸÑÿ©: ŸÜÿ≠ÿ™ÿßÿ¨ ÿßŸÑŸÖÿ≥ÿßÿ°ŸÑÿ© ÿπŸÜ ŸÇÿ±ÿßÿ±ÿßÿ™ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä\n",
    "\n",
    "**Remember the limitation from the previous notebook?**\n",
    "\n",
    "We learned counterfactual analysis for \"what if\" explanations. But we discovered:\n",
    "\n",
    "**How do we ensure accountability and responsibility for AI decisions?**\n",
    "\n",
    "**The Problem**: Transparent AI systems also need:\n",
    "- ‚ùå **Accountability frameworks** (who is responsible?)\n",
    "- ‚ùå **Responsibility mechanisms** (how to assign responsibility?)\n",
    "- ‚ùå **Audit trails** (how to track decisions?)\n",
    "- ‚ùå **Stakeholder accountability** (who answers for outcomes?)\n",
    "\n",
    "**We've learned:**\n",
    "- ‚úÖ How to use SHAP for explanations (Notebook 1)\n",
    "- ‚úÖ How to use LIME for fast explanations (Notebook 2)\n",
    "- ‚úÖ How to use counterfactuals for \"what if\" scenarios (Notebook 3)\n",
    "- ‚úÖ Multiple explanation methods\n",
    "\n",
    "**But we haven't learned:**\n",
    "- ‚ùå How to **define stakeholder responsibilities**\n",
    "- ‚ùå How to **create audit trails**\n",
    "- ‚ùå How to **establish responsibility mechanisms**\n",
    "- ‚ùå How to **ensure accountability** for AI decisions\n",
    "\n",
    "**We need accountability frameworks** to:\n",
    "1. Define stakeholder responsibilities\n",
    "2. Create audit trails\n",
    "3. Establish responsibility mechanisms\n",
    "4. Enable accountability for AI decisions\n",
    "\n",
    "**This notebook solves that problem** by teaching you accountability frameworks for AI systems!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Prerequisites (What You Need First) | ÿßŸÑŸÖÿ™ÿ∑ŸÑÿ®ÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- ‚úÖ **Example 1: SHAP Explanations** - Understanding explainability\n",
    "- ‚úÖ **Example 2: LIME Explanations** - Understanding local explanations\n",
    "- ‚úÖ **Example 3: Counterfactual Analysis** - Understanding \"what if\" scenarios\n",
    "- ‚úÖ **Basic Python knowledge**: Functions, data manipulation\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why accountability matters\n",
    "- Knowing how to structure accountability frameworks\n",
    "- Understanding stakeholder responsibilities\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Where This Notebook Fits | ŸÖŸÉÿßŸÜ Ÿáÿ∞ÿß ÿßŸÑÿØŸÅÿ™ÿ±\n",
    "\n",
    "**This is the FOURTH example in Unit 4** - it teaches you accountability!\n",
    "\n",
    "**Why this example FOURTH?**\n",
    "- **Before** you can ensure accountability, you need explainability (Examples 1-3)\n",
    "- **Before** you can implement HITL, you need accountability structures\n",
    "- **Before** you can build transparent systems, you need accountability\n",
    "\n",
    "**Builds on**: \n",
    "- üìì Example 1: SHAP Explanations (explainability)\n",
    "- üìì Example 2: LIME Explanations (local explanations)\n",
    "- üìì Example 3: Counterfactual Analysis (\"what if\" scenarios)\n",
    "\n",
    "**Leads to**: \n",
    "- üìì Example 5: Human-in-the-Loop (HITL approaches)\n",
    "- üìì Example 6: Transparency Tools (transparency frameworks)\n",
    "\n",
    "**Why this order?**\n",
    "1. Accountability provides **responsibility structures** (needed for ethical AI)\n",
    "2. Accountability teaches **stakeholder roles** (critical for governance)\n",
    "3. Accountability shows **audit mechanisms** (essential for transparency)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Who Is Responsible? | ÿßŸÑŸÇÿµÿ©: ŸÖŸÜ ÿßŸÑŸÖÿ≥ÿ§ŸàŸÑÿü\n",
    "\n",
    "Imagine you're using an AI system that makes a wrong decision. **Before** accountability frameworks, you wouldn't know who to hold responsible (developers? data scientists? users?). **After** implementing accountability frameworks, you have clear responsibilities, audit trails, and accountability mechanisms!\n",
    "\n",
    "Same with AI: **Before** we have explanations but no accountability, now we learn accountability frameworks - define responsibilities, create audit trails, establish accountability! **After** accountability frameworks, we have responsible and accountable AI systems!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Accountability Frameworks Matter | ŸÑŸÖÿßÿ∞ÿß ÿ™ŸáŸÖ ÿ£ÿ∑ÿ± ÿßŸÑŸÖÿ≥ÿßÿ°ŸÑÿ©ÿü\n",
    "\n",
    "Accountability frameworks are essential for ethical AI:\n",
    "- **Responsibility**: Define who is responsible for AI decisions\n",
    "- **Transparency**: Enable tracking and auditing of decisions\n",
    "- **Trust**: Build user confidence through accountability\n",
    "- **Compliance**: Meet regulatory requirements for accountability\n",
    "- **Ethics**: Ensure responsible AI development and deployment\n",
    "\n",
    "## Learning Objectives | ÿ£ŸáÿØÿßŸÅ ÿßŸÑÿ™ÿπŸÑŸÖ\n",
    "1. Understand accountability frameworks\n",
    "2. Learn stakeholder responsibilities\n",
    "3. Create audit trails\n",
    "4. Establish responsibility mechanisms\n",
    "5. Implement model cards and data sheets\n",
    "6. Build accountability structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:59:18.942183Z",
     "iopub.status.busy": "2025-12-26T16:59:18.941984Z",
     "iopub.status.idle": "2025-12-26T16:59:21.368421Z",
     "shell.execute_reply": "2025-12-26T16:59:21.368248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Unit 4 - Example 4: Accountability Frameworks\n",
      "================================================================================\n",
      "\n",
      "1. Stakeholder Responsibilities:\n",
      "\n",
      "Developers:\n",
      "  Accountability Level: 9\n",
      "  Responsibilities:\n",
      "    - Design fair and transparent algorithms\n",
      "    - Document model decisions and limitations\n",
      "    - Implement bias detection and mitigation\n",
      "    - Create model cards and documentation\n",
      "\n",
      "Data Scientists:\n",
      "  Accountability Level: 8\n",
      "  Responsibilities:\n",
      "    - Ensure data quality and representativeness\n",
      "    - Document data sources and preprocessing\n",
      "    - Identify potential biases in data\n",
      "    - Maintain data lineage\n",
      "\n",
      "Product Managers:\n",
      "  Accountability Level: 7\n",
      "  Responsibilities:\n",
      "    - Define ethical requirements\n",
      "    - Oversee deployment and monitoring\n",
      "    - Ensure compliance with regulations\n",
      "    - Manage stakeholder communication\n",
      "\n",
      "Legal:\n",
      "  Accountability Level: 9\n",
      "  Responsibilities:\n",
      "    - Ensure regulatory compliance\n",
      "    - Review model for legal risks\n",
      "    - Handle liability issues\n",
      "    - Manage data privacy requirements\n",
      "\n",
      "End Users:\n",
      "  Accountability Level: 5\n",
      "  Responsibilities:\n",
      "    - Use AI system responsibly\n",
      "    - Report issues and biases\n",
      "    - Provide feedback\n",
      "    - Understand system limitations\n",
      "\n",
      "2. Model Card:\n",
      "  Model: Loan Approval Classifier\n",
      "  Created: 2025-12-26\n",
      "  Performance: {'accuracy': 0.87, 'fairness_score': 0.92}\n",
      "\n",
      "3. Audit Trail:\n",
      "  Total audit entries: 7\n",
      "  Date range: 2025-11-26 19:59:20.218923 to 2025-12-08 19:59:20.218923\n",
      "\n",
      "4. Accountability Checklist:\n",
      "\n",
      "Pre-deployment:\n",
      "  [ ] Model documentation complete\n",
      "  [ ] Bias assessment performed\n",
      "  [ ] Fairness metrics calculated\n",
      "  [ ] Stakeholder review completed\n",
      "  [ ] Legal compliance verified\n",
      "\n",
      "Deployment:\n",
      "  [ ] Monitoring systems in place\n",
      "  [ ] Audit trail enabled\n",
      "  [ ] User notifications configured\n",
      "  [ ] Rollback plan prepared\n",
      "\n",
      "Post-deployment:\n",
      "  [ ] Regular performance monitoring\n",
      "  [ ] Fairness metrics tracking\n",
      "  [ ] User feedback collection\n",
      "  [ ] Periodic model audits\n",
      "  [ ] Incident response plan\n",
      "\n",
      "================================================================================\n",
      "Creating Visualizations...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: stakeholder_accountability.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: audit_timeline.png\n",
      "‚úÖ Saved: accountability_checklist.png\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. Clear stakeholder responsibilities ensure accountability\n",
      "2. Model cards document model characteristics and limitations\n",
      "3. Audit trails track all system decisions and changes\n",
      "4. Accountability checklists ensure comprehensive coverage\n",
      "5. Accountability frameworks are essential for trustworthy AI\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unit 4: Interpretability, Transparency, and Accountability\n",
    "Example 4: Accountability Frameworks\n",
    "This example demonstrates accountability frameworks for AI systems:\n",
    "- Key stakeholders and responsibilities\n",
    "- Mechanisms for tracking and auditing\n",
    "- Model cards and data sheets\n",
    "- Audit trails\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "# ============================================================================\n",
    "# STAKEHOLDER RESPONSIBILITIES\n",
    "# ============================================================================\n",
    "def define_stakeholder_responsibilities():\n",
    "    \"\"\"\n",
    "    Define key stakeholders and their responsibilities in AI accountability\n",
    "    \"\"\"\n",
    "    stakeholders = {\n",
    "        'Developers': {\n",
    "            'responsibilities': [\n",
    "                'Design fair and transparent algorithms', 'Document model decisions and limitations',\n",
    "                'Implement bias detection and mitigation',\n",
    "                'Create model cards and documentation'\n",
    "            ],\n",
    "            'accountability_level': 9\n",
    "        },\n",
    "        'Data Scientists': {\n",
    "            'responsibilities': [\n",
    "                'Ensure data quality and representativeness',\n",
    "                'Document data sources and preprocessing',\n",
    "                'Identify potential biases in data',\n",
    "                'Maintain data lineage'\n",
    "            ],\n",
    "            'accountability_level': 8\n",
    "        },\n",
    "        'Product Managers': {\n",
    "            'responsibilities': [\n",
    "                'Define ethical requirements',\n",
    "                'Oversee deployment and monitoring',\n",
    "                'Ensure compliance with regulations',\n",
    "                'Manage stakeholder communication'\n",
    "            ],\n",
    "            'accountability_level': 7\n",
    "        },\n",
    "        'Legal': {\n",
    "            'responsibilities': [\n",
    "                'Ensure regulatory compliance',\n",
    "                'Review model for legal risks',\n",
    "                'Handle liability issues',\n",
    "                'Manage data privacy requirements'\n",
    "            ],\n",
    "            'accountability_level': 9\n",
    "        },\n",
    "        'End Users': {\n",
    "            'responsibilities': [\n",
    "                'Use AI system responsibly',\n",
    "                'Report issues and biases',\n",
    "                'Provide feedback',\n",
    "                'Understand system limitations'\n",
    "            ],\n",
    "            'accountability_level': 5\n",
    "        }\n",
    "    }\n",
    "    return stakeholders\n",
    "# ============================================================================\n",
    "# MODEL CARD TEMPLATE\n",
    "# ============================================================================\n",
    "def create_model_card(model_name, model_type, performance_metrics, training_data_info, \n",
    "                     limitations, use_cases):\n",
    "    \"\"\"\n",
    "    Create a model card documenting key information about an AI model\n",
    "    \"\"\"\n",
    "    model\n",
    "card = {\n",
    "        'model_name': model_name,\n",
    "        'model_type': model_type,\n",
    "        'date_created': datetime.now().strftime('%Y-%m-%d'), 'performance_metrics': performance_metrics,\n",
    "        'training_data': training_data_info,\n",
    "        'limitations': limitations,\n",
    "        'intended_use_cases': use_cases,\n",
    "        'ethical_considerations': {\n",
    "            'bias_mitigation': 'Applied reweighing and fairness constraints',\n",
    "            'transparency': 'SHAP and LIME explanations available',\n",
    "            'accountability': 'Full audit trail maintained'\n",
    "        }\n",
    "    }\n",
    "    return model_card\n",
    "# ============================================================================\n",
    "# AUDIT TRAIL\n",
    "# ============================================================================\n",
    "def create_audit_trail():\n",
    "    \"\"\"\n",
    "    Create an audit trail for AI system decisions\n",
    "    \"\"\"\n",
    "    audit\n",
    "entries = []\n",
    "    # Simulate audit trail entries\n",
    "    base\n",
    "time = datetime.now() - timedelta(days=30)\n",
    "    events = [\n",
    "        {'timestamp': base_time, 'event': 'Model trained', 'user': 'Data Scientist', 'details': 'Initial model training'},\n",
    "        {'timestamp': base_time + timedelta(days=1), 'event': 'Bias check performed', 'user': 'Developer', 'details': 'Demographic parity: 0.05'},\n",
    "        {'timestamp': base_time + timedelta(days=2), 'event': 'Model deployed', 'user': 'Product Manager', 'details': 'Production deployment'},\n",
    "        {'timestamp': base_time + timedelta(days=5), 'event': 'Performance monitoring', 'user': 'System', 'details': 'Accuracy: 0.87'},\n",
    "        {'timestamp': base_time + timedelta(days=10), 'event': 'Bias detected', 'user': 'Monitoring System', 'details': 'Fairness metric degraded'},\n",
    "        {'timestamp': base_time + timedelta(days=11), 'event': 'Model retrained', 'user': 'Data Scientist', 'details': 'Retraining with fairness constraints'},\n",
    "        {'timestamp': base_time + timedelta(days=12), 'event': 'Model updated', 'user': 'Product Manager', 'details': 'New version deployed'},\n",
    "    ]\n",
    "    for event in events:\n",
    "        audit_entries.append({\n",
    "            'timestamp': event['timestamp'], 'event_type': event['event'],\n",
    "            'user': event['user'],\n",
    "            'details': event['details']\n",
    "        })\n",
    "    return pd.DataFrame(audit_entries)\n",
    "# ============================================================================\n",
    "# ACCOUNTABILITY FRAMEWORK\n",
    "# ============================================================================\n",
    "def accountability_framework_checklist():\n",
    "    \"\"\"\n",
    "    Create accountability framework checklist\n",
    "    \"\"\"\n",
    "    checklist = {\n",
    "        'Pre-deployment': [\n",
    "            'Model documentation complete', 'Bias assessment performed',\n",
    "            'Fairness metrics calculated',\n",
    "            'Stakeholder review completed',\n",
    "            'Legal compliance verified'\n",
    "        ],\n",
    "        'Deployment': [\n",
    "            'Monitoring systems in place',\n",
    "            'Audit trail enabled',\n",
    "            'User notifications configured',\n",
    "            'Rollback plan prepared'\n",
    "        ],\n",
    "        'Post-deployment': [\n",
    "            'Regular performance monitoring',\n",
    "            'Fairness metrics tracking',\n",
    "            'User feedback collection',\n",
    "            'Periodic model audits',\n",
    "            'Incident response plan'\n",
    "        ]\n",
    "    }\n",
    "    return checklist\n",
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "def plot_stakeholder_responsibilities(stakeholders):\n",
    "    \"\"\"\n",
    "    Plot stakeholder responsibility matrix\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    names = list(stakeholders.keys())\n",
    "    accountability = [stakeholders[name]['accountability_level'] for name in names]\n",
    "    num\n",
    "responsibilities = [len(stakeholders[name]['responsibilities']) for name in names]\n",
    "    scatter = ax.scatter(num_responsibilities, accountability, s=200, alpha=0.6, c=accountability, \n",
    "                        cmap='RdYlGn', edgecolors='black', linewidth=2)\n",
    "    for i, name in enumerate(names):\n",
    "        ax.annotate(name, (num_responsibilities[i], accountability[i]), \n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('Number of Responsibilities', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Accountability Level (1-10)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Stakeholder Accountability Matrix', fontsize=12, fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax, label='Accountability Level')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unit4-transparency-accountability', dpi=300, bbox\n",
    "inches ='tight')\n",
    "    print(\"‚úÖ Saved: stakeholder_accountability.png\")\n",
    "    plt.close()\n",
    "def plot_audit_timeline(audit_df):\n",
    "    \"\"\"\n",
    "    Plot audit trail timeline\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    # Convert timestamps to days since first event\n",
    "    first\n",
    "time = audit\n",
    "df['timestamp'].min()\n",
    "    audit_df['days_since_start'] = (audit_df['timestamp'] - first_time).dt.days\n",
    "    # Plot events\n",
    "    event\n",
    "types = audit\n",
    "df['event_type'].unique()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(event_types)))\n",
    "    color\n",
    "map = dict(zip(event_types, colors))\n",
    "    for idx, row in audit_df.iterrows():\n",
    "        ax.scatter(row['days_since_start'], idx, s=200, \n",
    "                 c= color\n",
    "map[row['event_type']], alpha=0.7, edgecolors='black')\n",
    "        ax.text(row['days_since_start'], idx, f\"  {row['event_type']}\", \n",
    "               va='center', fontsize=9)\n",
    "    ax.set_xlabel('Days Since First Event', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Event Index', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('AI System Audit Trail Timeline', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend\n",
    "elements = [Patch(facecolor= color\n",
    "map[et], label=et) for et in event_types]\n",
    "    ax.legend(handles= legend\n",
    "elements, loc='upper left', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unit4-transparency-accountability', dpi=300, bbox\n",
    "inches ='tight')\n",
    "    print(\"‚úÖ Saved: audit_timeline.png\")\n",
    "    plt.close()\n",
    "def plot_accountability_checklist(checklist):\n",
    "    \"\"\"\n",
    "    Plot accountability checklist status\n",
    "    \"\"\"\n",
    "    phases = list(checklist.keys())\n",
    "    items_per\n",
    "phase = [len(items) for items in checklist.values()]\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars = ax.bar(phases, items_per_phase, color=['#3498db', '#2ecc71', '#f39c12'], alpha=0.8)\n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, items_per_phase):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{int(count)} items', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Checklist Items', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Accountability Framework Checklist by Phase', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unit4-transparency-accountability', dpi=300, bbox\n",
    "inches ='tight')\n",
    "    print(\"‚úÖ Saved: accountability_checklist.png\")\n",
    "    plt.close()\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if_\n",
    "name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"Unit 4 - Example 4: Accountability Frameworks\")\n",
    "    print(\"=\"*80)\n",
    "    # Define stakeholders\n",
    "    print(\"\\n1. Stakeholder Responsibilities:\")\n",
    "    stakeholders = define\n",
    "stakeholder\n",
    "responsibilities()\n",
    "    for name, info in stakeholders.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Accountability Level: {info['accountability_level']}\")\n",
    "        print(f\"  Responsibilities:\")\n",
    "        for resp in info['responsibilities']:\n",
    "            print(f\"    - {resp}\")\n",
    "    # Create model card\n",
    "    print(\"\\n2. Model Card:\")\n",
    "    model\n",
    "card = create\n",
    "model\n",
    "card(\n",
    "        model\n",
    "name ='Loan Approval Classifier', model\n",
    "type ='Random Forest',\n",
    "        performance\n",
    "metrics ={'accuracy': 0.87, 'fairness_score': 0.92},\n",
    "        training_data\n",
    "info ={'samples': 10000, 'features': 10, 'date_range': '2023-01 to 2023-12'},\n",
    "        limitations=['May have bias for certain demographic groups', 'Requires periodic retraining'],\n",
    "        use\n",
    "cases =['Loan approval decisions', 'Credit risk assessment']\n",
    "    )\n",
    "    print(f\"  Model: {model_card['model_name']}\")\n",
    "    print(f\"  Created: {model_card['date_created']}\")\n",
    "    print(f\"  Performance: {model_card['performance_metrics']}\")\n",
    "    # Create audit trail\n",
    "    print(\"\\n3. Audit Trail:\")\n",
    "    audit\n",
    "df = create\n",
    "audit\n",
    "trail()\n",
    "    print(f\"  Total audit entries: {len(audit_df)}\")\n",
    "    print(f\"  Date range: {audit_df['timestamp'].min()} to {audit_df['timestamp'].max()}\")\n",
    "    # Accountability checklist\n",
    "    print(\"\\n4. Accountability Checklist:\")\n",
    "    checklist = accountability\n",
    "framework\n",
    "checklist()\n",
    "    for phase, items in checklist.items():\n",
    "        print(f\"\\n{phase}:\")\n",
    "        for item in items:\n",
    "            print(f\"  [ ] {item}\")\n",
    "    # Create visualizations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Creating Visualizations...\")\n",
    "    print(\"=\"*80)\n",
    "    plot_stakeholder_responsibilities(stakeholders)\n",
    "    plot_audit_timeline(audit_df)\n",
    "    plot_accountability_checklist(checklist)\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nKey Takeaways:\")\n",
    "    print(\"1. Clear stakeholder responsibilities ensure accountability\")\n",
    "    print(\"2. Model cards document model characteristics and limitations\")\n",
    "    print(\"3. Audit trails track all system decisions and changes\")\n",
    "    print(\"4. Accountability checklists ensure comprehensive coverage\")\n",
    "    print(\"5. Accountability frameworks are essential for trustworthy AI\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üö´ When Accountability Frameworks Hit a Limitation | ÿπŸÜÿØŸÖÿß ÿ™ÿµŸÑ ÿ£ÿ∑ÿ± ÿßŸÑŸÖÿ≥ÿßÿ°ŸÑÿ© ÿ•ŸÑŸâ ÿ≠ÿØ\n",
    "\n",
    "### The Limitation We Discovered\n",
    "\n",
    "We've learned accountability frameworks for defining responsibilities. **But there's still a challenge:**\n",
    "\n",
    "**How do we incorporate human judgment into AI decision-making?**\n",
    "\n",
    "Accountability frameworks work well when:\n",
    "- ‚úÖ We have clear responsibilities defined\n",
    "- ‚úÖ We have audit trails in place\n",
    "- ‚úÖ We have accountability mechanisms\n",
    "\n",
    "**But ethical AI systems also need:**\n",
    "- ‚ùå **Human oversight** (human judgment for critical decisions)\n",
    "- ‚ùå **Human-in-the-loop** (HITL) approaches\n",
    "- ‚ùå **Human review** for uncertain cases\n",
    "- ‚ùå **Human validation** of AI decisions\n",
    "\n",
    "### Why This Is a Problem\n",
    "\n",
    "When we have accountability but no human oversight:\n",
    "- Critical decisions may be made without human judgment\n",
    "- Uncertain cases may not get human review\n",
    "- AI decisions may lack human validation\n",
    "- We may miss important context that humans understand\n",
    "\n",
    "### The Solution: Human-in-the-Loop (HITL) Approaches\n",
    "\n",
    "We need **human-in-the-loop approaches** to:\n",
    "1. Incorporate human judgment into AI decisions\n",
    "2. Enable human review for uncertain cases\n",
    "3. Provide human oversight for critical decisions\n",
    "4. Combine AI efficiency with human judgment\n",
    "\n",
    "**This is exactly what we'll learn in the next notebook: Human-in-the-Loop Approaches!**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Steps | ÿßŸÑÿÆÿ∑Ÿàÿßÿ™ ÿßŸÑÿ™ÿßŸÑŸäÿ©\n",
    "\n",
    "**You've completed this notebook!** Now you understand:\n",
    "- ‚úÖ How to use SHAP, LIME, and counterfactuals (Notebooks 1-3)\n",
    "- ‚úÖ How to establish accountability frameworks (This notebook!)\n",
    "- ‚úÖ **The limitation**: We need human oversight!\n",
    "\n",
    "**Next notebook**: `05_hitl_approaches.ipynb`\n",
    "- Learn about human-in-the-loop approaches\n",
    "- Understand human oversight mechanisms\n",
    "- Implement HITL for critical decisions\n",
    "- Combine AI with human judgment\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
