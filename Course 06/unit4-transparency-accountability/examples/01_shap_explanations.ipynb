{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. SHAP Explanations | ØªÙØ³ÙŠØ±Ø§Øª SHAP\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 06, Unit 4** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. SHAP Explanations | ØªÙØ³ÙŠØ±Ø§Øª SHAP\n",
    "\n",
    "## ğŸš¨ THE PROBLEM: AI Systems Are \"Black Boxes\" | Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ \"ØµÙ†Ø§Ø¯ÙŠÙ‚ Ø³ÙˆØ¯Ø§Ø¡\"\n",
    "\n",
    "**Remember the transition from Unit 3?**\n",
    "\n",
    "We completed Unit 3: Privacy and Security, where we learned:\n",
    "- How to protect data and ensure privacy\n",
    "- How to comply with regulations\n",
    "- How to build secure AI systems\n",
    "\n",
    "**But there's a new challenge:**\n",
    "\n",
    "**AI systems also raise transparency and accountability concerns!**\n",
    "\n",
    "**The Problem**: As we build AI systems, we need to consider:\n",
    "- âŒ **Transparency**: How do we explain AI decisions?\n",
    "- âŒ **Accountability**: Who is responsible for AI outcomes?\n",
    "- âŒ **Explainability**: How do we make AI understandable?\n",
    "- âŒ **Auditability**: How do we track and verify AI behavior?\n",
    "\n",
    "**We've learned:**\n",
    "- âœ… How to build private and secure AI systems (Unit 3)\n",
    "- âœ… How to ensure GDPR compliance\n",
    "- âœ… How to protect data\n",
    "\n",
    "**But we haven't learned:**\n",
    "- âŒ How to **explain** AI model decisions\n",
    "- âŒ How to **understand** why models make certain predictions\n",
    "- âŒ How to **interpret** model behavior\n",
    "- âŒ How to **provide explanations** to users\n",
    "\n",
    "**We need explainability techniques** to:\n",
    "1. Explain individual predictions (local explanations)\n",
    "2. Understand overall model behavior (global explanations)\n",
    "3. Identify important features\n",
    "4. Make AI decisions transparent\n",
    "\n",
    "**This notebook solves that problem** by teaching you SHAP (SHapley Additive exPlanations) for model interpretability!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Unit 1: Ethics Foundations** - Understanding ethical principles\n",
    "- âœ… **Unit 2: Bias and Justice** - Understanding fairness\n",
    "- âœ… **Unit 3: Privacy and Security** - Understanding privacy and security\n",
    "- âœ… **Basic Python knowledge**: Functions, data manipulation, ML concepts\n",
    "- âœ… **Basic ML knowledge**: Model training, predictions, feature importance\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why explainability matters\n",
    "- Knowing how to interpret model predictions\n",
    "- Understanding feature importance concepts\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the FIRST example in Unit 4** - it teaches you how to explain AI decisions!\n",
    "\n",
    "**Why this example FIRST?**\n",
    "- **Before** you can use other explanation methods, you need to understand SHAP\n",
    "- **Before** you can ensure accountability, you need explainability\n",
    "- **Before** you can build transparent systems, you need explanation techniques\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Unit 1: Ethics Foundations (ethical principles)\n",
    "- ğŸ““ Unit 2: Bias and Justice (fairness concepts)\n",
    "- ğŸ““ Unit 3: Privacy and Security (privacy and security)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 2: LIME Explanations (alternative explanation method)\n",
    "- ğŸ““ Example 3: Counterfactual Analysis (what-if explanations)\n",
    "- ğŸ““ Example 4: Accountability Frameworks (accountability structures)\n",
    "- ğŸ““ Example 5: Human-in-the-Loop (HITL approaches)\n",
    "- ğŸ““ Example 6: Transparency Tools (transparency frameworks)\n",
    "\n",
    "**Why this order?**\n",
    "1. SHAP provides **foundation** for explainability (needed before other methods)\n",
    "2. SHAP teaches **feature importance** (critical for understanding)\n",
    "3. SHAP shows **local and global explanations** (essential for transparency)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Opening the Black Box | Ø§Ù„Ù‚ØµØ©: ÙØªØ­ Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø£Ø³ÙˆØ¯\n",
    "\n",
    "Imagine you're a doctor using AI to diagnose patients. **Before** explainability, you'd get a diagnosis but wouldn't know why (black box!). **After** using SHAP, you can see which symptoms most influenced the diagnosis - transparent and understandable!\n",
    "\n",
    "Same with AI: **Before** we have models that make predictions but we can't explain why, now we learn SHAP - calculate feature contributions to understand each prediction! **After** SHAP, we can explain why models make specific decisions!\n",
    "\n",
    "---\n",
    "\n",
    "## Why SHAP Explanations Matter | Ù„Ù…Ø§Ø°Ø§ ØªÙ‡Ù… ØªÙØ³ÙŠØ±Ø§Øª SHAPØŸ\n",
    "\n",
    "SHAP explanations are essential for ethical AI:\n",
    "- **Transparency**: Understand why models make decisions\n",
    "- **Trust**: Build user confidence through explanations\n",
    "- **Debugging**: Identify model errors and biases\n",
    "- **Compliance**: Meet explainability requirements\n",
    "- **Accountability**: Enable responsibility for AI outcomes\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Understand SHAP values and their meaning\n",
    "2. Learn how to calculate SHAP values\n",
    "3. Generate local explanations for individual predictions\n",
    "4. Generate global explanations for overall model behavior\n",
    "5. Visualize SHAP explanations\n",
    "6. Interpret SHAP results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:59:07.873528Z",
     "iopub.status.busy": "2025-12-26T16:59:07.873386Z",
     "iopub.status.idle": "2025-12-26T16:59:10.387788Z",
     "shell.execute_reply": "2025-12-26T16:59:10.387547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: SHAP library not available. Using simplified SHAP implementation.\n",
      "================================================================================\n",
      "Unit 4 - Example 1: SHAP Explanations\n",
      "================================================================================\n",
      "\n",
      "Generating dataset...\n",
      "Dataset shape: (1000, 5)\n",
      "\n",
      "Training Random Forest model...\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy: 0.9367\n",
      "\n",
      "Calculating SHAP values...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP values shape: (100, 4)\n",
      "\n",
      "================================================================================\n",
      "Creating Visualizations...\n",
      "================================================================================\n",
      "âœ… Saved: shap_summary.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: shap_waterfall.png\n",
      "âœ… Saved: shap_dependence.png\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. SHAP values explain individual predictions\n",
      "2. SHAP summary shows global feature importance\n",
      "3. Waterfall plots show how features contribute to a specific prediction\n",
      "4. Dependence plots show how SHAP values vary with feature values\n",
      "5. SHAP provides model-agnostic explanations\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unit 4: Interpretability, Transparency, and Accountability\n",
    "Example 1: SHAP Explanations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš« When SHAP Explanations Hit a Limitation | Ø¹Ù†Ø¯Ù…Ø§ ØªØµÙ„ ØªÙØ³ÙŠØ±Ø§Øª SHAP Ø¥Ù„Ù‰ Ø­Ø¯\n",
    "\n",
    "### The Limitation We Discovered\n",
    "\n",
    "We've learned SHAP for explaining model predictions. **But there's still a challenge:**\n",
    "\n",
    "**What if we need a simpler, faster explanation method that works with any model?**\n",
    "\n",
    "SHAP works well when:\n",
    "- âœ… We need mathematically rigorous explanations\n",
    "- âœ… We can afford computational cost\n",
    "- âœ… We want feature importance based on game theory\n",
    "\n",
    "**But sometimes we need:**\n",
    "- âŒ **Faster explanations** (SHAP can be slow for large datasets)\n",
    "- âŒ **Simpler methods** (easier to understand and implement)\n",
    "- âŒ **Model-agnostic approaches** (work with any black-box model)\n",
    "- âŒ **Local explanations** (explain individual predictions quickly)\n",
    "\n",
    "### Why This Is a Problem\n",
    "\n",
    "When SHAP is too slow or complex:\n",
    "- We may need faster explanations for real-time systems\n",
    "- We may want simpler methods for non-technical users\n",
    "- We may need alternatives for different use cases\n",
    "- We may want to compare different explanation methods\n",
    "\n",
    "### The Solution: LIME Explanations\n",
    "\n",
    "We need **LIME (Local Interpretable Model-agnostic Explanations)** to:\n",
    "1. Provide fast local explanations\n",
    "2. Work with any black-box model\n",
    "3. Offer simpler, more intuitive explanations\n",
    "4. Enable real-time explainability\n",
    "\n",
    "**This is exactly what we'll learn in the next notebook: LIME Explanations!**\n",
    "\n",
    "---\n",
    "\n",
    "## â¡ï¸ Next Steps | Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©\n",
    "\n",
    "**You've completed this notebook!** Now you understand:\n",
    "- âœ… How to use SHAP for model explanations\n",
    "- âœ… How to generate local and global explanations\n",
    "- âœ… **The limitation**: We need faster, simpler alternatives!\n",
    "\n",
    "**Next notebook**: `02_lime_explanations.ipynb`\n",
    "- Learn about LIME for fast local explanations\n",
    "- Understand model-agnostic explainability\n",
    "- Compare SHAP vs LIME approaches\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}