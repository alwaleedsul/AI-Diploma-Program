{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. LIME Explanations | ØªÙØ³ÙŠØ±Ø§Øª LIME\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 06, Unit 4** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. LIME Explanations | ØªÙØ³ÙŠØ±Ø§Øª LIME\n",
    "\n",
    "## ğŸš¨ THE PROBLEM: We Need Faster, Simpler Explanations | Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: Ù†Ø­ØªØ§Ø¬ ØªÙØ³ÙŠØ±Ø§Øª Ø£Ø³Ø±Ø¹ ÙˆØ£Ø¨Ø³Ø·\n",
    "\n",
    "**Remember the limitation from the previous notebook?**\n",
    "\n",
    "We learned SHAP for explaining model predictions. But we discovered:\n",
    "\n",
    "**What if we need a simpler, faster explanation method that works with any model?**\n",
    "\n",
    "**The Problem**: Sometimes we need:\n",
    "- âŒ **Faster explanations** (SHAP can be slow for large datasets)\n",
    "- âŒ **Simpler methods** (easier to understand and implement)\n",
    "- âŒ **Model-agnostic approaches** (work with any black-box model)\n",
    "- âŒ **Local explanations** (explain individual predictions quickly)\n",
    "\n",
    "**We've learned:**\n",
    "- âœ… How to use SHAP for model explanations (Notebook 1)\n",
    "- âœ… How to generate local and global explanations\n",
    "- âœ… Feature importance concepts\n",
    "\n",
    "**But we haven't learned:**\n",
    "- âŒ How to use **LIME** for fast local explanations\n",
    "- âŒ How to provide **simpler explanations** for non-technical users\n",
    "- âŒ How to use **model-agnostic** explanation methods\n",
    "- âŒ How to **compare different explanation methods**\n",
    "\n",
    "**We need LIME (Local Interpretable Model-agnostic Explanations)** to:\n",
    "1. Provide fast local explanations\n",
    "2. Work with any black-box model\n",
    "3. Offer simpler, more intuitive explanations\n",
    "4. Enable real-time explainability\n",
    "\n",
    "**This notebook solves that problem** by teaching you LIME for fast, model-agnostic explanations!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: SHAP Explanations** - Understanding explainability basics\n",
    "- âœ… **Basic Python knowledge**: Functions, data manipulation, ML concepts\n",
    "- âœ… **Basic ML knowledge**: Model training, predictions\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why we need alternative explanation methods\n",
    "- Knowing how LIME differs from SHAP\n",
    "- Understanding local interpretability concepts\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the SECOND example in Unit 4** - it teaches you faster, simpler explanations!\n",
    "\n",
    "**Why this example SECOND?**\n",
    "- **Before** you can use LIME effectively, you need to understand SHAP (Example 1)\n",
    "- **Before** you can compare methods, you need multiple approaches\n",
    "- **Before** you can use counterfactuals, you need local explanations\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: SHAP Explanations (we learned SHAP, now we learn LIME!)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 3: Counterfactual Analysis (what-if explanations)\n",
    "- ğŸ““ Example 4: Accountability Frameworks (accountability structures)\n",
    "- ğŸ““ Example 5: Human-in-the-Loop (HITL approaches)\n",
    "- ğŸ““ Example 6: Transparency Tools (transparency frameworks)\n",
    "\n",
    "**Why this order?**\n",
    "1. LIME provides **alternative approach** (complements SHAP)\n",
    "2. LIME enables **faster explanations** (critical for real-time systems)\n",
    "3. LIME shows **model-agnostic** methods (works with any model)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Fast and Simple Explanations | Ø§Ù„Ù‚ØµØ©: ØªÙØ³ÙŠØ±Ø§Øª Ø³Ø±ÙŠØ¹Ø© ÙˆØ¨Ø³ÙŠØ·Ø©\n",
    "\n",
    "Imagine you're a loan officer using AI to approve loans. **Before** LIME, you'd use SHAP but it might be too slow for real-time decisions. **After** using LIME, you get fast, simple explanations that show which factors matter most - quick and understandable!\n",
    "\n",
    "Same with AI: **Before** we have SHAP but it may be slow, now we learn LIME - fast local explanations using simple linear models! **After** LIME, we can explain predictions quickly for any model!\n",
    "\n",
    "---\n",
    "\n",
    "## Why LIME Explanations Matter | Ù„Ù…Ø§Ø°Ø§ ØªÙ‡Ù… ØªÙØ³ÙŠØ±Ø§Øª LIMEØŸ\n",
    "\n",
    "LIME explanations are essential for ethical AI:\n",
    "- **Speed**: Fast explanations for real-time systems\n",
    "- **Simplicity**: Easy to understand for non-technical users\n",
    "- **Model-Agnostic**: Works with any black-box model\n",
    "- **Local Focus**: Explains individual predictions clearly\n",
    "- **Flexibility**: Works with tabular, text, and image data\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Understand LIME and how it differs from SHAP\n",
    "2. Learn how to generate LIME explanations\n",
    "3. Apply LIME to tabular data\n",
    "4. Understand local interpretability concepts\n",
    "5. Compare LIME vs SHAP approaches\n",
    "6. Use LIME for real-time explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:59:11.792919Z",
     "iopub.status.busy": "2025-12-26T16:59:11.792775Z",
     "iopub.status.idle": "2025-12-26T16:59:13.760364Z",
     "shell.execute_reply": "2025-12-26T16:59:13.760117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: LIME library not available. Using simplified LIME implementation.\n",
      "================================================================================\n",
      "Unit 4 - Example 2: LIME Explanations\n",
      "================================================================================\n",
      "\n",
      "Generating dataset...\n",
      "Dataset shape: (1000, 5)\n",
      "\n",
      "Training Random Forest model...\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy: 0.9167\n",
      "\n",
      "Generating LIME explanations...\n",
      "Generated 10 LIME explanations\n",
      "\n",
      "================================================================================\n",
      "Creating Visualizations...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: lime_explanation.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: lime_comparison.png\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. LIME provides local, interpretable explanations\n",
      "2. LIME approximates complex models with simple linear models locally\n",
      "3. LIME works for any black-box model\n",
      "4. LIME explanations are instance-specific\n",
      "5. LIME helps understand individual predictions\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unit 4: Interpretability, Transparency, and Accountability\n",
    "Example 2: LIME Explanations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš« When LIME Explanations Hit a Limitation | Ø¹Ù†Ø¯Ù…Ø§ ØªØµÙ„ ØªÙØ³ÙŠØ±Ø§Øª LIME Ø¥Ù„Ù‰ Ø­Ø¯\n",
    "\n",
    "### The Limitation We Discovered\n",
    "\n",
    "We've learned LIME for fast, local explanations. **But there's still a challenge:**\n",
    "\n",
    "**What if we need to understand \"what if\" scenarios - how would predictions change if features were different?**\n",
    "\n",
    "LIME works well when:\n",
    "- âœ… We need fast local explanations\n",
    "- âœ… We want to understand current predictions\n",
    "- âœ… We can use model-agnostic methods\n",
    "\n",
    "**But we also need:**\n",
    "- âŒ **Counterfactual explanations** (what-if scenarios)\n",
    "- âŒ **Actionable insights** (what to change to get different outcomes)\n",
    "- âŒ **Alternative scenarios** (how predictions would change)\n",
    "- âŒ **Causal reasoning** (understanding cause-effect relationships)\n",
    "\n",
    "### Why This Is a Problem\n",
    "\n",
    "When we only have feature importance:\n",
    "- We know what features matter, but not what to change\n",
    "- We can't explore alternative scenarios\n",
    "- We can't understand \"what if\" questions\n",
    "- We can't provide actionable recommendations\n",
    "\n",
    "### The Solution: Counterfactual Analysis\n",
    "\n",
    "We need **counterfactual analysis** to:\n",
    "1. Understand \"what if\" scenarios\n",
    "2. Provide actionable insights\n",
    "3. Explore alternative outcomes\n",
    "4. Enable causal reasoning\n",
    "\n",
    "**This is exactly what we'll learn in the next notebook: Counterfactual Analysis!**\n",
    "\n",
    "---\n",
    "\n",
    "## â¡ï¸ Next Steps | Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©\n",
    "\n",
    "**You've completed this notebook!** Now you understand:\n",
    "- âœ… How to use SHAP for explanations (Notebook 1)\n",
    "- âœ… How to use LIME for fast explanations (This notebook!)\n",
    "- âœ… **The limitation**: We need \"what if\" scenarios!\n",
    "\n",
    "**Next notebook**: `03_counterfactual_analysis.ipynb`\n",
    "- Learn about counterfactual explanations\n",
    "- Understand \"what if\" scenarios\n",
    "- Explore alternative outcomes\n",
    "- Provide actionable insights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}