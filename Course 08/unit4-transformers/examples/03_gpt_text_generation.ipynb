{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-Style Text Generation\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Basic Python\n",
    "- ‚úÖ Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 08, Unit 4** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-Style Text Generation\n## AIAT 122 - Deep Learning\n\n## Learning Objectives\n\nBy completing this notebook, you will:\n- Understand GPT architecture and text generation\n- Implement text generation with pre-trained GPT models\n- Fine-tune GPT for specific tasks\n- Apply GPT to real-world text generation problems\n\n## Prerequisites\n\n- Python 3.8+\n- Understanding of transformers and attention mechanisms\n- Familiarity with Hugging Face Transformers library\n\n---\n\n## Introduction: Why GPT?\n\nGPT (Generative Pre-trained Transformer) models revolutionized text generation:\n\n- **Language Understanding**: Pre-trained on massive text corpora\n- **Text Generation**: Can generate coherent, context-aware text\n- **Task Adaptation**: Fine-tunable for specific applications\n- **Real-World Applications**: Story writing, code completion, chatbots, content creation\n\n**Real-World Application**: In this notebook, we'll use GPT for creative story generation and code completion, simulating real-world applications in:\n- **Content Creation**: Automated article writing, creative storytelling\n- **Software Development**: Code completion and generation\n- **Customer Service**: Conversational AI chatbots\n- **Education**: Personalized learning content generation\n\n**Industry Impact**: GPT models power ChatGPT, GitHub Copilot, and many production AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets -q\n",
    "\n",
    "import torch\n",
    "from transformers \n",
    "import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers \n",
    "import Trainer, TrainingArguments\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('‚úÖ Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Understanding GPT Architecture\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model (smaller version for demonstration)\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Set pad token\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"Model parameters: {model.num_parameters():,}\")\nprint(f\"Vocabulary size: {len(tokenizer)} tokens\")\nprint(f\"Max context length: {model.config.n_positions} tokens\")\nprint(\"\\n‚úÖ GPT-2 model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: Text Generation Basics\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_length=100, temperature=0.7, top_k=50, top_p=0.95):\n",
    " \n",
    "    \n",
    "    \"\"\"\n",
    " Generate text using GPT model.\n",
    " \n",
    " Args:\n",
    " prompt: Input text promptmodel: GPT model\n",
    " tokenizer: GPT tokenizer\n",
    " max_length: Maximum generation length\n",
    " temperature: Controls randomness (lower = more deterministic)\n",
    " top_k: Keep only top k tokenstop_p: Nucleus sampling threshold\n",
    " \"\"\"\n",
    " # Encode input\n",
    " inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    " \n",
    " # Generate\n",
    " with torch.no_grad():\n",
    " outputs = model.generate(\n",
    " inputs, max_length=max_length,\n",
    " temperature=temperature,\n",
    " top_k=top_k,\n",
    " top_p=top_p,\n",
    " do_sample=True,\n",
    " pad_token_id=tokenizer.eos_token_id\n",
    " )\n",
    " \n",
    " # Decode\n",
    " generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    " return generated_text\n",
    "\n",
    "# Example 1: Story generation\n",
    "prompt1 = \"Once upon a time, in a world where artificial intelligence\"\n",
    "generated1 = generate_text(prompt1, model, tokenizer, max_length=150)\n",
    "print(\"üìñ Story Generation:\")\n",
    "print(generated1)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Example 2: Code completion\n",
    "prompt2 = \"def calculate_fibonacci(n):\"\n",
    "generated2 = generate_text(prompt2, model, tokenizer, max_length=100, temperature=0.3)\n",
    "print(\"üíª Code Completion:\")\n",
    "print(generated2)\n",
    "print(\"\\n‚úÖ Text generation working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3: Real-World Application: Creative Writing Assistant\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world scenario: Content creation for marketing\n",
    "marketing_prompts = [\n",
    " \"Our new AI-powered product helps businesses\",\n",
    " \"The future of technology is\",\n",
    " \"Customer satisfaction is achieved through\"\n",
    "]\n",
    "\n",
    "print(\"üìù Marketing Content Generation:\\n\")\n",
    "for i, prompt in enumerate(marketing_prompts, 1):\n",
    " generated = generate_text(prompt, model, tokenizer, max_length=80, temperature=0.8)\n",
    " print(f\"Prompt {i}: {prompt}\")\n",
    " print(f\"Generated: {generated}\\n\")\n",
    " print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Real-world application demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4: Controlling Generation Quality\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different generation strategies\nprompt = \"The impact of artificial intelligence on healthcare\"\n\nprint(\"üîß Generation Strategies Comparison:\\n\")\n\n# Strategy 1: High temperature (creative)\ncreative = generate_text(prompt, model, tokenizer, temperature=1.2, top_p=0.9)\nprint(f\"Creative (temp=1.2): {creative[:200]}...\\n\")\n\n# Strategy 2: Low temperature (focused)\nfocused = generate_text(prompt, model, tokenizer, temperature=0.3, top_p=0.5)\nprint(f\"Focused (temp=0.3): {focused[:200]}...\\n\")\n\n# Strategy 3: Balanced\nbalanced = generate_text(prompt, model, tokenizer, temperature=0.7, top_p=0.95)\nprint(f\"Balanced (temp=0.7): {balanced[:200]}...\\n\")\n\nprint(\"‚úÖ Different strategies produce different styles!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Fine-tuning GPT for Specific Tasks\n\n**Real-World Application**: Companies fine-tune GPT models for:\n- Domain-specific content (legal, medical, technical)\n- Brand voice consistency\n- Task-specific outputs (summarization, Q&A)\n\n**Note**: Full fine-tuning requires significant resources. Here we demonstrate the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual example: Fine-tuning setup\n",
    "# In production, you would:\n",
    "# 1. Prepare domain-specific dataset\n",
    "# 2. Configure training arguments\n",
    "# 3. Fine-tune model\n",
    "# 4. Evaluate on test set\n",
    "\n",
    "print(\"üìö Fine-tuning Concept:\")\n",
    "print(\"\\n1. Prepare Dataset:\")\n",
    "print(\" - Collect domain-specific text (e.g., medical articles)\")\n",
    "print(\" - Format as text files or use Hugging Face datasets\")\n",
    "print(\"\\n2. Configure Training:\")\n",
    "print(\" - Learning rate: 5e-5\")\n",
    "print(\" - Batch size: 4-8 (depending on GPU)\")\n",
    "print(\" - Epochs: 3-5\")\n",
    "print(\"\\n3. Fine-tune Model:\")\n",
    "print(\" - Use Trainer API from transformers\")\n",
    "print(\" - Monitor loss and perplexity\")\n",
    "print(\"\\n4. Evaluate:\")\n",
    "print(\" - Test on held-out data\")\n",
    "print(\" - Compare with base model\")\n",
    "print(\"\\n‚úÖ Fine-tuning process understood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Use Cases\n\n### 1. Content Marketing\n- Generate blog posts, social media content\n- Maintain brand voice consistency\n- Scale content production\n\n### 2. Code Generation\n- GitHub Copilot-style code completion\n- Code documentation generation\n- Bug fix suggestions\n\n### 3. Conversational AI\n- Customer service chatbots\n- Virtual assistants\n- Personalized recommendations\n\n### 4. Education\n- Personalized learning content\n- Quiz generation\n- Explanation generation\n\n---\n\n## Key Takeaways\n\n1. **GPT Architecture**: Transformer-based decoder-only model\n2. **Text Generation**: Controlled by temperature, top-k, top-p\n3. **Fine-tuning**: Adapts pre-trained models to specific domains\n4. **Real-World Impact**: Powers many production AI systems\n\n## Next Steps\n\n- Explore larger GPT models (GPT-3, GPT-4 via API)\n- Fine-tune on your own dataset\n- Implement RAG (Retrieval-Augmented Generation)\n- Build production text generation pipeline\n\n---\n\n**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}