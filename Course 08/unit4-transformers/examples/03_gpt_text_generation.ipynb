{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-Style Text Generation\n## AIAT 122 - Deep Learning\n\n## Learning Objectives\n\nBy completing this notebook, you will:\n- Understand GPT architecture and text generation\n- Implement text generation with pre-trained GPT models\n- Fine-tune GPT for specific tasks\n- Apply GPT to real-world text generation problems\n\n## Prerequisites\n\n- Python 3.8+\n- Understanding of transformers and attention mechanisms\n- Familiarity with Hugging Face Transformers library\n\n---\n\n## Introduction: Why GPT?\n\nGPT (Generative Pre-trained Transformer) models revolutionized text generation:\n\n- **Language Understanding**: Pre-trained on massive text corpora\n- **Text Generation**: Can generate coherent, context-aware text\n- **Task Adaptation**: Fine-tunable for specific applications\n- **Real-World Applications**: Story writing, code completion, chatbots, content creation\n\n**Real-World Application**: In this notebook, we'll use GPT for creative story generation and code completion, simulating real-world applications in:\n- **Content Creation**: Automated article writing, creative storytelling\n- **Software Development**: Code completion and generation\n- **Customer Service**: Conversational AI chatbots\n- **Education**: Personalized learning content generation\n\n**Industry Impact**: GPT models power ChatGPT, GitHub Copilot, and many production AI systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers torch datasets -q\n\nimport torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\nfrom transformers import Trainer, TrainingArguments\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('\u2705 Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding GPT Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained GPT-2 model (smaller version for demonstration)\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Set pad token\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"Model parameters: {model.num_parameters():,}\")\nprint(f\"Vocabulary size: {len(tokenizer)} tokens\")\nprint(f\"Max context length: {model.config.n_positions} tokens\")\nprint(\"\\n\u2705 GPT-2 model loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Text Generation Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(prompt, model, tokenizer, max_length=100, temperature=0.7, top_k=50, top_p=0.95):\n    \"\"\"\n    Generate text using GPT model.\n    \n    Args:\n        prompt: Input text prompt\n        model: GPT model\n        tokenizer: GPT tokenizer\n        max_length: Maximum generation length\n        temperature: Controls randomness (lower = more deterministic)\n        top_k: Keep only top k tokens\n        top_p: Nucleus sampling threshold\n    \"\"\"\n    # Encode input\n    inputs = tokenizer.encode(prompt, return_tensors='pt')\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs,\n            max_length=max_length,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n\n# Example 1: Story generation\nprompt1 = \"Once upon a time, in a world where artificial intelligence\"\ngenerated1 = generate_text(prompt1, model, tokenizer, max_length=150)\nprint(\"\ud83d\udcd6 Story Generation:\")\nprint(generated1)\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Example 2: Code completion\nprompt2 = \"def calculate_fibonacci(n):\"\ngenerated2 = generate_text(prompt2, model, tokenizer, max_length=100, temperature=0.3)\nprint(\"\ud83d\udcbb Code Completion:\")\nprint(generated2)\nprint(\"\\n\u2705 Text generation working!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Real-World Application: Creative Writing Assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real-world scenario: Content creation for marketing\nmarketing_prompts = [\n    \"Our new AI-powered product helps businesses\",\n    \"The future of technology is\",\n    \"Customer satisfaction is achieved through\"\n]\n\nprint(\"\ud83d\udcdd Marketing Content Generation:\\n\")\nfor i, prompt in enumerate(marketing_prompts, 1):\n    generated = generate_text(prompt, model, tokenizer, max_length=80, temperature=0.8)\n    print(f\"Prompt {i}: {prompt}\")\n    print(f\"Generated: {generated}\\n\")\n    print(\"-\" * 60 + \"\\n\")\n\nprint(\"\u2705 Real-world application demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Controlling Generation Quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different generation strategies\nprompt = \"The impact of artificial intelligence on healthcare\"\n\nprint(\"\ud83d\udd27 Generation Strategies Comparison:\\n\")\n\n# Strategy 1: High temperature (creative)\ncreative = generate_text(prompt, model, tokenizer, temperature=1.2, top_p=0.9)\nprint(f\"Creative (temp=1.2): {creative[:200]}...\\n\")\n\n# Strategy 2: Low temperature (focused)\nfocused = generate_text(prompt, model, tokenizer, temperature=0.3, top_p=0.5)\nprint(f\"Focused (temp=0.3): {focused[:200]}...\\n\")\n\n# Strategy 3: Balanced\nbalanced = generate_text(prompt, model, tokenizer, temperature=0.7, top_p=0.95)\nprint(f\"Balanced (temp=0.7): {balanced[:200]}...\\n\")\n\nprint(\"\u2705 Different strategies produce different styles!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Fine-tuning GPT for Specific Tasks\n\n**Real-World Application**: Companies fine-tune GPT models for:\n- Domain-specific content (legal, medical, technical)\n- Brand voice consistency\n- Task-specific outputs (summarization, Q&A)\n\n**Note**: Full fine-tuning requires significant resources. Here we demonstrate the concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conceptual example: Fine-tuning setup\n# In production, you would:\n# 1. Prepare domain-specific dataset\n# 2. Configure training arguments\n# 3. Fine-tune model\n# 4. Evaluate on test set\n\nprint(\"\ud83d\udcda Fine-tuning Concept:\")\nprint(\"\\n1. Prepare Dataset:\")\nprint(\"   - Collect domain-specific text (e.g., medical articles)\")\nprint(\"   - Format as text files or use Hugging Face datasets\")\nprint(\"\\n2. Configure Training:\")\nprint(\"   - Learning rate: 5e-5\")\nprint(\"   - Batch size: 4-8 (depending on GPU)\")\nprint(\"   - Epochs: 3-5\")\nprint(\"\\n3. Fine-tune Model:\")\nprint(\"   - Use Trainer API from transformers\")\nprint(\"   - Monitor loss and perplexity\")\nprint(\"\\n4. Evaluate:\")\nprint(\"   - Test on held-out data\")\nprint(\"   - Compare with base model\")\nprint(\"\\n\u2705 Fine-tuning process understood!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Use Cases\n\n### 1. Content Marketing\n- Generate blog posts, social media content\n- Maintain brand voice consistency\n- Scale content production\n\n### 2. Code Generation\n- GitHub Copilot-style code completion\n- Code documentation generation\n- Bug fix suggestions\n\n### 3. Conversational AI\n- Customer service chatbots\n- Virtual assistants\n- Personalized recommendations\n\n### 4. Education\n- Personalized learning content\n- Quiz generation\n- Explanation generation\n\n---\n\n## Key Takeaways\n\n1. **GPT Architecture**: Transformer-based decoder-only model\n2. **Text Generation**: Controlled by temperature, top-k, top-p\n3. **Fine-tuning**: Adapts pre-trained models to specific domains\n4. **Real-World Impact**: Powers many production AI systems\n\n## Next Steps\n\n- Explore larger GPT models (GPT-3, GPT-4 via API)\n- Fine-tune on your own dataset\n- Implement RAG (Retrieval-Augmented Generation)\n- Build production text generation pipeline\n\n---\n\n**End of Notebook**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}