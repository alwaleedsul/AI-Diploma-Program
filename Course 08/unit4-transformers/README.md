# Unit 4: Transformers | المحولات
## AIAT 122 - Deep Learning

### Learning Objectives | أهداف التعلم

By the end of this unit, students will be able to:
- Understand the Transformer architecture
- Implement attention mechanisms
- Work with pre-trained transformer models
- Fine-tune transformers for specific tasks
- Apply transformers to various NLP and vision tasks

---

## Topics Covered | المواضيع المغطاة

Based on official curriculum (AIAT 122), this unit covers:

1. **Attention Mechanisms**
   - Self-attention
   - Multi-head attention
   - Scaled dot-product attention
   - Attention visualization

2. **Transformer Architecture**
   - Encoder-decoder structure
   - Positional encoding
   - Layer normalization
   - Feed-forward networks
   - Residual connections

3. **Pre-trained Transformer Models**
   - BERT (Bidirectional Encoder Representations)
   - GPT (Generative Pre-trained Transformer)
   - T5 (Text-to-Text Transfer Transformer)
   - Vision Transformers (ViT)

4. **Fine-tuning Transformers**
   - Transfer learning with transformers
   - Task-specific fine-tuning
   - Parameter-efficient fine-tuning
   - Prompt engineering

5. **Applications**
   - Natural language understanding
   - Text generation
   - Machine translation
   - Image classification with ViT
   - Multimodal transformers

---

## Unit Breakdown | تفصيل الوحدة

**Theoretical Hours:** 7  
**Practical Hours:** 13  
**Total Hours:** 20

### Theoretical Content | المحتوى النظري

- Understanding attention mechanisms
- Transformer architecture deep dive
- Pre-trained model families
- Fine-tuning strategies
- Recent advances in transformers

### Practical Content | المحتوى العملي

- Implementing attention from scratch
- Building a simple transformer
- Fine-tuning BERT for classification
- Using GPT for text generation
- Applying Vision Transformers

---

**Unit Duration:** 3 weeks  
**Difficulty:** Advanced  
**Prerequisites:** Units 1-3 completion, understanding of deep learning

**Created for:** AIAT 122 - Deep Learning  
**Last Updated:** 2025-01-10

