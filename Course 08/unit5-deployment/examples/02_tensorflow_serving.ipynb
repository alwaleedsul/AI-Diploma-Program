{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TensorFlow Serving for Model Deployment\n## AIAT 122 - Deep Learning\n\n## Learning Objectives\n\n- Understand TensorFlow Serving architecture\n- Deploy models with TensorFlow Serving\n- Create REST API endpoints\n- Test production model serving\n\n## Real-World Context\n\nDeploying models for production inference at scale.\n\n**Industry Impact**: TensorFlow Serving is used by Google, Airbnb, and many companies for production ML serving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tensorflow tensorflow-serving-api -q\nimport tensorflow as tf\nimport numpy as np\nprint(f'TensorFlow version: {tf.__version__}')\nprint('\u2705 Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Save Model in SavedModel Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple model for demonstration\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Save in SavedModel format\nmodel_path = './saved_model'\nmodel.save(model_path, save_format='tf')\nprint(f'\u2705 Model saved to {model_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: TensorFlow Serving Setup\n\n**Note**: Full TensorFlow Serving requires Docker. Here we demonstrate the concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\ud83d\udce6 TensorFlow Serving Setup:')\nprint('\\n1. Install TensorFlow Serving:')\nprint('   docker pull tensorflow/serving')\nprint('\\n2. Start serving container:')\nprint('   docker run -p 8501:8501 --mount type=bind,source=/path/to/model,target=/models/model -e MODEL_NAME=model tensorflow/serving')\nprint('\\n3. Test REST API:')\nprint('   curl -d \\'{\"instances\": [[1,2,3,...]]}\\' -X POST http://localhost:8501/v1/models/model:predict')\nprint('\\n\u2705 Serving setup understood!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: REST API Client Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\nimport json\n\n# Example REST API call (when serving is running)\ndef predict_rest_api(data, model_name='model', port=8501):\n    \"\"\"\n    Make prediction via REST API.\n    \n    Real-world: Production inference endpoint\n    \"\"\"\n    url = f'http://localhost:{port}/v1/models/{model_name}:predict'\n    payload = {'instances': data.tolist() if isinstance(data, np.ndarray) else data}\n    \n    response = requests.post(url, json=payload)\n    return response.json()\n\nprint('\u2705 REST API client ready!')\nprint('\\nReal-world: This is how production systems serve predictions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Applications\n\n- **Google**: Serves billions of predictions daily\n- **Airbnb**: Dynamic pricing models\n- **Uber**: ETA predictions\n- **Netflix**: Recommendation systems\n\n---\n\n**End of Notebook**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}