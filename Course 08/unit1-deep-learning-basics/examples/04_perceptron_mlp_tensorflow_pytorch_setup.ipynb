{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron, MLP, and Deep Learning Framework Setup\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand deep learning fundamentals compared to traditional ML\n",
    "- Implement basic perceptron from scratch\n",
    "- Build Multi-Layer Perceptron (MLP) models\n",
    "- Set up TensorFlow and PyTorch environments\n",
    "- Compare TensorFlow and PyTorch approaches\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Understanding of basic machine learning concepts\n",
    "- âœ… Python 3.8+ installed\n",
    "- âœ… Basic NumPy knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 08, Unit 1**:\n",
    "- Deep learning fundamentals compared to traditional ML\n",
    "- Setting up TensorFlow and PyTorch\n",
    "- Implementing basic perceptron and MLP\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 1 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Deep Learning** is a subset of machine learning that uses neural networks with multiple layers to learn hierarchical representations of data. Unlike traditional ML, deep learning can automatically discover features from raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš€ Google Colab Setup (Run this first if using Colab)\n",
    "# Ø¯Ù„ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯ Google Colab (Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„ Ù‡Ø°Ø§ Ø£ÙˆÙ„Ø§Ù‹ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… Colab)\n",
    "\n",
    "# Check if running on Colab\n",
    "try:\n",
    "    IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "except NameError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ðŸŒ Detected Google Colab environment\")\n",
    "    print(\"ðŸ“‹ To enable GPU:\")\n",
    "    print(\"   1. Click: Runtime â†’ Change runtime type\")\n",
    "    print(\"   2. Set Hardware accelerator: GPU\")\n",
    "    print(\"   3. Click Save\")\n",
    "    print(\"\\nâ³ Installing TensorFlow and PyTorch with GPU support...\")\n",
    "    print(\"   (This may take 3-5 minutes)\")\n",
    "    \n",
    "    # Install TensorFlow and PyTorch (Colab has GPU support built-in)\n",
    "    !pip install -q tensorflow torch torchvision\n",
    "    \n",
    "    print(\"\\nâœ… TensorFlow and PyTorch installed!\")\n",
    "    print(\"ðŸ’¡ GPU is automatically available in Colab - no extra setup needed!\")\n",
    "    print(\"ðŸ”„ Please restart runtime: Runtime â†’ Restart runtime\")\n",
    "else:\n",
    "    print(\"ðŸ’» Running on local machine\")\n",
    "    print(\"   GPU recommended for deep learning training (much faster!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try importing TensorFlow\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    HAS_TF = True\n",
    "    print(f\"âœ… TensorFlow {tf.__version__} imported successfully!\")\n",
    "except ImportError:\n",
    "    HAS_TF = False\n",
    "    print(\"âš ï¸  TensorFlow not available. Install with: pip install tensorflow\")\n",
    "\n",
    "# Try importing PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    HAS_TORCH = True\n",
    "    print(f\"âœ… PyTorch {torch.__version__} imported successfully!\")\n",
    "except ImportError:\n",
    "    HAS_TORCH = False\n",
    "    print(\"âš ï¸  PyTorch not available. Install with: pip install torch\")\n",
    "\n",
    "print(\"âœ… NumPy and Matplotlib ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Deep Learning vs Traditional ML\n",
    "\n",
    "Let's compare the fundamental differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Deep Learning vs Traditional ML\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison = {\n",
    "    \"Feature Engineering\": {\n",
    "        \"Traditional ML\": \"Manual feature extraction required\", \"Deep Learning\": \"Automatic feature learning from raw data\"\n",
    "    },\n",
    "    \"Data Requirements\": {\n",
    "        \"Traditional ML\": \"Works well with small to medium datasets\",\n",
    "        \"Deep Learning\": \"Requires large datasets for best performance\"\n",
    "    },\n",
    "    \"Model Complexity\": {\n",
    "        \"Traditional ML\": \"Simpler, more interpretable models\",\n",
    "        \"Deep Learning\": \"Complex, hierarchical representations\"\n",
    "    },\n",
    "    \"Performance\": {\n",
    "        \"Traditional ML\": \"Good for structured data\",\n",
    "        \"Deep Learning\": \"Excels with unstructured data (images, text, audio)\"\n",
    "    },\n",
    "    \"Training Time\": {\n",
    "        \"Traditional ML\": \"Faster training\",\n",
    "        \"Deep Learning\": \"Longer training, benefits from GPUs\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for aspect, details in comparison.items():\n",
    "    print(f\"\\n{aspect}:\")\n",
    "    print(f\"  Traditional ML: {details['Traditional ML']}\")\n",
    "    print(f\"  Deep Learning: {details['Deep Learning']}\")\n",
    "\n",
    "print(\"\\nâœ… Key Insight: Deep learning automatically learns features, making it powerful for complex patterns!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Basic Perceptron Implementation\n",
    "\n",
    "A perceptron is the simplest neural network - a single neuron with weights and bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"Simple Perceptron implementation from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, learning\n",
    "rate =0.1, n\n",
    "iterations =100):\n",
    "        self.learning\n",
    "rate = learning\n",
    "rate\n",
    "        self.n\n",
    "iterations = n\n",
    "iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the perceptron\"\"\"\n",
    "        n_samples, n\n",
    "features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for _ in range(self.n_iterations):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                # Forward pass\n",
    "                linear\n",
    "output = np.dot(x_i, self.weights) + self.bias\n",
    "                y\n",
    "predicted = self.activate(linear_output)\n",
    "                \n",
    "                # Update weights and bias\n",
    "                update = self.learning_rate * (y[idx] - y_predicted)\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "    \n",
    "    def activate(self, x):\n",
    "        \"\"\"Step activation function\"\"\"\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        linear\n",
    "output = np.dot(X, self.weights) + self.bias\n",
    "        return self.activate(linear_output)\n",
    "\n",
    "# Example: Simple AND gate\n",
    "print(\"=\" * 60)\n",
    "print(\"Perceptron: Learning AND Gate\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# AND gate truth table\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "# Train perceptron\n",
    "perceptron = Perceptron(learning\n",
    "rate =0.1, n\n",
    "iterations =100)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Test\n",
    "predictions = perceptron.predict(X)\n",
    "print(\"\\nPredictions:\")\n",
    "print(f\"Input [0,0]: {predictions[0]} (expected 0)\")\n",
    "print(f\"Input [0,1]: {predictions[1]} (expected 0)\")\n",
    "print(f\"Input [1,0]: {predictions[2]} (expected 0)\")\n",
    "print(f\"Input [1,1]: {predictions[3]} (expected 1)\")\n",
    "print(f\"\\nAccuracy: {np.mean(predictions == y) * 100:.0f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multi-Layer Perceptron (MLP) with TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MLP with TensorFlow/Keras\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Generate sample data\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    X, y = make\n",
    "classification(n\n",
    "samples =1000, n\n",
    "features =20, n\n",
    "classes =2, random\n",
    "state =42)\n",
    "    X_train, X_test, y_train, y\n",
    "test = train\n",
    "test\n",
    "split(X, y, test\n",
    "size =0.2, random\n",
    "state =42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X\n",
    "train = scaler.fit_transform(X_train)\n",
    "    X\n",
    "test = scaler.transform(X_test)\n",
    "    \n",
    "    # Build MLP model\n",
    "    model\n",
    "tf = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input\n",
    "shape =(20,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model_tf.compile(\n",
    "        optimizer='adam', loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model_tf.summary()\n",
    "    \n",
    "    # Train model (just a few epochs for demonstration)\n",
    "    print(\"\\nTraining model (5 epochs)...\")\n",
    "    history = model\n",
    "tf.fit(X_train, y_train, epochs=5, batch\n",
    "size =32, verbose=0)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test\n",
    "acc = model\n",
    "tf.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "    print(\"âœ… TensorFlow MLP created successfully!\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MLP with TensorFlow (Installation Required)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "    To use TensorFlow for MLP:\n",
    "    \n",
    "    1. Install TensorFlow:\n",
    "       pip install tensorflow\n",
    "    \n",
    "    2. Basic MLP structure:\n",
    "       model = tf.keras.Sequential([\n",
    "           tf.keras.layers.Dense(64, activation='relu', input\n",
    "shape =(n_features,)),\n",
    "           tf.keras.layers.Dense(32, activation='relu'),\n",
    "           tf.keras.layers.Dense(1, activation='sigmoid')  # for binary classification\n",
    "       ])\n",
    "       \n",
    "    3. Compile and train:\n",
    "       model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "       model.fit(X_train, y_train, epochs=10, batch\n",
    "size =32)\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Layer Perceptron (MLP) with PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TORCH:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MLP with PyTorch\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    # Generate sample data\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    X, y = make\n",
    "classification(n\n",
    "samples =1000, n\n",
    "features =20, n\n",
    "classes =2, random\n",
    "state =42)\n",
    "    X_train, X_test, y_train, y\n",
    "test = train\n",
    "test\n",
    "split(X, y, test\n",
    "size =0.2, random\n",
    "state =42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X\n",
    "train = scaler.fit_transform(X_train)\n",
    "    X\n",
    "test = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train\n",
    "tensor = torch.FloatTensor(X_train)\n",
    "    y_train\n",
    "tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "    X_test\n",
    "tensor = torch.FloatTensor(X_test)\n",
    "    y_test\n",
    "tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "    \n",
    "    # Define MLP model\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MLP, self).__init__()\n",
    "            self.fc1 = nn.Linear(20, 64)\n",
    "            self.fc2 = nn.Linear(64, 32)\n",
    "            self.fc3 = nn.Linear(32, 1)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.relu(self.fc2(x))\n",
    "            x = self.sigmoid(self.fc3(x))\n",
    "            return x\n",
    "    \n",
    "    # Create model\n",
    "    model\n",
    "torch = MLP()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model_torch.parameters(), lr=0.001)\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model_torch)\n",
    "    \n",
    "    # Train model (just a few epochs for demonstration)\n",
    "    print(\"\\nTraining model (5 epochs)...\")\n",
    "    model_torch.train()\n",
    "    for epoch in range(5):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model\n",
    "torch(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    model_torch.eval()\n",
    "    with torch.no_grad():\n",
    "        test\n",
    "outputs = model\n",
    "torch(X_test_tensor)\n",
    "        test\n",
    "preds = (test_outputs > 0.5).float()\n",
    "        test\n",
    "acc = (test\n",
    "preds == y\n",
    "test\n",
    "tensor).float().mean()\n",
    "        print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    print(\"âœ… PyTorch MLP created successfully!\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MLP with PyTorch (Installation Required)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "    To use PyTorch for MLP:\n",
    "    \n",
    "    1. Install PyTorch:\n",
    "       pip install torch\n",
    "    \n",
    "    2. Basic MLP structure:\n",
    "       class MLP(nn.Module):\n",
    "           def __init__(self):\n",
    "               super().__init__()\n",
    "               self.fc1 = nn.Linear(n_features, 64)\n",
    "               self.fc2 = nn.Linear(64, 32)\n",
    "               self.fc3 = nn.Linear(32, 1)\n",
    "           \n",
    "           def forward(self, x):\n",
    "               x = torch.relu(self.fc1(x))\n",
    "               x = torch.relu(self.fc2(x))\n",
    "               x = torch.sigmoid(self.fc3(x))\n",
    "               return x\n",
    "       \n",
    "    3. Train with optimizer:\n",
    "       optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "       for epoch in range(epochs):\n",
    "           optimizer.zero_grad()\n",
    "           outputs = model(X_train)\n",
    "           loss = criterion(outputs, y_train)\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Deep Learning vs Traditional ML**: Automatic feature learning, requires more data, better for unstructured data\n",
    "2. **Perceptron**: Single neuron, can learn simple linear patterns (e.g., AND gate)\n",
    "3. **MLP (Multi-Layer Perceptron)**: Multiple layers of neurons, can learn complex non-linear patterns\n",
    "4. **TensorFlow/Keras**: High-level API, easier to use, great for rapid prototyping\n",
    "5. **PyTorch**: More flexible, imperative style, better for research and custom architectures\n",
    "\n",
    "### Framework Comparison:\n",
    "- **TensorFlow**: Industry standard, production-ready, extensive ecosystem\n",
    "- **PyTorch**: Research-friendly, dynamic computation graphs, intuitive API\n",
    "\n",
    "### When to Use:\n",
    "- **TensorFlow**: Production deployment, large-scale systems, when you need TF Serving\n",
    "- **PyTorch**: Research, experimentation, when you need dynamic graphs\n",
    "\n",
    "**Reference:** Course 08, Unit 1: \"Deep learning fundamentals compared to traditional ML\", \"Setting up TensorFlow and PyTorch\", and \"Implementing basic perceptron and MLP\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}