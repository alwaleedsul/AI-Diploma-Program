{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 5 - Example 08: Deployment & Monitoring\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ðŸ”— Prerequisites\n",
    "\n",
    "- âœ… Examples 02â€“07 (Big Data theory, Dask, PySpark, RAPIDS, pipelines, optimization, large datasets)\n",
    "- âœ… Basic NumPy/Pandas, ML basics (Unit 4)\n",
    "\n",
    "**Previous:** Example 07 (Large datasets). **Next:** Example 09 (Model monitoring).\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "**DETAILED_UNIT_DESCRIPTIONS.md** (Unit 5): *\"Deploying ML models using Flask or FastAPI â€¦ Scaling and monitoring.\"*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 5 - Example 08: Deployment & Monitoring\n",
    "\n",
    "## ðŸ”— Solving the Problem from Example 07 | Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ù…Ù† Ø§Ù„Ù…Ø«Ø§Ù„ 07\n",
    "\n",
    "**Remember the dead end from Example 07?**\n",
    "- We learned large dataset handling strategies\n",
    "- But the model is ready - how do we deploy it for users?\n",
    "- We needed deployment and monitoring strategies\n",
    "\n",
    "**This notebook solves that problem!**\n",
    "- We'll learn **model deployment strategies**\n",
    "- We'll learn **monitoring and maintenance**\n",
    "- We'll learn **production deployment best practices**\n",
    "\n",
    "**This solves the deployment problem from Example 07!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T16:20:32.147268Z",
     "iopub.status.busy": "2026-01-24T16:20:32.147212Z",
     "iopub.status.idle": "2026-01-24T16:20:32.848823Z",
     "shell.execute_reply": "2026-01-24T16:20:32.848600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Example 08: Deployment & Monitoring | Ø§Ù„Ù†Ø´Ø± ÙˆØ§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©\n",
      "======================================================================\n",
      "\n",
      "ðŸ“š Prerequisites: Examples 02-07 completed, deployment knowledge\n",
      "ðŸ”— This is the EIGHTH example in Unit 5 - deployment and monitoring\n",
      "ðŸŽ¯ Goal: Master deploying and monitoring ML models\n",
      "Reference: Study 19.pdf before running this code example.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Example 08: Deployment & Monitoring | Ø§Ù„Ù†Ø´Ø± ÙˆØ§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nðŸ“š Prerequisites: Examples 02-07 completed, deployment knowledge\")\n",
    "print(\"ðŸ”— This is the EIGHTH example in Unit 5 - deployment and monitoring\")\n",
    "print(\"ðŸŽ¯ Goal: Master deploying and monitoring ML models\")\n",
    "print(\"Reference: Study 19.pdf before running this code example.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TRAIN MODEL FOR DEPLOYMENT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T16:20:32.862497Z",
     "iopub.status.busy": "2026-01-24T16:20:32.862361Z",
     "iopub.status.idle": "2026-01-24T16:20:32.865759Z",
     "shell.execute_reply": "2026-01-24T16:20:32.865565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Training Model for Deployment\n",
      "----------------------------------------------------------------------\n",
      "âœ“ Model trained successfully\n",
      "MSE: 0.0104, RÂ²: 0.9985\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. Training Model for Deployment\")\n",
    "print(\"-\" * 70)\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "X = np.random.randn(n_samples, 3)\n",
    "y = X[:, 0] * 2 + X[:, 1] * 1.5 - X[:, 2] * 0.5 + np.random.randn(n_samples) * 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"âœ“ Model trained successfully\")\n",
    "print(f\"MSE: {mse:.4f}, RÂ²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SAVE MODEL FOR DEPLOYMENT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T16:20:32.866810Z",
     "iopub.status.busy": "2026-01-24T16:20:32.866741Z",
     "iopub.status.idle": "2026-01-24T16:20:32.869053Z",
     "shell.execute_reply": "2026-01-24T16:20:32.868865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2. Saving Model\n",
      "----------------------------------------------------------------------\n",
      "âœ“ Model saved to deployed_model.pkl\n",
      "âœ“ Model metadata saved to model_metadata.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n2. Saving Model\")\n",
    "print(\"-\" * 70)\n",
    "# Save model using pickle\n",
    "model_path = 'deployed_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"âœ“ Model saved to {model_path}\")\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "'model_version': '1.0.0',\n",
    "    'deployed_at': datetime.now().isoformat(), 'training_date': datetime.now().isoformat(), 'metrics': {\n",
    "'mse': float(mse),\n",
    "'r2': float(r2)\n",
    "}, 'features': ['feature_0', 'feature_1', 'feature_2'],\n",
    "'model_type': 'LinearRegression'\n",
    "}\n",
    "metadata_path = 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"âœ“ Model metadata saved to {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DEPLOYMENT FUNCTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T16:20:32.869992Z",
     "iopub.status.busy": "2026-01-24T16:20:32.869937Z",
     "iopub.status.idle": "2026-01-24T16:20:32.872099Z",
     "shell.execute_reply": "2026-01-24T16:20:32.871905Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 19:20:32,870 - INFO - Making prediction for 5 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 19:20:32,870 - INFO - Prediction successful: 5 results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3. Deployment Function\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "âœ“ Deployment function tested successfully\n",
      "Sample predictions: [ 3.00910382  2.75354276 -2.06205368]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n3. Deployment Function\")\n",
    "print(\"-\" * 70)\n",
    "def predict(model, features):\n",
    "    \"\"\"\n",
    "    Make predictions using deployed model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Making prediction for {len(features)} samples\")\n",
    "        # Validate input\n",
    "        if len(features.shape) != 2:\n",
    "            raise ValueError(\"Features must be 2D array\")\n",
    "        # Make prediction\n",
    "        prediction = model.predict(features)\n",
    "        logger.info(f\"Prediction successful: {len(prediction)} results\")\n",
    "        return prediction\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction failed: {str(e)}\", exc_info=True)\n",
    "        raise e\n",
    "\n",
    "# Test deployment function\n",
    "test_features = X_test[:5]\n",
    "predictions = predict(model, test_features)\n",
    "print(f\"\\nâœ“ Deployment function tested successfully\")\n",
    "print(f\"Sample predictions: {predictions[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. API Deployment with Flask or FastAPI\n",
    "\n",
    "**Per DETAILED_UNIT_DESCRIPTIONS.md (Unit 5):** Deploy ML models using **Flask or FastAPI** to serve predictions in real-time, making them integrable with web applications.\n",
    "\n",
    "Below we define minimal REST APIs. Install if needed: `pip install flask fastapi uvicorn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T16:20:32.872967Z",
     "iopub.status.busy": "2026-01-24T16:20:32.872908Z",
     "iopub.status.idle": "2026-01-24T16:20:34.429600Z",
     "shell.execute_reply": "2026-01-24T16:20:34.429417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3b. API Deployment (Flask / FastAPI)\n",
      "----------------------------------------------------------------------\n",
      " * Serving Flask app '__main__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 19:20:32,918 - INFO - \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 19:20:32,918 - INFO - \u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 19:20:34,427 - INFO - 127.0.0.1 - - [24/Jan/2026 19:20:34] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Flask /predict response: {'predictions': [3.009103823581936, 2.753542759170331, -2.062053684283035]}\n"
     ]
    }
   ],
   "source": [
    "# Flask API example: serve predictions via REST\n",
    "print(\"\\n3b. API Deployment (Flask / FastAPI)\")\n",
    "print(\"-\" * 70)\n",
    "try:\n",
    "    from flask import Flask, request, jsonify\n",
    "    import threading\n",
    "    import time\n",
    "\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    @app.route(\"/predict\", methods=[\"POST\"])\n",
    "    def predict_api():\n",
    "        \"\"\"Serve model predictions. POST JSON: {\\\"features\\\": [[f1,f2,f3], ...]}\"\"\"\n",
    "        data = request.get_json()\n",
    "        if not data or \"features\" not in data:\n",
    "            return jsonify({\"error\": \"Expected JSON with 'features' key\"}), 400\n",
    "        X = np.array(data[\"features\"])\n",
    "        preds = model.predict(X)\n",
    "        return jsonify({\"predictions\": preds.tolist()})\n",
    "\n",
    "    def run_flask():\n",
    "        app.run(host=\"127.0.0.1\", port=5000, use_reloader=False, threaded=True)\n",
    "\n",
    "    thread = threading.Thread(target=run_flask, daemon=True)\n",
    "    thread.start()\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    import urllib.request\n",
    "    body = json.dumps({\"features\": X_test[:3].tolist()}).encode()\n",
    "    req = urllib.request.Request(\"http://127.0.0.1:5000/predict\", data=body, method=\"POST\", headers={\"Content-Type\": \"application/json\"})\n",
    "    with urllib.request.urlopen(req) as r:\n",
    "        out = json.loads(r.read().decode())\n",
    "    print(\"âœ“ Flask /predict response:\", out)\n",
    "except ImportError:\n",
    "    print(\"âš  Install Flask to run API example: pip install flask\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Flask demo skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T16:20:34.430573Z",
     "iopub.status.busy": "2026-01-24T16:20:34.430504Z",
     "iopub.status.idle": "2026-01-24T16:20:36.099611Z",
     "shell.execute_reply": "2026-01-24T16:20:36.099412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ FastAPI /predict response: {'predictions': [3.009103823581936, 2.753542759170331]}\n"
     ]
    }
   ],
   "source": [
    "# FastAPI example: same idea, async-friendly API\n",
    "try:\n",
    "    import threading\n",
    "    import time\n",
    "    import urllib.request\n",
    "    from typing import List\n",
    "    from fastapi import FastAPI\n",
    "    from pydantic import BaseModel\n",
    "    import uvicorn\n",
    "\n",
    "    api = FastAPI(title=\"ML Predictions\")\n",
    "\n",
    "    class FeaturesRequest(BaseModel):\n",
    "        features: List[List[float]]\n",
    "\n",
    "    @api.post(\"/predict\")\n",
    "    def predict_endpoint(req: FeaturesRequest):\n",
    "        X = np.array(req.features)\n",
    "        preds = model.predict(X)\n",
    "        return {\"predictions\": preds.tolist()}\n",
    "\n",
    "    def run_fastapi():\n",
    "        uvicorn.run(api, host=\"127.0.0.1\", port=8000, log_level=\"warning\")\n",
    "\n",
    "    thread2 = threading.Thread(target=run_fastapi, daemon=True)\n",
    "    thread2.start()\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    body2 = json.dumps({\"features\": X_test[:2].tolist()}).encode()\n",
    "    req2 = urllib.request.Request(\"http://127.0.0.1:8000/predict\", data=body2, method=\"POST\", headers={\"Content-Type\": \"application/json\"})\n",
    "    with urllib.request.urlopen(req2) as r2:\n",
    "        out2 = json.loads(r2.read().decode())\n",
    "    print(\"âœ“ FastAPI /predict response:\", out2)\n",
    "except NameError:\n",
    "    pass  # threading, urllib, etc. from Flask cell\n",
    "except ImportError:\n",
    "    print(\"âš  Install FastAPI + uvicorn to run: pip install fastapi uvicorn\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  FastAPI demo skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T16:20:36.100665Z",
     "iopub.status.busy": "2026-01-24T16:20:36.100602Z",
     "iopub.status.idle": "2026-01-24T16:20:36.101963Z",
     "shell.execute_reply": "2026-01-24T16:20:36.101773Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. MONITORING SETUP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T16:20:36.102906Z",
     "iopub.status.busy": "2026-01-24T16:20:36.102847Z",
     "iopub.status.idle": "2026-01-24T16:20:36.106906Z",
     "shell.execute_reply": "2026-01-24T16:20:36.106735Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 19:20:36,104 - INFO - Logged prediction: 3.009103823581936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 19:20:36,105 - INFO - Logged prediction: 2.753542759170331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 19:20:36,105 - INFO - Logged prediction: -2.062053684283035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 19:20:36,105 - INFO - Logged prediction: -3.695126217427459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 19:20:36,105 - INFO - Logged prediction: 3.1305121454342015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "4. Monitoring Setup\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Monitoring Statistics:\n",
      "  Total predictions: 5\n",
      "  Total errors: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n4. Monitoring Setup\")\n",
    "print(\"-\" * 70)\n",
    "class ModelMonitor:\n",
    "    \"\"\"Simple model monitoring class\"\"\"\n",
    "    def __init__(self):\n",
    "        self.predictions_log = []\n",
    "        self.errors_log = []\n",
    "    \n",
    "    def log_prediction(self, features, prediction, actual=None):\n",
    "        \"\"\"Log prediction for monitoring\"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now().isoformat(), 'features': features.tolist() if isinstance(features, np.ndarray) else features,\n",
    "            'prediction': float(prediction) if np.isscalar(prediction) else prediction.tolist(), 'actual': float(actual) if actual is not None and np.isscalar(actual) else None\n",
    "        }\n",
    "        self.predictions_log.append(log_entry)\n",
    "        logger.info(f\"Logged prediction: {log_entry['prediction']}\")\n",
    "    \n",
    "    def log_error(self, error_message):\n",
    "        \"\"\"Log error for monitoring\"\"\"\n",
    "        error_entry = {\n",
    "            'timestamp': datetime.now().isoformat(), 'error': error_message\n",
    "        }\n",
    "        self.errors_log.append(error_entry)\n",
    "        logger.error(f\"Logged error: {error_message}\")\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get monitoring statistics\"\"\"\n",
    "        return {\n",
    "            'total_predictions': len(self.predictions_log), 'total_errors': len(self.errors_log),\n",
    "            'latest_prediction': self.predictions_log[-1] if self.predictions_log else None\n",
    "        }\n",
    "\n",
    "monitor = ModelMonitor()\n",
    "# Simulate monitoring\n",
    "for i in range(5):\n",
    "    features = X_test[i:i+1]\n",
    "    pred = model.predict(features)[0]\n",
    "    actual = y_test.iloc[i] if hasattr(y_test, \"iloc\") else y_test[i]\n",
    "    monitor.log_prediction(features[0], pred, actual)\n",
    "\n",
    "stats = monitor.get_stats()\n",
    "print(f\"\\nMonitoring Statistics:\")\n",
    "print(f\"  Total predictions: {stats['total_predictions']}\")\n",
    "print(f\"  Total errors: {stats['total_errors']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. DEPLOYMENT CHECKLIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T16:20:36.107959Z",
     "iopub.status.busy": "2026-01-24T16:20:36.107900Z",
     "iopub.status.idle": "2026-01-24T16:20:36.109635Z",
     "shell.execute_reply": "2026-01-24T16:20:36.109482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5. Deployment Checklist\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Deployment Checklist:\n",
      "  âœ“ Model trained and validated\n",
      "  âœ“ Model saved and versioned\n",
      "  âœ“ Metadata documented\n",
      "  âœ“ Error handling implemented\n",
      "  âœ“ Logging configured\n",
      "  âœ“ Monitoring set up\n",
      "  âœ“ Documentation created\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n5. Deployment Checklist\")\n",
    "print(\"-\" * 70)\n",
    "checklist = {\n",
    "'Model trained and validated': True, 'Model saved and versioned': True,\n",
    "'Metadata documented': True,\n",
    "'Error handling implemented': True,\n",
    "'Logging configured': True,\n",
    "'Monitoring set up': True,\n",
    "'Documentation created': True\n",
    "}\n",
    "print(\"\\nDeployment Checklist:\")\n",
    "for item, status in checklist.items():\n",
    "    status_symbol = \"âœ“\" if status else \"âœ—\"\n",
    "    print(f\"  {status_symbol} {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. SUMMARY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T16:20:36.110687Z",
     "iopub.status.busy": "2026-01-24T16:20:36.110635Z",
     "iopub.status.idle": "2026-01-24T16:20:36.112526Z",
     "shell.execute_reply": "2026-01-24T16:20:36.112343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================================================================\n",
      "Summary\n",
      "======================================================================\n",
      "\n",
      "Key Concepts Covered:\n",
      "1. Model serialization and saving\n",
      "2. Deployment functions\n",
      "3. Monitoring and logging\n",
      "4. Version control\n",
      "5. Deployment best practices\n",
      "\n",
      "======================================================================\n",
      "Course Complete! All 19 examples created successfully!\n",
      " !      19 !\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"Summary\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nKey Concepts Covered:\")\n",
    "print(\"1. Model serialization and saving\")\n",
    "print(\"2. Deployment functions\")\n",
    "print(\"3. Monitoring and logging\")\n",
    "print(\"4. Version control\")\n",
    "print(\"5. Deployment best practices\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Course Complete! All 19 examples created successfully!\")\n",
    "print(\" !      19 !\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
