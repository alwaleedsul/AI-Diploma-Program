{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Distributed Data Processing with PySpark\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand PySpark for distributed data processing\n",
    "- Implement PySpark to perform distributed data processing on large datasets\n",
    "- Integrate PySpark with existing Python workflows\n",
    "- Compare PySpark with Dask for distributed computing\n",
    "- Apply PySpark to real-world large dataset scenarios\n",
    "\n",
    "## üîó Prerequisites\n",
    "\n",
    "- ‚úÖ Basic Python\n",
    "- ‚úÖ Basic NumPy/Pandas\n",
    "- ‚úÖ Understanding of distributed computing concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 05, Unit 5**:\n",
    "- Data Processing using PySpark: Implementing PySpark to perform distributed data processing on large datasets and integrating with existing Python workflows\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 5 Practical Content\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Story | ÿßŸÑŸÇÿµÿ©\n",
    "\n",
    "**BEFORE**: You know Dask for distributed computing but don't know Spark for big data processing.\n",
    "\n",
    "**AFTER**: You'll learn PySpark - industry-standard framework for distributed big data processing and analytics!\n",
    "\n",
    "**Why this matters**: Distributed Data Processing with PySpark is essential for building complete, professional data science solutions!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 5 - Example 03: Distributed Data Processing with PySpark | ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖŸàÿ≤ÿπÿ© ŸÖÿπ PySpark\n",
    "\n",
    "## üîó Building on Example 02 | ÿßŸÑÿ®ŸÜÿßÿ° ÿπŸÑŸâ ÿßŸÑŸÖÿ´ÿßŸÑ 14\n",
    "\n",
    "**From Example 02 (Dask):**\n",
    "- We learned Dask for distributed computing in Python\n",
    "- Dask works well for Python-native workflows\n",
    "- But for enterprise-scale distributed processing, we need Apache Spark\n",
    "\n",
    "**This notebook introduces:**\n",
    "- **PySpark** - Python API for Apache Spark\n",
    "- **Distributed data processing** on large datasets\n",
    "- **Integration** with existing Python workflows\n",
    "- **Enterprise-scale** distributed computing\n",
    "\n",
    "**This complements Dask with enterprise distributed computing!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:28:32.111996Z",
     "iopub.status.busy": "2026-01-26T15:28:32.111925Z",
     "iopub.status.idle": "2026-01-26T15:28:32.435693Z",
     "shell.execute_reply": "2026-01-26T15:28:32.435460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  PySpark not available. Install Spark for distributed processing:\n",
      "   Note: Requires Apache Spark installation\n",
      "   Continuing with pandas simulation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "# Try importing PySpark (requires Spark installation)\n",
    "# PySpark may not be available on all systems - we'll provide fallback\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "    PYSPARK_AVAILABLE = True\n",
    "    print(\"‚úÖ PySpark imported successfully!\")\n",
    "except ImportError:\n",
    "    PYSPARK_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  PySpark not available. Install Spark for distributed processing:\")\n",
    "    print(\"   Note: Requires Apache Spark installation\")\n",
    "    print(\"   Continuing with pandas simulation...\")\n",
    "\n",
    "# Always import pandas/numpy for fallback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to PySpark | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ£ŸàŸÑ: ŸÖŸÇÿØŸÖÿ© ÿ•ŸÑŸâ PySpark\n",
    "\n",
    "**PySpark** is the Python API for Apache Spark, a unified analytics engine for large-scale data processing.\n",
    "\n",
    "**Key Features:**\n",
    "- Distributed data processing across clusters\n",
    "- In-memory computing for faster processing\n",
    "- Integration with Hadoop ecosystem\n",
    "- Support for SQL, streaming, and machine learning\n",
    "\n",
    "**When to use PySpark vs Dask:**\n",
    "- **PySpark**: Enterprise clusters, Hadoop integration, SQL queries, streaming\n",
    "- **Dask**: Python-native workflows, smaller clusters, NumPy/Pandas compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:28:32.436925Z",
     "iopub.status.busy": "2026-01-26T15:28:32.436819Z",
     "iopub.status.idle": "2026-01-26T15:28:32.438606Z",
     "shell.execute_reply": "2026-01-26T15:28:32.438427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Example 03: Distributed Data Processing with PySpark\n",
      "======================================================================\n",
      "\n",
      "üìö Prerequisites: Example 02 (Dask) completed\n",
      "üîó This notebook covers PySpark for distributed data processing\n",
      "üéØ Goal: Master PySpark for enterprise-scale distributed computing\n",
      "\n",
      "‚ö†Ô∏è  PySpark not available - Using pandas simulation\n",
      "   (Install Spark to use actual distributed processing)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Example 03: Distributed Data Processing with PySpark\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüìö Prerequisites: Example 02 (Dask) completed\")\n",
    "print(\"üîó This notebook covers PySpark for distributed data processing\")\n",
    "print(\"üéØ Goal: Master PySpark for enterprise-scale distributed computing\\n\")\n",
    "\n",
    "if PYSPARK_AVAILABLE:\n",
    "    print(\"‚úÖ PySpark is available - Using real distributed processing\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  PySpark not available - Using pandas simulation\")\n",
    "    print(\"   (Install Spark to use actual distributed processing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating Spark Session | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ´ÿßŸÜŸä: ÿ•ŸÜÿ¥ÿßÿ° ÿ¨ŸÑÿ≥ÿ© Spark\n",
    "\n",
    "**SparkSession** is the entry point for PySpark applications.\n",
    "\n",
    "**Why SparkSession?**\n",
    "- Manages Spark context and configuration\n",
    "- Provides unified API for Spark SQL, DataFrames, and Datasets\n",
    "- Handles distributed execution across cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:28:32.439553Z",
     "iopub.status.busy": "2026-01-26T15:28:32.439496Z",
     "iopub.status.idle": "2026-01-26T15:28:32.441047Z",
     "shell.execute_reply": "2026-01-26T15:28:32.440867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  PySpark simulation mode - Using pandas for demonstration\n",
      "   (Same operations, but single-machine processing)\n"
     ]
    }
   ],
   "source": [
    "if PYSPARK_AVAILABLE:\n",
    "    # Create SparkSession for distributed processing\n",
    "    spark = (SparkSession.builder\n",
    "             .appName(\"Course05_PySpark_Example\")\n",
    "             .master(\"local[*]\")  # Use all available cores locally\n",
    "             .getOrCreate())\n",
    "    \n",
    "    print(\"‚úÖ SparkSession created successfully!\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "    print(f\"Spark master: {spark.sparkContext.master}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  PySpark simulation mode - Using pandas for demonstration\")\n",
    "    print(\"   (Same operations, but single-machine processing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating Sample Data | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ´ÿßŸÑÿ´: ÿ•ŸÜÿ¥ÿßÿ° ÿ®ŸäÿßŸÜÿßÿ™ ÿ™ÿ¨ÿ±Ÿäÿ®Ÿäÿ©\n",
    "\n",
    "We'll create a large dataset to demonstrate PySpark's distributed processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:28:32.441914Z",
     "iopub.status.busy": "2026-01-26T15:28:32.441861Z",
     "iopub.status.idle": "2026-01-26T15:28:32.448074Z",
     "shell.execute_reply": "2026-01-26T15:28:32.447894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample data created: 100,000 rows\n",
      "Data shape: (100000, 5)\n",
      "\n",
      "First few rows:\n",
      "   id    value1  value2 category      score\n",
      "0   0  0.496714      36        C  99.661504\n",
      "1   1 -0.138264      18        C  58.729074\n",
      "2   2  0.647689       5        A  72.476852\n",
      "3   3  1.523030      46        C  90.637787\n",
      "4   4 -0.234153      86        B  30.715934\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data for demonstration\n",
    "np.random.seed(42)\n",
    "n_rows = 100000  # Large dataset to show distributed processing benefits\n",
    "\n",
    "sample_data = pd.DataFrame({\n",
    "    'id': range(n_rows),\n",
    "    'value1': np.random.randn(n_rows),\n",
    "    'value2': np.random.randint(1, 100, n_rows),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], n_rows),\n",
    "    'score': np.random.uniform(0, 100, n_rows)\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Sample data created: {len(sample_data):,} rows\")\n",
    "print(f\"Data shape: {sample_data.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(sample_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Loading Data into PySpark | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ±ÿßÿ®ÿπ: ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ•ŸÑŸâ PySpark\n",
    "\n",
    "PySpark DataFrames are distributed collections of data organized into named columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:28:32.449041Z",
     "iopub.status.busy": "2026-01-26T15:28:32.448984Z",
     "iopub.status.idle": "2026-01-26T15:28:32.451614Z",
     "shell.execute_reply": "2026-01-26T15:28:32.451441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  PySpark simulation - Using pandas DataFrame\n",
      "Data shape: (100000, 5)\n",
      "\n",
      "First few rows:\n",
      "   id    value1  value2 category      score\n",
      "0   0  0.496714      36        C  99.661504\n",
      "1   1 -0.138264      18        C  58.729074\n",
      "2   2  0.647689       5        A  72.476852\n",
      "3   3  1.523030      46        C  90.637787\n",
      "4   4 -0.234153      86        B  30.715934\n"
     ]
    }
   ],
   "source": [
    "if PYSPARK_AVAILABLE:\n",
    "    # Save to CSV first (PySpark can read from various sources)\n",
    "    csv_path = 'sample_data_pyspark.csv'\n",
    "    sample_data.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Load into PySpark DataFrame\n",
    "    df_spark = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "    \n",
    "    print(\"‚úÖ Data loaded into PySpark DataFrame!\")\n",
    "    print(f\"Number of partitions: {df_spark.rdd.getNumPartitions()}\")\n",
    "    print(f\"Total rows: {df_spark.count():,}\")\n",
    "    print(\"\\nSchema:\")\n",
    "    df_spark.printSchema()\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    df_spark.show(5)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  PySpark simulation - Using pandas DataFrame\")\n",
    "    df_pandas = sample_data.copy()\n",
    "    print(f\"Data shape: {df_pandas.shape}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df_pandas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Distributed Data Processing Operations | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿÆÿßŸÖÿ≥: ÿπŸÖŸÑŸäÿßÿ™ ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖŸàÿ≤ÿπÿ©\n",
    "\n",
    "PySpark performs operations in a distributed manner across partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:28:32.452505Z",
     "iopub.status.busy": "2026-01-26T15:28:32.452443Z",
     "iopub.status.idle": "2026-01-26T15:28:32.459736Z",
     "shell.execute_reply": "2026-01-26T15:28:32.459555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PART 5: Distributed Data Processing Operations\n",
      "======================================================================\n",
      "\n",
      "1. Filtering data:\n",
      "   Rows where value2 > 50: 49,280\n",
      "\n",
      "2. Aggregations:\n",
      "   Grouped by category:\n",
      "          avg_score  count\n",
      "category                  \n",
      "A         49.987463  25002\n",
      "B         50.131388  24837\n",
      "C         49.840321  25078\n",
      "D         50.177236  25083\n",
      "\n",
      "3. Transformations:\n",
      "   Added new column 'value1_squared'\n",
      "   id    value1  value1_squared\n",
      "0   0  0.496714        0.246725\n",
      "1   1 -0.138264        0.019117\n",
      "2   2  0.647689        0.419500\n",
      "3   3  1.523030        2.319620\n",
      "4   4 -0.234153        0.054828\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 5: Distributed Data Processing Operations\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if PYSPARK_AVAILABLE:\n",
    "    # Filtering (distributed)\n",
    "    print(\"\\n1. Filtering data (distributed across partitions):\")\n",
    "    filtered = df_spark.filter(df_spark['value2'] > 50)\n",
    "    print(f\"   Rows where value2 > 50: {filtered.count():,}\")\n",
    "    \n",
    "    # Aggregations (distributed)\n",
    "    print(\"\\n2. Aggregations (distributed):\")\n",
    "    aggregated = df_spark.groupBy('category').agg(\n",
    "        F.avg('score').alias('avg_score'),\n",
    "        F.count('*').alias('count')\n",
    "    )\n",
    "    print(\"   Grouped by category:\")\n",
    "    aggregated.show()\n",
    "    \n",
    "    # Transformations (distributed)\n",
    "    print(\"\\n3. Transformations (distributed):\")\n",
    "    transformed = df_spark.withColumn('value1_squared', df_spark['value1'] ** 2)\n",
    "    print(\"   Added new column 'value1_squared'\")\n",
    "    transformed.select('id', 'value1', 'value1_squared').show(5)\n",
    "else:\n",
    "    # Pandas simulation\n",
    "    print(\"\\n1. Filtering data:\")\n",
    "    filtered = df_pandas[df_pandas['value2'] > 50]\n",
    "    print(f\"   Rows where value2 > 50: {len(filtered):,}\")\n",
    "    \n",
    "    print(\"\\n2. Aggregations:\")\n",
    "    aggregated = df_pandas.groupby('category').agg({\n",
    "        'score': 'mean',\n",
    "        'id': 'count'\n",
    "    }).rename(columns={'score': 'avg_score', 'id': 'count'})\n",
    "    print(\"   Grouped by category:\")\n",
    "    print(aggregated)\n",
    "    \n",
    "    print(\"\\n3. Transformations:\")\n",
    "    df_pandas['value1_squared'] = df_pandas['value1'] ** 2\n",
    "    print(\"   Added new column 'value1_squared'\")\n",
    "    print(df_pandas[['id', 'value1', 'value1_squared']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Performance Comparison | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ≥ÿßÿØÿ≥: ŸÖŸÇÿßÿ±ŸÜÿ© ÿßŸÑÿ£ÿØÿßÿ°\n",
    "\n",
    "Comparing PySpark (distributed) vs Pandas (single-machine) performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:28:32.460818Z",
     "iopub.status.busy": "2026-01-26T15:28:32.460749Z",
     "iopub.status.idle": "2026-01-26T15:28:32.464565Z",
     "shell.execute_reply": "2026-01-26T15:28:32.464387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PART 6: Performance Comparison\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è  Simulated PySpark (distributed) performance:\n",
      "   (PySpark would distribute this across multiple cores/machines)\n",
      "\n",
      "‚è±Ô∏è  Testing Pandas (single-machine) performance...\n",
      "   ‚úÖ Pandas (single-machine): 0.0018 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 6: Performance Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test complex operation: filtering + aggregation + transformation\n",
    "if PYSPARK_AVAILABLE:\n",
    "    print(\"\\n‚è±Ô∏è  Testing PySpark (distributed) performance...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result_spark = (df_spark\n",
    "                .filter(df_spark['value2'] > 50)\n",
    "                .groupBy('category')\n",
    "                .agg(F.avg('score').alias('avg_score'))\n",
    "                .collect())\n",
    "    \n",
    "    spark_time = time.time() - start_time\n",
    "    print(f\"   ‚úÖ PySpark (distributed): {spark_time:.4f} seconds\")\n",
    "    print(f\"   Results: {len(result_spark)} groups\")\n",
    "else:\n",
    "    print(\"\\n‚è±Ô∏è  Simulated PySpark (distributed) performance:\")\n",
    "    print(\"   (PySpark would distribute this across multiple cores/machines)\")\n",
    "    spark_time = 0.05  # Simulated faster time\n",
    "\n",
    "# Pandas (single-machine)\n",
    "print(\"\\n‚è±Ô∏è  Testing Pandas (single-machine) performance...\")\n",
    "start_time = time.time()\n",
    "\n",
    "result_pandas = (df_pandas[df_pandas['value2'] > 50]\n",
    "              .groupby('category')['score']\n",
    "              .mean())\n",
    "\n",
    "pandas_time = time.time() - start_time\n",
    "print(f\"   ‚úÖ Pandas (single-machine): {pandas_time:.4f} seconds\")\n",
    "\n",
    "# Comparison\n",
    "if PYSPARK_AVAILABLE:\n",
    "    speedup = pandas_time / spark_time\n",
    "    print(f\"\\nüìä Speedup: {speedup:.2f}x faster with PySpark\")\n",
    "    print(\"   (On larger datasets/clusters, PySpark shows even better performance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Integration with Python Workflows | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ≥ÿßÿ®ÿπ: ÿßŸÑÿ™ŸÉÿßŸÖŸÑ ŸÖÿπ ÿ≥Ÿäÿ± ÿπŸÖŸÑ Python\n",
    "\n",
    "PySpark integrates seamlessly with existing Python workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:28:32.465500Z",
     "iopub.status.busy": "2026-01-26T15:28:32.465442Z",
     "iopub.status.idle": "2026-01-26T15:28:32.467235Z",
     "shell.execute_reply": "2026-01-26T15:28:32.467067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PART 7: Integration with Python Workflows\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  PySpark simulation - Integration demonstration:\n",
      "   In real PySpark, you can:\n",
      "   1. Process large datasets distributed across cluster\n",
      "   2. Convert to Pandas for smaller subsets\n",
      "   3. Use with scikit-learn, pandas, numpy\n",
      "   4. Write results back to distributed storage\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 7: Integration with Python Workflows\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if PYSPARK_AVAILABLE:\n",
    "    # Convert PySpark DataFrame to Pandas (for integration)\n",
    "    print(\"\\n1. Converting PySpark DataFrame to Pandas:\")\n",
    "    df_pandas_from_spark = df_spark.limit(1000).toPandas()\n",
    "    print(f\"   Converted {len(df_pandas_from_spark):,} rows to Pandas\")\n",
    "    print(\"   Now you can use pandas/numpy/scikit-learn on this data\")\n",
    "    \n",
    "    # Use with existing Python libraries\n",
    "    print(\"\\n2. Using with existing Python libraries:\")\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df_pandas_from_spark[['value1', 'score']])\n",
    "    print(f\"   Scaled data shape: {scaled_data.shape}\")\n",
    "    print(\"   ‚úÖ PySpark integrates with scikit-learn, pandas, numpy!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  PySpark simulation - Integration demonstration:\")\n",
    "    print(\"   In real PySpark, you can:\")\n",
    "    print(\"   1. Process large datasets distributed across cluster\")\n",
    "    print(\"   2. Convert to Pandas for smaller subsets\")\n",
    "    print(\"   3. Use with scikit-learn, pandas, numpy\")\n",
    "    print(\"   4. Write results back to distributed storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Summary | ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ´ÿßŸÖŸÜ: ÿßŸÑŸÖŸÑÿÆÿµ\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **PySpark** provides distributed data processing for large datasets\n",
    "2. **Distributed operations** run across multiple cores/machines\n",
    "3. **Integration** with Python workflows (pandas, scikit-learn, numpy)\n",
    "4. **Performance** scales with cluster size\n",
    "\n",
    "**When to use PySpark:**\n",
    "- Very large datasets (billions of rows)\n",
    "- Enterprise clusters and Hadoop integration\n",
    "- SQL queries on distributed data\n",
    "- Streaming data processing\n",
    "\n",
    "**When to use Dask (from Example 02):**\n",
    "- Python-native workflows\n",
    "- Smaller clusters\n",
    "- NumPy/Pandas compatibility\n",
    "- Interactive data science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:28:32.468056Z",
     "iopub.status.busy": "2026-01-26T15:28:32.468008Z",
     "iopub.status.idle": "2026-01-26T15:28:32.469795Z",
     "shell.execute_reply": "2026-01-26T15:28:32.469615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SUMMARY: PySpark for Distributed Data Processing\n",
      "======================================================================\n",
      "\n",
      "‚úÖ What you learned:\n",
      "   1. PySpark for distributed data processing\n",
      "   2. Creating SparkSession and DataFrames\n",
      "   3. Distributed operations (filtering, aggregation, transformation)\n",
      "   4. Integration with Python workflows\n",
      "   5. Performance benefits of distributed processing\n",
      "\n",
      "üîó Next steps:\n",
      "   - Example 04: RAPIDS workflows (GPU acceleration)\n",
      "   - Example 04: Production pipelines\n",
      "   - Example 05: Performance optimization\n",
      "\n",
      "üí° To use PySpark:\n",
      "   1. Install Apache Spark: https://spark.apache.org/downloads.html\n",
      "   2. Install PySpark: pip install pyspark\n",
      "   3. Run this notebook again for distributed processing\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY: PySpark for Distributed Data Processing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ What you learned:\")\n",
    "print(\"   1. PySpark for distributed data processing\")\n",
    "print(\"   2. Creating SparkSession and DataFrames\")\n",
    "print(\"   3. Distributed operations (filtering, aggregation, transformation)\")\n",
    "print(\"   4. Integration with Python workflows\")\n",
    "print(\"   5. Performance benefits of distributed processing\")\n",
    "\n",
    "print(\"\\nüîó Next steps:\")\n",
    "print(\"   - Example 04: RAPIDS workflows (GPU acceleration)\")\n",
    "print(\"   - Example 04: Production pipelines\")\n",
    "print(\"   - Example 05: Performance optimization\")\n",
    "\n",
    "if PYSPARK_AVAILABLE:\n",
    "    spark.stop()\n",
    "    print(\"\\n‚úÖ SparkSession stopped\")\n",
    "else:\n",
    "    print(\"\\nüí° To use PySpark:\")\n",
    "    print(\"   1. Install Apache Spark: https://spark.apache.org/downloads.html\")\n",
    "    print(\"   2. Install PySpark: pip install pyspark\")\n",
    "    print(\"   3. Run this notebook again for distributed processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}