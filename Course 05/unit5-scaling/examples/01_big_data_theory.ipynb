{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Big Data Concepts & Distributed Computing Theory\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers **theoretical content** from **DETAILED_UNIT_DESCRIPTIONS.md** (Unit 5: Extending the Scope of Data Science):\n",
    "- Big Data characteristics (4 Vs)\n",
    "- Big Data technologies and challenges\n",
    "- Distributed systems, parallel computing, and MapReduce\n",
    "- Fault tolerance\n",
    "\n",
    "These concepts underpin Dask, PySpark, and RAPIDS workflows covered in later examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Big Data: The Four Vs\n",
    "\n",
    "**Volume** â€“ Scale of data (TB, PB). Traditional single-machine tools (e.g. pandas on one laptop) cannot store or process it.\n",
    "\n",
    "**Variety** â€“ Mixed types: structured (tables), semi-structured (JSON, logs), unstructured (text, images, video). Different storage and processing needs.\n",
    "\n",
    "**Velocity** â€“ Speed of data generation and ingestion (streaming, real-time). Batch processing alone is insufficient.\n",
    "\n",
    "**Veracity** â€“ Quality, completeness, and trustworthiness. Big data often includes noise, missing values, and inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Story | Ø§Ù„Ù‚ØµØ©\n",
    "\n",
    "**BEFORE**: You can work with small datasets but don't understand challenges of big data.\n",
    "\n",
    "**AFTER**: You'll understand big data concepts: volume, velocity, variety, and strategies for handling large-scale data!\n",
    "\n",
    "**Why this matters**: Big Data Concepts & Distributed Computing Theory is essential for building complete, professional data science solutions!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Big Data Technologies & Challenges\n",
    "\n",
    "**Technologies:** Distributed storage (HDFS, S3), distributed processing (Apache Spark, Dask), message queues (Kafka), GPU acceleration (RAPIDS).\n",
    "\n",
    "**Challenges:**\n",
    "- Cost and complexity of distributed clusters\n",
    "- Data locality and network bottlenecks\n",
    "- Consistency, security, and governance at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distributed Computing: MapReduce & Parallel Processing\n",
    "\n",
    "**MapReduce** is a programming model for processing large datasets in parallel:\n",
    "- **Map:** Apply a function to each partition; produce keyâ€“value pairs.\n",
    "- **Shuffle:** Group by key.\n",
    "- **Reduce:** Aggregate per key.\n",
    "\n",
    "Spark and Dask implement MapReduce-style operations (e.g. `groupby`, `apply`) over distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Inputs & ðŸ“¤ Outputs | Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø®Ø±Ø¬Ø§Øª\n",
    "\n",
    "**Inputs:** What we use in this notebook\n",
    "\n",
    "- Concepts: 4 Vs, MapReduce, fault tolerance\n- Optional MapReduce-style code\n",
    "\n",
    "**Outputs:** What you'll see when you run the cells\n",
    "\n",
    "- Theory recap\n- Optional code demos\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:28:27.993966Z",
     "iopub.status.busy": "2026-01-26T15:28:27.993668Z",
     "iopub.status.idle": "2026-01-26T15:28:28.005057Z",
     "shell.execute_reply": "2026-01-26T15:28:28.004595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapReduce-style (key, count): {'A': 4, 'B': 4, 'C': 2}\n"
     ]
    }
   ],
   "source": [
    "# Conceptual MapReduce-style pattern (single-machine illustration)\n",
    "from collections import defaultdict\n",
    "\n",
    "def map_fn(item):\n",
    "    \"\"\"Map: emit (category, 1) for each item.\"\"\"\n",
    "    category = item.get(\"category\", \"unknown\")\n",
    "    return (category, 1)\n",
    "\n",
    "def reduce_fn(key, values):\n",
    "    \"\"\"Reduce: sum counts per key.\"\"\"\n",
    "    return (key, sum(values))\n",
    "\n",
    "data = [{\"id\": i, \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"][i % 5]} for i in range(10)]\n",
    "mapped = [map_fn(d) for d in data]\n",
    "groups = defaultdict(list)\n",
    "for k, v in mapped:\n",
    "    groups[k].append(v)\n",
    "reduced = [reduce_fn(k, vals) for k, vals in groups.items()]\n",
    "print(\"MapReduce-style (key, count):\", dict(reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fault Tolerance\n",
    "\n",
    "In distributed systems, nodes can fail. **Fault tolerance** means the system continues to work:\n",
    "- **Replication:** Store data on multiple nodes; if one fails, others serve it.\n",
    "- **Checkpointing:** Save intermediate results so tasks can be re-run after failure.\n",
    "- **Idempotency:** Re-running a task produces the same result; safe to retry.\n",
    "\n",
    "Spark and Dask provide fault-tolerant execution; RAPIDS typically assumes reliable GPU hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Example 02:** Dask for distributed computing\n",
    "- **Example 03:** PySpark for distributed data processing\n",
    "- **Example 04:** RAPIDS for GPU-accelerated workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}