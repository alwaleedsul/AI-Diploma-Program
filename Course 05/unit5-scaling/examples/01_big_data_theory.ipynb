{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 5 - Big Data Concepts & Distributed Computing Theory\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers **theoretical content** from **DETAILED_UNIT_DESCRIPTIONS.md** (Unit 5: Extending the Scope of Data Science):\n",
    "- Big Data characteristics (4 Vs)\n",
    "- Big Data technologies and challenges\n",
    "- Distributed systems, parallel computing, and MapReduce\n",
    "- Fault tolerance\n",
    "\n",
    "These concepts underpin Dask, PySpark, and RAPIDS workflows covered in later examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Big Data: The Four Vs\n",
    "\n",
    "**Volume** – Scale of data (TB, PB). Traditional single-machine tools (e.g. pandas on one laptop) cannot store or process it.\n",
    "\n",
    "**Variety** – Mixed types: structured (tables), semi-structured (JSON, logs), unstructured (text, images, video). Different storage and processing needs.\n",
    "\n",
    "**Velocity** – Speed of data generation and ingestion (streaming, real-time). Batch processing alone is insufficient.\n",
    "\n",
    "**Veracity** – Quality, completeness, and trustworthiness. Big data often includes noise, missing values, and inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Big Data Technologies & Challenges\n",
    "\n",
    "**Technologies:** Distributed storage (HDFS, S3), distributed processing (Apache Spark, Dask), message queues (Kafka), GPU acceleration (RAPIDS).\n",
    "\n",
    "**Challenges:**\n",
    "- Cost and complexity of distributed clusters\n",
    "- Data locality and network bottlenecks\n",
    "- Consistency, security, and governance at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distributed Computing: MapReduce & Parallel Processing\n",
    "\n",
    "**MapReduce** is a programming model for processing large datasets in parallel:\n",
    "- **Map:** Apply a function to each partition; produce key–value pairs.\n",
    "- **Shuffle:** Group by key.\n",
    "- **Reduce:** Aggregate per key.\n",
    "\n",
    "Spark and Dask implement MapReduce-style operations (e.g. `groupby`, `apply`) over distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual MapReduce-style pattern (single-machine illustration)\n",
    "from collections import defaultdict\n",
    "\n",
    "def map_fn(item):\n",
    "    \"\"\"Map: emit (category, 1) for each item.\"\"\"\n",
    "    category = item.get(\"category\", \"unknown\")\n",
    "    return (category, 1)\n",
    "\n",
    "def reduce_fn(key, values):\n",
    "    \"\"\"Reduce: sum counts per key.\"\"\"\n",
    "    return (key, sum(values))\n",
    "\n",
    "data = [{\"id\": i, \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"][i % 5]} for i in range(10)]\n",
    "mapped = [map_fn(d) for d in data]\n",
    "groups = defaultdict(list)\n",
    "for k, v in mapped:\n",
    "    groups[k].append(v)\n",
    "reduced = [reduce_fn(k, vals) for k, vals in groups.items()]\n",
    "print(\"MapReduce-style (key, count):\", dict(reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fault Tolerance\n",
    "\n",
    "In distributed systems, nodes can fail. **Fault tolerance** means the system continues to work:\n",
    "- **Replication:** Store data on multiple nodes; if one fails, others serve it.\n",
    "- **Checkpointing:** Save intermediate results so tasks can be re-run after failure.\n",
    "- **Idempotency:** Re-running a task produces the same result; safe to retry.\n",
    "\n",
    "Spark and Dask provide fault-tolerant execution; RAPIDS typically assumes reliable GPU hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Example 02:** Dask for distributed computing\n",
    "- **Example 03:** PySpark for distributed data processing\n",
    "- **Example 04:** RAPIDS for GPU-accelerated workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
