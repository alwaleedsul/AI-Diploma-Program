{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. Data Pipeline Automation | Ø£ØªÙ…ØªØ© Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Build automated data processing pipelines\n",
    "- Schedule and orchestrate data workflows\n",
    "- Handle errors and retries in pipelines\n",
    "- Monitor pipeline execution and performance\n",
    "- Automate repetitive data tasks\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Example 17: Production Pipelines (understand pipeline structure)\n",
    "- âœ… Example 18: Performance Optimization (optimize pipelines)\n",
    "- âœ… Understanding of data processing workflows\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 05, Unit 5** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 17: Production Pipelines** - Understand pipeline structure!\n",
    "- âœ… **Example 18: Performance Optimization** - Optimize before automating!\n",
    "- âœ… **Understanding of workflows**: What are data processing steps?\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding pipeline design\n",
    "- Knowing what to automate\n",
    "- Understanding scheduling and orchestration\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the FINAL example in Unit 5: Scaling and Production**\n",
    "\n",
    "**Why pipeline automation?**\n",
    "- **After** building pipelines, we automate them\n",
    "- **Automation** saves time and reduces errors\n",
    "- **Scheduling** ensures pipelines run regularly\n",
    "- **Final step** in production ML systems\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 17: Production Pipelines (pipeline structure)\n",
    "- ğŸ““ Example 18: Performance Optimization (optimized pipelines)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Production ML systems\n",
    "- ğŸ““ Automated data science workflows\n",
    "\n",
    "**Why this order?**\n",
    "1. Automation comes after building and optimizing\n",
    "2. Essential for production systems\n",
    "3. Final step in scaling data science\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: From Manual to Automatic | Ø§Ù„Ù‚ØµØ©: Ù…Ù† Ø§Ù„ÙŠØ¯ÙˆÙŠ Ø¥Ù„Ù‰ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ\n",
    "\n",
    "Imagine you have a task you do daily (manual pipeline). **Before** automation, you do it manually every day. **After** automation, it runs automatically - saves time, reduces errors!\n",
    "\n",
    "Same with data pipelines: **After** building pipelines, we automate - schedule runs, handle errors, monitor execution. **After** automation, pipelines run reliably!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Pipeline Automation Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… Ø£ØªÙ…ØªØ© Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨\n",
    "\n",
    "Pipeline automation is essential because:\n",
    "- **Efficiency**: Saves time on repetitive tasks\n",
    "- **Reliability**: Reduces human errors\n",
    "- **Scalability**: Handles large workloads\n",
    "- **Production**: Essential for production systems\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: What should I automate?**\n",
    "  - Answer: Repetitive tasks, regular data processing, model retraining\n",
    "  - Example: Daily data updates, weekly model retraining\n",
    "  - Rule: If you do it regularly, automate it!\n",
    "  \n",
    "- **Q: How do I handle errors in automated pipelines?**\n",
    "  - Answer: Use try/except, logging, retries, alerts\n",
    "  - Example: Retry failed steps, log errors, send alerts\n",
    "  - Benefit: Pipelines continue even when errors occur\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Data pipeline automation** transforms manual data processing into automated, scheduled workflows. This ensures reliable, efficient data processing in production environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. Data Pipeline Automation | Ø£ØªÙ…ØªØ© Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**All concepts are explained in the code comments below - you can learn everything from this notebook alone!**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Automating Data Workflows | Ø£ØªÙ…ØªØ© Ø³ÙŠØ± Ø¹Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**Manual data processing is slow and error-prone!**\n",
    "- Processing data manually takes too much time\n",
    "- Errors happen when steps are forgotten\n",
    "- We need automated, reliable pipelines\n",
    "\n",
    "**This notebook teaches pipeline automation!**\n",
    "- We'll learn **pipeline design** - structure data workflows\n",
    "- We'll learn **scheduling** - run pipelines automatically\n",
    "- We'll learn **error handling** - make pipelines robust\n",
    "- We'll learn **monitoring** - track pipeline health\n",
    "\n",
    "**This enables production-ready data pipelines!**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Design automated data processing pipelines\n",
    "2. Implement error handling and retries\n",
    "3. Schedule pipeline execution\n",
    "4. Monitor pipeline performance and health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T00:53:05.271252Z",
     "iopub.status.busy": "2026-01-24T00:53:05.271196Z",
     "iopub.status.idle": "2026-01-24T00:53:05.451092Z",
     "shell.execute_reply": "2026-01-24T00:53:05.450901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported!\n",
      "\n",
      "ğŸ“š This notebook covers:\n",
      "   - Pipeline design\n",
      "   - Error handling\n",
      "   - Pipeline monitoring\n",
      "   - Automation strategies\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(\"\\nğŸ“š This notebook covers:\")\n",
    "print(\"   - Pipeline design\")\n",
    "print(\"   - Error handling\")\n",
    "print(\"   - Pipeline monitoring\")\n",
    "print(\"   - Automation strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T00:53:05.463545Z",
     "iopub.status.busy": "2026-01-24T00:53:05.463438Z",
     "iopub.status.idle": "2026-01-24T00:53:05.465658Z",
     "shell.execute_reply": "2026-01-24T00:53:05.465476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "1. Pipeline Design | ØªØµÙ…ÙŠÙ… Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨\n",
      "======================================================================\n",
      "\n",
      "âœ… Pipeline steps defined!\n",
      "   - Extract: Get data from source\n",
      "   - Transform: Process and clean data\n",
      "   - Load: Save processed data\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define Pipeline Steps\n",
    "print(\"=\" * 70)\n",
    "print(\"1. Pipeline Design | ØªØµÙ…ÙŠÙ… Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def step1_extract():\n",
    "    \"\"\"Extract data from source\"\"\"\n",
    "    print(\"  â†’ Step 1: Extracting data...\")\n",
    "    # Simulate data extraction\n",
    "    data = pd.DataFrame({\n",
    "        'id': range(1, 101),\n",
    "        'value': np.random.rand(100) * 100\n",
    "    })\n",
    "    time.sleep(0.1)  # Simulate processing time\n",
    "    print(f\"     âœ“ Extracted {len(data)} rows\")\n",
    "    return data\n",
    "\n",
    "def step2_transform(data):\n",
    "    \"\"\"Transform data\"\"\"\n",
    "    print(\"  â†’ Step 2: Transforming data...\")\n",
    "    data['value_normalized'] = (data['value'] - data['value'].mean()) / data['value'].std()\n",
    "    time.sleep(0.1)\n",
    "    print(f\"     âœ“ Transformed {len(data)} rows\")\n",
    "    return data\n",
    "\n",
    "def step3_load(data):\n",
    "    \"\"\"Load data to destination\"\"\"\n",
    "    print(\"  â†’ Step 3: Loading data...\")\n",
    "    # Simulate saving\n",
    "    time.sleep(0.1)\n",
    "    print(f\"     âœ“ Loaded {len(data)} rows\")\n",
    "    return True\n",
    "\n",
    "print(\"\\nâœ… Pipeline steps defined!\")\n",
    "print(\"   - Extract: Get data from source\")\n",
    "print(\"   - Transform: Process and clean data\")\n",
    "print(\"   - Load: Save processed data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-24T00:53:05.466492Z",
     "iopub.status.busy": "2026-01-24T00:53:05.466442Z",
     "iopub.status.idle": "2026-01-24T00:53:05.783876Z",
     "shell.execute_reply": "2026-01-24T00:53:05.783674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "2. Pipeline Execution | ØªÙ†ÙÙŠØ° Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨\n",
      "======================================================================\n",
      "\n",
      "ğŸš€ Starting pipeline at 2026-01-24 03:53:05\n",
      "  â†’ Step 1: Extracting data...\n",
      "     âœ“ Extracted 100 rows\n",
      "  â†’ Step 2: Transforming data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     âœ“ Transformed 100 rows\n",
      "  â†’ Step 3: Loading data...\n",
      "     âœ“ Loaded 100 rows\n",
      "\n",
      "âœ… Pipeline completed successfully!\n",
      "   Duration: 0.32 seconds\n",
      "\n",
      "ğŸ’¡ Tip: Schedule this pipeline to run automatically (e.g., daily, hourly)!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Execute Pipeline\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2. Pipeline Execution | ØªÙ†ÙÙŠØ° Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def run_pipeline():\n",
    "    \"\"\"Execute complete pipeline with error handling\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    print(f\"\\nğŸš€ Starting pipeline at {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract\n",
    "        data = step1_extract()\n",
    "        \n",
    "        # Transform\n",
    "        data = step2_transform(data)\n",
    "        \n",
    "        # Load\n",
    "        success = step3_load(data)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\nâœ… Pipeline completed successfully!\")\n",
    "        print(f\"   Duration: {duration:.2f} seconds\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Pipeline failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run pipeline\n",
    "success = run_pipeline()\n",
    "\n",
    "print(\"\\nğŸ’¡ Tip: Schedule this pipeline to run automatically (e.g., daily, hourly)!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
