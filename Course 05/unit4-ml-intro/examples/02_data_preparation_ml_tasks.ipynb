{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Data Preparation for ML Tasks | ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù…Ù‡Ø§Ù… ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Clean and prepare data for machine learning\n",
    "- Handle missing values (detection, imputation, removal)\n",
    "- Encode categorical variables (Label, One-Hot encoding)\n",
    "- Prepare features for ML models\n",
    "- Split data into train/test sets\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Example 1: Pandas Data Manipulation (need manipulation skills)\n",
    "- âœ… Unit 2: Data Cleaning (cleaning techniques)\n",
    "- âœ… Understanding of ML concepts (what is training data?)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook covers practical activities from **Course 05, Unit 4**:\n",
    "- Cleaning and preparing data for ML tasks (handling missing values, encoding categorical variables)\n",
    "- **Source:** `DETAILED_UNIT_DESCRIPTIONS.md` - Unit 4 Practical Content\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Pandas Data Manipulation** - You need manipulation skills!\n",
    "- âœ… **Unit 2: Data Cleaning** - You need cleaning techniques!\n",
    "- âœ… **Understanding of ML**: What is training? What is testing?\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why data needs preparation\n",
    "- Knowing which encoding to use\n",
    "- Understanding train/test split\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is a key example in Unit 4: Introduction to Machine Learning**\n",
    "\n",
    "**Why data preparation for ML?**\n",
    "- **Before** building models, data must be prepared\n",
    "- **ML models** require specific data formats\n",
    "- **Preparation** is essential for model success\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: Pandas Data Manipulation (manipulation skills)\n",
    "- ğŸ““ Unit 2: Data Cleaning (cleaning techniques)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 3: Implementing ML Models (needs prepared data!)\n",
    "- ğŸ““ All ML model training\n",
    "\n",
    "**Why this order?**\n",
    "1. Data preparation is the first ML step (essential)\n",
    "2. Proper preparation ensures model success\n",
    "3. These skills are prerequisites for all ML work\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Preparing Ingredients for a Recipe | Ø§Ù„Ù‚ØµØ©: ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ù„Ù„ÙˆØµÙØ©\n",
    "\n",
    "Imagine you're cooking. **Before** you can cook, you prepare ingredients - clean vegetables, measure quantities, organize everything. **After** preparing, ingredients are ready to cook!\n",
    "\n",
    "Same with ML: **Before** building models, we prepare data - clean, encode, split. **After** preparing, data is ready for models!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Preparation for ML Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "Data preparation is essential because:\n",
    "- **Model Requirements**: ML models need specific data formats\n",
    "- **Performance**: Well-prepared data improves model performance\n",
    "- **Success**: Proper preparation is key to ML success\n",
    "- **Standard Practice**: All ML workflows start with preparation\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: Why split data into train/test?**\n",
    "  - Answer: Train on training set, evaluate on test set (unseen data)\n",
    "  - Example: 80% train, 20% test\n",
    "  - Benefit: Tests if model generalizes to new data\n",
    "  \n",
    "- **Q: When do I encode vs scale?**\n",
    "  - Answer: Encode categorical (text â†’ numbers), scale numerical (normalize range)\n",
    "  - Example: Color (categorical) â†’ encode, Age (numerical) â†’ scale\n",
    "  - Rule: Categories â†’ encode, Numbers â†’ scale\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Data preparation for ML** transforms raw data into ML-ready format. This involves cleaning, handling missing values, encoding categorical variables, and splitting data - all essential steps before building ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Inputs & ğŸ“¤ Outputs | Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø®Ø±Ø¬Ø§Øª\n",
    "\n",
    "**Inputs:** What we use in this notebook\n",
    "\n",
    "- Raw data\n- pandas, sklearn\n",
    "\n",
    "**Outputs:** What you'll see when you run the cells\n",
    "\n",
    "- Train/test splits\n- Encoded/scaled features\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PART 1: CREATE SAMPLE DATA WITH ISSUES | Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ© Ø¨Ù…Ø´Ø§ÙƒÙ„\n# ============================================================================\nprint(\"\\nğŸ“Š PART 1: Create Sample Data | Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ©\")\nprint(\"-\" * 70)\n\n# Create data with missing values and categorical variables\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'age': [25, 30, np.nan, 35, 28, np.nan, 32],\n    'salary': [5000, 7000, 6000, np.nan, 5500, 8000, 7500],\n    'city': ['Riyadh', 'Jeddah', 'Riyadh', 'Dammam', 'Jeddah', 'Riyadh', 'Dammam'],\n    'experience': [2, 5, 3, 8, 2, 6, 4],\n    'target': [0, 1, 0, 1, 0, 1, 1]  # Target variable for ML\n})\n\nprint(\"Original data (with missing values and categories):\")\nprint(df)\nprint(f\"\\nMissing values:\")\nprint(df.isnull().sum())\n\n# ============================================================================\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PART 2: HANDLE MISSING VALUES | Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"PART 2: Handle Missing Values | Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\")\nprint(\"=\" * 70)\n\n# Method 1: Fill with mean\nprint(\"\\nâœ… Example 1: Fill Missing Values with Mean\")\nprint(\"-\" * 70)\ndf_filled_mean = df.copy()\ndf_filled_mean['age'].fillna(df_filled_mean['age'].mean(), inplace=True)\ndf_filled_mean['salary'].fillna(df_filled_mean['salary'].mean(), inplace=True)\nprint(\"After filling with mean:\")\nprint(df_filled_mean)\nprint(f\"\\nMissing values remaining: {df_filled_mean.isnull().sum().sum()}\")\n\n# Method 2: Using SimpleImputer\nprint(\"\\nâœ… Example 2: Using SimpleImputer (Scikit-learn)\")\nprint(\"-\" * 70)\nimputer = SimpleImputer(strategy='mean')\nnumeric_cols = ['age', 'salary', 'experience']\ndf_imputed = df.copy()\ndf_imputed[numeric_cols] = imputer.fit_transform(df[numeric_cols])\nprint(\"After imputation:\")\nprint(df_imputed[numeric_cols])\n\n# ============================================================================\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PART 3: ENCODE CATEGORICAL VARIABLES | ØªØ´ÙÙŠØ± Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"PART 3: Encode Categorical Variables | ØªØ´ÙÙŠØ± Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©\")\nprint(\"=\" * 70)\n\n# Label Encoding\nprint(\"\\nâœ… Example 3: Label Encoding\")\nprint(\"-\" * 70)\nlabel_encoder = LabelEncoder()\ndf_encoded = df_imputed.copy()\ndf_encoded['city_encoded'] = label_encoder.fit_transform(df_encoded['city'])\nprint(\"Label encoded 'city':\")\nprint(df_encoded[['city', 'city_encoded']])\nprint(f\"\\nEncoding mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n\n# One-Hot Encoding\nprint(\"\\nâœ… Example 4: One-Hot Encoding\")\nprint(\"-\" * 70)\ndf_onehot = pd.get_dummies(df_imputed, columns=['city'], prefix='city')\nprint(\"One-Hot encoded 'city':\")\nprint(df_onehot[['city_Riyadh', 'city_Jeddah', 'city_Dammam']])\n\n# ============================================================================\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PART 4: DATA SPLITTING | ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"PART 4: Data Splitting | ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\")\nprint(\"=\" * 70)\n\n# Prepare features and target\nX = df_onehot[['age', 'salary', 'experience', 'city_Riyadh', 'city_Jeddah', 'city_Dammam']]\ny = df_onehot['target']\n\n# Train/Test Split\nprint(\"\\nâœ… Example 5: Train/Test Split\")\nprint(\"-\" * 70)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Original data: {X.shape[0]} samples\")\nprint(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/X.shape[0]*100:.1f}%)\")\nprint(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/X.shape[0]*100:.1f}%)\")\nprint(\"\\nğŸ’¡ Training set: Used to train model\")\nprint(\"ğŸ’¡ Test set: Used to evaluate model (unseen data)\")\n\n# ============================================================================\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PART 5: SCALING (OPTIONAL BUT RECOMMENDED) | Ø§Ù„Ù‚ÙŠØ§Ø³ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„ÙƒÙ† Ù…ÙˆØµÙ‰ Ø¨Ù‡)\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"PART 5: Feature Scaling | Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª\")\nprint(\"=\" * 70)\n\n# Scale numerical features\nprint(\"\\nâœ… Example 6: Feature Scaling\")\nprint(\"-\" * 70)\nscaler = StandardScaler()\nnumeric_features = ['age', 'salary', 'experience']\nX_train_scaled = X_train.copy()\nX_test_scaled = X_test.copy()\n\nX_train_scaled[numeric_features] = scaler.fit_transform(X_train[numeric_features])\nX_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])\n\nprint(\"Scaled training data (first 3 rows):\")\nprint(X_train_scaled.head(3))\nprint(\"\\nğŸ’¡ Scaling: Makes features comparable (mean=0, std=1)\")\n\n# ============================================================================\n# SUMMARY | Ø§Ù„Ù…Ù„Ø®Øµ\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Summary | Ø§Ù„Ù…Ù„Ø®Øµ\")\nprint(\"=\" * 70)\nprint(\"\"\"\nâœ… What you learned:\n   1. Handle Missing Values: Fill with mean, use SimpleImputer\n   2. Encode Categorical: Label Encoding, One-Hot Encoding\n   3. Data Splitting: Train/Test split (80/20 typical)\n   4. Feature Scaling: StandardScaler (mean=0, std=1)\n\nğŸ¯ Key Takeaways:\n   - Missing Values: Fill or impute before ML\n   - Encoding: Convert categories to numbers (ML needs numbers)\n   - Splitting: Train on training set, test on test set\n   - Scaling: Normalize features for better performance\n\nğŸ“š Next Steps:\n   - Example 3: Implementing ML Models (use prepared data!)\n   - Train models on prepared data\n   - Evaluate model performance\n\"\"\")\nprint(\"âœ… Data preparation concepts understood!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}