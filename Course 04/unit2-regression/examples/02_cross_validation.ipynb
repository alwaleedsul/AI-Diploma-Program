{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Cross-Validation for Model Evaluation | Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Unit 1: All examples** - Data processing and basic regression\n",
    "- âœ… **Unit 2, Example 1: Ridge and Lasso** - Understanding model evaluation\n",
    "- âœ… **Understanding of train-test split**: Why we split data\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why cross-validation is better than simple train-test split\n",
    "- Knowing when to use K-Fold vs Leave-One-Out\n",
    "- Understanding how cross-validation helps with model selection\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is Unit 2, Example 2** - it improves how we evaluate models!\n",
    "\n",
    "**Why this example SECOND in Unit 2?**\n",
    "- **Before** you can properly evaluate models, you need to understand basic evaluation\n",
    "- **Before** you can select the best model, you need reliable evaluation methods\n",
    "- **Before** you can tune hyperparameters, you need cross-validation\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Unit 1, Example 3: Train-Test Split (we know basic splitting)\n",
    "- ğŸ““ Unit 2, Example 1: Ridge/Lasso (we evaluated models with simple split)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Unit 5, Example 1: Grid Search (uses cross-validation for hyperparameter tuning)\n",
    "- ğŸ““ All ML projects (cross-validation is the gold standard for evaluation!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Cross-validation improves **model evaluation** (more reliable than single split)\n",
    "2. Cross-validation enables **model comparison** (fair comparison between models)\n",
    "3. Cross-validation is essential for **hyperparameter tuning** (used in grid search)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Getting Multiple Opinions | Ø§Ù„Ù‚ØµØ©: Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢Ø±Ø§Ø¡ Ù…ØªØ¹Ø¯Ø¯Ø©\n",
    "\n",
    "Imagine you're hiring someone. **Before** cross-validation, you ask one person's opinion (single train-test split) - might be biased! **After** cross-validation, you ask multiple people (multiple folds) and average their opinions - much more reliable!\n",
    "\n",
    "Same with machine learning: **Before** cross-validation, we evaluate on one test set (might be lucky/unlucky). **After** cross-validation, we evaluate on multiple test sets and average - much more reliable!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Cross-Validation Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ØŸ\n",
    "\n",
    "Cross-validation is the gold standard for model evaluation:\n",
    "- **More Reliable**: Uses all data for both training and testing (in different folds)\n",
    "- **Less Variance**: Averages results across multiple folds\n",
    "- **Better Model Selection**: Fairly compares different models\n",
    "- **Detects Overfitting**: Shows if model performance varies across folds\n",
    "- **Industry Standard**: Used in all professional ML projects\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Applications | Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©\n",
    "\n",
    "**Cross-Validation is used in EVERY ML project to ensure reliable model evaluation!** Here's where it's critical:\n",
    "\n",
    "### ğŸ’° Finance & Banking Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ù…Ø§Ù„ÙŠ ÙˆØ§Ù„Ù…ØµØ±ÙÙŠ\n",
    "- **Credit Risk Models**: CV ensures loan default models work across different customer segments\n",
    "- **Fraud Detection**: CV validates that fraud models generalize to new transaction patterns\n",
    "- **Algorithmic Trading**: CV tests trading strategies on different market periods\n",
    "- **Portfolio Optimization**: CV validates investment models across different market conditions\n",
    "- **Regulatory Compliance**: CV provides reliable performance estimates for regulators\n",
    "\n",
    "### ğŸ¥ Healthcare & Medical Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµØ­ÙŠ ÙˆØ§Ù„Ø·Ø¨ÙŠ\n",
    "- **Disease Diagnosis**: CV ensures diagnostic models work across different patient populations\n",
    "- **Drug Efficacy Studies**: CV validates drug response models across different patient groups\n",
    "- **Medical Imaging**: CV tests image classification models on different scanners/hospitals\n",
    "- **Clinical Trials**: CV validates treatment outcome predictions across different trial sites\n",
    "- **Patient Risk Stratification**: CV ensures risk models work across different demographics\n",
    "\n",
    "### ğŸ“Š Marketing & E-commerce Sector | Ù‚Ø·Ø§Ø¹ Ø§Ù„ØªØ³ÙˆÙŠÙ‚ ÙˆØ§Ù„ØªØ¬Ø§Ø±Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©\n",
    "- **Customer Churn Prediction**: CV tests models across different customer segments/time periods\n",
    "- **Recommendation Systems**: CV validates recommendations work across different user groups\n",
    "- **Ad Campaign Optimization**: CV tests ad performance models across different campaigns\n",
    "- **Price Optimization**: CV validates pricing models across different market conditions\n",
    "- **A/B Testing**: CV helps validate A/B test results across different user segments\n",
    "\n",
    "### ğŸ­ Manufacturing & Quality Control | Ø§Ù„ØªØµÙ†ÙŠØ¹ ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¬ÙˆØ¯Ø©\n",
    "- **Quality Prediction**: CV ensures quality models work across different production batches\n",
    "- **Predictive Maintenance**: CV validates failure prediction models across different equipment\n",
    "- **Supply Chain Forecasting**: CV tests demand models across different seasons/regions\n",
    "- **Defect Detection**: CV ensures defect models work across different product lines\n",
    "- **Process Optimization**: CV validates optimization models across different production conditions\n",
    "\n",
    "### ğŸ”¬ Scientific Research & Data Analysis | Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ù„Ù…ÙŠ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "- **Genomics Research**: CV validates gene-disease associations across different populations\n",
    "- **Climate Modeling**: CV tests climate models across different time periods/regions\n",
    "- **Drug Discovery**: CV validates compound effectiveness across different experiments\n",
    "- **Social Science**: CV ensures survey models work across different demographics\n",
    "- **Epidemiology**: CV validates disease spread models across different regions\n",
    "\n",
    "### ğŸ¯ Hyperparameter Tuning (Used in ALL ML Projects)\n",
    "- **Grid Search**: CV evaluates each hyperparameter combination (used in Unit 5)\n",
    "- **Random Search**: CV validates random hyperparameter samples\n",
    "- **Model Selection**: CV compares different algorithms (SVM vs Random Forest vs XGBoost)\n",
    "- **Feature Selection**: CV validates which features are most important\n",
    "- **Ensemble Methods**: CV tests different ensemble combinations\n",
    "\n",
    "### ğŸ›ï¸ Government & Public Safety Sector (Ministry of Interior) | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¹Ø§Ù…Ø© (ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©)\n",
    "- **Threat Detection Models**: CV ensures threat models work across different threat types/time periods â†’ counter-terrorism\n",
    "- **Traffic Management Systems**: CV tests traffic models across different locations/time periods â†’ traffic management\n",
    "- **Crime Prediction Models**: CV validates crime models across different areas/demographics â†’ law enforcement\n",
    "- **Emergency Response Systems**: CV tests emergency models across different incident types â†’ emergency services\n",
    "- **Border Security Models**: CV validates border models across different crossing points/time periods â†’ border control\n",
    "- **Surveillance Systems**: CV tests surveillance models across different locations/cameras â†’ security monitoring\n",
    "- **Traffic Violation Detection**: CV validates violation models across different road types â†’ traffic enforcement\n",
    "- **Access Control Systems**: CV tests access models across different facilities/personnel â†’ internal organization\n",
    "- **Identity Verification**: CV validates identity models across different demographics â†’ government facilities\n",
    "- **Traffic Flow Models**: CV tests flow models across different traffic conditions â†’ smart traffic systems\n",
    "\n",
    "### ğŸ’¡ Why Cross-Validation is Essential:\n",
    "- **Limited Data**: When you have small datasets, CV uses all data efficiently\n",
    "- **Unreliable Single Split**: One train-test split might be lucky/unlucky\n",
    "- **Model Comparison**: Fairly compares different models/algorithms\n",
    "- **Hyperparameter Tuning**: Essential for finding best hyperparameters\n",
    "- **Production Confidence**: Gives confidence that model will work on new data\n",
    "\n",
    "### ğŸ“ˆ When to Use Cross-Validation:\n",
    "âœ… **Use Cross-Validation when:**\n",
    "- Have limited data (small dataset)\n",
    "- Need reliable model evaluation\n",
    "- Comparing different models/algorithms\n",
    "- Tuning hyperparameters (Grid Search, Random Search)\n",
    "- Need confidence in model performance\n",
    "- Want to detect overfitting\n",
    "\n",
    "âŒ **Don't use Cross-Validation when:**\n",
    "- Have very large dataset (single split is sufficient)\n",
    "- Data has temporal structure (use time-series CV instead)\n",
    "- Need fast evaluation (CV is slower)\n",
    "- Data is streaming (use online evaluation instead)\n",
    "\n",
    "### ğŸ”„ Types of Cross-Validation:\n",
    "- **K-Fold CV**: Most common (5-fold or 10-fold)\n",
    "- **Stratified K-Fold**: For imbalanced classification\n",
    "- **Leave-One-Out CV**: For very small datasets\n",
    "- **Time Series CV**: For temporal data\n",
    "- **Group K-Fold**: When samples are grouped\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Implement K-Fold Cross-Validation\n",
    "2. Use cross-validation for model comparison\n",
    "3. Understand Leave-One-Out Cross-Validation (LOOCV)\n",
    "4. Compare models using cross-validation scores\n",
    "5. Visualize cross-validation splits and results\n",
    "6. Know when to use each cross-validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "\n",
      "ğŸ“š What each cross-validation tool does:\n",
      "   - KFold: Splits data into K equal folds\n",
      "   - cross_val_score: Computes scores for each fold\n",
      "   - cross_validate: Computes multiple metrics at once\n",
      "   - LeaveOneOut: Each sample is its own test set (expensive but thorough)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us perform cross-validation\n",
    "\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np   # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For visualizations\n",
    "from sklearn.model_selection import (\n",
    "    KFold,              # K-Fold cross-validation (splits data into K folds)\n",
    "    StratifiedKFold,    # Stratified K-Fold (maintains class distribution)\n",
    "    LeaveOneOut,        # Leave-One-Out CV (each sample is a fold)\n",
    "    cross_val_score,    # Computes cross-validation scores\n",
    "    cross_validate,     # Computes multiple metrics with CV\n",
    "    train_test_split    # Simple train-test split (for comparison)\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso  # Models to evaluate\n",
    "from sklearn.preprocessing import StandardScaler  # For scaling features\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer  # Evaluation metrics\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each cross-validation tool does:\")\n",
    "print(\"   - KFold: Splits data into K equal folds\")\n",
    "print(\"   - cross_val_score: Computes scores for each fold\")\n",
    "print(\"   - cross_validate: Computes multiple metrics at once\")\n",
    "print(\"   - LeaveOneOut: Each sample is its own test set (expensive but thorough)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤” Why Cross-Validation? Why Not Just Use MSE? | Ù„Ù…Ø§Ø°Ø§ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ØŸ Ù„Ù…Ø§Ø°Ø§ Ù„Ø§ Ù†Ø³ØªØ®Ø¯Ù… MSE ÙÙ‚Ø·ØŸ\n",
    "\n",
    "**This is the MOST IMPORTANT question to understand!** Before we learn HOW to do cross-validation, let's understand WHY we need it.\n",
    "\n",
    "### ğŸ  Why Cross-Validation Matters for Housing Price Prediction\n",
    "\n",
    "**Real Problem with California Housing Data:**\n",
    "- Housing prices vary dramatically by region (coastal vs inland, urban vs rural)\n",
    "- Single split might have mostly expensive houses (San Francisco area) â†’ MSE looks high\n",
    "- Single split might have mostly cheap houses (rural areas) â†’ MSE looks low\n",
    "- **You don't know if your model is good or just lucky with the split!**\n",
    "\n",
    "**Solution:** Cross-validation tests on different regions (different folds) â†’ average performance across all regions â†’ reliable estimate!\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ The Problem with Just Using MSE (or Any Single Metric) on One Test Set\n",
    "\n",
    "Imagine you evaluate your model with just one metric on one test set. What could go wrong?\n",
    "\n",
    "#### Problem 1: **Lucky or Unlucky Split** ğŸ²\n",
    "- **Issue:** Single split might have \"easy\" or \"hard\" examples by chance\n",
    "- **Housing Example:** \n",
    "  - Split 1: Test set has mostly predictable houses â†’ MSE = 0.50 (looks great!)\n",
    "  - Split 2: Test set has mostly unpredictable houses â†’ MSE = 0.57 (looks bad!)\n",
    "  - **Which one is the \"real\" performance?** ğŸ¤·\n",
    "- **Generic Example:** Split 1: MSE = 50 (lucky), Split 2: MSE = 150 (unlucky) â†’ Which is real?\n",
    "- **Solution:** Cross-validation tests on MULTIPLE splits â†’ average = reliable estimate!\n",
    "\n",
    "#### Problem 2: **Overfitting to One Test Set** ğŸ¯\n",
    "- **Issue:** Model might perform well on one specific test set but fail on others\n",
    "- **Example:** Model A: MSE = 100 (Test 1), MSE = 95 (Test 2) â†’ Avg = 97.5\n",
    "- **Example:** Model B: MSE = 90 (Test 1), MSE = 110 (Test 2) â†’ Avg = 100\n",
    "- **Problem:** Single split makes Model B look better, but Model A is actually better!\n",
    "- **Solution:** Cross-validation evaluates on MULTIPLE test sets â†’ fair comparison!\n",
    "\n",
    "#### Problem 3: **High Variance in Evaluation** ğŸ“Š\n",
    "- **Issue:** Single split = ONE evaluation with unknown reliability\n",
    "- **Example:** Single split: MSE = 100 (but is it really 100? or 80? or 120?)\n",
    "- **Solution:** Cross-validation: MSE = 100 Â± 20 â†’ you know the range [80-120]!\n",
    "\n",
    "#### Problem 4: **Wasting Data** ğŸ“‰\n",
    "- **Issue:** Simple split uses 80% train, 20% test â†’ test set only used once\n",
    "- **Example:** 1000 samples â†’ 800 train, 200 test (test discarded after one evaluation)\n",
    "- **Solution:** Cross-validation uses ALL data for both training AND testing (in different folds)\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Why Cross-Validation Solves These Problems\n",
    "\n",
    "**Cross-validation systematically addresses all four problems:**\n",
    "\n",
    "1. **Multiple Evaluations = Reliable Average** ğŸ“ˆ\n",
    "   - Split data into 5 folds, evaluate 5 times, average the scores\n",
    "   - Example: Folds give MSE = [50, 150, 100, 120, 80] â†’ Mean = 100, Std = 35\n",
    "   - No more \"lucky\" or \"unlucky\" single splits!\n",
    "\n",
    "2. **Fair Model Comparison** âš–ï¸\n",
    "   - All models tested on EXACTLY the same test sets\n",
    "   - No bias from specific test set â†’ fair comparison guaranteed!\n",
    "\n",
    "3. **Confidence Intervals** ğŸ“Š\n",
    "   - Mean Â± Std shows range of performance\n",
    "   - Not just one number, but a range you can trust!\n",
    "\n",
    "4. **Efficient Data Usage** ğŸ’¯\n",
    "   - All samples used for training (in different folds)\n",
    "   - All samples used for testing (in different folds)\n",
    "   - 100% data usage vs 80% in simple split!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Key Insight: MSE is NOT the Problem!\n",
    "\n",
    "**Important:** MSE (or RÂ², or any metric) is NOT the problem!\n",
    "\n",
    "**The problem is:** Using MSE on a SINGLE test set gives unreliable results!\n",
    "\n",
    "**The solution:** Use MSE (or any metric) on MULTIPLE test sets (cross-validation) â†’ reliable results!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Comparison: Single Split vs Cross-Validation\n",
    "\n",
    "| Aspect | Single Train-Test Split | Cross-Validation |\n",
    "|--------|-------------------------|------------------|\n",
    "| **Number of Evaluations** | 1 | K (typically 5 or 10) |\n",
    "| **Reliability** | Low (one evaluation) | High (multiple evaluations) |\n",
    "| **Variance** | Unknown | Known (std across folds) |\n",
    "| **Data Usage** | 80% train, 20% test | 100% used for both |\n",
    "| **Model Comparison** | Unfair (different test sets) | Fair (same test sets) |\n",
    "| **Confidence** | Low (one number) | High (mean Â± std) |\n",
    "| **Overfitting Risk** | High (one test set) | Low (multiple test sets) |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Real-World Analogy\n",
    "\n",
    "**Single Split = Asking One Person's Opinion**\n",
    "- You ask one person: \"Is this candidate good?\" â†’ \"Yes!\" âœ…\n",
    "- **Problem:** Maybe they're biased or just lucky?\n",
    "- **Solution:** Ask 5 people and average their opinions â†’ more reliable!\n",
    "\n",
    "**Cross-Validation = Asking Multiple People's Opinions**\n",
    "- You ask 5 people: \"Is this candidate good?\"\n",
    "- Answers: Yes, No, Yes, Yes, No â†’ Average: 60% positive\n",
    "- **Much more reliable!** You know the range of opinions!\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Summary: Why Cross-Validation?\n",
    "\n",
    "1. **More Reliable** âœ… - Multiple evaluations â†’ average is more reliable\n",
    "2. **Fair Comparison** âœ… - All models tested on same test sets\n",
    "3. **Confidence Intervals** âœ… - Mean Â± Std shows range of performance\n",
    "4. **Efficient Data Usage** âœ… - All data used for both training and testing\n",
    "5. **Industry Standard** âœ… - Used in all professional ML projects\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ Common Misunderstanding: \"Small Variance = CV Not Needed\"\n",
    "\n",
    "**Students often think:** \"If variance is small, cross-validation doesn't matter\"\n",
    "\n",
    "**But this is WRONG!** Here's why:\n",
    "\n",
    "**Without CV:**\n",
    "- You get ONE number (e.g., MSE = 0.56)\n",
    "- But you DON'T KNOW: Is it really 0.56? Or could it be 0.50? Or 0.57?\n",
    "- You have NO IDEA about reliability or confidence interval\n",
    "- You don't know if this single split was typical or extreme\n",
    "\n",
    "**With CV:**\n",
    "- You get MEAN Â± STD (e.g., MSE = 0.53 Â± 0.02)\n",
    "- You KNOW: True performance is between 0.51 and 0.55 (95% confidence)\n",
    "- You have a RELIABLE estimate with confidence interval\n",
    "- You know if your single split was typical or extreme\n",
    "\n",
    "**Key Insight:** \n",
    "- The point isn't that variance is **large** - it's that you **KNOW** the variance!\n",
    "- Even with small variance, CV tells you the TRUE performance range\n",
    "- Without CV, you have ONE number with UNKNOWN reliability\n",
    "- With CV, you have MEAN Â± STD with KNOWN reliability\n",
    "\n",
    "**Example:**\n",
    "- Single split: \"MSE = 0.56\" â†’ Is it really 0.56? Or 0.50? Or 0.57? ğŸ¤·\n",
    "- Cross-validation: \"MSE = 0.53 Â± 0.02\" â†’ True performance is 0.51-0.55 âœ…\n",
    "\n",
    "**Note:** We'll demonstrate both small variance (California Housing) and high variance (smaller subset) cases to show why CV matters in both scenarios!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Common Student Questions\n",
    "\n",
    "**Q: Why not just use MSE on training set?**\n",
    "- **Answer:** Training MSE is biased (model knows the data it was trained on)\n",
    "- **Solution:** Need test set (unseen data) â†’ Better: Multiple test sets (cross-validation)\n",
    "\n",
    "**Q: Why not just use a larger test set?**\n",
    "- **Answer:** Larger test set = less training data â†’ worse model!\n",
    "- **Solution:** Cross-validation uses ALL data for training AND testing (best of both worlds)\n",
    "\n",
    "**Q: Why not just use MSE multiple times on different splits?**\n",
    "- **Answer:** That's EXACTLY what cross-validation does! ğŸ¯\n",
    "- **Cross-validation = systematic way to use MSE on multiple splits**\n",
    "\n",
    "**Q: Is MSE the problem?**\n",
    "- **Answer:** NO! MSE is a great metric!\n",
    "- **The problem:** Using MSE on a SINGLE test set\n",
    "- **The solution:** Using MSE on MULTIPLE test sets (cross-validation)\n",
    "\n",
    "**Q: Is cross-validation always better than simple split?**\n",
    "- **Answer:** Usually yes, but not always!\n",
    "- **For very large datasets (>100K samples):** Simple split might be sufficient (single split is reliable enough)\n",
    "- **For small-medium datasets (<10K samples):** Cross-validation is essential (single split unreliable)\n",
    "- **For our California Housing (20K samples):** Cross-validation is recommended for reliable evaluation\n",
    "\n",
    "**Q: Why does cross-validation take longer?**\n",
    "- **Answer:** Cross-validation trains the model K times (5 times for 5-fold CV)\n",
    "- **Simple split:** Train once, test once = 1 model training\n",
    "- **5-fold CV:** Train 5 times, test 5 times = 5 model trainings\n",
    "- **Trade-off:** More computation time for more reliable evaluation\n",
    "- **Tip:** Use `n_jobs=-1` parameter to parallelize CV (faster on multi-core machines)\n",
    "\n",
    "**Q: When should I use cross-validation vs simple split?**\n",
    "- **Use Cross-Validation when:**\n",
    "  - Dataset < 10,000 samples (need reliable evaluation)\n",
    "  - Comparing multiple models (need fair comparison)\n",
    "  - Tuning hyperparameters (need reliable performance estimate)\n",
    "  - Limited data (need to use all data efficiently)\n",
    "- **Use Simple Split when:**\n",
    "  - Very large dataset (>100K samples, single split reliable enough)\n",
    "  - Quick baseline evaluation needed\n",
    "  - Computational constraints (very expensive models)\n",
    "- **See Decision Framework (Cell 25) for detailed guide!**\n",
    "\n",
    "**Q: Why do I get different results each time I run cross-validation?**\n",
    "- **Answer:** Random shuffling creates different folds each time\n",
    "- **Solution:** Set `random_state` parameter (e.g., `random_state=42`) for reproducible results\n",
    "- **Example:** `KFold(n_splits=5, shuffle=True, random_state=42)` â†’ same folds every time\n",
    "- **Note:** Without `random_state`, folds change â†’ results vary slightly (but mean should be similar)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Next Steps\n",
    "\n",
    "Now that you understand **WHY** cross-validation is needed, let's learn **HOW** to do it!\n",
    "\n",
    "**Remember:** \n",
    "- âœ… MSE (or any metric) is fine - use it!\n",
    "- âœ… The problem is using it on a SINGLE test set\n",
    "- âœ… The solution is using it on MULTIPLE test sets (cross-validation)\n",
    "- âœ… Cross-validation makes your evaluation RELIABLE and FAIR!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Quick Start (Optional) | Ø¨Ø¯Ø¡ Ø³Ø±ÙŠØ¹ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n",
    "\n",
    "**Want to jump right in?** Run the code cells below to see cross-validation in action, then come back to read the detailed explanations!\n",
    "\n",
    "**Quick Path:**\n",
    "1. Run Cell 4: Load data\n",
    "2. Run Cell 6: Prepare data\n",
    "3. Run Cell 8: Simple split (baseline)\n",
    "4. Run Cell 15: K-Fold CV (main method)\n",
    "5. Run Cell 17: Compare models\n",
    "\n",
    "**Full Path:** Read all explanations from the beginning for complete understanding!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¥ Loading California Housing dataset...\n",
      "ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§Ù„ÙŠÙÙˆØ±Ù†ÙŠØ§ Ù„Ù„Ø¥Ø³ÙƒØ§Ù†...\n",
      "\n",
      "âœ… Real-world California Housing data loaded!\n",
      "   ğŸ“Š This is REAL data from the 1990 California census\n",
      "   ğŸ“ˆ Contains 20640 housing districts with 8 features\n",
      "   ğŸ’° Target: Median House Value (in $100,000s)\n",
      "\n",
      "ğŸ“„ First 5 rows:\n",
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  target  \n",
      "0    -122.23   4.526  \n",
      "1    -122.22   3.585  \n",
      "2    -122.24   3.521  \n",
      "3    -122.25   3.413  \n",
      "4    -122.25   3.422  \n",
      "\n",
      "ğŸ“Š Dataset Info:\n",
      "   Features: MedInc, HouseAge, AveRooms, AveBedrms... and 4 more\n",
      "   Target range: $0.15 - $5.00 (in $100,000s)\n",
      "\n",
      "ğŸ” Notice:\n",
      "   - This is REAL housing data from 1990 California census\n",
      "   - 8 features: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n",
      "   - Perfect for comparing simple train-test split vs cross-validation!\n",
      "   - Cross-validation will give more reliable evaluation on this real data!\n",
      "   - Complete solutions provided for every step!\n",
      "\n",
      "ğŸ’¡ Why Cross-Validation Matters for This Dataset:\n",
      "   - Housing prices vary by region (coastal vs inland, urban vs rural)\n",
      "   - Single split might have mostly expensive areas â†’ model looks bad\n",
      "   - Single split might have mostly cheap areas â†’ model looks good\n",
      "   - Cross-validation tests on different regions â†’ reliable estimate!\n",
      "   - We'll see this in action: different splits give different scores!\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Load real-world California Housing dataset\n",
    "# This is REAL data perfect for demonstrating cross-validation with complete solutions!\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "print(\"\\nğŸ“¥ Loading California Housing dataset...\")\n",
    "print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§Ù„ÙŠÙÙˆØ±Ù†ÙŠØ§ Ù„Ù„Ø¥Ø³ÙƒØ§Ù†...\")\n",
    "\n",
    "housing_data = fetch_california_housing()\n",
    "\n",
    "# SOLUTION: Create DataFrame from real-world data\n",
    "df = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\n",
    "df['target'] = housing_data.target  # Median House Value\n",
    "\n",
    "print(f\"\\nâœ… Real-world California Housing data loaded!\")\n",
    "print(f\"   ğŸ“Š This is REAL data from the 1990 California census\")\n",
    "print(f\"   ğŸ“ˆ Contains {len(df)} housing districts with {len(df.columns)-1} features\")\n",
    "print(f\"   ğŸ’° Target: Median House Value (in $100,000s)\")\n",
    "print(f\"\\nğŸ“„ First 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nğŸ“Š Dataset Info:\")\n",
    "print(f\"   Features: {', '.join(housing_data.feature_names[:4])}... and {len(housing_data.feature_names)-4} more\")\n",
    "print(f\"   Target range: ${df['target'].min():.2f} - ${df['target'].max():.2f} (in $100,000s)\")\n",
    "print(\"\\nğŸ” Notice:\")\n",
    "print(\"   - This is REAL housing data from 1990 California census\")\n",
    "print(\"   - 8 features: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\")\n",
    "print(\"   - Perfect for comparing simple train-test split vs cross-validation!\")\n",
    "print(\"   - Cross-validation will give more reliable evaluation on this real data!\")\n",
    "print(\"   - Complete solutions provided for every step!\")\n",
    "print(\"\\nğŸ’¡ Why Cross-Validation Matters for This Dataset:\")\n",
    "print(\"   - Housing prices vary by region (coastal vs inland, urban vs rural)\")\n",
    "print(\"   - Single split might have mostly expensive areas â†’ model looks bad\")\n",
    "print(\"   - Single split might have mostly cheap areas â†’ model looks good\")\n",
    "print(\"   - Cross-validation tests on different regions â†’ reliable estimate!\")\n",
    "print(\"   - We'll see this in action: different splits give different scores!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Understanding the Dataset | ÙÙ‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø¬Ø§Ù„\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (medical, financial, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: 20,640 rows Ã— 8 columns (samples Ã— features)\n",
    "- **Feature Types**: All numerical (float64) - continuous values\n",
    "- **Target Type**: Regression (predicting continuous value: house price)\n",
    "- **Task**: Predict median house value based on features\n",
    "- **Data Quality**: Real-world data with regional variations (perfect for cross-validation demonstration)\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Large dataset** â†’ Good for cross-validation (enough data for multiple folds)\n",
    "- **Regional variations** â†’ Shows why cross-validation is needed (different regions = different performance)\n",
    "- **Regression task** â†’ We'll use regression metrics (MSE, RMSE, RÂ²) with cross-validation\n",
    "- **Real-world data** â†’ Demonstrates real-world scenario where single split is unreliable\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ÙÙ‡Ù… Ù…Ø¬Ø§Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø§Ø®ØªØµØ§Ø±)\n",
    "\n",
    "**What is this data?** California housing prices from 1990 census.\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For cross-validation**: Regional variations (coastal vs inland) â†’ different folds have different characteristics\n",
    "- **For model evaluation**: Need reliable evaluation across different regions â†’ cross-validation provides this\n",
    "- **For demonstration**: Shows why single split is unreliable (different regions = different MSE)\n",
    "\n",
    "**Domain Context** (Brief):\n",
    "- **MedInc**: Median income (varies by region - coastal areas higher)\n",
    "- **Latitude, Longitude**: Location (coastal areas more expensive)\n",
    "- **Regional Variation**: Prices vary dramatically by region (San Francisco vs rural areas)\n",
    "- **Why CV matters**: Single split might get mostly coastal (expensive) or mostly rural (cheap) â†’ unreliable MSE\n",
    "\n",
    "**ğŸ’¡ Key Point for CS Students**: You don't need to be a real estate expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types, variations)\n",
    "- Recognizing **data variations** (different regions = different characteristics)\n",
    "- Understanding why **cross-validation is needed** (single split unreliable with variations)\n",
    "- Choosing the right **evaluation method** based on structure, not domain knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Data for Modeling | Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù†Ù…Ø°Ø¬Ø©\n",
    "\n",
    "**BEFORE**: We have the California Housing dataset loaded, but we need to extract features (X) and target (y) for modeling.\n",
    "\n",
    "**AFTER**: We'll extract features and target, then scale them for cross-validation!\n",
    "\n",
    "**Why this step?** Models need features (X) and target (y) as separate arrays. Scaling ensures fair comparison across features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Data prepared for modeling:\n",
      "   Features (X): 8 features\n",
      "   Target (y): 20640 samples\n",
      "   Feature names: MedInc, HouseAge, AveRooms, AveBedrms... and more\n",
      "   Feature shape: (20640, 8)\n",
      "   Target shape: (20640,)\n",
      "\n",
      "   âœ… Features scaled!\n",
      "   Before scaling - Mean range: [-119.57, 1425.48]\n",
      "   After scaling - Mean: [0. 0. 0.]... (all â‰ˆ0)\n",
      "   After scaling - Std: [1. 1. 1.]... (all â‰ˆ1)\n",
      "   âœ… All features now have meanâ‰ˆ0 and stdâ‰ˆ1 (standardized)!\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Prepare features (X) and target (y) from the loaded data\n",
    "# Extract features and target from DataFrame\n",
    "X = df[housing_data.feature_names].values  # Features as numpy array\n",
    "y = df['target'].values  # Target as numpy array\n",
    "\n",
    "print(f\"\\nâœ… Data prepared for modeling:\")\n",
    "print(f\"   Features (X): {X.shape[1]} features\")\n",
    "print(f\"   Target (y): {len(y)} samples\")\n",
    "print(f\"   Feature names: {', '.join(housing_data.feature_names[:4])}... and more\")\n",
    "print(f\"   Feature shape: {X.shape}\")\n",
    "print(f\"   Target shape: {y.shape}\")\n",
    "\n",
    "# SOLUTION: Scale features\n",
    "# IMPORTANT: Regularization and cross-validation require scaled features!\n",
    "# StandardScaler standardizes features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "# .fit_transform(data)\n",
    "# - Two operations in one: .fit() then .transform()\n",
    "#   1. .fit(): Learns parameters from data (mean/std, categories, etc.)\n",
    "#   2. .transform(): Applies transformation using learned parameters\n",
    "# - Use on training data\n",
    "# - For test data, use only .transform() (don't refit!)\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(f\"\\n   âœ… Features scaled!\")\n",
    "print(f\"   Before scaling - Mean range: [{X.mean(axis=0).min():.2f}, {X.mean(axis=0).max():.2f}]\")\n",
    "print(f\"   After scaling - Mean: {X_scaled.mean(axis=0).round(2)[:3]}... (all â‰ˆ0)\")\n",
    "print(f\"   After scaling - Std: {X_scaled.std(axis=0).round(2)[:3]}... (all â‰ˆ1)\")\n",
    "print(f\"   âœ… All features now have meanâ‰ˆ0 and stdâ‰ˆ1 (standardized)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Simple Train-Test Split (Baseline) | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨Ø³ÙŠØ· (Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³)\n",
    "\n",
    "**BEFORE**: We've been using simple train-test split. Let's see its limitations.\n",
    "\n",
    "**AFTER**: We'll see that single split gives one score, but cross-validation gives multiple scores and an average!\n",
    "\n",
    "**Why start with simple split?** It's what we know. We'll compare it to cross-validation to see the improvement!\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: Why is simple train-test split not enough?**\n",
    "  - Answer: Single split = one evaluation (might be lucky/unlucky!)\n",
    "  - Problem: Different splits give different scores (high variance)\n",
    "  - Solution: Cross-validation uses multiple splits â†’ average (more reliable)\n",
    "- **Q: Why not just use more test data?**\n",
    "  - Answer: More test data = less training data â†’ worse model!\n",
    "  - Cross-validation uses ALL data for training AND testing (in different folds)\n",
    "  - Best of both worlds: More training data + multiple evaluations\n",
    "- **Q: How many folds should I use?**\n",
    "  - Answer: Common choices: 5-fold (good balance) or 10-fold (more thorough)\n",
    "  - More folds = more evaluations but slower\n",
    "  - Rule of thumb: Use 5-fold for most cases, 10-fold for small datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "1. Simple Train-Test Split (Baseline) - COMPLETE SOLUTION\n",
      "Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨Ø³ÙŠØ· ØªØ¯Ø±ÙŠØ¨-Ø§Ø®ØªØ¨Ø§Ø± (Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³) - Ø­Ù„ ÙƒØ§Ù…Ù„\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Simple Train-Test Split Results:\n",
      "   Training samples: 16512, Test samples: 4128\n",
      "   Test MSE: 0.5559\n",
      "   Test RÂ²: 0.5758\n",
      "\n",
      "   âš ï¸  Problem: This is just ONE evaluation!\n",
      "   - If we're lucky with this split, score looks good\n",
      "   - If we're unlucky, score looks bad\n",
      "   - Cross-validation will give us multiple evaluations!\n",
      "   - Cross-validation averages multiple evaluations â†’ more reliable!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. Simple Train-Test Split (Baseline) - COMPLETE SOLUTION\")\n",
    "print(\"Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨Ø³ÙŠØ· ØªØ¯Ø±ÙŠØ¨-Ø§Ø®ØªØ¨Ø§Ø± (Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³) - Ø­Ù„ ÙƒØ§Ù…Ù„\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# SOLUTION: Simple train-test split (what we've been using)\n",
    "# Why show this? To compare with cross-validation!\n",
    "# train_test_split(X, y, test_size=0.2, random_state=123  # Any number works - just for reproducibility, stratify=y)\n",
    "# - Splits data into training and testing sets\n",
    "# - X: Features (input variables), y: Target (output variable)\n",
    "# - test_size=0.2: 20% for testing, 80% for training\n",
    "# - random_state=123  # Any number works - just for reproducibility: Seed for reproducibility (same split every time)\n",
    "# - stratify=y: Maintains class distribution in train/test (for classification)\n",
    "# - Returns: X_train, X_test, y_train, y_test\n",
    "\n",
    "# SOLUTION: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=123  # Any number works - just for reproducibility\n",
    ")\n",
    "\n",
    "# SOLUTION: Train model\n",
    "model_simple = LinearRegression()\n",
    "model_simple.fit(X_train, y_train)\n",
    "\n",
    "# SOLUTION: Make predictions\n",
    "y_pred = model_simple.predict(X_test)\n",
    "\n",
    "# SOLUTION: Calculate metrics\n",
    "mse_simple = mean_squared_error(y_test, y_pred)\n",
    "r2_simple = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nğŸ“Š Simple Train-Test Split Results:\")\n",
    "print(f\"   Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "print(f\"   Test MSE: {mse_simple:.4f}\")\n",
    "print(f\"   Test RÂ²: {r2_simple:.4f}\")\n",
    "print(f\"\\n   âš ï¸  Problem: This is just ONE evaluation!\")\n",
    "print(f\"   - If we're lucky with this split, score looks good\")\n",
    "print(f\"   - If we're unlucky, score looks bad\")\n",
    "print(f\"   - Cross-validation will give us multiple evaluations!\")\n",
    "print(f\"   - Cross-validation averages multiple evaluations â†’ more reliable!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ” DEMONSTRATION: The Problem with Single Split\n",
      "Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ: Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ÙˆØ§Ø­Ø¯\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Testing 10 different random splits to show variance:\n",
      "   (This demonstrates WHY we need cross-validation!)\n",
      "   Split 1 (random_state=0): MSE = 0.53, RÂ² = 0.5943\n",
      "   Split 2 (random_state=1): MSE = 0.53, RÂ² = 0.5966\n",
      "   Split 3 (random_state=2): MSE = 0.54, RÂ² = 0.6014\n",
      "   Split 4 (random_state=3): MSE = 0.54, RÂ² = 0.5931\n",
      "   Split 5 (random_state=4): MSE = 0.53, RÂ² = 0.5964\n",
      "   Split 6 (random_state=5): MSE = 0.54, RÂ² = 0.6113\n",
      "   Split 7 (random_state=6): MSE = 0.54, RÂ² = 0.5947\n",
      "   Split 8 (random_state=7): MSE = 0.53, RÂ² = 0.6075\n",
      "   Split 9 (random_state=8): MSE = 0.50, RÂ² = 0.6179\n",
      "   Split 10 (random_state=9): MSE = 0.57, RÂ² = 0.5743\n",
      "\n",
      "ğŸ“Š Statistics across 10 different splits:\n",
      "   Mean MSE: 0.53 Â± 0.02\n",
      "   Mean RÂ²: 0.5988 Â± 0.0113\n",
      "   MSE Range: [0.50, 0.57]\n",
      "   RÂ² Range: [0.5743, 0.6179]\n",
      "\n",
      "âš ï¸  KEY INSIGHT: Look at the VARIATION!\n",
      "   - Single split (random_state=42) gave us: MSE = 0.56, RÂ² = 0.5758\n",
      "   - But across 10 different splits: MSE ranges from 0.50 to 0.57\n",
      "   - RÂ² ranges from 0.5743 to 0.6179\n",
      "   - Standard deviation: MSE Â± 0.02, RÂ² Â± 0.0113\n",
      "\n",
      "ğŸ’¡ This is EXACTLY why we need cross-validation!\n",
      "   - Single split = ONE number (might be lucky/unlucky)\n",
      "   - Cross-validation = MEAN Â± STD (reliable estimate with confidence interval)\n",
      "   - Cross-validation does this SYSTEMATICALLY (not random splits)\n",
      "   - Cross-validation uses ALL data efficiently (each sample tested once)\n",
      "\n",
      "âœ… Solution: Use cross-validation to get reliable estimate:\n",
      "   - Instead of: 'MSE = 0.56' (one number, unknown reliability)\n",
      "   - We get: 'MSE = 0.53 Â± 0.02' (mean Â± std, known reliability)\n",
      "   - This tells us the TRUE performance, not just one lucky/unlucky split!\n",
      "\n",
      "âš ï¸  CRITICAL INSIGHT: Why CV Matters Even with Small Variance!\n",
      "   - You might think: 'Variance is small (0.50 to 0.57), so CV doesn't matter'\n",
      "   - BUT: Without CV, you get ONE number (0.56) - is it reliable?\n",
      "   - With CV, you get MEAN Â± STD (0.53 Â± 0.02) - you KNOW the range!\n",
      "   - The point isn't that variance is large - it's that you KNOW the variance!\n",
      "   - Single split (0.56) is within range, but you don't know if it's typical or extreme\n",
      "   - CV tells you: 'True performance is 0.53 Â± 0.02' - RELIABLE estimate!\n",
      "   - Confidence interval: [0.50, 0.57]\n",
      "   - This is MUCH more informative than just one number!\n"
     ]
    }
   ],
   "source": [
    "# DEMONSTRATION: Why Single Split is Unreliable\n",
    "# This shows the PROBLEM with using just one train-test split\n",
    "# We'll do 10 different random splits and see how much the scores vary!\n",
    "# NOTE: This comes AFTER the simple split to use mse_simple and r2_simple for comparison\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” DEMONSTRATION: The Problem with Single Split\")\n",
    "print(\"Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ: Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ÙˆØ§Ø­Ø¯\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ“Š Testing 10 different random splits to show variance:\")\n",
    "print(\"   (This demonstrates WHY we need cross-validation!)\")\n",
    "\n",
    "# Store scores from multiple splits\n",
    "multiple_splits_mse = []\n",
    "multiple_splits_r2 = []\n",
    "\n",
    "# Try 10 different random splits\n",
    "for seed in range(10):\n",
    "    X_train_demo, X_test_demo, y_train_demo, y_test_demo = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=seed\n",
    "    )\n",
    "    \n",
    "    model_demo = LinearRegression()\n",
    "    model_demo.fit(X_train_demo, y_train_demo)\n",
    "    y_pred_demo = model_demo.predict(X_test_demo)\n",
    "    \n",
    "    mse_demo = mean_squared_error(y_test_demo, y_pred_demo)\n",
    "    r2_demo = r2_score(y_test_demo, y_pred_demo)\n",
    "    \n",
    "    multiple_splits_mse.append(mse_demo)\n",
    "    multiple_splits_r2.append(r2_demo)\n",
    "    \n",
    "    print(f\"   Split {seed+1} (random_state={seed}): MSE = {mse_demo:.2f}, RÂ² = {r2_demo:.4f}\")\n",
    "\n",
    "# Calculate statistics\n",
    "mean_mse_multi = np.mean(multiple_splits_mse)\n",
    "std_mse_multi = np.std(multiple_splits_mse)\n",
    "mean_r2_multi = np.mean(multiple_splits_r2)\n",
    "std_r2_multi = np.std(multiple_splits_r2)\n",
    "\n",
    "print(f\"\\nğŸ“Š Statistics across 10 different splits:\")\n",
    "print(f\"   Mean MSE: {mean_mse_multi:.2f} Â± {std_mse_multi:.2f}\")\n",
    "print(f\"   Mean RÂ²: {mean_r2_multi:.4f} Â± {std_r2_multi:.4f}\")\n",
    "print(f\"   MSE Range: [{min(multiple_splits_mse):.2f}, {max(multiple_splits_mse):.2f}]\")\n",
    "print(f\"   RÂ² Range: [{min(multiple_splits_r2):.4f}, {max(multiple_splits_r2):.4f}]\")\n",
    "\n",
    "print(f\"\\nâš ï¸  KEY INSIGHT: Look at the VARIATION!\")\n",
    "print(f\"   - Single split (random_state=123  # Any number works (42, 123, 2024, etc.) - just for reproducibility) gave us: MSE = {mse_simple:.2f}, RÂ² = {r2_simple:.4f}\")\n",
    "print(f\"   - But across 10 different splits: MSE ranges from {min(multiple_splits_mse):.2f} to {max(multiple_splits_mse):.2f}\")\n",
    "print(f\"   - RÂ² ranges from {min(multiple_splits_r2):.4f} to {max(multiple_splits_r2):.4f}\")\n",
    "print(f\"   - Standard deviation: MSE Â± {std_mse_multi:.2f}, RÂ² Â± {std_r2_multi:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ This is EXACTLY why we need cross-validation!\")\n",
    "print(f\"   - Single split = ONE number (might be lucky/unlucky)\")\n",
    "print(f\"   - Cross-validation = MEAN Â± STD (reliable estimate with confidence interval)\")\n",
    "print(f\"   - Cross-validation does this SYSTEMATICALLY (not random splits)\")\n",
    "print(f\"   - Cross-validation uses ALL data efficiently (each sample tested once)\")\n",
    "\n",
    "print(f\"\\nâœ… Solution: Use cross-validation to get reliable estimate:\")\n",
    "print(f\"   - Instead of: 'MSE = {mse_simple:.2f}' (one number, unknown reliability)\")\n",
    "print(f\"   - We get: 'MSE = {mean_mse_multi:.2f} Â± {std_mse_multi:.2f}' (mean Â± std, known reliability)\")\n",
    "print(f\"   - This tells us the TRUE performance, not just one lucky/unlucky split!\")\n",
    "\n",
    "print(f\"\\nâš ï¸  CRITICAL INSIGHT: Why CV Matters Even with Small Variance!\")\n",
    "print(f\"   - You might think: 'Variance is small ({min(multiple_splits_mse):.2f} to {max(multiple_splits_mse):.2f}), so CV doesn't matter'\")\n",
    "print(f\"   - BUT: Without CV, you get ONE number ({mse_simple:.2f}) - is it reliable?\")\n",
    "print(f\"   - With CV, you get MEAN Â± STD ({mean_mse_multi:.2f} Â± {std_mse_multi:.2f}) - you KNOW the range!\")\n",
    "print(f\"   - The point isn't that variance is large - it's that you KNOW the variance!\")\n",
    "print(f\"   - Single split ({mse_simple:.2f}) is within range, but you don't know if it's typical or extreme\")\n",
    "print(f\"   - CV tells you: 'True performance is {mean_mse_multi:.2f} Â± {std_mse_multi:.2f}' - RELIABLE estimate!\")\n",
    "print(f\"   - Confidence interval: [{mean_mse_multi - 2*std_mse_multi:.2f}, {mean_mse_multi + 2*std_mse_multi:.2f}]\")\n",
    "print(f\"   - This is MUCH more informative than just one number!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Comparison: Small vs High Variance | Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©: ØªØ¨Ø§ÙŠÙ† ØµØºÙŠØ± Ù…Ù‚Ø§Ø¨Ù„ ØªØ¨Ø§ÙŠÙ† Ø¹Ø§Ù„ÙŠ\n",
    "\n",
    "**BEFORE**: We saw California Housing shows relatively small variance (0.50-0.57).\n",
    "\n",
    "**AFTER**: We'll see a case with HIGH variance to understand when CV is CRITICAL!\n",
    "\n",
    "**Why show both?**\n",
    "- **Small variance** (California Housing): CV matters because you need to KNOW the variance\n",
    "- **High variance** (smaller/noisier data): CV is CRITICAL because variance is large\n",
    "- **Both cases**: CV provides reliable estimates, not just one number!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ” DEMONSTRATION: High Variance Case - When CV is CRITICAL!\n",
      "Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ: Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø§Ù„Ø¹Ø§Ù„ÙŠ - Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† CV Ø­Ø§Ø³Ù…Ø§Ù‹!\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Using smaller subset (500 samples) of REAL California Housing data:\n",
      "   - This is STILL REAL data from 1990 California census\n",
      "   - Just using first 500 samples (smaller datasets show more variance)\n",
      "   - Same features, same target - just smaller size\n",
      "\n",
      "   Dataset: 500 samples of REAL California Housing data\n",
      "   (Smaller datasets = more variance across splits)\n",
      "   Testing 10 different random splits to show HIGH variance:\n",
      "   Split 1 (random_state=0): MSE = 0.28, RÂ² = 0.7171\n",
      "   Split 2 (random_state=1): MSE = 0.22, RÂ² = 0.7726\n",
      "   Split 3 (random_state=2): MSE = 0.50, RÂ² = 0.5411\n",
      "   Split 4 (random_state=3): MSE = 0.23, RÂ² = 0.7798\n",
      "   Split 5 (random_state=4): MSE = 0.45, RÂ² = 0.5213\n",
      "   Split 6 (random_state=5): MSE = 0.21, RÂ² = 0.7926\n",
      "   Split 7 (random_state=6): MSE = 0.20, RÂ² = 0.7893\n",
      "   Split 8 (random_state=7): MSE = 0.38, RÂ² = 0.6177\n",
      "   Split 9 (random_state=8): MSE = 0.46, RÂ² = 0.6182\n",
      "   Split 10 (random_state=9): MSE = 0.44, RÂ² = 0.6269\n",
      "\n",
      "ğŸ“Š Statistics across 10 different splits (HIGH VARIANCE CASE):\n",
      "   Mean MSE: 0.34 Â± 0.11\n",
      "   Mean RÂ²: 0.6777 Â± 0.0998\n",
      "   MSE Range: [0.20, 0.50]\n",
      "   RÂ² Range: [0.5213, 0.7926]\n",
      "\n",
      "ğŸ“Š COMPARISON: Small Variance vs High Variance\n",
      "============================================================\n",
      "\n",
      "1ï¸âƒ£  California Housing (Full Dataset - 20K samples):\n",
      "   - MSE Range: [0.50, 0.57] (small variance)\n",
      "   - Std: Â± 0.02\n",
      "   - CV still matters: You need to KNOW the variance!\n",
      "\n",
      "2ï¸âƒ£  Smaller Subset (500 samples of REAL data - HIGH variance):\n",
      "   - Still REAL California Housing data (just smaller subset)\n",
      "   - MSE Range: [0.20, 0.50] (HIGH variance)\n",
      "   - Std: Â± 0.11\n",
      "   - CV is CRITICAL: Variance is large, single split is very unreliable!\n",
      "\n",
      "ğŸ’¡ KEY INSIGHT: Both Cases Need Cross-Validation!\n",
      "   - Small variance: CV tells you the TRUE range (even if small)\n",
      "   - High variance: CV is CRITICAL (single split could be very wrong)\n",
      "   - In BOTH cases: CV gives reliable estimate with confidence interval\n",
      "   - Without CV: You have ONE number with UNKNOWN reliability\n",
      "   - With CV: You have MEAN Â± STD with KNOWN reliability\n",
      "\n",
      "âœ… Conclusion:\n",
      "   - Small variance (0.50-0.57): CV still matters â†’ you know the range\n",
      "   - High variance (0.20-0.50): CV is CRITICAL â†’ single split very unreliable\n",
      "   - ALWAYS use CV for reliable model evaluation!\n"
     ]
    }
   ],
   "source": [
    "# DEMONSTRATION: High Variance Case - When CV is CRITICAL!\n",
    "# This shows a scenario where variance is LARGE, making CV essential\n",
    "# We'll use a smaller subset of the SAME REAL California Housing data to demonstrate high variance\n",
    "# NOTE: This is still REAL data - just a smaller subset (first 500 samples)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” DEMONSTRATION: High Variance Case - When CV is CRITICAL!\")\n",
    "print(\"Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ: Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø§Ù„Ø¹Ø§Ù„ÙŠ - Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† CV Ø­Ø§Ø³Ù…Ø§Ù‹!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ“Š Using smaller subset (500 samples) of REAL California Housing data:\")\n",
    "print(\"   - This is STILL REAL data from 1990 California census\")\n",
    "print(\"   - Just using first 500 samples (smaller datasets show more variance)\")\n",
    "print(\"   - Same features, same target - just smaller size\")\n",
    "\n",
    "# Use smaller subset for higher variance demonstration\n",
    "# This is STILL REAL California Housing data - just first 500 samples\n",
    "n_small = 500\n",
    "X_small_high_var = X_scaled[:n_small]  # First 500 samples of REAL California Housing\n",
    "y_small_high_var = y[:n_small]  # First 500 samples of REAL California Housing\n",
    "\n",
    "print(f\"\\n   Dataset: {n_small} samples of REAL California Housing data\")\n",
    "print(\"   (Smaller datasets = more variance across splits)\")\n",
    "print(\"   Testing 10 different random splits to show HIGH variance:\")\n",
    "\n",
    "# Store scores from multiple splits\n",
    "high_var_mse = []\n",
    "high_var_r2 = []\n",
    "\n",
    "# Try 10 different random splits\n",
    "for seed in range(10):\n",
    "    X_train_hv, X_test_hv, y_train_hv, y_test_hv = train_test_split(\n",
    "        X_small_high_var, y_small_high_var, test_size=0.2, random_state=seed\n",
    "    )\n",
    "    \n",
    "    model_hv = LinearRegression()\n",
    "    model_hv.fit(X_train_hv, y_train_hv)\n",
    "    y_pred_hv = model_hv.predict(X_test_hv)\n",
    "    \n",
    "    mse_hv = mean_squared_error(y_test_hv, y_pred_hv)\n",
    "    r2_hv = r2_score(y_test_hv, y_pred_hv)\n",
    "    \n",
    "    high_var_mse.append(mse_hv)\n",
    "    high_var_r2.append(r2_hv)\n",
    "    \n",
    "    print(f\"   Split {seed+1} (random_state={seed}): MSE = {mse_hv:.2f}, RÂ² = {r2_hv:.4f}\")\n",
    "\n",
    "# Calculate statistics\n",
    "mean_mse_hv = np.mean(high_var_mse)\n",
    "std_mse_hv = np.std(high_var_mse)\n",
    "mean_r2_hv = np.mean(high_var_r2)\n",
    "std_r2_hv = np.std(high_var_r2)\n",
    "\n",
    "print(f\"\\nğŸ“Š Statistics across 10 different splits (HIGH VARIANCE CASE):\")\n",
    "print(f\"   Mean MSE: {mean_mse_hv:.2f} Â± {std_mse_hv:.2f}\")\n",
    "print(f\"   Mean RÂ²: {mean_r2_hv:.4f} Â± {std_r2_hv:.4f}\")\n",
    "print(f\"   MSE Range: [{min(high_var_mse):.2f}, {max(high_var_mse):.2f}]\")\n",
    "print(f\"   RÂ² Range: [{min(high_var_r2):.4f}, {max(high_var_r2):.4f}]\")\n",
    "\n",
    "# Compare with California Housing (full dataset)\n",
    "print(f\"\\nğŸ“Š COMPARISON: Small Variance vs High Variance\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1ï¸âƒ£  California Housing (Full Dataset - 20K samples):\")\n",
    "print(f\"   - MSE Range: [0.50, 0.57] (small variance)\")\n",
    "print(f\"   - Std: Â± 0.02\")\n",
    "print(f\"   - CV still matters: You need to KNOW the variance!\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£  Smaller Subset (500 samples of REAL data - HIGH variance):\")\n",
    "print(f\"   - Still REAL California Housing data (just smaller subset)\")\n",
    "print(f\"   - MSE Range: [{min(high_var_mse):.2f}, {max(high_var_mse):.2f}] (HIGH variance)\")\n",
    "print(f\"   - Std: Â± {std_mse_hv:.2f}\")\n",
    "print(f\"   - CV is CRITICAL: Variance is large, single split is very unreliable!\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ KEY INSIGHT: Both Cases Need Cross-Validation!\")\n",
    "print(f\"   - Small variance: CV tells you the TRUE range (even if small)\")\n",
    "print(f\"   - High variance: CV is CRITICAL (single split could be very wrong)\")\n",
    "print(f\"   - In BOTH cases: CV gives reliable estimate with confidence interval\")\n",
    "print(f\"   - Without CV: You have ONE number with UNKNOWN reliability\")\n",
    "print(f\"   - With CV: You have MEAN Â± STD with KNOWN reliability\")\n",
    "\n",
    "print(f\"\\nâœ… Conclusion:\")\n",
    "print(f\"   - Small variance (0.50-0.57): CV still matters â†’ you know the range\")\n",
    "print(f\"   - High variance ({min(high_var_mse):.2f}-{max(high_var_mse):.2f}): CV is CRITICAL â†’ single split very unreliable\")\n",
    "print(f\"   - ALWAYS use CV for reliable model evaluation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: K-Fold Cross-Validation | Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ K-Fold\n",
    "\n",
    "**BEFORE**: Simple split gives one evaluation that might be lucky or unlucky.\n",
    "\n",
    "**AFTER**: K-Fold CV splits data into K folds, trains K times, and averages results - much more reliable!\n",
    "\n",
    "**Why K-Fold?**\n",
    "- **Splits data into K folds**: Each fold becomes test set once\n",
    "- **Trains K times**: Uses all data for both training and testing\n",
    "- **Averages results**: More reliable than single evaluation\n",
    "- **Standard choice**: K=5 or K=10 are most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. K-Fold Cross-Validation\n",
      "Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ K-Fold\n",
      "============================================================\n",
      "\n",
      "   âœ… K-Fold CV created (5 folds)\n",
      "   How it works:\n",
      "   - Split data into 5 equal parts (folds)\n",
      "   - Use fold 1 as test, folds 2-5 as train â†’ score 1\n",
      "\n",
      "   ğŸ“ Common Student Questions:\n",
      "   Q: Why K=5? Why not 3 or 10?\n",
      "      Answer: K=5 is a good balance:\n",
      "      - Too few folds (K=3): Less reliable, fewer evaluations\n",
      "      - Too many folds (K=10): More reliable but slower\n",
      "      - K=5: Good balance of reliability and speed\n",
      "   Q: Why shuffle the data?\n",
      "      Answer: Shuffling randomizes order â†’ prevents bias from data order\n",
      "      Example: If data sorted by date, first fold = old data, last fold = new data\n",
      "      Shuffling ensures each fold has mix of all data types\n",
      "   Q: Why does cross-validation take longer?\n",
      "      Answer: CV trains model K times (5 times for 5-fold)\n",
      "      - Simple split: 1 training â†’ 1 evaluation\n",
      "      - 5-fold CV: 5 trainings â†’ 5 evaluations\n",
      "      - Trade-off: More time for more reliable results\n",
      "   Q: Why do I get different results each time?\n",
      "      Answer: Random shuffling creates different folds\n",
      "      - Solution: Set random_state=42 for reproducible results\n",
      "      - Without random_state: Folds change â†’ results vary slightly\n",
      "   - Use fold 2 as test, folds 1,3-5 as train â†’ score 2\n",
      "   - ... repeat for all 5 folds\n",
      "   - Average all 5 scores â†’ final reliable score!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. K-Fold Cross-Validation\")\n",
    "print(\"Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ K-Fold\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create K-Fold cross-validator\n",
    "# n_splits=5 means we split data into 5 folds\n",
    "# shuffle=True randomizes the order (better for evaluation)\n",
    "# random_state=123  # Any number works (42, 123, 2024, etc.) - just for reproducibility for reproducibility\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=123  # Any number works (42, 123, 2024, etc.) - just for reproducibility)\n",
    "model_kfold = LinearRegression()\n",
    "\n",
    "print(\"\\n   âœ… K-Fold CV created (5 folds)\")\n",
    "print(\"   How it works:\")\n",
    "print(\"   - Split data into 5 equal parts (folds)\")\n",
    "print(\"   - Use fold 1 as test, folds 2-5 as train â†’ score 1\")\n",
    "print(\"\\n   ğŸ“ Common Student Questions:\")\n",
    "print(\"   Q: Why K=5? Why not 3 or 10?\")\n",
    "print(\"      Answer: K=5 is a good balance:\")\n",
    "print(\"      - Too few folds (K=3): Less reliable, fewer evaluations\")\n",
    "print(\"      - Too many folds (K=10): More reliable but slower\")\n",
    "print(\"      - K=5: Good balance of reliability and speed\")\n",
    "print(\"   Q: Why shuffle the data?\")\n",
    "print(\"      Answer: Shuffling randomizes order â†’ prevents bias from data order\")\n",
    "print(\"      Example: If data sorted by date, first fold = old data, last fold = new data\")\n",
    "print(\"      Shuffling ensures each fold has mix of all data types\")\n",
    "print(\"   Q: Why does cross-validation take longer?\")\n",
    "print(\"      Answer: CV trains model K times (5 times for 5-fold)\")\n",
    "print(\"      - Simple split: 1 training â†’ 1 evaluation\")\n",
    "print(\"      - 5-fold CV: 5 trainings â†’ 5 evaluations\")\n",
    "print(\"      - Trade-off: More time for more reliable results\")\n",
    "print(\"   Q: Why do I get different results each time?\")\n",
    "print(\"      Answer: Random shuffling creates different folds\")\n",
    "print(\"      - Solution: Set random_state=123  # Any number works - just for reproducibility for reproducible results\")\n",
    "print(\"      - Without random_state: Folds change â†’ results vary slightly\")\n",
    "print(\"   - Use fold 2 as test, folds 1,3-5 as train â†’ score 2\")\n",
    "print(\"   - ... repeat for all 5 folds\")\n",
    "print(\"   - Average all 5 scores â†’ final reliable score!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â“ How to Choose the Number of Folds (K)? | ÙƒÙŠÙ ØªØ®ØªØ§Ø± Ø¹Ø¯Ø¯ Ø§Ù„Ø·ÙŠØ§Øª (K)ØŸ\n",
    "\n",
    "**BEFORE**: You know K-Fold uses K folds, but don't know how to choose K.\n",
    "\n",
    "**AFTER**: You'll know exactly how to decide between K=3, K=5, K=10, or other values!\n",
    "\n",
    "**This is a VERY common student question!** Let's answer it comprehensively.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Common K Values | Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù€ K\n",
    "\n",
    "| K Value | When to Use | Pros | Cons |\n",
    "|---------|-------------|------|------|\n",
    "| **K=3** | Very large datasets, quick evaluation | â€¢ Fast<br>â€¢ Less computation | â€¢ Less reliable<br>â€¢ Fewer evaluations<br>â€¢ Higher variance |\n",
    "| **K=5** | **Standard choice** (most common) âœ… | â€¢ Good balance<br>â€¢ Reliable<br>â€¢ Fast enough | â€¢ May not be enough for very small datasets |\n",
    "| **K=10** | Small-medium datasets, need reliability | â€¢ Very reliable<br>â€¢ More evaluations<br>â€¢ Lower variance | â€¢ Slower (10 evaluations)<br>â€¢ More computation |\n",
    "| **K=n (LOOCV)** | Very small datasets (< 50 samples) | â€¢ Maximum reliability<br>â€¢ Uses all data | â€¢ Very slow<br>â€¢ High variance<br>â€¢ Expensive |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Rule of Thumb | Ù‚Ø§Ø¹Ø¯Ø© Ø¹Ø§Ù…Ø©\n",
    "\n",
    "**Default Choice: K=5** âœ…\n",
    "- Works well for most problems\n",
    "- Good balance of reliability and speed\n",
    "- Industry standard\n",
    "\n",
    "**When to Use K=10:**\n",
    "- Small-medium datasets (100-5,000 samples)\n",
    "- Need more reliable estimate\n",
    "- Can afford extra computation time\n",
    "- Model comparison or hyperparameter tuning\n",
    "\n",
    "**When to Use K=3:**\n",
    "- Very large datasets (> 50,000 samples)\n",
    "- Need quick evaluation\n",
    "- Computational constraints\n",
    "- Simple split might be sufficient anyway\n",
    "\n",
    "**When to Use K=n (LOOCV):**\n",
    "- Very small datasets (< 50 samples)\n",
    "- Need maximum data usage\n",
    "- Can afford n evaluations\n",
    "\n",
    "---\n",
    "\n",
    "### âš–ï¸ Trade-offs: Bias vs Variance vs Computation | Ø§Ù„Ù…Ù‚Ø§ÙŠØ¶Ø§Øª\n",
    "\n",
    "#### **Bias (Underfitting Risk):**\n",
    "- **K too small (K=3)**: Each fold has more training data â†’ **lower bias** âœ…\n",
    "- **K too large (K=10)**: Each fold has less training data â†’ **slightly higher bias** âš ï¸\n",
    "- **K=n (LOOCV)**: Maximum training data â†’ **lowest bias** âœ…\n",
    "\n",
    "#### **Variance (Reliability):**\n",
    "- **K too small (K=3)**: Fewer evaluations â†’ **higher variance** âš ï¸\n",
    "- **K too large (K=10)**: More evaluations â†’ **lower variance** âœ…\n",
    "- **K=n (LOOCV)**: Most evaluations â†’ **but high variance** (each test set = 1 sample) âš ï¸\n",
    "\n",
    "#### **Computation Time:**\n",
    "- **K=3**: Fast (3 model trainings) âœ…\n",
    "- **K=5**: Moderate (5 model trainings) âœ…\n",
    "- **K=10**: Slower (10 model trainings) âš ï¸\n",
    "- **K=n (LOOCV)**: Very slow (n model trainings) âŒ\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ˆ Visual Guide: K vs Performance | Ø¯Ù„ÙŠÙ„ Ù…Ø±Ø¦ÙŠ\n",
    "\n",
    "```\n",
    "Reliability (Lower Variance)\n",
    "    â†‘\n",
    "    |     K=10 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    |                          â”‚\n",
    "    |     K=5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "    |                  â”‚       â”‚\n",
    "    |     K=3 â”€â”€â”€â”€â”    â”‚       â”‚\n",
    "    |              â”‚    â”‚       â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â†’ Computation Time\n",
    "     Fast          Moderate    Slow\n",
    "\n",
    "Bias (Underfitting Risk)\n",
    "    â†‘\n",
    "    |     K=3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    |                         â”‚\n",
    "    |     K=5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "    |                  â”‚      â”‚\n",
    "    |     K=10 â”€â”€â”€â”€â”   â”‚      â”‚\n",
    "    |              â”‚   â”‚      â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â†’ K Value\n",
    "     Low Bias      Moderate  Higher Bias\n",
    "```\n",
    "\n",
    "**Key Insight:**\n",
    "- **K=5** is the **sweet spot**: Good reliability, acceptable bias, reasonable speed\n",
    "- **K=10** is better for reliability but slower\n",
    "- **K=3** is faster but less reliable\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Decision Framework | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "#### **Step 1: Check Dataset Size**\n",
    "\n",
    "```\n",
    "Dataset Size â†’ Recommended K\n",
    "â”œâ”€ < 50 samples â†’ K=n (LOOCV) or K=10\n",
    "â”œâ”€ 50-100 samples â†’ K=10\n",
    "â”œâ”€ 100-1,000 samples â†’ K=5 or K=10\n",
    "â”œâ”€ 1,000-10,000 samples â†’ K=5 (standard)\n",
    "â””â”€ > 10,000 samples â†’ K=5 or K=3 (or simple split)\n",
    "```\n",
    "\n",
    "#### **Step 2: Check Computation Time**\n",
    "\n",
    "```\n",
    "Model Training Time â†’ Recommended K\n",
    "â”œâ”€ Fast (< 1 second) â†’ K=10 (can afford more folds)\n",
    "â”œâ”€ Moderate (1-10 seconds) â†’ K=5 (good balance)\n",
    "â””â”€ Slow (> 10 seconds) â†’ K=5 or K=3 (limit folds)\n",
    "```\n",
    "\n",
    "#### **Step 3: Check Your Goal**\n",
    "\n",
    "```\n",
    "Goal â†’ Recommended K\n",
    "â”œâ”€ Quick evaluation â†’ K=3 or K=5\n",
    "â”œâ”€ Model comparison â†’ K=5 or K=10 (need reliability)\n",
    "â”œâ”€ Hyperparameter tuning â†’ K=5 or K=10 (need reliability)\n",
    "â””â”€ Final model evaluation â†’ K=5 or K=10 (need reliability)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### **Example 1: House Price Prediction (20,000 samples)**\n",
    "- **Dataset**: 20,000 samples (large)\n",
    "- **Model**: Linear Regression (fast)\n",
    "- **Goal**: Model evaluation\n",
    "- **Decision**: **K=5** âœ…\n",
    "- **Reasoning**: Large dataset, fast model â†’ K=5 is standard and sufficient\n",
    "\n",
    "#### **Example 2: Medical Diagnosis (200 samples)**\n",
    "- **Dataset**: 200 samples (small-medium)\n",
    "- **Model**: Random Forest (moderate speed)\n",
    "- **Goal**: Reliable evaluation (medical application)\n",
    "- **Decision**: **K=10** âœ…\n",
    "- **Reasoning**: Small dataset, need reliability â†’ K=10 gives more reliable estimate\n",
    "\n",
    "#### **Example 3: Image Classification (100,000 samples)**\n",
    "- **Dataset**: 100,000 samples (very large)\n",
    "- **Model**: Deep Neural Network (slow)\n",
    "- **Goal**: Quick evaluation\n",
    "- **Decision**: **K=3** or **Simple Split** âœ…\n",
    "- **Reasoning**: Very large dataset, slow model â†’ K=3 or simple split is sufficient\n",
    "\n",
    "#### **Example 4: Rare Disease Study (30 samples)**\n",
    "- **Dataset**: 30 samples (very small)\n",
    "- **Model**: Logistic Regression (fast)\n",
    "- **Goal**: Maximum reliability\n",
    "- **Decision**: **K=n (LOOCV)** âœ…\n",
    "- **Reasoning**: Very small dataset, need maximum data usage â†’ LOOCV is best\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Default: K=5** âœ…\n",
    "   - Works for most problems\n",
    "   - Good balance of reliability and speed\n",
    "   - Industry standard\n",
    "\n",
    "2. **Use K=10 when:**\n",
    "   - Small-medium datasets (100-5,000 samples)\n",
    "   - Need more reliable estimate\n",
    "   - Can afford extra computation\n",
    "\n",
    "3. **Use K=3 when:**\n",
    "   - Very large datasets (> 50,000 samples)\n",
    "   - Need quick evaluation\n",
    "   - Computational constraints\n",
    "\n",
    "4. **Use K=n (LOOCV) when:**\n",
    "   - Very small datasets (< 50 samples)\n",
    "   - Need maximum data usage\n",
    "   - Can afford n evaluations\n",
    "\n",
    "5. **Trade-offs:**\n",
    "   - **More folds (K=10)**: More reliable, slower, slightly higher bias\n",
    "   - **Fewer folds (K=3)**: Less reliable, faster, lower bias\n",
    "   - **K=5**: Sweet spot for most cases\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Common Student Questions | Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©\n",
    "\n",
    "**Q: Is there a formula to calculate K?**\n",
    "- **Answer**: No exact formula, but rule of thumb: K=5 for most cases, K=10 for small datasets\n",
    "\n",
    "**Q: Can I use K=20 or K=50?**\n",
    "- **Answer**: Usually not recommended. K=10 is already very reliable. More folds = slower with diminishing returns\n",
    "\n",
    "**Q: What if my dataset has 1,000 samples?**\n",
    "- **Answer**: K=5 is standard, but K=10 is also fine if you need more reliability\n",
    "\n",
    "**Q: Does K affect the final model?**\n",
    "- **Answer**: No! K only affects **evaluation**. The final model is trained on ALL data (after evaluation)\n",
    "\n",
    "**Q: Should I always use K=5?**\n",
    "- **Answer**: K=5 is a good default, but adjust based on dataset size and computation time\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Remember: K=5 is the standard choice, but adjust based on your specific situation!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5: Manual Cross-Validation Implementation (Complete Solution) | Ø§Ù„Ø®Ø·ÙˆØ© 3.5: ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ ÙŠØ¯ÙˆÙŠØ§Ù‹ (Ø­Ù„ ÙƒØ§Ù…Ù„)\n",
    "\n",
    "**BEFORE**: You've seen how `cross_val_score` works automatically.\n",
    "\n",
    "**AFTER**: You'll understand exactly what happens inside cross-validation by implementing it manually!\n",
    "\n",
    "**Why this matters**: Understanding the manual process helps you:\n",
    "- **Know what's happening** under the hood\n",
    "- **Debug issues** when cross-validation doesn't work\n",
    "- **Customize** cross-validation for special cases\n",
    "- **Build confidence** in using cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Manual K-Fold Cross-Validation Implementation\n",
      "ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ K-Fold ÙŠØ¯ÙˆÙŠØ§Ù‹\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Manual Cross-Validation Process:\n",
      "\n",
      "   Fold 1:\n",
      "      Train size: 16512, Val size: 4128\n",
      "      MSE: 0.5559, RÂ²: 0.5758\n",
      "\n",
      "   Fold 2:\n",
      "      Train size: 16512, Val size: 4128\n",
      "      MSE: 0.5277, RÂ²: 0.6137\n",
      "\n",
      "   Fold 3:\n",
      "      Train size: 16512, Val size: 4128\n",
      "      MSE: 0.5093, RÂ²: 0.6086\n",
      "\n",
      "   Fold 4:\n",
      "      Train size: 16512, Val size: 4128\n",
      "      MSE: 0.5049, RÂ²: 0.6213\n",
      "\n",
      "   Fold 5:\n",
      "      Train size: 16512, Val size: 4128\n",
      "      MSE: 0.5552, RÂ²: 0.5875\n",
      "\n",
      "âœ… Manual Cross-Validation Results:\n",
      "   Mean MSE: 0.5306 (+/- 0.0435)\n",
      "   Mean RÂ²: 0.6014 (+/- 0.0340)\n",
      "\n",
      "ğŸ’¡ Key Takeaway:\n",
      "   - This manual implementation shows exactly what cross-validation does!\n",
      "   - cross_val_score() does exactly the same thing automatically\n",
      "   - It is just a convenient shortcut that does all 5 steps automatically\n",
      "   - Now you understand what happens inside cross-validation!\n",
      "   - We will compare these results with cross_val_score() in the next cell!\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Manual K-Fold Cross-Validation Implementation\n",
    "# This shows exactly what cross_val_score does internally!\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Manual K-Fold Cross-Validation Implementation\")\n",
    "print(\"ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ K-Fold ÙŠØ¯ÙˆÙŠØ§Ù‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# SOLUTION: Step 1 - Create K-Fold splitter\n",
    "kfold_manual = KFold(n_splits=5, shuffle=True, random_state=123  # Any number works (42, 123, 2024, etc.) - just for reproducibility)\n",
    "\n",
    "# SOLUTION: Step 2 - Initialize lists to store scores\n",
    "manual_mse_scores = []\n",
    "manual_r2_scores = []\n",
    "\n",
    "# SOLUTION: Step 3 - Loop through each fold\n",
    "print(\"\\nğŸ“Š Manual Cross-Validation Process:\")\n",
    "for fold_num, (train_idx, val_idx) in enumerate(kfold_manual.split(X_scaled), 1):\n",
    "    # SOLUTION: Step 3a - Split data for this fold\n",
    "    X_train_fold = X_scaled[train_idx]\n",
    "    X_val_fold = X_scaled[val_idx]\n",
    "    y_train_fold = y[train_idx]\n",
    "    y_val_fold = y[val_idx]\n",
    "    \n",
    "    # SOLUTION: Step 3b - Train model on training fold\n",
    "    model_fold = LinearRegression()\n",
    "    model_fold.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # SOLUTION: Step 3c - Make predictions on validation fold\n",
    "    y_pred_fold = model_fold.predict(X_val_fold)\n",
    "    \n",
    "    # SOLUTION: Step 3d - Calculate metrics for this fold\n",
    "    mse_fold = mean_squared_error(y_val_fold, y_pred_fold)\n",
    "    r2_fold = r2_score(y_val_fold, y_pred_fold)\n",
    "    \n",
    "    # SOLUTION: Step 3e - Store scores\n",
    "    manual_mse_scores.append(mse_fold)\n",
    "    manual_r2_scores.append(r2_fold)\n",
    "    \n",
    "    print(f\"\\n   Fold {fold_num}:\")\n",
    "    print(f\"      Train size: {len(train_idx)}, Val size: {len(val_idx)}\")\n",
    "    print(f\"      MSE: {mse_fold:.4f}, RÂ²: {r2_fold:.4f}\")\n",
    "\n",
    "# SOLUTION: Step 4 - Calculate mean and std across all folds\n",
    "mean_mse_manual = np.mean(manual_mse_scores)\n",
    "std_mse_manual = np.std(manual_mse_scores)\n",
    "mean_r2_manual = np.mean(manual_r2_scores)\n",
    "std_r2_manual = np.std(manual_r2_scores)\n",
    "\n",
    "print(f\"\\nâœ… Manual Cross-Validation Results:\")\n",
    "print(f\"   Mean MSE: {mean_mse_manual:.4f} (+/- {std_mse_manual * 2:.4f})\")\n",
    "print(f\"   Mean RÂ²: {mean_r2_manual:.4f} (+/- {std_r2_manual * 2:.4f})\")\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ’¡ Key Takeaway:\")\n",
    "print(f\"   - This manual implementation shows exactly what cross-validation does!\")\n",
    "print(f\"   - cross_val_score() does exactly the same thing automatically\")\n",
    "print(f\"   - It is just a convenient shortcut that does all 5 steps automatically\")\n",
    "print(f\"   - Now you understand what happens inside cross-validation!\")\n",
    "print(f\"   - We will compare these results with cross_val_score() in the next cell!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š 5-Fold Cross-Validation Results:\n",
      "Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ 5-Fold:\n",
      "\n",
      "ğŸ“ˆ MSE Scores (for each fold):\n",
      "   Fold 1: 0.5559\n",
      "   Fold 2: 0.5277\n",
      "   Fold 3: 0.5093\n",
      "   Fold 4: 0.5049\n",
      "   Fold 5: 0.5552\n",
      "\n",
      "ğŸ“ˆ RÂ² Scores (for each fold):\n",
      "   Fold 1: 0.5758\n",
      "   Fold 2: 0.6137\n",
      "   Fold 3: 0.6086\n",
      "   Fold 4: 0.6213\n",
      "   Fold 5: 0.5875\n",
      "\n",
      "âœ… Summary Statistics:\n",
      "   Mean MSE: 0.5306 (+/- 0.0435)\n",
      "   Mean RÂ²: 0.6014 (+/- 0.0340)\n",
      "\n",
      "   ğŸ“Š Comparison with Simple Split:\n",
      "   - Simple Split RÂ²: 0.5758\n",
      "   - CV Mean RÂ²: 0.6014\n",
      "   - CV gives us confidence interval: 0.5674 to 0.6354\n",
      "   - Much more informative than single score!\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Perform cross-validation\n",
    "# cross_val_score automatically:\n",
    "# 1. Splits data using kfold\n",
    "# 2. Trains model on each training fold\n",
    "# 3. Evaluates on each test fold\n",
    "# 4. Returns scores for all folds\n",
    "\n",
    "# SOLUTION: Get MSE scores (negative because sklearn maximizes scores)\n",
    "cv_scores_mse = cross_val_score(model_kfold, X_scaled, y,\n",
    "                                cv=kfold, scoring='neg_mean_squared_error')\n",
    "# Note: 'neg_mean_squared_error' returns negative MSE (sklearn convention)\n",
    "# We'll convert to positive when displaying\n",
    "\n",
    "# SOLUTION: Get RÂ² scores\n",
    "cv_scores_r2 = cross_val_score(model_kfold, X_scaled, y,\n",
    "                               cv=kfold, scoring='r2')\n",
    "\n",
    "print(f\"\\nğŸ“Š 5-Fold Cross-Validation Results:\")\n",
    "print(f\"Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ 5-Fold:\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ MSE Scores (for each fold):\")\n",
    "for i, score in enumerate(cv_scores_mse):\n",
    "    print(f\"   Fold {i+1}: {-score:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ RÂ² Scores (for each fold):\")\n",
    "for i, score in enumerate(cv_scores_r2):\n",
    "    print(f\"   Fold {i+1}: {score:.4f}\")\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "# Mean = average performance across all folds\n",
    "# Std = how much performance varies (lower is better - more stable!)\n",
    "mean_mse = -cv_scores_mse.mean()\n",
    "std_mse = cv_scores_mse.std()\n",
    "mean_r2 = cv_scores_r2.mean()\n",
    "std_r2 = cv_scores_r2.std()\n",
    "\n",
    "print(f\"\\nâœ… Summary Statistics:\")\n",
    "print(f\"   Mean MSE: {mean_mse:.4f} (+/- {std_mse * 2:.4f})\")\n",
    "print(f\"   Mean RÂ²: {mean_r2:.4f} (+/- {std_r2 * 2:.4f})\")\n",
    "print(f\"\\n   ğŸ“Š Comparison with Simple Split:\")\n",
    "print(f\"   - Simple Split RÂ²: {r2_simple:.4f}\")\n",
    "print(f\"   - CV Mean RÂ²: {mean_r2:.4f}\")\n",
    "print(f\"   - CV gives us confidence interval: {mean_r2 - std_r2*2:.4f} to {mean_r2 + std_r2*2:.4f}\")\n",
    "print(f\"   - Much more informative than single score!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. Cross-Validate with Multiple Metrics\n",
      "Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ Ø¨Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…ØªØ¹Ø¯Ø¯Ø©\n",
      "============================================================\n",
      "\n",
      "ğŸ“š Difference between cross_val_score() and cross_validate():\n",
      "   - cross_val_score(): Returns scores for ONE metric (simpler)\n",
      "   - cross_validate(): Returns scores for MULTIPLE metrics (more flexible)\n",
      "   - Both use the same cross-validation process, just different output format\n",
      "   - Use cross_val_score() when you need one metric\n",
      "   - Use cross_validate() when you need multiple metrics (like here)\n",
      "\n",
      "Cross-Validation Results (Multiple Metrics):\n",
      "Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ (Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…ØªØ¹Ø¯Ø¯Ø©):\n",
      "\n",
      "Test MSE: 0.5306 (+/- 0.0435)\n",
      "Test MAE: 0.5317 (+/- 0.0169)\n",
      "Test RÂ²: 0.6014 (+/- 0.0340)\n",
      "\n",
      "ğŸ’¡ Why Multiple Metrics?\n",
      "   - MSE: Penalizes large errors more (sensitive to outliers)\n",
      "   - MAE: Treats all errors equally (less sensitive to outliers)\n",
      "   - RÂ²: Shows how much variance is explained (interpretable)\n",
      "   - Different metrics tell different stories about model performance!\n"
     ]
    }
   ],
   "source": [
    "# 3. Cross-Validate with Multiple Metrics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. Cross-Validate with Multiple Metrics\")\n",
    "print(\"Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ Ø¨Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…ØªØ¹Ø¯Ø¯Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ“š Difference between cross_val_score() and cross_validate():\")\n",
    "print(\"   - cross_val_score(): Returns scores for ONE metric (simpler)\")\n",
    "print(\"   - cross_validate(): Returns scores for MULTIPLE metrics (more flexible)\")\n",
    "print(\"   - Both use the same cross-validation process, just different output format\")\n",
    "print(\"   - Use cross_val_score() when you need one metric\")\n",
    "print(\"   - Use cross_validate() when you need multiple metrics (like here)\")\n",
    "\n",
    "scoring = {\n",
    "    'mse': 'neg_mean_squared_error',\n",
    "    'mae': 'neg_mean_absolute_error',\n",
    "    'r2': 'r2'\n",
    "}\n",
    "cv_results = cross_validate(model_kfold, X_scaled, y,\n",
    "                           cv=kfold, scoring=scoring, return_train_score=True)\n",
    "print(\"\\nCross-Validation Results (Multiple Metrics):\")\n",
    "print(\"Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ (Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…ØªØ¹Ø¯Ø¯Ø©):\")\n",
    "print(f\"\\nTest MSE: {-cv_results['test_mse'].mean():.4f} (+/- {cv_results['test_mse'].std() * 2:.4f})\")\n",
    "print(f\"Test MAE: {-cv_results['test_mae'].mean():.4f} (+/- {cv_results['test_mae'].std() * 2:.4f})\")\n",
    "print(f\"Test RÂ²: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std() * 2:.4f})\")\n",
    "print(\"\\nğŸ’¡ Why Multiple Metrics?\")\n",
    "print(\"   - MSE: Penalizes large errors more (sensitive to outliers)\")\n",
    "print(\"   - MAE: Treats all errors equally (less sensitive to outliers)\")\n",
    "print(\"   - RÂ²: Shows how much variance is explained (interpretable)\")\n",
    "print(\"   - Different metrics tell different stories about model performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Why Visualize K-Fold Splits? | Ù„Ù…Ø§Ø°Ø§ ØªØµÙˆØ± ØªÙ‚Ø³ÙŠÙ…Ø§Øª K-FoldØŸ\n",
    "\n",
    "**BEFORE**: You understand K-Fold conceptually, but haven't seen how data is actually split.\n",
    "\n",
    "**AFTER**: You'll see visually how each fold uses different data for training and validation!\n",
    "\n",
    "**Why visualize?**\n",
    "- **See the splits**: Visual confirmation of how data is divided\n",
    "- **Understand shuffling**: See why shuffling ensures each fold has a mix of data\n",
    "- **Build intuition**: Visual learning helps understand the concept better\n",
    "- **Verify process**: Confirm that each fold uses different data\n",
    "\n",
    "**What you'll learn:**\n",
    "- How each fold gets different training/validation data\n",
    "- Why shuffling matters (visible in the plot)\n",
    "- How all data is used (each sample appears in different folds)\n",
    "- The systematic nature of K-Fold (not random!)\n",
    "\n",
    "**Note:** This visualization shows only 2 features (Feature 0 and Feature 1) for clarity, but the actual dataset has 8 features. This is a 2D projection to help you visualize the concept!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. Comparing Different Models with Cross-Validation\n",
      "Ù…Ù‚Ø§Ø±Ù†Ø© Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØªÙ„ÙØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹\n",
      "============================================================\n",
      "\n",
      "Model Comparison:\n",
      "            Model  Mean MSE  Std MSE   Mean RÂ²\n",
      "Linear Regression  0.530572 0.021771  0.601378\n",
      "      Ridge (Î±=1)  0.530565 0.021762  0.601384\n",
      "     Ridge (Î±=10)  0.530506 0.021680  0.601429\n",
      "    Lasso (Î±=0.1)  0.674294 0.013741  0.493429\n",
      "      Lasso (Î±=1)  1.331687 0.023900 -0.000282\n",
      "\n",
      "============================================================\n",
      "ğŸ’¡ Interpreting Cross-Validation Results | ØªÙØ³ÙŠØ± Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Best Model by MSE: Ridge (Î±=10)\n",
      "   - Mean MSE: 0.5305\n",
      "   - Std MSE: 0.0217\n",
      "   - Lower MSE = better predictions\n",
      "\n",
      "ğŸ“Š Best Model by RÂ²: Ridge (Î±=10)\n",
      "   - Mean RÂ²: 0.6014\n",
      "   - Higher RÂ² = explains more variance\n",
      "\n",
      "ğŸ” Understanding Standard Deviation:\n",
      "   - Std MSE shows variability across folds\n",
      "   - Lower std = more consistent performance\n",
      "   - High std = model performance varies a lot\n",
      "   - Linear Regression: Std/MSE ratio = 0.04 (âœ… Very consistent)\n",
      "   - Ridge (Î±=1): Std/MSE ratio = 0.04 (âœ… Very consistent)\n",
      "   - Ridge (Î±=10): Std/MSE ratio = 0.04 (âœ… Very consistent)\n",
      "   - Lasso (Î±=0.1): Std/MSE ratio = 0.02 (âœ… Very consistent)\n",
      "   - Lasso (Î±=1): Std/MSE ratio = 0.02 (âœ… Very consistent)\n",
      "\n",
      "ğŸ“Š Regularization Analysis:\n",
      "   - Best Ridge: Ridge (Î±=10) (MSE: 0.5305)\n",
      "   - Alpha = 1.0 vs 10.0: Higher alpha is better\n",
      "   - Best Lasso: Lasso (Î±=0.1) (MSE: 0.6743)\n",
      "   - Alpha = 0.1 vs 1.0: Lower alpha is better\n",
      "\n",
      "ğŸ“š What This Teaches Us:\n",
      "   - Cross-validation gives reliable model comparison\n",
      "   - Mean shows average performance, Std shows consistency\n",
      "   - Lower std = more reliable model\n",
      "   - Compare models using both mean and std\n",
      "   - Regularization (Ridge/Lasso) may or may not improve performance\n",
      "   - Alpha (regularization strength) needs tuning - not always better\n",
      "   - CV prevents overfitting to a single train/test split\n"
     ]
    }
   ],
   "source": [
    "# 4. Comparing Different Models with Cross-Validation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Comparing Different Models with Cross-Validation\")\n",
    "print(\"Ù…Ù‚Ø§Ø±Ù†Ø© Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØªÙ„ÙØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹\")\n",
    "print(\"=\" * 60)\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge (Î±=1)': Ridge(alpha=1.0),\n",
    "    'Ridge (Î±=10)': Ridge(alpha=10.0),\n",
    "    'Lasso (Î±=0.1)': Lasso(alpha=0.1),\n",
    "    'Lasso (Î±=1)': Lasso(alpha=1.0)\n",
    "}\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_scaled, y,\n",
    "                                cv=kfold, scoring='neg_mean_squared_error')\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Mean MSE': -cv_scores.mean(),\n",
    "        'Std MSE': cv_scores.std(),\n",
    "        'Mean RÂ²': cross_val_score(model, X_scaled, y, cv=kfold, scoring='r2').mean()\n",
    "    })\n",
    "# pd.DataFrame(data)\n",
    "# - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "# - data: Dictionary where keys become column names, values become column data\n",
    "#   - Each key-value pair: key = column name, value = list of values for that column\n",
    "# - Returns DataFrame with rows and columns\n",
    "# - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Add interpretation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ’¡ Interpreting Cross-Validation Results | ØªÙØ³ÙŠØ± Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_mse_idx = results_df['Mean MSE'].idxmin()\n",
    "best_r2_idx = results_df['Mean RÂ²'].idxmax()\n",
    "best_model_mse = results_df.loc[best_mse_idx, 'Model']\n",
    "best_model_r2 = results_df.loc[best_r2_idx, 'Model']\n",
    "\n",
    "print(f\"\\nğŸ“Š Best Model by MSE: {best_model_mse}\")\n",
    "print(f\"   - Mean MSE: {results_df.loc[best_mse_idx, 'Mean MSE']:.4f}\")\n",
    "print(f\"   - Std MSE: {results_df.loc[best_mse_idx, 'Std MSE']:.4f}\")\n",
    "print(f\"   - Lower MSE = better predictions\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Best Model by RÂ²: {best_model_r2}\")\n",
    "print(f\"   - Mean RÂ²: {results_df.loc[best_r2_idx, 'Mean RÂ²']:.4f}\")\n",
    "print(f\"   - Higher RÂ² = explains more variance\")\n",
    "\n",
    "print(f\"\\nğŸ” Understanding Standard Deviation:\")\n",
    "print(f\"   - Std MSE shows variability across folds\")\n",
    "print(f\"   - Lower std = more consistent performance\")\n",
    "print(f\"   - High std = model performance varies a lot\")\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    std_ratio = row['Std MSE'] / row['Mean MSE']\n",
    "    if std_ratio < 0.1:\n",
    "        consistency = \"âœ… Very consistent\"\n",
    "    elif std_ratio < 0.2:\n",
    "        consistency = \"âœ… Consistent\"\n",
    "    else:\n",
    "        consistency = \"âš ï¸  Variable\"\n",
    "    print(f\"   - {row['Model']}: Std/MSE ratio = {std_ratio:.2f} ({consistency})\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Regularization Analysis:\")\n",
    "ridge_models = results_df[results_df['Model'].str.contains('Ridge')]\n",
    "lasso_models = results_df[results_df['Model'].str.contains('Lasso')]\n",
    "\n",
    "if len(ridge_models) > 0:\n",
    "    best_ridge = ridge_models.loc[ridge_models['Mean MSE'].idxmin()]\n",
    "    print(f\"   - Best Ridge: {best_ridge['Model']} (MSE: {best_ridge['Mean MSE']:.4f})\")\n",
    "    print(f\"   - Alpha = 1.0 vs 10.0: {'Lower alpha is better' if best_ridge['Model'].endswith('1)') else 'Higher alpha is better'}\")\n",
    "\n",
    "if len(lasso_models) > 0:\n",
    "    best_lasso = lasso_models.loc[lasso_models['Mean MSE'].idxmin()]\n",
    "    print(f\"   - Best Lasso: {best_lasso['Model']} (MSE: {best_lasso['Mean MSE']:.4f})\")\n",
    "    print(f\"   - Alpha = 0.1 vs 1.0: {'Lower alpha is better' if best_lasso['Model'].endswith('0.1)') else 'Higher alpha is better'}\")\n",
    "\n",
    "print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "print(f\"   - Cross-validation gives reliable model comparison\")\n",
    "print(f\"   - Mean shows average performance, Std shows consistency\")\n",
    "print(f\"   - Lower std = more reliable model\")\n",
    "print(f\"   - Compare models using both mean and std\")\n",
    "print(f\"   - Regularization (Ridge/Lasso) may or may not improve performance\")\n",
    "print(f\"   - Alpha (regularization strength) needs tuning - not always better\")\n",
    "print(f\"   - CV prevents overfitting to a single train/test split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model (lowest MSE): Ridge (Î±=10)\n"
     ]
    }
   ],
   "source": [
    "# Find best model\n",
    "best_model_name = results_df.loc[results_df['Mean MSE'].idxmin(), 'Model']\n",
    "print(f\"\\nBest Model (lowest MSE): {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. K-Fold Visualization\n",
      "ØªØµÙˆØ± K-Fold\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 5. K-Fold Visualization\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. K-Fold Visualization\")\n",
    "print(\"ØªØµÙˆØ± K-Fold\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# SOLUTION: Visualize K-Fold splits\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This shows how data is divided into training and validation sets for each fold\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# NOTE: We show only 2 features (Feature 0 = MedInc, Feature 1 = HouseAge) for 2D visualization\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# The actual dataset has 8 features, but 2D plot helps visualize the concept!\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubplots\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m axes \u001b[38;5;241m=\u001b[39m axes\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m      9\u001b[0m kfold_viz \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/pyplot.py:1502\u001b[0m, in \u001b[0;36msubplots\u001b[0;34m(nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;124;03mCreate a figure and a set of subplots.\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \n\u001b[1;32m   1500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m fig \u001b[38;5;241m=\u001b[39m figure(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfig_kw)\n\u001b[0;32m-> 1502\u001b[0m axs \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubplots\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubplot_kw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubplot_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgridspec_kw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgridspec_kw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mwidth_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth_ratios\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fig, axs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/figure.py:917\u001b[0m, in \u001b[0;36mFigureBase.subplots\u001b[0;34m(self, nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw)\u001b[0m\n\u001b[1;32m    914\u001b[0m     gridspec_kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth_ratios\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m width_ratios\n\u001b[1;32m    916\u001b[0m gs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_gridspec(nrows, ncols, figure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgridspec_kw)\n\u001b[0;32m--> 917\u001b[0m axs \u001b[38;5;241m=\u001b[39m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubplots\u001b[49m\u001b[43m(\u001b[49m\u001b[43msharex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m                  \u001b[49m\u001b[43msubplot_kw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubplot_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m axs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/gridspec.py:299\u001b[0m, in \u001b[0;36mGridSpecBase.subplots\u001b[0;34m(self, sharex, sharey, squeeze, subplot_kw)\u001b[0m\n\u001b[1;32m    297\u001b[0m         subplot_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msharex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m shared_with[sharex]\n\u001b[1;32m    298\u001b[0m         subplot_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msharey\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m shared_with[sharey]\n\u001b[0;32m--> 299\u001b[0m         axarr[row, col] \u001b[38;5;241m=\u001b[39m \u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_subplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msubplot_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# turn off redundant tick labeling\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sharex \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/figure.py:768\u001b[0m, in \u001b[0;36mFigureBase.add_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m         args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])))\n\u001b[1;32m    766\u001b[0m     projection_class, pkw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_projection_requirements(\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 768\u001b[0m     ax \u001b[38;5;241m=\u001b[39m \u001b[43mprojection_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     key \u001b[38;5;241m=\u001b[39m (projection_class, pkw)\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_axes_internal(ax, key)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/axes/_base.py:683\u001b[0m, in \u001b[0;36m_AxesBase.__init__\u001b[0;34m(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, *args, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_axisbelow(mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.axisbelow\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rasterization_zorder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 683\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;66;03m# funcs used to format x and y - fall back on major formatters\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt_xdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/axes/_base.py:1395\u001b[0m, in \u001b[0;36m_AxesBase.clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcla()\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__clear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/axes/_base.py:1382\u001b[0m, in \u001b[0;36m_AxesBase.__clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshare\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)(share)\n\u001b[1;32m   1381\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1382\u001b[0m         \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1383\u001b[0m         axis\u001b[38;5;241m.\u001b[39m_set_lim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, auto\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_transScale()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/axis.py:782\u001b[0m, in \u001b[0;36mAxis._set_scale\u001b[0;34m(self, value, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_default_locators_and_formatters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misDefault_majloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misDefault_minloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/scale.py:105\u001b[0m, in \u001b[0;36mLinearScale.set_default_locators_and_formatters\u001b[0;34m(self, axis)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_default_locators_and_formatters\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# docstring inherited\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     axis\u001b[38;5;241m.\u001b[39mset_major_locator(\u001b[43mAutoLocator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    106\u001b[0m     axis\u001b[38;5;241m.\u001b[39mset_major_formatter(ScalarFormatter())\n\u001b[1;32m    107\u001b[0m     axis\u001b[38;5;241m.\u001b[39mset_minor_formatter(NullFormatter())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/ticker.py:2900\u001b[0m, in \u001b[0;36mAutoLocator.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2898\u001b[0m     nbins \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2899\u001b[0m     steps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2.5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m-> 2900\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/ticker.py:2008\u001b[0m, in \u001b[0;36mMaxNLocator.__init__\u001b[0;34m(self, nbins, **kwargs)\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nbins \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2007\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnbins\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m nbins\n\u001b[0;32m-> 2008\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/ticker.py:2068\u001b[0m, in \u001b[0;36mMaxNLocator.set_params\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2067\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_steps(steps)\n\u001b[0;32m-> 2068\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_staircase\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m   2070\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_integer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/course2/lib/python3.8/site-packages/matplotlib/ticker.py:2029\u001b[0m, in \u001b[0;36mMaxNLocator._staircase\u001b[0;34m(steps)\u001b[0m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   2026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_staircase\u001b[39m(steps):\n\u001b[1;32m   2027\u001b[0m     \u001b[38;5;66;03m# Make an extended staircase within which the needed step will be\u001b[39;00m\n\u001b[1;32m   2028\u001b[0m     \u001b[38;5;66;03m# found.  This is probably much larger than necessary.\u001b[39;00m\n\u001b[0;32m-> 2029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SOLUTION: Visualize K-Fold splits\n",
    "# This shows how data is divided into training and validation sets for each fold\n",
    "# NOTE: We show only 2 features (Feature 0 = MedInc, Feature 1 = HouseAge) for 2D visualization\n",
    "# The actual dataset has 8 features, but 2D plot helps visualize the concept!\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "kfold_viz = KFold(n_splits=5, shuffle=True, random_state=123  # Any number works (42, 123, 2024, etc.) - just for reproducibility)\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold_viz.split(X_scaled)):\n",
    "    ax = axes[fold_idx]\n",
    "    \n",
    "    # SOLUTION: Plot training data (blue dots)\n",
    "    # Training data = data used to train the model for this fold (80% of data)\n",
    "    ax.scatter(X_scaled[train_idx, 0], X_scaled[train_idx, 1],\n",
    "              alpha=0.5, label='Train', s=30, color='blue')\n",
    "    \n",
    "    # SOLUTION: Plot validation data (red X's)\n",
    "    # Validation data = data used to test the model for this fold (20% of data)\n",
    "    ax.scatter(X_scaled[val_idx, 0], X_scaled[val_idx, 1],\n",
    "              alpha=0.8, label='Validation', s=50, marker='x', color='red')\n",
    "    \n",
    "    ax.set_title(f'Fold {fold_idx + 1} (Train: {len(train_idx)}, Val: {len(val_idx)})')\n",
    "    ax.set_xlabel('Feature 0 (scaled) - MedInc')\n",
    "    ax.set_ylabel('Feature 1 (scaled) - HouseAge')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide the last subplot (we only have 5 folds)\n",
    "axes[-1].axis('off')\n",
    "plt.suptitle('K-Fold Cross-Validation: How Data is Split into 5 Folds', \n",
    "             fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('kfold_visualization.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(\"\\nâœ… Plot saved as 'kfold_visualization.png'\")\n",
    "print(\"\\nğŸ“Š What This Visualization Shows:\")\n",
    "print(\"   - Blue dots = Training data for each fold (80% of data)\")\n",
    "print(\"   - Red X's = Validation data for each fold (20% of data)\")\n",
    "print(\"   - Each fold uses DIFFERENT data for training/validation!\")\n",
    "print(\"   - Notice: Each sample appears as validation in ONE fold, training in FOUR folds\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Observations:\")\n",
    "print(\"   - Fold 1: Different validation set than Fold 2, 3, 4, 5\")\n",
    "print(\"   - Shuffling ensures each fold has a MIX of all data types\")\n",
    "print(\"   - All data is used: Each sample tested once, trained 4 times\")\n",
    "print(\"   - This is why CV is reliable: Tests on different data each time!\")\n",
    "\n",
    "print(\"\\nâš ï¸  Note: This shows only 2 features (2D projection) for visualization.\")\n",
    "print(\"   The actual dataset has 8 features, but 2D helps you see the concept!\")\n",
    "print(\"   - Feature 0 = MedInc (Median Income)\")\n",
    "print(\"   - Feature 1 = HouseAge (House Age)\")\n",
    "print(\"   - In reality, we use all 8 features for modeling!\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. Leave-One-Out Cross-Validation (LOOCV)\n",
      "Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ Ø¨Ø¥Ø®Ø±Ø§Ø¬ ÙˆØ§Ø­Ø¯ (LOOCV)\n",
      "============================================================\n",
      "\n",
      "Note: LOOCV is computationally expensive for large datasets\n",
      "Ù…Ù„Ø§Ø­Ø¸Ø©: LOOCV Ù…ÙƒÙ„Ù Ø­Ø³Ø§Ø¨ÙŠØ§Ù‹ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©\n"
     ]
    }
   ],
   "source": [
    "# 6. Leave-One-Out Cross-Validation (LOOCV)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. Leave-One-Out Cross-Validation (LOOCV)\")\n",
    "print(\"Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ Ø¨Ø¥Ø®Ø±Ø§Ø¬ ÙˆØ§Ø­Ø¯ (LOOCV)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNote: LOOCV is computationally expensive for large datasets\")\n",
    "print(\"Ù…Ù„Ø§Ø­Ø¸Ø©: LOOCV Ù…ÙƒÙ„Ù Ø­Ø³Ø§Ø¨ÙŠØ§Ù‹ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Using 30 samples for LOOCV demonstration\n",
      "   (LOOCV with all 20640 samples would take too long)\n",
      "\n",
      "âœ… LOOCV Results (n=30):\n",
      "   Each fold: 1 test sample, 29 training samples\n",
      "   Total folds: 30 (one for each sample)\n",
      "\n",
      "ğŸ“Š MSE Results (standard metric for LOOCV):\n",
      "   Mean MSE: 0.3040\n",
      "   Std MSE: 0.6278\n",
      "   Min MSE: 0.0002\n",
      "   Max MSE: 3.5534\n",
      "\n",
      "ğŸ’¡ Why MSE instead of RÂ² for LOOCV?\n",
      "   - RÂ² requires at least 2 samples in test set to calculate variance\n",
      "   - LOOCV test sets have only 1 sample â†’ RÂ² is undefined (SS_tot = 0)\n",
      "   - MSE works perfectly with single predictions â†’ standard for LOOCV\n",
      "   - MSE directly measures prediction error, which is what we want!\n",
      "\n",
      "ğŸ’¡ Key Points:\n",
      "   - LOOCV gives the most thorough evaluation (every sample tested)\n",
      "   - Each model is trained on n-1 samples and tested on 1 sample\n",
      "   - Very reliable but slow (n model trainings required)\n",
      "   - Best for small datasets where you need maximum data usage!\n",
      "   - MSE is the standard metric for LOOCV (RÂ² not applicable)\n",
      "\n",
      "ğŸ“Š Comparison: LOOCV vs 5-Fold CV (on same 30 samples):\n",
      "   LOOCV Mean MSE: 0.3040\n",
      "   5-Fold Mean MSE: 0.2562\n",
      "   Difference: 0.0478\n",
      "   ğŸ’¡ Both methods give similar results, but LOOCV is more thorough!\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Leave-One-Out Cross-Validation (LOOCV)\n",
    "# LOOCV is expensive for large datasets, so we use a smaller subset for demonstration\n",
    "# In practice, LOOCV is typically used with datasets < 100 samples\n",
    "\n",
    "# Use first 30 samples for LOOCV demonstration (faster, still shows the concept)\n",
    "n_loocv = 30\n",
    "X_small = X_scaled[:n_loocv]\n",
    "y_small = y[:n_loocv]\n",
    "\n",
    "print(f\"\\nğŸ“Š Using {n_loocv} samples for LOOCV demonstration\")\n",
    "print(f\"   (LOOCV with all {len(y)} samples would take too long)\")\n",
    "\n",
    "# SOLUTION: Create LeaveOneOut cross-validator\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "# SOLUTION: Perform LOOCV - each fold uses 1 sample for testing, n-1 for training\n",
    "# IMPORTANT: RÂ² cannot be calculated with single test samples (SS_tot = 0)\n",
    "# Therefore, we use MSE as the standard metric for LOOCV\n",
    "loocv_mse_scores = -cross_val_score(LinearRegression(), X_small, y_small,\n",
    "                                     cv=loocv, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(f\"\\nâœ… LOOCV Results (n={len(y_small)}):\")\n",
    "print(f\"   Each fold: 1 test sample, {len(y_small)-1} training samples\")\n",
    "print(f\"   Total folds: {len(y_small)} (one for each sample)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š MSE Results (standard metric for LOOCV):\")\n",
    "print(f\"   Mean MSE: {loocv_mse_scores.mean():.4f}\")\n",
    "print(f\"   Std MSE: {loocv_mse_scores.std():.4f}\")\n",
    "print(f\"   Min MSE: {loocv_mse_scores.min():.4f}\")\n",
    "print(f\"   Max MSE: {loocv_mse_scores.max():.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Why MSE instead of RÂ² for LOOCV?\")\n",
    "print(f\"   - RÂ² requires at least 2 samples in test set to calculate variance\")\n",
    "print(f\"   - LOOCV test sets have only 1 sample â†’ RÂ² is undefined (SS_tot = 0)\")\n",
    "print(f\"   - MSE works perfectly with single predictions â†’ standard for LOOCV\")\n",
    "print(f\"   - MSE directly measures prediction error, which is what we want!\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Key Points:\")\n",
    "print(f\"   - LOOCV gives the most thorough evaluation (every sample tested)\")\n",
    "print(f\"   - Each model is trained on n-1 samples and tested on 1 sample\")\n",
    "print(f\"   - Very reliable but slow (n model trainings required)\")\n",
    "print(f\"   - Best for small datasets where you need maximum data usage!\")\n",
    "print(f\"   - MSE is the standard metric for LOOCV (RÂ² not applicable)\")\n",
    "\n",
    "# SOLUTION: Compare LOOCV with K-Fold for the same subset\n",
    "print(f\"\\nğŸ“Š Comparison: LOOCV vs 5-Fold CV (on same {n_loocv} samples):\")\n",
    "kfold_small = KFold(n_splits=5, shuffle=True, random_state=123  # Any number works (42, 123, 2024, etc.) - just for reproducibility)\n",
    "kfold_scores = -cross_val_score(LinearRegression(), X_small, y_small,\n",
    "                                cv=kfold_small, scoring='neg_mean_squared_error')\n",
    "print(f\"   LOOCV Mean MSE: {loocv_mse_scores.mean():.4f}\")\n",
    "print(f\"   5-Fold Mean MSE: {kfold_scores.mean():.4f}\")\n",
    "print(f\"   Difference: {abs(loocv_mse_scores.mean() - kfold_scores.mean()):.4f}\")\n",
    "print(f\"   ğŸ’¡ Both methods give similar results, but LOOCV is more thorough!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. Cross-Validation Score Distribution\n",
      "ØªÙˆØ²ÙŠØ¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 7. Cross-Validation Score Distribution\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"7. Cross-Validation Score Distribution\")\n",
    "print(\"ØªÙˆØ²ÙŠØ¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Scores collected from 5 folds:\n",
      "   Fold 1: RÂ² = 0.5758\n",
      "   Fold 2: RÂ² = 0.6137\n",
      "   Fold 3: RÂ² = 0.6086\n",
      "   Fold 4: RÂ² = 0.6213\n",
      "   Fold 5: RÂ² = 0.5875\n",
      "\n",
      "   Mean RÂ²: 0.6014\n",
      "   Std RÂ²: 0.0170\n",
      "   Range: [0.5758, 0.6213]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Get scores for all folds\n",
    "# This collects RÂ² scores from each fold for visualization\n",
    "# We'll use these scores to create a bar chart showing score distribution\n",
    "\n",
    "all_scores = []\n",
    "for train_idx, val_idx in kfold.split(X_scaled):\n",
    "    model_temp = LinearRegression()\n",
    "    model_temp.fit(X_scaled[train_idx], y[train_idx])\n",
    "    pred = model_temp.predict(X_scaled[val_idx])\n",
    "    score = r2_score(y[val_idx], pred)\n",
    "    all_scores.append(score)\n",
    "\n",
    "print(f\"\\nğŸ“Š Scores collected from {len(all_scores)} folds:\")\n",
    "for i, score in enumerate(all_scores, 1):\n",
    "    print(f\"   Fold {i}: RÂ² = {score:.4f}\")\n",
    "print(f\"\\n   Mean RÂ²: {np.mean(all_scores):.4f}\")\n",
    "print(f\"   Std RÂ²: {np.std(all_scores):.4f}\")\n",
    "print(f\"   Range: [{min(all_scores):.4f}, {max(all_scores):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Why Visualize Score Distribution? | Ù„Ù…Ø§Ø°Ø§ ØªØµÙˆØ± ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŸ\n",
    "\n",
    "**BEFORE**: You've seen the scores for each fold, but haven't visualized the distribution.\n",
    "\n",
    "**AFTER**: You'll see visually how consistent (or variable) model performance is across folds!\n",
    "\n",
    "**Why visualize?**\n",
    "- **See variation**: Visual representation of how much scores vary\n",
    "- **Understand consistency**: See if model performance is stable or variable\n",
    "- **Compare models**: Visual comparison is easier than numbers\n",
    "- **Build intuition**: Visual learning helps understand variance\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to interpret score variation visually\n",
    "- What \"consistent\" vs \"variable\" performance looks like\n",
    "- How to use mean Â± std for model evaluation\n",
    "- When to be concerned about high variation\n",
    "\n",
    "**What to look for:**\n",
    "- **Good**: Bars close together, small shaded area â†’ consistent performance âœ…\n",
    "- **Concerning**: Bars far apart, large shaded area â†’ variable performance âš ï¸\n",
    "- **Mean line**: Average performance across all folds\n",
    "- **Shaded area**: Â±1 standard deviation (68% of scores fall here)\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Decision Framework - When to Use Each Cross-Validation Method | Ø§Ù„Ø®Ø·ÙˆØ© 8: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø© ØªØ­Ù‚Ù‚ Ù…ØªÙ‚Ø§Ø·Ø¹\n",
    "\n",
    "**BEFORE**: You've learned different cross-validation methods, but when should you use each one?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose the right cross-validation method for any situation!\n",
    "\n",
    "**Why this matters**: Using the wrong cross-validation method can:\n",
    "- **Unreliable evaluation** â†’ Wrong method gives biased estimates\n",
    "- **Wasted computation** â†’ Using expensive methods when simple ones work\n",
    "- **Poor model selection** â†’ Wrong evaluation leads to wrong model choice\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework: Which Cross-Validation Method? | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±: Ø£ÙŠ Ø·Ø±ÙŠÙ‚Ø© ØªØ­Ù‚Ù‚ Ù…ØªÙ‚Ø§Ø·Ø¹ØŸ\n",
    "\n",
    "**Key Question**: Should I use **K-FOLD**, **STRATIFIED K-FOLD**, **LEAVE-ONE-OUT**, or **SIMPLE TRAIN-TEST SPLIT**?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "What type of problem do you have?\n",
    "â”œâ”€ CLASSIFICATION â†’ Check class balance\n",
    "â”‚   â”œâ”€ Balanced classes â†’ Use STRATIFIED K-FOLD âœ…\n",
    "â”‚   â”‚   â””â”€ Why? Maintains class distribution in each fold\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€ Imbalanced classes â†’ Use STRATIFIED K-FOLD âœ…\n",
    "â”‚       â””â”€ Why? Critical for imbalanced data\n",
    "â”‚\n",
    "â””â”€ REGRESSION â†’ Check dataset size\n",
    "    â”œâ”€ Small dataset (< 100 samples) â†’ Use LEAVE-ONE-OUT âœ…\n",
    "    â”‚   â””â”€ Why? Maximum data usage, most reliable\n",
    "    â”‚\n",
    "    â”œâ”€ Medium dataset (100-10,000) â†’ Use K-FOLD (K=5 or 10) âœ…\n",
    "    â”‚   â””â”€ Why? Good balance of reliability and speed\n",
    "    â”‚\n",
    "    â””â”€ Large dataset (> 10,000) â†’ Use SIMPLE TRAIN-TEST SPLIT or K-FOLD (K=5) âœ…\n",
    "        â””â”€ Why? Large datasets, simple split is often sufficient\n",
    "```\n",
    "\n",
    "#### Detailed Decision Process:\n",
    "\n",
    "```\n",
    "Step 1: Problem Type\n",
    "â”œâ”€ Classification â†’ Go to Step 2\n",
    "â””â”€ Regression â†’ Go to Step 3\n",
    "\n",
    "Step 2: Classification - Check Class Balance\n",
    "â”œâ”€ Balanced classes (similar counts) â†’ Use STRATIFIED K-FOLD (K=5 or 10)\n",
    "â”‚   â””â”€ Why? Maintains class distribution\n",
    "â”‚\n",
    "â””â”€ Imbalanced classes (very different counts) â†’ Use STRATIFIED K-FOLD (K=5 or 10)\n",
    "    â””â”€ Why? Critical to maintain distribution in imbalanced data\n",
    "\n",
    "Step 3: Regression - Check Dataset Size\n",
    "â”œâ”€ Very small (< 50 samples) â†’ Use LEAVE-ONE-OUT (LOOCV)\n",
    "â”‚   â””â”€ Why? Maximum data usage, most reliable for small data\n",
    "â”‚\n",
    "â”œâ”€ Small (50-100 samples) â†’ Use K-FOLD (K=10) or LEAVE-ONE-OUT\n",
    "â”‚   â””â”€ Why? Need more folds for reliable estimate\n",
    "â”‚\n",
    "â”œâ”€ Medium (100-10,000 samples) â†’ Use K-FOLD (K=5 or 10)\n",
    "â”‚   â””â”€ Why? Standard choice, good balance\n",
    "â”‚\n",
    "â””â”€ Large (> 10,000 samples) â†’ Use SIMPLE TRAIN-TEST SPLIT or K-FOLD (K=5)\n",
    "    â””â”€ Why? Large datasets, simple split often sufficient\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Comparison Table: Cross-Validation Methods | Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Simple Train-Test Split** | Large datasets, quick evaluation | â€¢ Fast<br>â€¢ Simple<br>â€¢ One evaluation | â€¢ Less reliable<br>â€¢ Single split may be lucky/unlucky<br>â€¢ Doesn't use all data | Dataset with 50,000+ samples |\n",
    "| **K-Fold (K=5)** | Medium datasets, standard choice | â€¢ Reliable<br>â€¢ Uses all data<br>â€¢ Good balance | â€¢ 5 evaluations (slower)<br>â€¢ May not maintain class distribution | Dataset with 1,000-10,000 samples |\n",
    "| **K-Fold (K=10)** | Small-medium datasets, more reliable | â€¢ Very reliable<br>â€¢ Uses all data<br>â€¢ More folds = better estimate | â€¢ 10 evaluations (slower)<br>â€¢ More computation | Dataset with 100-5,000 samples |\n",
    "| **Stratified K-Fold** | Classification problems (any size) | â€¢ Maintains class distribution<br>â€¢ Reliable for classification<br>â€¢ Works with imbalanced data | â€¢ Only for classification<br>â€¢ Slightly more complex | Any classification problem |\n",
    "| **Leave-One-Out (LOOCV)** | Very small datasets | â€¢ Maximum data usage<br>â€¢ Most reliable for small data<br>â€¢ Unbiased estimate | â€¢ Very slow (n evaluations)<br>â€¢ High variance<br>â€¢ Expensive | Dataset with < 50 samples |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When to Use Each Method | Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø©\n",
    "\n",
    "#### Use Simple Train-Test Split when:\n",
    "1. **Large Dataset** âœ…\n",
    "   - More than 10,000 samples\n",
    "   - Single split is reliable enough\n",
    "   - **Example**: 50,000+ samples, quick evaluation needed\n",
    "\n",
    "2. **Quick Evaluation** âœ…\n",
    "   - Need fast results\n",
    "   - Don't need maximum reliability\n",
    "   - **Example**: Initial model testing, baseline evaluation\n",
    "\n",
    "3. **Computational Constraints** âœ…\n",
    "   - Very expensive models (deep learning)\n",
    "   - Can't afford multiple evaluations\n",
    "   - **Example**: Training neural networks\n",
    "\n",
    "#### Use K-Fold (K=5) when:\n",
    "1. **Medium Dataset** âœ…\n",
    "   - 1,000-10,000 samples\n",
    "   - Standard choice for most problems\n",
    "   - **Example**: Most regression problems\n",
    "\n",
    "2. **Balanced Reliability/Speed** âœ…\n",
    "   - Need reliable evaluation\n",
    "   - Don't want too slow\n",
    "   - **Example**: Model comparison, hyperparameter tuning\n",
    "\n",
    "#### Use K-Fold (K=10) when:\n",
    "1. **Small-Medium Dataset** âœ…\n",
    "   - 100-5,000 samples\n",
    "   - Need more reliable estimate\n",
    "   - **Example**: Limited data, need best estimate\n",
    "\n",
    "2. **Model Selection** âœ…\n",
    "   - Comparing multiple models\n",
    "   - Need reliable comparison\n",
    "   - **Example**: Choosing between different algorithms\n",
    "\n",
    "#### Use Stratified K-Fold when:\n",
    "1. **Classification Problem** âœ…\n",
    "   - Any classification task\n",
    "   - Maintains class distribution\n",
    "   - **Example**: Binary or multiclass classification\n",
    "\n",
    "2. **Imbalanced Classes** âœ…\n",
    "   - Classes have very different counts\n",
    "   - Critical to maintain distribution\n",
    "   - **Example**: Fraud detection (99% normal, 1% fraud)\n",
    "\n",
    "3. **Small Classification Dataset** âœ…\n",
    "   - Small dataset with classes\n",
    "   - Need to maintain distribution\n",
    "   - **Example**: Medical diagnosis with limited data\n",
    "\n",
    "#### Use Leave-One-Out (LOOCV) when:\n",
    "1. **Very Small Dataset** âœ…\n",
    "   - Less than 50 samples\n",
    "   - Need maximum data usage\n",
    "   - **Example**: Rare disease study with 30 patients\n",
    "\n",
    "2. **Maximum Reliability Needed** âœ…\n",
    "   - Small dataset, need best estimate\n",
    "   - Can afford computation time\n",
    "   - **Example**: Expensive data collection, need best evaluation\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ When NOT to Use Each Method | Ù…ØªÙ‰ Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø©\n",
    "\n",
    "#### Don't use Simple Train-Test Split when:\n",
    "1. **Small Dataset** âŒ\n",
    "   - Less than 1,000 samples\n",
    "   - Single split unreliable\n",
    "   - **Use Instead**: K-Fold or LOOCV\n",
    "\n",
    "2. **Model Selection** âŒ\n",
    "   - Comparing multiple models\n",
    "   - Need reliable comparison\n",
    "   - **Use Instead**: K-Fold Cross-Validation\n",
    "\n",
    "#### Don't use K-Fold when:\n",
    "1. **Very Small Dataset** âŒ\n",
    "   - Less than 50 samples\n",
    "   - K folds may have too few samples per fold\n",
    "   - **Use Instead**: Leave-One-Out (LOOCV)\n",
    "\n",
    "2. **Imbalanced Classification** âŒ\n",
    "   - Classes very imbalanced\n",
    "   - K-Fold may not maintain distribution\n",
    "   - **Use Instead**: Stratified K-Fold\n",
    "\n",
    "#### Don't use Stratified K-Fold when:\n",
    "1. **Regression Problem** âŒ\n",
    "   - Predicting continuous values\n",
    "   - No classes to stratify\n",
    "   - **Use Instead**: Regular K-Fold\n",
    "\n",
    "#### Don't use Leave-One-Out when:\n",
    "1. **Large Dataset** âŒ\n",
    "   - More than 100 samples\n",
    "   - Too slow, high variance\n",
    "   - **Use Instead**: K-Fold (K=5 or 10)\n",
    "\n",
    "2. **Computational Constraints** âŒ\n",
    "   - Expensive models\n",
    "   - Can't afford n evaluations\n",
    "   - **Use Instead**: K-Fold (K=5)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: House Price Prediction (5,000 samples) âœ… K-FOLD (K=5)\n",
    "- **Problem**: Regression (predicting price)\n",
    "- **Dataset Size**: 5,000 samples (medium)\n",
    "- **Decision**: âœ… Use K-Fold (K=5)\n",
    "- **Reasoning**: Medium dataset, regression problem, standard choice\n",
    "\n",
    "#### Example 2: Customer Churn Prediction (10,000 samples, imbalanced) âœ… STRATIFIED K-FOLD\n",
    "- **Problem**: Classification (churn/not churn)\n",
    "- **Dataset Size**: 10,000 samples\n",
    "- **Class Balance**: Imbalanced (95% not churn, 5% churn)\n",
    "- **Decision**: âœ… Use Stratified K-Fold (K=5)\n",
    "- **Reasoning**: Classification, imbalanced classes, need to maintain distribution\n",
    "\n",
    "#### Example 3: Rare Disease Diagnosis (30 samples) âœ… LEAVE-ONE-OUT\n",
    "- **Problem**: Classification (disease/healthy)\n",
    "- **Dataset Size**: 30 samples (very small)\n",
    "- **Decision**: âœ… Use Leave-One-Out (LOOCV)\n",
    "- **Reasoning**: Very small dataset, need maximum data usage, can afford computation\n",
    "\n",
    "#### Example 4: Image Classification (100,000 samples) âœ… SIMPLE SPLIT or K-FOLD (K=5)\n",
    "- **Problem**: Classification (10 classes)\n",
    "- **Dataset Size**: 100,000 samples (large)\n",
    "- **Decision**: âœ… Use Simple Train-Test Split or K-Fold (K=5)\n",
    "- **Reasoning**: Large dataset, simple split often sufficient, or K=5 for more reliability\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Classification â†’ Stratified K-Fold** - Always use stratified for classification\n",
    "2. **Small data â†’ More folds** - Use K=10 or LOOCV for small datasets\n",
    "3. **Large data â†’ Simple split** - Simple split often sufficient for large datasets\n",
    "4. **Standard choice â†’ K=5** - Good default for most problems\n",
    "5. **Imbalanced â†’ Stratified** - Critical for imbalanced classification\n",
    "6. **Model selection â†’ K-Fold** - Use K-Fold for comparing models\n",
    "7. **Balance reliability/speed** - Choose method based on dataset size and needs\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario 1**: Predicting house prices with 8,000 samples\n",
    "- **Problem**: Regression\n",
    "- **Dataset Size**: 8,000 (medium)\n",
    "- **Decision**: âœ… K-Fold (K=5) - standard choice for medium regression\n",
    "\n",
    "**Scenario 2**: Predicting customer churn with 2,000 samples (imbalanced: 90% not churn)\n",
    "- **Problem**: Classification\n",
    "- **Dataset Size**: 2,000 (medium)\n",
    "- **Class Balance**: Imbalanced\n",
    "- **Decision**: âœ… Stratified K-Fold (K=5) - classification with imbalanced classes\n",
    "\n",
    "**Scenario 3**: Medical diagnosis with 25 patients\n",
    "- **Problem**: Classification\n",
    "- **Dataset Size**: 25 (very small)\n",
    "- **Decision**: âœ… Leave-One-Out (LOOCV) - very small dataset, need maximum data usage\n",
    "\n",
    "**Scenario 4**: Image classification with 200,000 images\n",
    "- **Problem**: Classification\n",
    "- **Dataset Size**: 200,000 (very large)\n",
    "- **Decision**: âœ… Simple Train-Test Split or K-Fold (K=5) - large dataset, simple split sufficient\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Unit 5, Example 1: Grid Search** - Uses cross-validation for hyperparameter tuning\n",
    "- ğŸ““ **Unit 3: Classification** - Stratified K-Fold for all classification problems\n",
    "- ğŸ““ **All ML Projects** - Cross-validation is the gold standard for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Plot saved as 'cv_score_distribution.png'\n",
      "\n",
      "ğŸ“Š What This Visualization Shows:\n",
      "   - Each bar = RÂ² score for one fold (height = performance)\n",
      "   - Red dashed line = Mean RÂ² across all folds (average performance)\n",
      "   - Red shaded area = Â±1 standard deviation (68% confidence interval)\n",
      "   - Value labels = Exact RÂ² score for each fold\n",
      "\n",
      "ğŸ’¡ How to Interpret This Plot:\n",
      "   - Mean RÂ²: 0.6014 (average performance)\n",
      "   - Std: 0.0170 (variation across folds)\n",
      "   - Range: [0.5758, 0.6213] (min to max)\n",
      "\n",
      "   Consistency: âœ… Very consistent\n",
      "   Model performance is very stable across folds - reliable model!\n",
      "\n",
      "ğŸ“ˆ What to Look For:\n",
      "   - Bars close together â†’ Consistent performance (good!)\n",
      "   - Bars far apart â†’ Variable performance (investigate!)\n",
      "   - Mean line in middle â†’ Balanced performance\n",
      "   - Small shaded area â†’ Low variance (reliable!)\n",
      "   - Large shaded area â†’ High variance (less reliable)\n",
      "\n",
      "ğŸ¯ How to Use This:\n",
      "   - Compare different models: Lower std = more reliable\n",
      "   - Check consistency: Small range = stable model\n",
      "   - Confidence interval: Mean Â± Std shows performance range\n",
      "   - Model selection: Choose models with consistent performance!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“š Summary: What You Learned | Ø§Ù„Ù…Ù„Ø®Øµ: Ù…Ø§ ØªØ¹Ù„Ù…ØªÙ‡\n",
      "============================================================\n",
      "\n",
      "âœ… Complete Solutions Provided:\n",
      "   1. âœ… Simple train-test split (baseline comparison)\n",
      "   2. âœ… K-Fold cross-validation (5-fold)\n",
      "   3. âœ… Manual cross-validation implementation (step-by-step)\n",
      "   4. âœ… Multiple metrics with cross_validate()\n",
      "   5. âœ… Model comparison using cross-validation\n",
      "   6. âœ… K-Fold visualization (how data is split)\n",
      "   7. âœ… Leave-One-Out CV (LOOCV) for small datasets\n",
      "   8. âœ… Score distribution visualization\n",
      "\n",
      "ğŸ’¡ Key Takeaways:\n",
      "   - Cross-validation gives more reliable evaluation than single split\n",
      "   - K-Fold (K=5) is the standard choice for most problems\n",
      "   - LOOCV is best for very small datasets (< 50 samples)\n",
      "   - Cross-validation enables fair model comparison\n",
      "   - All solutions are complete and runnable!\n",
      "\n",
      "============================================================\n",
      "Example 2 Complete! âœ“\n",
      "Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 2! âœ“\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Visualize score distribution across folds\n",
    "# This bar chart shows how RÂ² scores vary across the 5 folds\n",
    "# Helps us understand if model performance is consistent or variable\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# SOLUTION: Create bar chart showing RÂ² score for each fold\n",
    "# Each bar represents the RÂ² score from one fold (height = performance)\n",
    "bars = plt.bar(range(1, len(all_scores) + 1), all_scores, \n",
    "               alpha=0.7, edgecolor='black', color='steelblue')\n",
    "\n",
    "# SOLUTION: Add mean line\n",
    "# Red dashed line shows the average RÂ² across all folds\n",
    "mean_score = np.mean(all_scores)\n",
    "plt.axhline(mean_score, color='r', linestyle='--', linewidth=2,\n",
    "           label=f'Mean RÂ²: {mean_score:.4f}')\n",
    "\n",
    "# SOLUTION: Add standard deviation bands\n",
    "# Red shaded area shows Â±1 standard deviation (68% of scores fall here)\n",
    "std_score = np.std(all_scores)\n",
    "plt.axhspan(mean_score - std_score, mean_score + std_score, \n",
    "           alpha=0.2, color='red', label=f'Â±1 Std: {std_score:.4f}')\n",
    "\n",
    "# SOLUTION: Add value labels on bars\n",
    "# Shows exact RÂ² value for each fold\n",
    "for i, (bar, score) in enumerate(zip(bars, all_scores)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{score:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xlabel('Fold Number', fontsize=12)\n",
    "plt.ylabel('RÂ² Score', fontsize=12)\n",
    "plt.title('Cross-Validation Score Distribution Across 5 Folds', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.ylim([min(all_scores) - 0.1, max(all_scores) + 0.1])\n",
    "plt.xticks(range(1, len(all_scores) + 1))\n",
    "plt.tight_layout()\n",
    "plt.savefig('cv_score_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nâœ… Plot saved as 'cv_score_distribution.png'\")\n",
    "print(\"\\nğŸ“Š What This Visualization Shows:\")\n",
    "print(\"   - Each bar = RÂ² score for one fold (height = performance)\")\n",
    "print(\"   - Red dashed line = Mean RÂ² across all folds (average performance)\")\n",
    "print(\"   - Red shaded area = Â±1 standard deviation (68% confidence interval)\")\n",
    "print(\"   - Value labels = Exact RÂ² score for each fold\")\n",
    "\n",
    "print(\"\\nğŸ’¡ How to Interpret This Plot:\")\n",
    "mean_score = np.mean(all_scores)\n",
    "std_score = np.std(all_scores)\n",
    "print(f\"   - Mean RÂ²: {mean_score:.4f} (average performance)\")\n",
    "print(f\"   - Std: {std_score:.4f} (variation across folds)\")\n",
    "print(f\"   - Range: [{min(all_scores):.4f}, {max(all_scores):.4f}] (min to max)\")\n",
    "\n",
    "# Automatic interpretation based on standard deviation\n",
    "if std_score < 0.05:\n",
    "    consistency = \"âœ… Very consistent\"\n",
    "    interpretation = \"Model performance is very stable across folds - reliable model!\"\n",
    "elif std_score < 0.1:\n",
    "    consistency = \"âœ… Consistent\"\n",
    "    interpretation = \"Model performance is fairly stable - good model!\"\n",
    "else:\n",
    "    consistency = \"âš ï¸  Variable\"\n",
    "    interpretation = \"Model performance varies across folds - may need investigation!\"\n",
    "\n",
    "print(f\"\\n   Consistency: {consistency}\")\n",
    "print(f\"   {interpretation}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ What to Look For:\")\n",
    "print(\"   - Bars close together â†’ Consistent performance (good!)\")\n",
    "print(\"   - Bars far apart â†’ Variable performance (investigate!)\")\n",
    "print(\"   - Mean line in middle â†’ Balanced performance\")\n",
    "print(\"   - Small shaded area â†’ Low variance (reliable!)\")\n",
    "print(\"   - Large shaded area â†’ High variance (less reliable)\")\n",
    "\n",
    "print(\"\\nğŸ¯ How to Use This:\")\n",
    "print(\"   - Compare different models: Lower std = more reliable\")\n",
    "print(\"   - Check consistency: Small range = stable model\")\n",
    "print(\"   - Confidence interval: Mean Â± Std shows performance range\")\n",
    "print(\"   - Model selection: Choose models with consistent performance!\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“š Summary: What You Learned | Ø§Ù„Ù…Ù„Ø®Øµ: Ù…Ø§ ØªØ¹Ù„Ù…ØªÙ‡\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nâœ… Complete Solutions Provided:\")\n",
    "print(\"   1. âœ… Simple train-test split (baseline comparison)\")\n",
    "print(\"   2. âœ… K-Fold cross-validation (5-fold)\")\n",
    "print(\"   3. âœ… Manual cross-validation implementation (step-by-step)\")\n",
    "print(\"   4. âœ… Multiple metrics with cross_validate()\")\n",
    "print(\"   5. âœ… Model comparison using cross-validation\")\n",
    "print(\"   6. âœ… K-Fold visualization (how data is split)\")\n",
    "print(\"   7. âœ… Leave-One-Out CV (LOOCV) for small datasets\")\n",
    "print(\"   8. âœ… Score distribution visualization\")\n",
    "print(\"\\nğŸ’¡ Key Takeaways:\")\n",
    "print(\"   - Cross-validation gives more reliable evaluation than single split\")\n",
    "print(\"   - K-Fold (K=5) is the standard choice for most problems\")\n",
    "print(\"   - LOOCV is best for very small datasets (< 50 samples)\")\n",
    "print(\"   - Cross-validation enables fair model comparison\")\n",
    "print(\"   - All solutions are complete and runnable!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 2 Complete! âœ“\")\n",
    "print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 2! âœ“\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
