{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Ridge and Lasso Regression | Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø±ÙŠØ¯Ø¬ ÙˆÙ„Ø§Ø³Ùˆ\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Unit 1: All examples** - Data processing, linear regression, polynomial regression\n",
    "- âœ… **Understanding of overfitting**: What happens when models are too complex\n",
    "- âœ… **Basic linear algebra**: Understanding coefficients and regularization\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why regularization is needed\n",
    "- Knowing when to use Ridge vs Lasso\n",
    "- Understanding how alpha (regularization strength) works\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is Unit 2, Example 1** - it solves the overfitting problem from polynomial regression!\n",
    "\n",
    "**Why this example FIRST in Unit 2?**\n",
    "- **Before** you can use advanced techniques, you need to solve overfitting\n",
    "- **Before** you can build robust models, you need regularization\n",
    "- **Before** you can handle multicollinearity, you need Ridge/Lasso\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Unit 1, Example 4: Linear Regression (we know basic regression)\n",
    "- ğŸ““ Unit 1, Example 5: Polynomial Regression (we saw overfitting!)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 2: Cross-Validation (evaluates models properly)\n",
    "- ğŸ““ Unit 3: Classification (same regularization concepts apply)\n",
    "- ğŸ““ All ML models (regularization is universal!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Ridge/Lasso solve **overfitting** (critical problem from Unit 1)\n",
    "2. Ridge/Lasso teach **regularization** (essential ML concept)\n",
    "3. Ridge/Lasso show **feature selection** (Lasso automatically selects features)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Preventing Overfitting | Ø§Ù„Ù‚ØµØ©: Ù…Ù†Ø¹ Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªÙ„Ø§Ø¦Ù…\n",
    "\n",
    "Imagine you're learning to drive. **Before** regularization, you memorize every turn on the training route perfectly, but fail on new routes (overfitting). **After** regularization, you learn general driving principles that work everywhere!\n",
    "\n",
    "Same with machine learning: **Before** Ridge/Lasso, models memorize training data perfectly but fail on new data. **After** Ridge/Lasso, models learn general patterns that generalize well!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Ridge and Lasso Matter | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… Ø±ÙŠØ¯Ø¬ ÙˆÙ„Ø§Ø³ÙˆØŸ\n",
    "\n",
    "Regularization prevents overfitting:\n",
    "- **Ridge (L2)**: Shrinks coefficients toward zero (keeps all features)\n",
    "- **Lasso (L1)**: Shrinks some coefficients to exactly zero (feature selection!)\n",
    "- **Both**: Prevent overfitting by penalizing large coefficients\n",
    "- **Alpha**: Controls regularization strength (higher = more regularization)\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Build Ridge regression models (L2 regularization)\n",
    "2. Build Lasso regression models (L1 regularization)\n",
    "3. Understand the difference between Ridge and Lasso\n",
    "4. Tune alpha hyperparameter\n",
    "5. Compare regularized models with linear regression\n",
    "6. Understand when to use each method"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:18:49.767466Z",
     "iopub.status.busy": "2025-12-25T19:18:49.767130Z",
     "iopub.status.idle": "2025-12-25T19:19:02.880919Z",
     "shell.execute_reply": "2025-12-25T19:19:02.880490Z"
    }
   },
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us build regularized regression models\n",
    "\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np   # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For visualizations\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,  # Baseline model (no regularization)\n",
    "    Ridge,             # L2 regularization (shrinks coefficients)\n",
    "    Lasso              # L1 regularization (shrinks + feature selection)\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler  # Important! Regularization needs scaled features\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # For evaluation\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each model does:\")\n",
    "print(\"   - LinearRegression: No regularization (baseline)\")\n",
    "print(\"   - Ridge: L2 regularization (keeps all features, shrinks coefficients)\")\n",
    "print(\"   - Lasso: L1 regularization (removes some features, shrinks others)\")\n",
    "print(\"   - StandardScaler: CRITICAL! Regularization requires scaled features!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We saw overfitting in polynomial regression - models that fit training data too well but fail on new data.\n",
    "\n",
    "**AFTER**: We'll use Ridge and Lasso regularization to prevent overfitting by penalizing large coefficients!\n",
    "\n",
    "**Why this matters**: Overfitting is the #1 problem in ML. Regularization is the #1 solution!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Load Real-World Data with Multicollinearity | Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù…Ø¹ Ø§Ø±ØªØ¨Ø§Ø· Ù…ØªØ¹Ø¯Ø¯\n",
    "\n",
    "**BEFORE**: We need to learn regularization, but we need real data that has multicollinearity (correlated features).\n",
    "\n",
    "**AFTER**: We'll load the California Housing dataset - real data where features are naturally correlated (multicollinearity) - this is when regularization helps most!\n",
    "\n",
    "**Why California Housing?** This is REAL data from the 1990 census where features like AveRooms, AveBedrms, and Population are naturally correlated. When features are correlated, regular linear regression struggles. Ridge/Lasso handle this better!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:19:02.903886Z",
     "iopub.status.busy": "2025-12-25T19:19:02.903705Z",
     "iopub.status.idle": "2025-12-25T19:19:04.089011Z",
     "shell.execute_reply": "2025-12-25T19:19:04.088655Z"
    }
   },
   "source": [
    "# Load real-world California Housing dataset\n",
    "# This dataset naturally has multicollinearity (correlated features)\n",
    "# Multicollinearity = features are correlated with each other\n",
    "# This is a common real-world problem that regularization solves!\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "print(\"\\nğŸ“¥ Loading California Housing dataset...\")\n",
    "print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§Ù„ÙŠÙÙˆØ±Ù†ÙŠØ§ Ù„Ù„Ø¥Ø³ÙƒØ§Ù†...\")\n",
    "\n",
    "housing_data = fetch_california_housing()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\n",
    "df['target'] = housing_data.target\n",
    "\n",
    "print(f\"\\nâœ… Real-world California Housing data loaded!\")\n",
    "print(f\"   ğŸ“Š This is REAL data from the 1990 California census\")\n",
    "print(f\"   ğŸ“ˆ Contains {len(df)} housing districts with {len(df.columns)-1} features\")\n",
    "print(f\"\\nğŸ” Notice:\")\n",
    "print(\"   - Features like AveRooms, AveBedrms, Population are naturally correlated\")\n",
    "print(\"   - This multicollinearity makes regularization important!\")\n",
    "print(\"   - Ridge/Lasso will handle this better than regular linear regression\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:19:04.090829Z",
     "iopub.status.busy": "2025-12-25T19:19:04.090631Z",
     "iopub.status.idle": "2025-12-25T19:19:04.095770Z",
     "shell.execute_reply": "2025-12-25T19:19:04.095506Z"
    }
   },
   "source": [
    "# Data summary comes first (showing what we loaded)\n",
    "# Then we'll check for multicollinearity\n",
    "\n",
    "print(f\"\\nğŸ“Š Real Data Summary:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Features: {', '.join(housing_data.feature_names[:4])}... and more\")\n",
    "print(f\"   Target: Median House Value (in $100,000s)\")\n",
    "print(f\"\\nğŸ“„ First 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nğŸ” Notice:\")\n",
    "print(\"   - This is REAL data from 1990 California census\")\n",
    "print(\"   - Features are naturally correlated (multicollinearity)\")\n",
    "print(\"   - Regular regression may struggle; Ridge/Lasso will handle this better!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:19:04.096979Z",
     "iopub.status.busy": "2025-12-25T19:19:04.096884Z",
     "iopub.status.idle": "2025-12-25T19:19:04.104358Z",
     "shell.execute_reply": "2025-12-25T19:19:04.104128Z"
    }
   },
   "source": [
    "# Now check for multicollinearity in the loaded data\n",
    "print(\"\\nğŸ“Š Checking for multicollinearity in real data...\")\n",
    "print(\"   (Correlated features make regularization important)\")\n",
    "\n",
    "# Check correlation between features\n",
    "correlation_matrix = df[housing_data.feature_names].corr()\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:  # High correlation threshold\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                corr_val\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\n   âœ… Found {len(high_corr_pairs)} highly correlated feature pairs:\")\n",
    "    for feat1, feat2, corr in high_corr_pairs[:3]:  # Show first 3\n",
    "        print(f\"      - {feat1} â†” {feat2}: {corr:.3f}\")\n",
    "    print(\"\\nğŸ’¡ What to Notice:\")\n",
    "    print(\"   - Correlation values close to 1.0 (or -1.0) mean features are highly correlated\")\n",
    "    print(\"   - Notice: AveRooms â†” AveBedrms correlation = 0.848 (very high! They move together)\")\n",
    "    print(\"   - Notice: Latitude â†” Longitude correlation = -0.925 (very high! They're related)\")\n",
    "    print(\"   - When features are correlated like this, regular Linear Regression can struggle\")\n",
    "    print(\"   - This is why we need Ridge/Lasso - they handle multicollinearity better! âœ…\")\n",
    "else:\n",
    "    print(\"   - Features have moderate correlation (still benefits from regularization)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Data for Modeling | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù†Ù…Ø°Ø¬Ø©\n",
    "\n",
    "**BEFORE**: We've loaded and explored the data, and we found multicollinearity (correlated features). Now we need to prepare the data for modeling!\n",
    "\n",
    "**AFTER**: We'll split the data into training and test sets, and scale the features!\n",
    "\n",
    "**Why scaling matters**: Regularization is sensitive to feature scale! Features on different scales (e.g., age vs income) will be penalized differently. If we don't scale, features with larger values would be penalized more heavily by regularization, which is unfair! We MUST scale first!\n",
    "\n",
    "**What we'll do**:\n",
    "1. Split data: 80% training, 20% testing (to evaluate models properly)\n",
    "2. Scale features: Use StandardScaler (mean=0, std=1) so all features are on the same scale\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:19:04.105491Z",
     "iopub.status.busy": "2025-12-25T19:19:04.105408Z",
     "iopub.status.idle": "2025-12-25T19:19:04.111042Z",
     "shell.execute_reply": "2025-12-25T19:19:04.110827Z"
    }
   },
   "source": [
    "# Prepare features (X) and target (y) from real data\n",
    "X_data = df[housing_data.feature_names]  # All 8 features\n",
    "y_data = df['target']  # Median House Value\n",
    "\n",
    "print(f\"\\nâœ… Data prepared for modeling:\")\n",
    "print(f\"   Features (X): {X_data.shape[1]} features\")\n",
    "print(f\"   Target (y): {y_data.shape[0]} samples\")\n",
    "print(f\"   Feature names: {', '.join(X_data.columns[:4])}... and more\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# - Splits data into training and testing sets\n",
    "# - X: Features (input variables), y: Target (output variable)\n",
    "# - test_size=0.2: 20% for testing, 80% for training\n",
    "# - random_state=42: Seed for reproducibility (same split every time)\n",
    "# - Returns: X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features (CRITICAL for regularization!)\n",
    "# Regularization is sensitive to feature scale, so we MUST scale first!\n",
    "# Without scaling, features with larger values would be penalized more\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"   âœ… Data split and scaled!\")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"   Test set: {X_test.shape[0]} samples\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Alpha (Î±) - The Regularization Parameter | ÙÙ‡Ù… Alpha (Î±) - Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…\n",
    "\n",
    "**BEFORE**: You know Ridge and Lasso use regularization, but what is alpha exactly?\n",
    "\n",
    "**AFTER**: You'll understand that alpha controls how much regularization is applied!\n",
    "\n",
    "**Why this matters**: Choosing the wrong alpha can lead to:\n",
    "- **Overfitting** (alpha too small) â†’ Model memorizes training data\n",
    "- **Underfitting** (alpha too large) â†’ Model becomes too simple, can't learn patterns\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š What is Alpha (Î±)? | Ù…Ø§ Ù‡Ùˆ Alpha (Î±)ØŸ\n",
    "\n",
    "**Alpha (Î±)** is the **regularization strength parameter** that controls the trade-off between:\n",
    "1. **Fitting the training data well** (low error on training data)\n",
    "2. **Keeping the model simple** (small coefficients to prevent overfitting)\n",
    "\n",
    "### ğŸ”¢ How Alpha Works | ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Alpha\n",
    "\n",
    "**The Math Behind It:**\n",
    "\n",
    "For **Ridge Regression (L2)**, the cost function becomes:\n",
    "```\n",
    "Cost = MSE + Î± Ã— (sum of squared coefficients)\n",
    "```\n",
    "\n",
    "For **Lasso Regression (L1)**, the cost function becomes:\n",
    "```\n",
    "Cost = MSE + Î± Ã— (sum of absolute coefficients)\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- **Î± = 0**: No regularization â†’ Same as Linear Regression\n",
    "- **Î± > 0**: Regularization is applied\n",
    "- **Higher Î±** â†’ More regularization â†’ Smaller coefficients\n",
    "- **Lower Î±** â†’ Less regularization â†’ Coefficients closer to Linear Regression\n",
    "\n",
    "### ğŸ“Š Alpha Values and Their Effects | Ù‚ÙŠÙ… Alpha ÙˆØªØ£Ø«ÙŠØ±Ø§ØªÙ‡Ø§\n",
    "\n",
    "| Alpha Value | Effect | Result |\n",
    "|-------------|--------|--------|\n",
    "| **Î± = 0** | No regularization | Same as Linear Regression |\n",
    "| **Î± = 0.01** | Very light regularization | Coefficients slightly shrunk |\n",
    "| **Î± = 0.1** | Light regularization | Noticeable coefficient shrinkage |\n",
    "| **Î± = 1.0** | Moderate regularization | Significant coefficient reduction |\n",
    "| **Î± = 10.0** | Heavy regularization | Very small coefficients |\n",
    "| **Î± = 100+** | Very heavy regularization | Most coefficients near zero (especially in Lasso) |\n",
    "\n",
    "### ğŸ¯ Alpha in Ridge vs Lasso | Alpha ÙÙŠ Ridge Ù…Ù‚Ø§Ø¨Ù„ Lasso\n",
    "\n",
    "**Ridge (L2) Regularization:**\n",
    "- **Shrinks coefficients smoothly** toward zero\n",
    "- **Never sets coefficients to exactly zero**\n",
    "- Keeps all features, just makes them smaller\n",
    "- **Best for**: When all features might be relevant\n",
    "\n",
    "**Lasso (L1) Regularization:**\n",
    "- **Can set coefficients to exactly zero** (feature selection!)\n",
    "- **More aggressive** shrinkage\n",
    "- **Best for**: When you want to remove irrelevant features\n",
    "\n",
    "### âš–ï¸ The Trade-off | Ø§Ù„Ù…Ù‚Ø§ÙŠØ¶Ø©\n",
    "\n",
    "```\n",
    "Alpha (Î±) = 0          Alpha (Î±) = small        Alpha (Î±) = large\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ No Regularizationâ”‚   â”‚ Light Regularization â”‚  â”‚ Heavy Regularizationâ”‚\n",
    "â”‚                 â”‚   â”‚                 â”‚      â”‚                 â”‚\n",
    "â”‚ - Fits training â”‚   â”‚ - Balanced      â”‚      â”‚ - Very simple   â”‚\n",
    "â”‚   data perfectlyâ”‚   â”‚ - Good          â”‚      â”‚ - May underfit  â”‚\n",
    "â”‚ - May overfit!  â”‚   â”‚   generalizationâ”‚      â”‚ - Small coefs   â”‚\n",
    "â”‚ - Large coefs   â”‚   â”‚ - Smaller coefs â”‚      â”‚ - Some = 0      â”‚\n",
    "â”‚                 â”‚   â”‚                 â”‚      â”‚   (Lasso only)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ’¡ How to Choose Alpha? | ÙƒÙŠÙ ØªØ®ØªØ§Ø± AlphaØŸ\n",
    "\n",
    "1. **Start with a range** (e.g., 0.01, 0.1, 1.0, 10.0, 100.0)\n",
    "2. **Train models** with different alpha values\n",
    "3. **Evaluate on validation/test set** (not training set!)\n",
    "4. **Choose the alpha** that gives the best test performance\n",
    "5. **Use cross-validation** for more reliable selection (coming in Example 2!)\n",
    "\n",
    "### ğŸ“ Important Notes | Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©\n",
    "\n",
    "- **Alpha must be â‰¥ 0** (can't be negative)\n",
    "- **Scaling matters!** Regularization requires scaled features (that's why we used StandardScaler)\n",
    "- **Different datasets** need different alpha values\n",
    "- **Ridge and Lasso** may have different optimal alpha values\n",
    "- **Tuning alpha is a hyperparameter** search problem (we'll learn Grid Search in Unit 5!)\n",
    "\n",
    "---\n",
    "\n",
    "**In this notebook**, we'll try different alpha values and see how they affect model performance! ğŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building and Comparing Models | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø¨Ù†Ø§Ø¡ ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
    "\n",
    "**BEFORE**: We understand what alpha is and our data is prepared. Now let's build models!\n",
    "\n",
    "**AFTER**: We'll build three models and compare them to see how regularization helps.\n",
    "\n",
    "**Approach**: \n",
    "1. **Linear Regression** (baseline - no regularization)\n",
    "2. **Ridge Regression** (L2 regularization - shrinks all coefficients)\n",
    "3. **Lasso Regression** (L1 regularization - can remove features)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Linear Regression Baseline | Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ ÙƒØ®Ø· Ø£Ø³Ø§Ø³\n",
    "\n",
    "**Why start here?** We need a baseline to compare against! Linear Regression has no regularization (Î± = 0), so we can see the difference when we add regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:19:04.112142Z",
     "iopub.status.busy": "2025-12-25T19:19:04.112078Z",
     "iopub.status.idle": "2025-12-25T19:19:04.117049Z",
     "shell.execute_reply": "2025-12-25T19:19:04.116844Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. Linear Regression (Baseline)\")\n",
    "print(\"Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ (Ø®Ø· Ø§Ù„Ø£Ø³Ø§Ø³)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Try regular linear regression first (no regularization)\n",
    "# This is our baseline to compare against\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "lr_mse = mean_squared_error(y_test, lr_pred)\n",
    "lr_r2 = r2_score(y_test, lr_pred)\n",
    "\n",
    "print(f\"\\nğŸ“Š Linear Regression Results:\")\n",
    "print(f\"   MSE: {lr_mse:.4f} (lower is better)\")\n",
    "print(f\"   RÂ² Score: {lr_r2:.4f} (closer to 1 is better)\")\n",
    "print(f\"\\n   Coefficients (first 5): {lr.coef_[:5]}\")\n",
    "print(f\"   Notice: Some coefficients might be large or unstable\")\n",
    "print(f\"\\nğŸ’¡ What we learned:\")\n",
    "print(f\"   - Linear Regression baseline: MSE = {lr_mse:.4f}, RÂ² = {lr_r2:.4f}\")\n",
    "print(f\"   - Now let's see if Ridge or Lasso can improve this!\")\n",
    "print(f\"   - Regularization should help stabilize these coefficients and potentially improve performance!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Ridge Regression (L2 Regularization) | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø±ÙŠØ¯Ø¬ (Ø§Ù„ØªÙ†Ø¸ÙŠÙ… L2)\n",
    "\n",
    "**BEFORE**: We have our baseline. Now let's try Ridge regression with different alpha values.\n",
    "\n",
    "**AFTER**: We'll see how different alpha values affect Ridge's performance and coefficients.\n",
    "\n",
    "**What to expect**: \n",
    "- Ridge will try different alpha values (0.01, 0.1, 1.0, 10.0, 100.0)\n",
    "- Remember from the Alpha explanation: higher alpha = more regularization = smaller coefficients\n",
    "- Ridge keeps ALL features but shrinks them toward zero\n",
    "- We'll find the best alpha that minimizes test MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:19:04.118040Z",
     "iopub.status.busy": "2025-12-25T19:19:04.117979Z",
     "iopub.status.idle": "2025-12-25T19:19:04.124706Z",
     "shell.execute_reply": "2025-12-25T19:19:04.124519Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Ridge Regression (L2 Regularization)\")\n",
    "print(\"Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø±ÙŠØ¯Ø¬ (Ø§Ù„ØªÙ†Ø¸ÙŠÙ… L2)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ridge regression with different alpha values\n",
    "# Alpha (Î±) = regularization strength parameter (see Alpha explanation section above!)\n",
    "# - Higher alpha = more regularization = smaller coefficients\n",
    "# - Lower alpha = less regularization = closer to Linear Regression\n",
    "# Why try different alphas? We need to find the best balance between fitting the data and preventing overfitting!\n",
    "# We'll evaluate each alpha on the test set and pick the one with lowest MSE\n",
    "alphas = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "ridge_results = []\n",
    "\n",
    "print(\"\\n   Trying different alpha values...\")\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)  # Create Ridge model with this alpha\n",
    "    ridge.fit(X_train_scaled, y_train)  # Train on scaled data\n",
    "    ridge_pred = ridge.predict(X_test_scaled)  # Predict on test data\n",
    "    \n",
    "    mse = mean_squared_error(y_test, ridge_pred)\n",
    "    r2 = r2_score(y_test, ridge_pred)\n",
    "    \n",
    "    ridge_results.append({\n",
    "        'alpha': alpha,\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'model': ridge\n",
    "    })\n",
    "    print(f\"   Alpha {alpha:6.2f}: MSE = {mse:.4f}, RÂ² = {r2:.4f}\")\n",
    "\n",
    "# Add interpretation after showing all results\n",
    "print(\"\\nğŸ’¡ What to Notice:\")\n",
    "print(\"   - Look at how MSE and RÂ² change as alpha increases\")\n",
    "print(\"   - Notice: Very low alpha (0.01) gives similar results to Linear Regression (MSE = 0.5559)\")\n",
    "print(\"   - Notice: The changes are SMALL (0.5559 â†’ 0.5533) - but regularization still helps!\")\n",
    "print(\"   - Why small changes? The original model wasn't heavily overfitting, so regularization's benefit is modest\")\n",
    "print(\"   - Even small improvements (0.0026 lower MSE) can be meaningful - regularization stabilizes the model!\")\n",
    "print(\"   - Best alpha: 100.0 gives MSE = 0.5533 (lowest error, best performance)\")\n",
    "print(\"   - This teaches us: Regularization doesn't always dramatically improve performance, but it makes models more stable!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Lasso Regression (L1 Regularization) | Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù„Ø§Ø³Ùˆ (Ø§Ù„ØªÙ†Ø¸ÙŠÙ… L1)\n",
    "\n",
    "**BEFORE**: We've seen how Ridge works. Now let's try Lasso!\n",
    "\n",
    "**AFTER**: We'll see how Lasso differs from Ridge - it can actually remove features!\n",
    "\n",
    "**Key difference from Ridge**:\n",
    "- **Ridge**: Shrinks all coefficients toward zero (keeps all features)\n",
    "- **Lasso**: Can set coefficients to EXACTLY zero (removes features automatically!)\n",
    "- This is Lasso's special power: **automatic feature selection**\n",
    "- We'll count how many features Lasso keeps vs removes\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:19:04.125649Z",
     "iopub.status.busy": "2025-12-25T19:19:04.125593Z",
     "iopub.status.idle": "2025-12-25T19:19:04.136547Z",
     "shell.execute_reply": "2025-12-25T19:19:04.136206Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. Lasso Regression (L1 Regularization)\")\n",
    "print(\"Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù„Ø§Ø³Ùˆ (Ø§Ù„ØªÙ†Ø¸ÙŠÙ… L1)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Lasso regression with different alpha values\n",
    "# Lasso does feature selection - sets some coefficients to zero!\n",
    "lasso_results = []\n",
    "\n",
    "print(\"\\n   Trying different alpha values...\")\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha)  # Create Lasso model\n",
    "    lasso.fit(X_train_scaled, y_train)  # Train on scaled data\n",
    "    lasso_pred = lasso.predict(X_test_scaled)  # Predict\n",
    "    \n",
    "    mse = mean_squared_error(y_test, lasso_pred)\n",
    "    r2 = r2_score(y_test, lasso_pred)\n",
    "    \n",
    "    # Count non-zero coefficients (features that Lasso kept)\n",
    "    # Why check this? Lasso removes features by setting coefficients to zero!\n",
    "    n_features = np.sum(np.abs(lasso.coef_) > 0.01)\n",
    "    \n",
    "    lasso_results.append({\n",
    "        'alpha': alpha,\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'n_features': n_features,  # How many features Lasso kept\n",
    "        'model': lasso\n",
    "    })\n",
    "    print(f\"   Alpha {alpha:6.2f}: MSE = {mse:.4f}, RÂ² = {r2:.4f}, Features = {n_features}/{len(X_data.columns)}\")\n",
    "\n",
    "# Add interpretation after showing all results\n",
    "print(\"\\nğŸ’¡ What to Notice:\")\n",
    "print(\"   - Look at the 'Features' column: This shows how many features Lasso KEPT (non-zero coefficients)\")\n",
    "print(\"   - Notice: As alpha increases, fewer features are used (Lasso removes more features!)\")\n",
    "print(\"   - Notice: High alpha (1.0+) removes ALL features (Features = 0/8) - this is too much regularization!\")\n",
    "print(\"   - âš ï¸ CRITICAL: When alpha â‰¥ 1.0, RÂ² becomes NEGATIVE (RÂ² = -0.0002)!\")\n",
    "print(\"     â†’ Negative RÂ² means the model is WORSE than just predicting the mean!\")\n",
    "print(\"     â†’ This happens because Lasso removed ALL features (no model left!)\")\n",
    "print(\"     â†’ This is what OVER-REGULARIZATION looks like - alpha is too high!\")\n",
    "print(\"   - Notice: Low alpha (0.01) keeps most features (Features = 7/8) - good balance\")\n",
    "print(\"   - Compare with Ridge: Ridge always uses ALL 8 features, Lasso can remove some!\")\n",
    "print(\"   - Which alpha performed best? Look for the lowest MSE value above! ğŸ‘†\")\n",
    "print(\"   - Best alpha = 0.01 (lowest MSE = 0.5483, highest RÂ² = 0.5816, keeps 7/8 features)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:19:04.138455Z",
     "iopub.status.busy": "2025-12-25T19:19:04.138344Z",
     "iopub.status.idle": "2025-12-25T19:19:04.142334Z",
     "shell.execute_reply": "2025-12-25T19:19:04.141286Z"
    }
   },
   "source": [
    "# Find best Ridge alpha (the one with lowest test MSE)\n",
    "# We tried different alpha values above, now let's pick the best one\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” Finding Best Ridge Alpha\")\n",
    "print(\"Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù‚ÙŠÙ…Ø© Alpha Ù„Ø±ÙŠØ¬\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_ridge = min(ridge_results, key=lambda x: x['mse'])\n",
    "print(f\"\\nâœ… Best Ridge Model:\")\n",
    "print(f\"   Alpha (Î±): {best_ridge['alpha']}\")\n",
    "print(f\"   Test MSE: {best_ridge['mse']:.4f} (lowest error)\")\n",
    "print(f\"   Test RÂ²: {best_ridge['r2']:.4f} (higher is better)\")\n",
    "print(f\"\\nğŸ’¡ Insight: Ridge with Î±={best_ridge['alpha']} gives the best performance!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:19:04.144666Z",
     "iopub.status.busy": "2025-12-25T19:19:04.144514Z",
     "iopub.status.idle": "2025-12-25T19:19:04.147383Z",
     "shell.execute_reply": "2025-12-25T19:19:04.146808Z"
    }
   },
   "source": [
    "# Find best Lasso alpha (the one with lowest test MSE)\n",
    "# We tried different alpha values above, now let's pick the best one\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” Finding Best Lasso Alpha\")\n",
    "print(\"Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù‚ÙŠÙ…Ø© Alpha Ù„Ù„Ø§Ø³Ùˆ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_lasso = min(lasso_results, key=lambda x: x['mse'])\n",
    "print(f\"\\nâœ… Best Lasso Model:\")\n",
    "print(f\"   Alpha (Î±): {best_lasso['alpha']}\")\n",
    "print(f\"   Test MSE: {best_lasso['mse']:.4f} (lowest error)\")\n",
    "print(f\"   Test RÂ²: {best_lasso['r2']:.4f} (higher is better)\")\n",
    "print(f\"   Features used: {best_lasso['n_features']}/{len(X_data.columns)}\")\n",
    "print(f\"\\nğŸ’¡ Insight: Lasso with Î±={best_lasso['alpha']} gives the best performance!\")\n",
    "if best_lasso['n_features'] < len(X_data.columns):\n",
    "    print(f\"   Plus: Lasso automatically removed {len(X_data.columns) - best_lasso['n_features']} features (feature selection!)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model Comparison | Ø§Ù„Ø®Ø·ÙˆØ© 4: Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
    "\n",
    "**BEFORE**: We've built three models (Linear, Ridge, Lasso) and found the best alpha for Ridge and Lasso. \n",
    "\n",
    "**AFTER**: Now let's compare all three models side-by-side to see which performs best!\n",
    "\n",
    "**What we'll compare**:\n",
    "- **Linear Regression**: Our baseline (no regularization, Î± = 0)\n",
    "- **Best Ridge Model**: Best performing Ridge with optimal alpha\n",
    "- **Best Lasso Model**: Best performing Lasso with optimal alpha\n",
    "\n",
    "**Why compare?** This helps us understand:\n",
    "- Does regularization actually help? (Ridge/Lasso vs Linear)\n",
    "- Which regularization works better? (Ridge vs Lasso)\n",
    "- What are the trade-offs? (performance vs simplicity)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:19:04.150759Z",
     "iopub.status.busy": "2025-12-25T19:19:04.150507Z",
     "iopub.status.idle": "2025-12-25T19:19:04.160523Z",
     "shell.execute_reply": "2025-12-25T19:19:04.159435Z"
    }
   },
   "source": [
    "# 4. Comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Model Comparison\")\n",
    "print(\"Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\")\n",
    "print(\"=\" * 60)\n",
    "# pd.DataFrame(data)\n",
    "# - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "# - data: Dictionary where keys become column names, values become column data\n",
    "#   - Each key-value pair: key = column name, value = list of values for that column\n",
    "# - Returns DataFrame with rows and columns\n",
    "# - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', f'Ridge (Î±={best_ridge[\"alpha\"]})',\n",
    "              f'Lasso (Î±={best_lasso[\"alpha\"]})'],\n",
    "    'Test MSE': [lr_mse, best_ridge['mse'], best_lasso['mse']],\n",
    "    'Test RÂ²': [lr_r2, best_ridge['r2'], best_lasso['r2']]\n",
    "})\n",
    "print(\"\\nComparison Table:\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\nğŸ’¡ Quick Look - What to Notice in the Table Above:\")\n",
    "print(\"   - Look at the Test MSE column: Which model has the LOWEST MSE? (Lower is better!)\")\n",
    "print(\"   - Look at the Test RÂ² column: Which model has the HIGHEST RÂ²? (Higher is better!)\")\n",
    "print(\"   - Notice: All three models have SIMILAR performance (values are close to each other)\")\n",
    "print(\"   - Notice: The differences are SMALL - this is normal and realistic!\")\n",
    "print(\"   - Notice: Lasso has the best MSE (0.5483) and RÂ² (0.5816) - regularization helped!\")\n",
    "print(\"   - ğŸ’¡ Learning point: Small improvements (0.0076 MSE difference) are meaningful in practice!\")\n",
    "print(\"   - Regularization's value isn't always huge performance gains - it's about stability and robustness!\")\n",
    "\n",
    "# Add interpretation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ’¡ Interpreting the Comparison | ØªÙØ³ÙŠØ± Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find best model\n",
    "best_mse_idx = comparison['Test MSE'].idxmin()  # Lower MSE is better\n",
    "best_r2_idx = comparison['Test RÂ²'].idxmax()    # Higher RÂ² is better (FIXED: was idxmin)\n",
    "best_model_mse = comparison.loc[best_mse_idx, 'Model']\n",
    "best_model_r2 = comparison.loc[best_r2_idx, 'Model']\n",
    "\n",
    "print(f\"\\nğŸ“Š Best Model by MSE: {best_model_mse}\")\n",
    "print(f\"   - Lowest error: {comparison['Test MSE'].min():.4f}\")\n",
    "print(f\"   - This model has the smallest prediction errors\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Best Model by RÂ²: {best_model_r2}\")\n",
    "print(f\"   - Highest RÂ²: {comparison['Test RÂ²'].max():.4f}\")\n",
    "print(f\"   - This model explains the most variance\")\n",
    "\n",
    "print(f\"\\nğŸ” Key Observations:\")\n",
    "mse_diff = comparison['Test MSE'].max() - comparison['Test MSE'].min()\n",
    "r2_diff = comparison['Test RÂ²'].max() - comparison['Test RÂ²'].min()\n",
    "\n",
    "if mse_diff < 0.01:\n",
    "    print(f\"   - âœ… All models perform similarly (MSE difference: {mse_diff:.4f})\")\n",
    "    print(f\"   - Regularization shows SMALL improvements - this is actually normal and realistic!\")\n",
    "    print(f\"   - Why small changes? The original Linear Regression model wasn't heavily overfitting\")\n",
    "    print(f\"   - This teaches us: Regularization doesn't always create dramatic improvements\")\n",
    "    print(f\"   - But regularization still helps: It stabilizes coefficients and prevents overfitting!\")\n",
    "    print(f\"   - Even small improvements matter in real-world applications\")\n",
    "else:\n",
    "    print(f\"   - âš ï¸  Significant performance difference (MSE range: {mse_diff:.4f})\")\n",
    "    print(f\"   - Regularization {'improved' if best_model_mse != 'Linear Regression' else 'did not improve'} performance\")\n",
    "    print(f\"   - This suggests the original model had overfitting issues that regularization solved!\")\n",
    "\n",
    "if r2_diff < 0.01:\n",
    "    print(f\"   - âœ… RÂ² scores are very close (difference: {r2_diff:.4f})\")\n",
    "else:\n",
    "    print(f\"   - âš ï¸  RÂ² scores differ (range: {r2_diff:.4f})\")\n",
    "\n",
    "print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "print(f\"   - Compare models using multiple metrics (MSE and RÂ²)\")\n",
    "print(f\"   - Lower MSE = better predictions (less error)\")\n",
    "print(f\"   - Higher RÂ² = better fit (explains more variance)\")\n",
    "print(f\"   - Regularization (Ridge/Lasso) helps when there's overfitting\")\n",
    "print(f\"   - âš ï¸ Important: Small improvements (like 0.0076 MSE difference) are NORMAL and REALISTIC!\")\n",
    "print(f\"   - Not every dataset will show dramatic improvements - that's okay!\")\n",
    "print(f\"   - Regularization's value: It stabilizes models and prevents overfitting, even if performance gains are modest\")\n",
    "print(f\"   - Lasso has advantage: automatic feature selection (removes irrelevant features)\")\n",
    "print(f\"   - In practice, regularization is about robustness and generalization, not just performance numbers\")\n",
    "\n",
    "# Check if Lasso removed features\n",
    "if best_lasso['n_features'] < len(X_data.columns):\n",
    "    print(f\"\\nğŸ’¡ Lasso Feature Selection:\")\n",
    "    print(f\"   - Lasso used only {best_lasso['n_features']}/{len(X_data.columns)} features\")\n",
    "    print(f\"   - Removed {len(X_data.columns) - best_lasso['n_features']} features (set coefficients to 0)\")\n",
    "    print(f\"   - This is Lasso's unique advantage: automatic feature selection!\")\n",
    "    print(f\"   - Simpler model (fewer features) = easier to interpret\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:19:04.162344Z",
     "iopub.status.busy": "2025-12-25T19:19:04.162193Z",
     "iopub.status.idle": "2025-12-25T19:19:04.170182Z",
     "shell.execute_reply": "2025-12-25T19:19:04.169521Z"
    }
   },
   "source": [
    "# 5. Coefficient Comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. Coefficient Comparison\")\n",
    "print(\"Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "# pd.DataFrame(data)\n",
    "# - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "# - data: Dictionary where keys become column names, values become column data\n",
    "#   - Each key-value pair: key = column name, value = list of values for that column\n",
    "# - Returns DataFrame with rows and columns\n",
    "# - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': X_data.columns,\n",
    "    'Linear': lr.coef_,\n",
    "    'Ridge': best_ridge['model'].coef_,\n",
    "    'Lasso': best_lasso['model'].coef_\n",
    "})\n",
    "print(\"\\nCoefficient Comparison (first 5 features):\")\n",
    "print(coef_comparison.head().to_string(index=False))\n",
    "\n",
    "print(\"\\nğŸ’¡ Quick Look - What to Notice in the Table Above:\")\n",
    "print(\"   - Look at the 'Population' row: Linear= -0.002308, Ridge= -0.000024, Lasso= -0.000000\")\n",
    "print(\"   - Notice: Lasso set Population coefficient to EXACTLY 0.000000 (removed this feature!)\")\n",
    "print(\"   - Notice: Ridge shrunk it to -0.000024 (very small, but NOT zero - still keeps the feature)\")\n",
    "print(\"   - Look at other features: Compare how Ridge shrinks vs Lasso removes coefficients\")\n",
    "print(\"   - This shows Lasso's feature selection power - it can completely remove features! ğŸ¯\")\n",
    "print(\"\\nLasso shrinks many coefficients to zero (feature selection)\")\n",
    "print(\"Ù„Ø§Ø³Ùˆ ÙŠÙ‚Ù„Øµ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ± (Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª)\")\n",
    "\n",
    "# Add interpretation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ’¡ Understanding Coefficient Differences | ÙÙ‡Ù… Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count zero coefficients\n",
    "lasso_zeros = (np.abs(best_lasso['model'].coef_) < 0.01).sum()\n",
    "ridge_zeros = (np.abs(best_ridge['model'].coef_) < 0.01).sum()\n",
    "linear_zeros = (np.abs(lr.coef_) < 0.01).sum()\n",
    "\n",
    "print(f\"\\nğŸ“Š Coefficient Analysis:\")\n",
    "print(f\"   - Linear Regression: {linear_zeros} coefficients near zero\")\n",
    "print(f\"   - Ridge: {ridge_zeros} coefficients near zero (shrinks but keeps all)\")\n",
    "print(f\"   - Lasso: {lasso_zeros} coefficients set to zero (feature selection!)\")\n",
    "\n",
    "print(f\"\\nğŸ” What This Shows:\")\n",
    "print(f\"   - Ridge: Shrinks coefficients toward 0 but keeps all features\")\n",
    "print(f\"   - Lasso: Can completely remove features (coefficient = 0)\")\n",
    "print(f\"   - Lasso's sparsity: Only {best_lasso['n_features']} features have non-zero coefficients\")\n",
    "\n",
    "# Compare coefficient magnitudes\n",
    "coef_diff_ridge = np.abs(lr.coef_ - best_ridge['model'].coef_)\n",
    "coef_diff_lasso = np.abs(lr.coef_ - best_lasso['model'].coef_)\n",
    "\n",
    "print(f\"\\nğŸ“Š Coefficient Shrinking:\")\n",
    "print(f\"   - Ridge average change: {coef_diff_ridge.mean():.4f}\")\n",
    "print(f\"   - Lasso average change: {coef_diff_lasso.mean():.4f}\")\n",
    "if coef_diff_lasso.mean() > coef_diff_ridge.mean():\n",
    "    print(f\"   - Lasso shrinks coefficients more aggressively\")\n",
    "\n",
    "print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "print(f\"   - Regularization reduces coefficient magnitudes (prevents overfitting)\")\n",
    "print(f\"   - Ridge: Gentle shrinking, keeps all features\")\n",
    "print(f\"   - Lasso: Aggressive shrinking, removes irrelevant features\")\n",
    "print(f\"   - Smaller coefficients = simpler model = better generalization\")\n",
    "print(f\"   - Lasso is useful when you have many features (automatic feature selection)\")\n",
    "print(f\"   - Ridge is useful when all features might be relevant\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Coefficient Comparison | Ø§Ù„Ø®Ø·ÙˆØ© 5: Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª\n",
    "\n",
    "**BEFORE**: We've compared model performance (MSE, RÂ²). But what's happening \"under the hood\" with the coefficients?\n",
    "\n",
    "**AFTER**: We'll see how Ridge and Lasso actually change the coefficients compared to Linear Regression!\n",
    "\n",
    "**Why compare coefficients?**\n",
    "- **Understand regularization's effect**: How does it shrink coefficients?\n",
    "- **See Ridge vs Lasso difference**: Ridge shrinks smoothly, Lasso can set to zero\n",
    "- **Verify feature selection**: Lasso should have some coefficients exactly equal to zero\n",
    "- **Better intuition**: Seeing coefficient changes helps understand why regularization works!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Decision Framework - When to Use Ridge vs Lasso | Ø§Ù„Ø®Ø·ÙˆØ© 7: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… Ø±ÙŠØ¯Ø¬ Ù…Ù‚Ø§Ø¨Ù„ Ù„Ø§Ø³Ùˆ\n",
    "\n",
    "**BEFORE**: You've learned how to build Ridge and Lasso models, but when should you use each one?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose between Ridge, Lasso, or regular Linear Regression!\n",
    "\n",
    "**Why this matters**: Using the wrong regularization method can:\n",
    "- **Miss important features** â†’ Lasso removes features you need\n",
    "- **Keep irrelevant features** â†’ Ridge keeps all features even when some are noise\n",
    "- **Poor performance** â†’ Wrong method leads to worse predictions\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework: Ridge vs Lasso vs Linear Regression | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±: Ø±ÙŠØ¯Ø¬ Ù…Ù‚Ø§Ø¨Ù„ Ù„Ø§Ø³Ùˆ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ\n",
    "\n",
    "**Key Question**: Should I use **LINEAR REGRESSION**, **RIDGE**, or **LASSO**?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Do you have overfitting?\n",
    "â”œâ”€ NO â†’ Use LINEAR REGRESSION âœ…\n",
    "â”‚   â””â”€ Why? No need for regularization if model generalizes well\n",
    "â”‚\n",
    "â””â”€ YES â†’ Check your situation:\n",
    "    â”œâ”€ Many features (>20)? â†’ Continue to next step\n",
    "    â”‚\n",
    "    â”œâ”€ Need feature selection? â†’ Use LASSO âœ…\n",
    "    â”‚   â””â”€ Why? Lasso removes irrelevant features automatically\n",
    "    â”‚\n",
    "    â”œâ”€ Multicollinearity present? â†’ Use RIDGE âœ…\n",
    "    â”‚   â””â”€ Why? Ridge handles correlated features better\n",
    "    â”‚\n",
    "    â”œâ”€ All features important? â†’ Use RIDGE âœ…\n",
    "    â”‚   â””â”€ Why? Ridge keeps all features, just shrinks them\n",
    "    â”‚\n",
    "    â””â”€ Want interpretability? â†’ Use LASSO âœ…\n",
    "        â””â”€ Why? Fewer features = simpler model\n",
    "```\n",
    "\n",
    "#### Detailed Decision Process:\n",
    "\n",
    "```\n",
    "Step 1: Check if regularization is needed\n",
    "â”œâ”€ Train RÂ² >> Test RÂ²? â†’ YES, overfitting present\n",
    "â”‚   â””â”€ Use Ridge or Lasso\n",
    "â”‚\n",
    "â””â”€ Train RÂ² â‰ˆ Test RÂ²? â†’ NO, no overfitting\n",
    "    â””â”€ Use Linear Regression (simpler)\n",
    "\n",
    "Step 2: If overfitting, choose regularization type\n",
    "â”œâ”€ Do you have many features (>20)?\n",
    "â”‚   â”œâ”€ YES â†’ Continue to step 3\n",
    "â”‚   â””â”€ NO â†’ Try Ridge first (simpler)\n",
    "â”‚\n",
    "â”œâ”€ Do you need feature selection?\n",
    "â”‚   â”œâ”€ YES â†’ Use LASSO\n",
    "â”‚   â”‚   â””â”€ Why? Automatically removes irrelevant features\n",
    "â”‚   â””â”€ NO â†’ Continue to step 4\n",
    "â”‚\n",
    "â”œâ”€ Is there multicollinearity (correlated features)?\n",
    "â”‚   â”œâ”€ YES â†’ Use RIDGE\n",
    "â”‚   â”‚   â””â”€ Why? Ridge handles correlations better\n",
    "â”‚   â””â”€ NO â†’ Continue to step 5\n",
    "â”‚\n",
    "â””â”€ Are all features potentially important?\n",
    "    â”œâ”€ YES â†’ Use RIDGE\n",
    "    â”‚   â””â”€ Why? Keeps all features, just shrinks them\n",
    "    â””â”€ NO â†’ Use LASSO\n",
    "        â””â”€ Why? Removes irrelevant features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Comparison Table: Linear vs Ridge vs Lasso | Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Linear Regression** | No overfitting, few features, interpretable | â€¢ Simple<br>â€¢ Fast<br>â€¢ Interpretable<br>â€¢ No hyperparameters | â€¢ Can overfit<br>â€¢ Sensitive to outliers<br>â€¢ Can't handle many features | Small dataset, < 10 features |\n",
    "| **Ridge (L2)** | Overfitting, multicollinearity, all features important | â€¢ Prevents overfitting<br>â€¢ Handles multicollinearity<br>â€¢ Keeps all features<br>â€¢ Stable | â€¢ Doesn't remove features<br>â€¢ All features contribute<br>â€¢ Less interpretable | Many correlated features, all potentially important |\n",
    "| **Lasso (L1)** | Overfitting, many features, need feature selection | â€¢ Prevents overfitting<br>â€¢ Automatic feature selection<br>â€¢ More interpretable<br>â€¢ Simpler models | â€¢ Can remove important features<br>â€¢ Unstable with correlated features<br>â€¢ May over-regularize | High-dimensional data, feature selection needed |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When to Use Each Method | Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø©\n",
    "\n",
    "#### Use Linear Regression when:\n",
    "1. **No Overfitting** âœ…\n",
    "   - Train and test performance are similar\n",
    "   - Model generalizes well\n",
    "   - **Example**: Small dataset, few features, good performance\n",
    "\n",
    "2. **Few Features** âœ…\n",
    "   - Less than 10-15 features\n",
    "   - All features are important\n",
    "   - **Example**: House price from size, bedrooms, age\n",
    "\n",
    "3. **Interpretability Critical** âœ…\n",
    "   - Need to understand exact coefficients\n",
    "   - No regularization complexity needed\n",
    "   - **Example**: Medical diagnosis, regulatory compliance\n",
    "\n",
    "#### Use Ridge Regression when:\n",
    "1. **Overfitting Present** âœ…\n",
    "   - Train RÂ² much higher than test RÂ²\n",
    "   - Model memorizes training data\n",
    "   - **Example**: Polynomial regression with high degree\n",
    "\n",
    "2. **Multicollinearity** âœ…\n",
    "   - Features are highly correlated\n",
    "   - Ridge handles correlations better than Lasso\n",
    "   - **Example**: House features (size, rooms, area all correlated)\n",
    "\n",
    "3. **All Features Important** âœ…\n",
    "   - Don't want to remove any features\n",
    "   - Just want to shrink coefficients\n",
    "   - **Example**: All features are domain-relevant\n",
    "\n",
    "4. **Many Features** âœ…\n",
    "   - 20+ features\n",
    "   - Need regularization but want to keep all features\n",
    "   - **Example**: 50+ features from feature engineering\n",
    "\n",
    "#### Use Lasso Regression when:\n",
    "1. **Feature Selection Needed** âœ…\n",
    "   - Many features, some are noise\n",
    "   - Want automatic feature selection\n",
    "   - **Example**: 100+ features, need to find important ones\n",
    "\n",
    "2. **Sparse Solution** âœ…\n",
    "   - Expect only few features matter\n",
    "   - Want interpretable model\n",
    "   - **Example**: Gene expression data (few genes matter)\n",
    "\n",
    "3. **High-Dimensional Data** âœ…\n",
    "   - More features than samples\n",
    "   - Need to reduce dimensionality\n",
    "   - **Example**: Text data with thousands of features\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ When NOT to Use Each Method | Ù…ØªÙ‰ Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø©\n",
    "\n",
    "#### Don't use Linear Regression when:\n",
    "1. **Severe Overfitting** âŒ\n",
    "   - Train RÂ² >> Test RÂ²\n",
    "   - **Use Instead**: Ridge or Lasso\n",
    "\n",
    "2. **Many Features** âŒ\n",
    "   - 50+ features\n",
    "   - **Use Instead**: Ridge or Lasso\n",
    "\n",
    "3. **Multicollinearity** âŒ\n",
    "   - Highly correlated features\n",
    "   - **Use Instead**: Ridge\n",
    "\n",
    "#### Don't use Ridge when:\n",
    "1. **Feature Selection Needed** âŒ\n",
    "   - Want to remove irrelevant features\n",
    "   - **Use Instead**: Lasso\n",
    "\n",
    "2. **Sparse Solution Expected** âŒ\n",
    "   - Only few features matter\n",
    "   - **Use Instead**: Lasso\n",
    "\n",
    "#### Don't use Lasso when:\n",
    "1. **Multicollinearity Present** âŒ\n",
    "   - Features are highly correlated\n",
    "   - Lasso may randomly select one\n",
    "   - **Use Instead**: Ridge\n",
    "\n",
    "2. **All Features Important** âŒ\n",
    "   - Don't want to remove any features\n",
    "   - **Use Instead**: Ridge\n",
    "\n",
    "3. **More Features than Samples** âŒ\n",
    "   - Lasso can select at most n features (n = samples)\n",
    "   - **Use Instead**: Ridge or Elastic Net\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: House Price Prediction (10 features) âœ… LINEAR REGRESSION\n",
    "- **Features**: Size, bedrooms, age, location, etc. (10 total)\n",
    "- **Overfitting**: No (train RÂ² = 0.85, test RÂ² = 0.83)\n",
    "- **Decision**: âœ… Use Linear Regression\n",
    "- **Reasoning**: No overfitting, few features, all important\n",
    "\n",
    "#### Example 2: House Price Prediction (50 features) âœ… RIDGE\n",
    "- **Features**: Size, bedrooms, age, location, neighborhood stats, etc. (50 total)\n",
    "- **Overfitting**: Yes (train RÂ² = 0.95, test RÂ² = 0.75)\n",
    "- **Multicollinearity**: Yes (size, rooms, area all correlated)\n",
    "- **Decision**: âœ… Use Ridge Regression\n",
    "- **Reasoning**: Overfitting, many features, multicollinearity, all features potentially important\n",
    "\n",
    "#### Example 3: Gene Expression Analysis (1000 features) âœ… LASSO\n",
    "- **Features**: 1000 genes, only 10-20 matter\n",
    "- **Overfitting**: Yes (train RÂ² = 0.98, test RÂ² = 0.60)\n",
    "- **Feature Selection**: Critical (need to find important genes)\n",
    "- **Decision**: âœ… Use Lasso Regression\n",
    "- **Reasoning**: Many features, need feature selection, sparse solution expected\n",
    "\n",
    "#### Example 4: Sales Prediction (30 features, some noise) âš ï¸ TRY BOTH\n",
    "- **Features**: 30 features, some are noise\n",
    "- **Overfitting**: Yes (train RÂ² = 0.92, test RÂ² = 0.78)\n",
    "- **Decision**: âš ï¸ Try both Ridge and Lasso, compare\n",
    "- **Reasoning**: Overfitting present, some features may be noise, try both methods\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Start with Linear Regression** - Always try simplest model first\n",
    "2. **Check for overfitting** - Compare train vs test performance\n",
    "3. **Ridge for multicollinearity** - When features are correlated\n",
    "4. **Lasso for feature selection** - When you need to remove features\n",
    "5. **Tune alpha** - Critical hyperparameter for both methods\n",
    "6. **Scale features first** - Regularization requires scaled features\n",
    "7. **Try both** - Sometimes try Ridge and Lasso, pick the best\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario 1**: Predicting house prices with 8 features (size, bedrooms, age, etc.)\n",
    "- **Overfitting**: No (train RÂ² = 0.88, test RÂ² = 0.86)\n",
    "- **Decision**: âœ… Linear Regression (no overfitting, few features)\n",
    "\n",
    "**Scenario 2**: Predicting sales with 50 features, many correlated\n",
    "- **Overfitting**: Yes (train RÂ² = 0.94, test RÂ² = 0.76)\n",
    "- **Multicollinearity**: Yes (many correlated features)\n",
    "- **Decision**: âœ… Ridge Regression (overfitting, multicollinearity, all features important)\n",
    "\n",
    "**Scenario 3**: Predicting disease from 500 gene expressions\n",
    "- **Overfitting**: Yes (train RÂ² = 0.97, test RÂ² = 0.65)\n",
    "- **Feature Selection**: Critical (only few genes matter)\n",
    "- **Decision**: âœ… Lasso Regression (many features, need feature selection, sparse solution)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Example 2: Cross-Validation** - For proper evaluation of Ridge/Lasso models\n",
    "- ğŸ““ **Unit 3: Classification** - Same regularization concepts apply to classification\n",
    "- ğŸ““ **Unit 5, Example 1: Grid Search** - For tuning alpha hyperparameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualization | Ø§Ù„Ø®Ø·ÙˆØ© 6: Ø§Ù„ØªØµÙˆØ±\n",
    "\n",
    "**BEFORE**: We've compared models and coefficients using numbers. Now let's visualize the results!\n",
    "\n",
    "**AFTER**: We'll create plots showing:\n",
    "1. How alpha affects model performance (MSE vs Alpha)\n",
    "2. How coefficients differ between models (coefficient magnitudes)\n",
    "\n",
    "**Why visualize?**\n",
    "- **See patterns visually**: Graphs make it easier to understand the relationships\n",
    "- **Compare at a glance**: Visual comparison is often clearer than numbers\n",
    "- **Alpha effect**: See how different alpha values affect Ridge and Lasso\n",
    "- **Coefficient differences**: Visualize how Ridge shrinks vs Lasso removes coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T19:19:04.171869Z",
     "iopub.status.busy": "2025-12-25T19:19:04.171773Z",
     "iopub.status.idle": "2025-12-25T19:19:04.677282Z",
     "shell.execute_reply": "2025-12-25T19:19:04.677074Z"
    }
   },
   "source": [
    "# 6. Visualization\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. Visualization\")\n",
    "print(\"Ø§Ù„ØªØµÙˆØ±\")\n",
    "print(\"=\" * 60)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Alpha vs MSE\n",
    "axes[0].semilogx([r['alpha'] for r in ridge_results],\n",
    "                 [r['mse'] for r in ridge_results],\n",
    "                 'o-', label='Ridge', linewidth=2)\n",
    "axes[0].semilogx([l['alpha'] for l in lasso_results],\n",
    "                 [l['mse'] for l in lasso_results],\n",
    "                 's-', label='Lasso', linewidth=2)\n",
    "axes[0].axhline(lr_mse, color='r', linestyle='--', label='Linear Regression')\n",
    "axes[0].set_xlabel('Alpha (Regularization Strength)')\n",
    "axes[0].set_ylabel('Test MSE')\n",
    "axes[0].set_title('Regularization vs Model Performance')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Coefficient magnitudes\n",
    "n_features = len(lr.coef_)  # Get actual number of features from the model\n",
    "axes[1].bar(range(n_features), np.abs(lr.coef_), alpha=0.7, label='Linear', width=0.25)\n",
    "axes[1].bar([i + 0.25 for i in range(n_features)], np.abs(best_ridge['model'].coef_),\n",
    "            alpha=0.7, label='Ridge', width=0.25)\n",
    "axes[1].bar([i + 0.5 for i in range(n_features)], np.abs(best_lasso['model'].coef_),\n",
    "            alpha=0.7, label='Lasso', width=0.25)\n",
    "axes[1].set_xlabel('Feature Index')\n",
    "axes[1].set_ylabel('Absolute Coefficient Value')\n",
    "axes[1].set_title('Coefficient Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ridge_lasso_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nâœ“ Plot saved as 'ridge_lasso_comparison.png'\")\n",
    "plt.show()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 1 Complete! âœ“\")\n",
    "print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 1! âœ“\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}