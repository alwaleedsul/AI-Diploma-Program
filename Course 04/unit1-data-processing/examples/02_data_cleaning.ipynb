{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Data Cleaning | \u062a\u0646\u0638\u064a\u0641 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a\n",
    "\n",
    "## \ud83d\udcda Prerequisites (What You Need First) | \u0627\u0644\u0645\u062a\u0637\u0644\u0628\u0627\u062a \u0627\u0644\u0623\u0633\u0627\u0633\u064a\u0629\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- \u2705 **Example 1: Data Loading and Exploration** - You need to know how to identify data quality issues first!\n",
    "- \u2705 **Basic pandas knowledge**: DataFrames, indexing, filtering\n",
    "- \u2705 **Understanding of data quality**: What are missing values, duplicates, outliers?\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why data cleaning is necessary\n",
    "- Knowing which cleaning method to use\n",
    "- Understanding the impact of cleaning on your data\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd17 Where This Notebook Fits | \u0645\u0643\u0627\u0646 \u0647\u0630\u0627 \u0627\u0644\u062f\u0641\u062a\u0631\n",
    "\n",
    "**This is the SECOND example** - it fixes the problems we found in Example 1!\n",
    "\n",
    "**Why this example SECOND?**\n",
    "- **Before** you can preprocess data, you need to clean it\n",
    "- **Before** you can build models, you need clean data\n",
    "- **Before** you can make predictions, you need to fix data quality issues\n",
    "\n",
    "**Builds on**: \n",
    "- \ud83d\udcd3 Example 1: Data Loading and Exploration (we found the problems, now we fix them!)\n",
    "\n",
    "**Leads to**: \n",
    "- \ud83d\udcd3 Example 3: Data Preprocessing (needs clean data to work with)\n",
    "- \ud83d\udcd3 Example 4: Linear Regression (needs clean, preprocessed data)\n",
    "- \ud83d\udcd3 All ML models (all need clean data!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Data cleaning fixes **quality issues** (needed before preprocessing)\n",
    "2. Data cleaning teaches you **when to remove vs. impute** (critical decision-making)\n",
    "3. Data cleaning shows you **the impact of outliers** (affects model accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Cleaning Before Cooking | \u0627\u0644\u0642\u0635\u0629: \u0627\u0644\u062a\u0646\u0638\u064a\u0641 \u0642\u0628\u0644 \u0627\u0644\u0637\u0628\u062e\n",
    "\n",
    "Imagine you're cooking a meal. **Before** you can cook, you need to clean your ingredients - remove spoiled items, wash vegetables, check for foreign objects. **After** cleaning everything, you can prepare a safe, delicious meal!\n",
    "\n",
    "Same with machine learning: **Before** building models, we clean our data - remove duplicates, handle missing values, fix outliers. **After** cleaning, we can build accurate, reliable models!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Cleaning Matters | \u0644\u0645\u0627\u0630\u0627 \u064a\u0647\u0645 \u062a\u0646\u0638\u064a\u0641 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a\u061f\n",
    "\n",
    "Data cleaning is essential for accurate models:\n",
    "- **Missing Values**: Break ML algorithms - must be handled\n",
    "- **Duplicates**: Bias your models (same data counted twice)\n",
    "- **Outliers**: Skew predictions and statistics\n",
    "- **Wrong Data Types**: Cause errors in calculations\n",
    "- **Dirty Data = Bad Models**: No amount of ML can fix fundamentally bad data\n",
    "\n",
    "## Learning Objectives | \u0623\u0647\u062f\u0627\u0641 \u0627\u0644\u062a\u0639\u0644\u0645\n",
    "1. Handle missing values (remove or impute)\n",
    "2. Remove duplicate rows\n",
    "3. Detect and handle outliers\n",
    "4. Convert data types correctly\n",
    "5. Understand trade-offs between different cleaning methods\n",
    "6. Know when to remove vs. when to fix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us clean and analyze data\n",
    "\n",
    "import pandas as pd  # For data manipulation (cleaning operations)\n",
    "import numpy as np   # For numerical operations (handling NaN, calculations)\n",
    "import matplotlib.pyplot as plt  # For visualizations (seeing outliers)\n",
    "import seaborn as sns  # For statistical plots (data quality visualization)\n",
    "\n",
    "print(\"\u2705 Libraries imported successfully!\")\n",
    "print(\"\\n\ud83d\udcda What each library does:\")\n",
    "print(\"   - pandas: Clean data (remove, fill, filter)\")\n",
    "print(\"   - numpy: Handle missing values (NaN operations)\")\n",
    "print(\"   - matplotlib: Visualize data quality issues\")\n",
    "print(\"   - seaborn: Create beautiful quality check plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | \u0627\u0644\u062c\u0632\u0621 \u0627\u0644\u0623\u0648\u0644: \u0625\u0639\u062f\u0627\u062f \u0627\u0644\u0645\u0634\u0647\u062f\n",
    "\n",
    "**BEFORE**: We explored our data in Example 1 and found problems - missing values, duplicates, outliers.\n",
    "\n",
    "**AFTER**: We'll clean the data by fixing all these issues, making it ready for preprocessing and modeling!\n",
    "\n",
    "**Why this matters**: Dirty data produces unreliable models. Cleaning is non-negotiable for good ML results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real-world Titanic dataset with natural data quality issues\n",
    "# The Titanic dataset is famous for having missing values, which makes it perfect for learning data cleaning!\n",
    "# This is REAL data from the 1912 Titanic disaster\n",
    "\n",
    "print(\"\\n\ud83d\udce5 Loading Titanic dataset...\")\n",
    "print(\"\u062a\u062d\u0645\u064a\u0644 \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a \u062a\u0627\u064a\u062a\u0627\u0646\u064a\u0643...\")\n",
    "\n",
    "# Load from public URL (well-known dataset)\n",
    "titanic_url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "df = pd.read_csv(titanic_url)\n",
    "\n",
    "# pd.read_csv(url)\n",
    "# - pd.read_csv(): Reads CSV file from URL or local path\n",
    "# - This is REAL historical data from the Titanic passenger manifest\n",
    "# - Returns DataFrame with passenger information\n",
    "\n",
    "print(f\"\\n\u2705 Real-world Titanic dataset loaded!\")\n",
    "print(f\"   \ud83d\udcca This is REAL data from the 1912 Titanic passenger manifest\")\n",
    "print(f\"   \ud83d\udcc8 Contains {len(df)} passengers with {len(df.columns)} features\")\n",
    "print(f\"\\n\ud83d\udd0d Notice:\")\n",
    "print(\"   - This dataset naturally has missing values (Age, Cabin, Embarked)\")\n",
    "print(\"   - Real data often has data quality issues - this is normal!\")\n",
    "print(\"   - We'll learn to clean this real data in the following steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Real-World Data with Issues | \u0627\u0644\u062e\u0637\u0648\u0629 1: \u062a\u062d\u0645\u064a\u0644 \u0628\u064a\u0627\u0646\u0627\u062a \u0627\u0644\u0639\u0627\u0644\u0645 \u0627\u0644\u062d\u0642\u064a\u0642\u064a \u0628\u0645\u0634\u0627\u0643\u0644\n",
    "\n",
    "**BEFORE**: We need to learn cleaning techniques, but we need data with real-world problems to practice on.\n",
    "\n",
    "**AFTER**: We'll load the Titanic dataset - a famous real-world dataset that naturally has missing values, making it perfect for learning data cleaning!\n",
    "\n",
    "**Why use Titanic?** This is REAL historical data from 1912 with natural data quality issues. Real datasets have these problems! We need to learn how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows in the real data\n",
    "# Real datasets sometimes have duplicates from data entry errors or merging issues\n",
    "\n",
    "# First, let's see if there are any natural duplicates\n",
    "initial_count = len(df)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Checking for duplicates...\")\n",
    "print(f\"   Initial rows: {initial_count}\")\n",
    "print(f\"   Duplicate rows found: {duplicate_count}\")\n",
    "\n",
    "# For demonstration, we'll add a few duplicates to show the cleaning process\n",
    "# (In real projects, you'd check if duplicates should be removed or kept)\n",
    "if duplicate_count == 0:\n",
    "    # Add 2 duplicates for demonstration purposes\n",
    "    df = pd.concat([df, df.iloc[[0, 1]]], ignore_index=True)\n",
    "    print(\"   - Added 2 duplicate rows for demonstration\")\n",
    "else:\n",
    "    print(\"   - Dataset already contains duplicates (real-world scenario!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in the real data\n",
    "# Real datasets often have outliers from errors (typos, measurement mistakes) or rare events\n",
    "\n",
    "# df.shape\n",
    "# - Returns tuple (rows, columns): (number_of_rows, number_of_columns)\n",
    "print(\"\\n\ud83d\udcca Original Data Shape:\", df.shape)\n",
    "print(\"\u0634\u0643\u0644 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0627\u0644\u0623\u0635\u0644\u064a\u0629:\", df.shape)\n",
    "\n",
    "# Check for potential outliers in Age (should be reasonable, e.g., 0-120)\n",
    "if 'Age' in df.columns:\n",
    "    age_outliers = df[(df['Age'] > 100) | (df['Age'] < 0)]\n",
    "    print(f\"\\n\ud83d\udd0d Checking for age outliers...\")\n",
    "    print(f\"   Age range: {df['Age'].min():.1f} to {df['Age'].max():.1f}\")\n",
    "    if len(age_outliers) > 0:\n",
    "        print(f\"   Found {len(age_outliers)} potential age outliers\")\n",
    "    else:\n",
    "        print(\"   No obvious age outliers found\")\n",
    "\n",
    "# For demonstration, we'll add one impossible age value to show outlier handling\n",
    "# (In real projects, check if outliers are errors or valid rare cases)\n",
    "if 'Age' in df.columns and df['Age'].max() < 120:\n",
    "    df.loc[0, 'Age'] = 150  # Add impossible age for demonstration\n",
    "    print(\"   - Added 1 impossible age value (150) for demonstration\")\n",
    "\n",
    "# df.head(10)\n",
    "# - Returns first 10 rows of DataFrame\n",
    "# - head(n): Shows first n rows (default is 5)\n",
    "# - Useful for quick data inspection\n",
    "print(\"\\n\ud83d\udcc4 Original Data (first 10 rows):\")\n",
    "print(df.head(10))\n",
    "print(f\"\\n\ud83d\udccb Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see how many missing values we have\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Handling Missing Values\")\n",
    "print(\"\u0645\u0639\u0627\u0644\u062c\u0629 \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n\ud83d\udd0d Missing values before cleaning:\")\n",
    "print(\"\u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629 \u0642\u0628\u0644 \u0627\u0644\u062a\u0646\u0638\u064a\u0641:\")\n",
    "\n",
    "# df.isnull().sum()\n",
    "# - df.isnull(): Returns DataFrame with True/False (True = missing value, False = not missing)\n",
    "# - .sum(): Sums True values (counts missing values) for each column\n",
    "# - Returns Series with column names and count of missing values\n",
    "# - Alternative: df.isna() does the same thing\n",
    "missing_before = df.isnull().sum()\n",
    "print(missing_before)\n",
    "\n",
    "# missing_before.sum()\n",
    "# - .sum() on Series: Adds up all values in the Series\n",
    "# - This gives total missing values across all columns\n",
    "print(f\"\\n   Total missing values: {missing_before.sum()}\")\n",
    "\n",
    "# df.shape[0] * df.shape[1]\n",
    "# - df.shape[0]: Number of rows\n",
    "# - df.shape[1]: Number of columns\n",
    "# - shape[0] * shape[1]: Total number of cells in DataFrame\n",
    "# - Used to calculate percentage of missing values\n",
    "print(f\"   Percentage missing: {(missing_before.sum() / (df.shape[0] * df.shape[1]) * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Remove rows with missing values\n",
    "# .dropna() removes any row that has at least one missing value\n",
    "# Why use this? If missing values are rare, it's better to remove than guess\n",
    "\n",
    "print(\"\\n--- Method 1: Remove rows with missing values ---\")\n",
    "print(\"--- \u0627\u0644\u0637\u0631\u064a\u0642\u0629 1: \u062d\u0630\u0641 \u0627\u0644\u0635\u0641\u0648\u0641 \u0630\u0627\u062a \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629 ---\")\n",
    "\n",
    "# df.dropna()\n",
    "# - Removes rows that contain ANY missing values (NaN/None)\n",
    "# - Returns new DataFrame (doesn't modify original unless inplace=True)\n",
    "# - Parameters:\n",
    "#   - axis=0 (default): Drop rows (axis=1 would drop columns)\n",
    "#   - how='any' (default): Drop if ANY value is missing\n",
    "#   - subset=None: Check all columns (can specify columns to check)\n",
    "#   - inplace=False: Return new DataFrame (True modifies original)\n",
    "df_removed = df.dropna()\n",
    "\n",
    "# df.shape[0] - df_removed.shape[0]\n",
    "# - df.shape[0]: Original number of rows\n",
    "# - df_removed.shape[0]: Number of rows after removal\n",
    "# - Difference = number of rows removed\n",
    "rows_removed = df.shape[0] - df_removed.shape[0]\n",
    "print(f\"\u2705 Rows after removal: {df_removed.shape[0]} (removed {rows_removed} rows)\")\n",
    "print(f\"\u0627\u0644\u0635\u0641\u0648\u0641 \u0628\u0639\u062f \u0627\u0644\u062d\u0630\u0641: {df_removed.shape[0]} (\u062a\u0645 \u062d\u0630\u0641 {rows_removed} \u0635\u0641)\")\n",
    "print(f\"   Data loss: {(rows_removed / df.shape[0] * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Handling Missing Values | \u0627\u0644\u062e\u0637\u0648\u0629 2: \u0645\u0639\u0627\u0644\u062c\u0629 \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629\n",
    "\n",
    "**BEFORE**: We have missing values (NaN/None) that will break our ML models.\n",
    "\n",
    "**AFTER**: We'll either remove rows with missing values OR fill them with reasonable estimates!\n",
    "\n",
    "**Why handle missing values?** \n",
    "- ML algorithms cannot work with missing data\n",
    "- Missing values indicate incomplete information\n",
    "- We must decide: **Remove** (if few missing) or **Impute** (if many missing)\n",
    "\n",
    "**Two main strategies:**\n",
    "1. **Remove**: Drop rows/columns with missing values (good if <5% missing)\n",
    "2. **Impute**: Fill missing values with mean/median/mode (good if >5% missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Fill missing values (imputation)\n",
    "# We'll use a copy so we don't modify the original\n",
    "print(\"\\n--- Method 2: Fill missing values ---\")\n",
    "print(\"--- \u0627\u0644\u0637\u0631\u064a\u0642\u0629 2: \u0645\u0644\u0621 \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629 ---\")\n",
    "\n",
    "# df.copy()\n",
    "# - Creates a deep copy of DataFrame (independent copy, not a reference)\n",
    "# - Changes to copy don't affect original\n",
    "# - Important: Without copy(), both variables point to same data\n",
    "# - Alternative: df.copy(deep=True) is explicit (deep=True is default)\n",
    "df_filled = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numeric columns with mean",
    "# \u0645\u0644\u0621 \u0627\u0644\u0623\u0639\u0645\u062f\u0629 \u0627\u0644\u0631\u0642\u0645\u064a\u0629 \u0628\u0627\u0644\u0645\u062a\u0648\u0633\u0637",
    "",
    "# df_filled['Age'].fillna(df_filled['Age'].mean(), inplace=True)",
    "# - df_filled['Age']: Selects 'age' column (returns Series)",
    "# - .fillna(): Fills missing values (NaN) with specified value",
    "#   - Parameter: Value to fill (here: mean of age column)",
    "#   - inplace=True: Modifies DataFrame directly (False returns new Series)",
    "# - df_filled['Age'].mean(): Calculates mean (average) of age column",
    "#   - .mean(): Returns average of all non-missing values",
    "#   - Ignores NaN values automatically",
    "# Result: All missing ages replaced with average age",
    "df_filled['Age'].fillna(df_filled['Age'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and remove duplicate rows\n",
    "# .duplicated() finds rows that are exact duplicates\n",
    "# .drop_duplicates() removes them, keeping the first occurrence\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. Removing Duplicates\")\n",
    "print(\"\u0625\u0632\u0627\u0644\u0629 \u0627\u0644\u062a\u0643\u0631\u0627\u0631\u0627\u062a\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# df_filled.duplicated().sum()\n",
    "# - df_filled.duplicated(): Returns boolean Series (True = duplicate row, False = unique)\n",
    "#   - Checks if each row is identical to a previous row\n",
    "#   - First occurrence marked as False, duplicates as True\n",
    "#   - Parameters:\n",
    "#     - subset=None: Check all columns (can specify columns to check)\n",
    "#     - keep='first' (default): Mark first as False, rest as True\n",
    "# - .sum(): Counts True values (number of duplicate rows)\n",
    "num_duplicates = df_filled.duplicated().sum()\n",
    "print(f\"\\n\ud83d\udd0d Number of duplicates: {num_duplicates}\")\n",
    "print(f\"\u0639\u062f\u062f \u0627\u0644\u062a\u0643\u0631\u0627\u0631\u0627\u062a: {num_duplicates}\")\n",
    "\n",
    "# df_filled.drop_duplicates()\n",
    "# - Removes duplicate rows, keeps first occurrence\n",
    "# - Returns new DataFrame (doesn't modify original unless inplace=True)\n",
    "# - Parameters:\n",
    "#   - subset=None: Check all columns for duplicates\n",
    "#   - keep='first' (default): Keep first, remove rest ('last' keeps last, False removes all)\n",
    "#   - inplace=False: Return new DataFrame\n",
    "df_no_duplicates = df_filled.drop_duplicates()\n",
    "print(f\"\\n\u2705 Rows after removing duplicates: {df_no_duplicates.shape[0]}\")\n",
    "print(f\"\u0627\u0644\u0635\u0641\u0648\u0641 \u0628\u0639\u062f \u0625\u0632\u0627\u0644\u0629 \u0627\u0644\u062a\u0643\u0631\u0627\u0631\u0627\u062a: {df_no_duplicates.shape[0]}\")\n",
    "print(f\"   Removed {num_duplicates} duplicate row(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill categorical columns with mode",
    "# \u0645\u0644\u0621 \u0627\u0644\u0623\u0639\u0645\u062f\u0629 \u0627\u0644\u0641\u0626\u0648\u064a\u0629 \u0628\u0627\u0644\u0642\u064a\u0645\u0629 \u0627\u0644\u0623\u0643\u062b\u0631 \u062a\u0643\u0631\u0627\u0631\u0627\u064b",
    "",
    "# df_filled['Embarked'].fillna(df_filled['Embarked'].mode()[0], inplace=True)",
    "# - df_filled['Embarked']: Selects 'department' column",
    "# - .mode(): Returns Series with most frequent value(s)",
    "#   - Mode = most common value (for categorical data)",
    "#   - Returns Series (can have multiple modes if tie)",
    "# - [0]: Gets first mode value (if multiple modes, takes first)",
    "# - .fillna(): Fills missing values with mode",
    "# - inplace=True: Modifies DataFrame directly",
    "# Result: All missing departments replaced with most common department",
    "df_filled['Embarked'].fillna(df_filled['Embarked'].mode()[0], inplace=True)",
    "print(\"\\nMissing values after filling:\")",
    "print(\"\u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629 \u0628\u0639\u062f \u0627\u0644\u0645\u0644\u0621:\")",
    "print(df_filled.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR (Interquartile Range) Method for outlier detection\n",
    "# This is a statistical method that identifies values far from the median\n",
    "\n",
    "print(\"\\n--- IQR Method for Outlier Detection ---\")\n",
    "print(\"--- \u0637\u0631\u064a\u0642\u0629 \u0627\u0644\u0645\u062f\u0649 \u0627\u0644\u0631\u0628\u064a\u0639\u064a \u0644\u0644\u0643\u0634\u0641 \u0639\u0646 \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0634\u0627\u0630\u0629 ---\")\n",
    "\n",
    "def detect_outliers_iqr(series):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method.\n",
    "    Returns True for outliers, False for normal values.\n",
    "    \"\"\"\n",
    "    # series.quantile(0.25)\n",
    "    # - Calculates 25th percentile (Q1) - value below which 25% of data falls\n",
    "    # - quantile(q): Returns value at quantile q (0.0 to 1.0)\n",
    "    # - 0.25 = 25th percentile, 0.5 = median, 0.75 = 75th percentile\n",
    "    Q1 = series.quantile(0.25)  # 25th percentile\n",
    "    \n",
    "    # series.quantile(0.75)\n",
    "    # - Calculates 75th percentile (Q3) - value below which 75% of data falls\n",
    "    Q3 = series.quantile(0.75)  # 75th percentile\n",
    "    \n",
    "    # IQR = Q3 - Q1\n",
    "    # - Interquartile Range: Spread of middle 50% of data\n",
    "    # - Measures variability, less sensitive to outliers than range\n",
    "    IQR = Q3 - Q1  # Interquartile Range\n",
    "    \n",
    "    # Q1 - 1.5 * IQR\n",
    "    # - Lower fence: Values below this are considered outliers\n",
    "    # - 1.5 is standard multiplier (can be adjusted)\n",
    "    lower_bound = Q1 - 1.5 * IQR  # Lower fence\n",
    "    \n",
    "    # Q3 + 1.5 * IQR\n",
    "    # - Upper fence: Values above this are considered outliers\n",
    "    upper_bound = Q3 + 1.5 * IQR  # Upper fence\n",
    "    \n",
    "    # (series < lower_bound) | (series > upper_bound)\n",
    "    # - Boolean indexing: Creates boolean Series (True = outlier, False = normal)\n",
    "    # - | : Logical OR operator (element-wise)\n",
    "    # - Returns True for values outside [lower_bound, upper_bound]\n",
    "    # Values outside the fences are outliers\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "print(\"   \u2705 IQR function defined\")\n",
    "print(\"   This will identify values that are too far from the median\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Handling Outliers\n",
    "# \u0645\u0639\u0627\u0644\u062c\u0629 \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0634\u0627\u0630\u0629\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Handling Outliers\")\n",
    "print(\"\u0645\u0639\u0627\u0644\u062c\u0629 \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0634\u0627\u0630\u0629\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in salary",
    "print(\"\\nOutliers in salary column:\")",
    "print(\"\u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0634\u0627\u0630\u0629 \u0641\u064a \u0639\u0645\u0648\u062f \u0627\u0644\u0631\u0627\u062a\u0628:\")",
    "",
    "# detect_outliers_iqr(df_no_duplicates['Fare'])",
    "# - Calls function defined earlier",
    "# - df_no_duplicates['Fare']: Passes salary column as Series",
    "# - Returns boolean Series (True = outlier, False = normal)",
    "fare_outliers = detect_outliers_iqr(df_no_duplicates['Fare'])",
    "",
    "# fare_outliers.sum()",
    "# - Counts True values (number of outliers)",
    "print(f\"Number of outliers: {fare_outliers.sum()}\")",
    "",
    "# df_no_duplicates[fare_outliers][['name', 'salary']]",
    "# - df_no_duplicates[fare_outliers]: Boolean indexing - selects rows where fare_outliers is True",
    "#   - Boolean Series used as filter: True rows kept, False rows removed",
    "# - [['name', 'salary']]: Selects only 'name' and 'salary' columns",
    "#   - Double brackets [[]] returns DataFrame (single [] returns Series)",
    "# Result: Shows only outlier rows with name and salary columns",
    "print(df_no_duplicates[fare_outliers][['name', 'salary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "\n",
    "# df_no_duplicates[~salary_outliers].copy()\n",
    "# - ~salary_outliers: NOT operator (~) inverts boolean Series\n",
    "#   - True becomes False, False becomes True\n",
    "#   - Selects rows that are NOT outliers (keeps normal values)\n",
    "# - [~salary_outliers]: Boolean indexing - filters DataFrame\n",
    "# - .copy(): Creates independent copy (good practice)\n",
    "# Result: DataFrame with outliers removed\n",
    "df_clean = df_no_duplicates[~salary_outliers].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also remove impossible age values\n",
    "\n",
    "# df_clean[df_clean['age'] <= 100].copy()\n",
    "# - df_clean['age'] <= 100: Creates boolean Series (True if age <= 100, False otherwise)\n",
    "#   - Comparison operator (<=) applied element-wise to all values\n",
    "# - df_clean[boolean_series]: Boolean indexing - keeps rows where condition is True\n",
    "# - .copy(): Creates independent copy\n",
    "# Result: Keeps only rows where age is 100 or less (removes impossible ages like 150)\n",
    "df_clean = df_clean[df_clean['age'] <= 100].copy()\n",
    "print(f\"\\nRows after removing outliers: {df_clean.shape[0]}\")\n",
    "print(f\"\u0627\u0644\u0635\u0641\u0648\u0641 \u0628\u0639\u062f \u0625\u0632\u0627\u0644\u0629 \u0627\u0644\u0642\u064a\u0645 \u0627\u0644\u0634\u0627\u0630\u0629: {df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Data Type Conversion\n",
    "# \u062a\u062d\u0648\u064a\u0644 \u0623\u0646\u0648\u0627\u0639 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. Data Type Conversion\")\n",
    "print(\"\u062a\u062d\u0648\u064a\u0644 \u0623\u0646\u0648\u0627\u0639 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nData types before conversion:\")\n",
    "print(\"\u0623\u0646\u0648\u0627\u0639 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0642\u0628\u0644 \u0627\u0644\u062a\u062d\u0648\u064a\u0644:\")\n",
    "\n",
    "# df_clean.dtypes\n",
    "# - Returns Series showing data type of each column\n",
    "# - Common types: int64, float64, object (string), bool, datetime64\n",
    "# - Useful for checking if types are correct (e.g., numbers stored as strings)\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert experience_years to int (rounding)\n",
    "\n",
    "# df_clean['experience_years'].round().astype(int)\n",
    "# - df_clean['experience_years']: Selects 'experience_years' column\n",
    "# - .round(): Rounds decimal values to nearest integer\n",
    "#   - No parameter = rounds to 0 decimal places\n",
    "#   - .round(2) would round to 2 decimal places\n",
    "# - .astype(int): Converts data type to integer\n",
    "#   - astype(): Changes data type of Series\n",
    "#   - int: Integer type (int64 in pandas)\n",
    "#   - Alternative: .astype('int64') or .astype(float) for float\n",
    "# Result: Converts float values like 2.5 to integer 3 (after rounding)\n",
    "df_clean['experience_years'] = df_clean['experience_years'].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary of cleaning process\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. Cleaning Summary\")\n",
    "print(\"\u0645\u0644\u062e\u0635 \u0627\u0644\u062a\u0646\u0638\u064a\u0641\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "original_rows = df.shape[0]\n",
    "final_rows = df_clean.shape[0]\n",
    "rows_removed = original_rows - final_rows\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Original rows: {original_rows}\")\n",
    "print(f\"\u0627\u0644\u0635\u0641\u0648\u0641 \u0627\u0644\u0623\u0635\u0644\u064a\u0629: {original_rows}\")\n",
    "print(f\"\u2705 Final cleaned rows: {final_rows}\")\n",
    "print(f\"\u0627\u0644\u0635\u0641\u0648\u0641 \u0627\u0644\u0646\u0647\u0627\u0626\u064a\u0629 \u0628\u0639\u062f \u0627\u0644\u062a\u0646\u0638\u064a\u0641: {final_rows}\")\n",
    "print(f\"\\n\ud83d\uddd1\ufe0f  Rows removed: {rows_removed} ({(rows_removed/original_rows*100):.1f}%)\")\n",
    "print(f\"\u0627\u0644\u0635\u0641\u0648\u0641 \u0627\u0644\u0645\u062d\u0630\u0648\u0641\u0629: {rows_removed}\")\n",
    "\n",
    "print(\"\\n\ud83d\udcc4 Cleaned Data (first 10 rows):\")\n",
    "print(\"\u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0627\u0644\u0646\u0638\u064a\u0641\u0629:\")\n",
    "print(df_clean.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2705 Example 2 Complete! \u2713\")\n",
    "print(\"\u0627\u0643\u062a\u0645\u0644 \u0627\u0644\u0645\u062b\u0627\u0644 2! \u2713\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\ud83c\udf93 What you accomplished:\")\n",
    "print(\"   \u2705 Handled missing values (imputed with mean/mode)\")\n",
    "print(\"   \u2705 Removed duplicate rows\")\n",
    "print(\"   \u2705 Detected and removed outliers\")\n",
    "print(\"   \u2705 Converted data types correctly\")\n",
    "print(\"   \u2705 Created clean, model-ready data!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Decision Framework - When to Remove vs. When to Fix | \u0627\u0644\u062e\u0637\u0648\u0629 6: \u0625\u0637\u0627\u0631 \u0627\u0644\u0642\u0631\u0627\u0631 - \u0645\u062a\u0649 \u0646\u062d\u0630\u0641 \u0648\u0645\u062a\u0649 \u0646\u0635\u0644\u062d\n",
    "\n",
    "**BEFORE**: You've learned different cleaning methods, but when should you use each one?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose the right cleaning method for any situation!\n",
    "\n",
    "**Why this matters**: Making the wrong cleaning decision can:\n",
    "- **Remove too much data** \u2192 Lose valuable information\n",
    "- **Keep bad data** \u2192 Break your models\n",
    "- **Use wrong method** \u2192 Introduce bias or errors\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfaf Decision Framework for Missing Values | \u0625\u0637\u0627\u0631 \u0627\u0644\u0642\u0631\u0627\u0631 \u0644\u0644\u0642\u064a\u0645 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629\n",
    "\n",
    "**Key Question**: Should I **REMOVE** or **IMPUTE** missing values?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Is missing data < 5% of total?\n",
    "\u251c\u2500 YES \u2192 REMOVE (dropna)\n",
    "\u2502   \u2514\u2500 Why? Small loss, keeps data \"pure\"\n",
    "\u2502\n",
    "\u2514\u2500 NO \u2192 Is missing data random or systematic?\n",
    "    \u251c\u2500 RANDOM \u2192 IMPUTE (fillna with mean/median/mode)\n",
    "    \u2502   \u2514\u2500 Why? Random missing = no bias, safe to estimate\n",
    "    \u2502\n",
    "    \u2514\u2500 SYSTEMATIC \u2192 INVESTIGATE FIRST\n",
    "        \u2514\u2500 Why? Systematic missing might indicate important pattern\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Remove** | < 5% missing, random | \u2022 No bias introduced<br>\u2022 Keeps data \"pure\"<br>\u2022 Simple | \u2022 Loses data<br>\u2022 Can't use if many missing | Age missing in 2% of records |\n",
    "| **Impute (Mean/Median)** | > 5% missing, numeric, random | \u2022 Keeps all rows<br>\u2022 Preserves sample size<br>\u2022 Works for numeric | \u2022 Can introduce bias<br>\u2022 Assumes normal distribution | Salary missing in 15% of records |\n",
    "| **Impute (Mode)** | > 5% missing, categorical, random | \u2022 Keeps all rows<br>\u2022 Preserves sample size<br>\u2022 Works for categories | \u2022 Can create artificial patterns<br>\u2022 May over-represent common values | Department missing in 10% of records |\n",
    "| **Investigate** | Systematic missing | \u2022 Finds root cause<br>\u2022 Prevents bias<br>\u2022 Better decisions | \u2022 Takes time<br>\u2022 Requires domain knowledge | All salaries missing for one department |\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfaf Decision Framework for Outliers | \u0625\u0637\u0627\u0631 \u0627\u0644\u0642\u0631\u0627\u0631 \u0644\u0644\u0642\u064a\u0645 \u0627\u0644\u0634\u0627\u0630\u0629\n",
    "\n",
    "**Key Question**: Should I **REMOVE** or **KEEP** outliers?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Is the outlier a data entry error?\n",
    "\u251c\u2500 YES \u2192 REMOVE\n",
    "\u2502   \u2514\u2500 Example: Age = 150, Salary = 500000 (typo)\n",
    "\u2502\n",
    "\u2514\u2500 NO \u2192 Is the outlier a rare but valid event?\n",
    "    \u251c\u2500 YES \u2192 KEEP (but handle separately)\n",
    "    \u2502   \u2514\u2500 Example: CEO salary in employee dataset\n",
    "    \u2502\n",
    "    \u2514\u2500 NO \u2192 Does it affect model performance?\n",
    "        \u251c\u2500 YES \u2192 REMOVE or TRANSFORM\n",
    "        \u2502   \u2514\u2500 Example: Extreme values breaking linear regression\n",
    "        \u2502\n",
    "        \u2514\u2500 NO \u2192 KEEP\n",
    "            \u2514\u2500 Example: Outlier in non-critical feature\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Remove** | Data entry errors, impossible values | \u2022 Removes noise<br>\u2022 Improves model accuracy<br>\u2022 Simple | \u2022 Loses information<br>\u2022 May remove valid rare events | Age = 150, Salary = 500000 |\n",
    "| **Cap/Clip** | Valid but extreme values | \u2022 Keeps data<br>\u2022 Reduces impact<br>\u2022 Preserves distribution | \u2022 Arbitrary threshold<br>\u2022 May hide important patterns | Cap salary at 99th percentile |\n",
    "| **Transform** | Skewed distributions | \u2022 Normalizes data<br>\u2022 Keeps all values<br>\u2022 Better for ML | \u2022 Changes interpretation<br>\u2022 More complex | Log transform for income |\n",
    "| **Keep** | Rare but valid events | \u2022 Preserves reality<br>\u2022 No information loss<br>\u2022 Important for analysis | \u2022 Can skew models<br>\u2022 May need special handling | CEO in employee dataset |\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfaf Decision Framework for Duplicates | \u0625\u0637\u0627\u0631 \u0627\u0644\u0642\u0631\u0627\u0631 \u0644\u0644\u062a\u0643\u0631\u0627\u0631\u0627\u062a\n",
    "\n",
    "**Key Question**: Should I **REMOVE** all duplicates or **INVESTIGATE** first?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Are duplicates exact copies?\n",
    "\u251c\u2500 YES \u2192 REMOVE (keep first)\n",
    "\u2502   \u2514\u2500 Why? No new information, wastes space\n",
    "\u2502\n",
    "\u2514\u2500 NO \u2192 Are duplicates near-duplicates (typos)?\n",
    "    \u251c\u2500 YES \u2192 FIX (merge or correct)\n",
    "    \u2502   \u2514\u2500 Why? Same entity, different spelling\n",
    "    \u2502\n",
    "    \u2514\u2500 NO \u2192 Are duplicates valid (same person, different records)?\n",
    "        \u2514\u2500 YES \u2192 KEEP (but flag for analysis)\n",
    "            \u2514\u2500 Why? Important information (e.g., repeat customers)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udcca Real-World Examples | \u0623\u0645\u062b\u0644\u0629 \u0645\u0646 \u0627\u0644\u0639\u0627\u0644\u0645 \u0627\u0644\u062d\u0642\u064a\u0642\u064a\n",
    "\n",
    "#### Example 1: E-commerce Dataset\n",
    "- **Missing values in \"price\"**: 20% missing\n",
    "  - **Decision**: IMPUTE with median (too much to remove, random missing)\n",
    "  - **Reason**: Random missing prices, median preserves distribution\n",
    "\n",
    "#### Example 2: Medical Dataset\n",
    "- **Outlier in \"age\"**: One patient age = 200\n",
    "  - **Decision**: REMOVE (impossible value)\n",
    "  - **Reason**: Data entry error, no one lives to 200\n",
    "\n",
    "#### Example 3: Customer Dataset\n",
    "- **Missing values in \"email\"**: 3% missing\n",
    "  - **Decision**: REMOVE (small percentage)\n",
    "  - **Reason**: Email is critical, can't impute, small loss acceptable\n",
    "\n",
    "#### Example 4: Sales Dataset\n",
    "- **Outlier in \"revenue\"**: One sale = $1,000,000 (normal range: $10-$1000)\n",
    "  - **Decision**: INVESTIGATE FIRST\n",
    "  - **Reason**: Could be valid (enterprise sale) or error (extra zero)\n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Key Takeaways | \u0627\u0644\u0646\u0642\u0627\u0637 \u0627\u0644\u0631\u0626\u064a\u0633\u064a\u0629\n",
    "\n",
    "1. **Always investigate first** - Understand WHY data is missing/outlier/duplicate\n",
    "2. **Consider data loss** - Removing >10% of data is usually too much\n",
    "3. **Consider bias** - Systematic missing/outliers may indicate important patterns\n",
    "4. **Test both approaches** - Sometimes try removing AND imputing, compare results\n",
    "5. **Document decisions** - Write down WHY you chose each method (for reproducibility)\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udf93 Practice Decision-Making | \u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u062a\u062e\u0627\u0630 \u0627\u0644\u0642\u0631\u0627\u0631\n",
    "\n",
    "**Scenario**: You have a dataset with:\n",
    "- 8% missing values in \"income\" column\n",
    "- 2 outliers in \"age\" (ages 0 and 200)\n",
    "- 5 duplicate rows\n",
    "\n",
    "**Your task**: Decide what to do for each issue and explain why!\n",
    "\n",
    "**Answers**:\n",
    "1. **Income (8% missing)**: IMPUTE with median (too much to remove, likely random)\n",
    "2. **Age outliers**: REMOVE (impossible values - data entry errors)\n",
    "3. **Duplicates**: REMOVE (exact copies, no new information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Example: Comparing Different Cleaning Approaches\n",
    "# \u0645\u062b\u0627\u0644 \u0639\u0645\u0644\u064a: \u0645\u0642\u0627\u0631\u0646\u0629 \u0637\u0631\u0642 \u0627\u0644\u062a\u0646\u0638\u064a\u0641 \u0627\u0644\u0645\u062e\u062a\u0644\u0641\u0629\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Practical Example: Decision-Making in Action\")\n",
    "print(\"\u0645\u062b\u0627\u0644 \u0639\u0645\u0644\u064a: \u0627\u062a\u062e\u0627\u0630 \u0627\u0644\u0642\u0631\u0627\u0631 \u0641\u064a \u0627\u0644\u0645\u0645\u0627\u0631\u0633\u0629\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a scenario with different data quality issues\n",
    "print(\"\\n\ud83d\udcca Scenario: Dataset with multiple issues\")\n",
    "print(\"   - 15% missing values in 'income' (too much to remove)\")\n",
    "print(\"   - 2% missing values in 'email' (can remove)\")\n",
    "print(\"   - 1 outlier: age = 200 (impossible, should remove)\")\n",
    "print(\"   - 3 duplicates (should remove)\")\n",
    "\n",
    "# Simulate the decision-making process\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 1: Income missing (15%)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   \u274c Can't remove: Would lose 15% of data (too much!)\")\n",
    "print(\"   \u2705 Should impute: Use median (preserves distribution)\")\n",
    "print(\"   \ud83d\udcdd Reason: Random missing, large percentage, numeric data\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 2: Email missing (2%)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   \u2705 Can remove: Only 2% loss (acceptable)\")\n",
    "print(\"   \u274c Can't impute: Email is unique, can't estimate\")\n",
    "print(\"   \ud83d\udcdd Reason: Small percentage, critical field, can't fill\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 3: Age outlier (age = 200)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   \u2705 Should remove: Impossible value (data entry error)\")\n",
    "print(\"   \u274c Can't keep: Would break age-based models\")\n",
    "print(\"   \ud83d\udcdd Reason: No human lives to 200, clearly an error\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 4: Duplicates (3 rows)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   \u2705 Should remove: Exact duplicates, no new information\")\n",
    "print(\"   \ud83d\udcdd Reason: Wastes space, can bias models (same data counted twice)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2705 Decision Framework Applied Successfully!\")\n",
    "print(\"\u062a\u0645 \u062a\u0637\u0628\u064a\u0642 \u0625\u0637\u0627\u0631 \u0627\u0644\u0642\u0631\u0627\u0631 \u0628\u0646\u062c\u0627\u062d!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert age to int\n",
    "\n",
    "# df_clean['age'].round().astype(int)\n",
    "# - df_clean['age']: Selects 'age' column\n",
    "# - .round(): Rounds decimal values (e.g., 40.5 \u2192 41)\n",
    "# - .astype(int): Converts to integer type\n",
    "# Result: Converts float ages to integer ages\n",
    "df_clean['age'] = df_clean['age'].round().astype(int)\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(\"\u0623\u0646\u0648\u0627\u0639 \u0627\u0644\u0628\u064a\u0627\u0646\u0627\u062a \u0628\u0639\u062f \u0627\u0644\u062a\u062d\u0648\u064a\u0644:\")\n",
    "\n",
    "# df_clean.dtypes\n",
    "# - Shows data types after conversion\n",
    "# - Should show int64 for age and experience_years now\n",
    "print(df_clean.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}