{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Data Cleaning | ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Data Loading and Exploration** - You need to know how to identify data quality issues first!\n",
    "- âœ… **Basic pandas knowledge**: DataFrames, indexing, filtering\n",
    "- âœ… **Understanding of data quality**: What are missing values, duplicates, outliers?\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why data cleaning is necessary\n",
    "- Knowing which cleaning method to use\n",
    "- Understanding the impact of cleaning on your data\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the SECOND example** - it fixes the problems we found in Example 1!\n",
    "\n",
    "**Why this example SECOND?**\n",
    "- **Before** you can preprocess data, you need to clean it\n",
    "- **Before** you can build models, you need clean data\n",
    "- **Before** you can make predictions, you need to fix data quality issues\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: Data Loading and Exploration (we found the problems, now we fix them!)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 3: Data Preprocessing (needs clean data to work with)\n",
    "- ğŸ““ Example 4: Linear Regression (needs clean, preprocessed data)\n",
    "- ğŸ““ All ML models (all need clean data!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Data cleaning fixes **quality issues** (needed before preprocessing)\n",
    "2. Data cleaning teaches you **when to remove vs. impute** (critical decision-making)\n",
    "3. Data cleaning shows you **the impact of outliers** (affects model accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Cleaning Before Cooking | Ø§Ù„Ù‚ØµØ©: Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ù‚Ø¨Ù„ Ø§Ù„Ø·Ø¨Ø®\n",
    "\n",
    "Imagine you're cooking a meal. **Before** you can cook, you need to clean your ingredients - remove spoiled items, wash vegetables, check for foreign objects. **After** cleaning everything, you can prepare a safe, delicious meal!\n",
    "\n",
    "Same with machine learning: **Before** building models, we clean our data - remove duplicates, handle missing values, fix outliers. **After** cleaning, we can build accurate, reliable models!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Cleaning Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ\n",
    "\n",
    "Data cleaning is essential for accurate models:\n",
    "- **Missing Values**: Break ML algorithms - must be handled\n",
    "- **Duplicates**: Bias your models (same data counted twice)\n",
    "- **Outliers**: Skew predictions and statistics\n",
    "- **Wrong Data Types**: Cause errors in calculations\n",
    "- **Dirty Data = Bad Models**: No amount of ML can fix fundamentally bad data\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Handle missing values (remove or impute)\n",
    "2. Remove duplicate rows\n",
    "3. Detect and handle outliers\n",
    "4. Convert data types correctly\n",
    "5. Understand trade-offs between different cleaning methods\n",
    "6. Know when to remove vs. when to fix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "\n",
      "ğŸ“š What each library does:\n",
      "   - pandas: Clean data (remove, fill, filter)\n",
      "   - numpy: Handle missing values (NaN operations)\n",
      "   - matplotlib: Visualize data quality issues\n",
      "   - seaborn: Create beautiful quality check plots\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us clean and analyze data\n",
    "\n",
    "import pandas as pd  # For data manipulation (cleaning operations)\n",
    "import numpy as np   # For numerical operations (handling NaN, calculations)\n",
    "import matplotlib.pyplot as plt  # For visualizations (seeing outliers)\n",
    "import seaborn as sns  # For statistical plots (data quality visualization)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each library does:\")\n",
    "print(\"   - pandas: Clean data (remove, fill, filter)\")\n",
    "print(\"   - numpy: Handle missing values (NaN operations)\")\n",
    "print(\"   - matplotlib: Visualize data quality issues\")\n",
    "print(\"   - seaborn: Create beautiful quality check plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We explored our data in Example 1 and found problems - missing values, duplicates, outliers.\n",
    "\n",
    "**AFTER**: We'll clean the data by fixing all these issues, making it ready for preprocessing and modeling!\n",
    "\n",
    "**Why this matters**: Dirty data produces unreliable models. Cleaning is non-negotiable for good ML results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Creating sample data with common issues...\n",
      "Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ© Ø¨Ù…Ø´Ø§ÙƒÙ„ Ø´Ø§Ø¦Ø¹Ø©...\n",
      "âœ… Sample data created with intentional issues:\n",
      "   - Missing values in 'age' and 'department'\n",
      "   - We'll add duplicates and outliers next\n"
     ]
    }
   ],
   "source": [
    "# Create sample data with common data quality issues\n",
    "# In real projects, you'd load this from a file and discover these issues during exploration\n",
    "\n",
    "print(\"\\n1. Creating sample data with common issues...\")\n",
    "print(\"Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ© Ø¨Ù…Ø´Ø§ÙƒÙ„ Ø´Ø§Ø¦Ø¹Ø©...\")\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data = {\n",
    "    'id': range(1, 21),\n",
    "    'name': [f'Person_{i}' for i in range(1, 21)],\n",
    "    'age': [25, 30, None, 35, 40, None, 28, 32, 45, 50,\n",
    "            22, None, 38, 42, 29, 33, 48, None, 27, 31],  # Missing values (None = NaN)\n",
    "    'salary': [50000, 60000, 55000, 70000, 80000, 65000, 58000,\n",
    "               72000, 90000, 95000, 52000, 68000, 75000, 85000,\n",
    "               57000, 73000, 92000, 62000, 54000, 71000],\n",
    "    'department': ['IT', 'HR', 'IT', 'Finance', 'IT', 'HR', 'IT',\n",
    "                   'Finance', 'IT', 'HR', 'IT', None, 'Finance', 'IT',\n",
    "                   'HR', 'IT', 'Finance', 'IT', None, 'HR'],  # Missing values\n",
    "    'experience_years': [2, 5, 3, 8, 10, 4, 2.5, 6, 12, 15,\n",
    "                         1, 5.5, 9, 11, 3, 7, 13, 4.5, 2, 6]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"âœ… Sample data created with intentional issues:\")\n",
    "print(\"   - Missing values in 'age' and 'department'\")\n",
    "print(\"   - We'll add duplicates and outliers next\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample Data with Issues | Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ© Ø¨Ù…Ø´Ø§ÙƒÙ„\n",
    "\n",
    "**BEFORE**: We need to learn cleaning techniques, but we need \"dirty\" data to practice on.\n",
    "\n",
    "**AFTER**: We'll create sample data with common real-world problems (missing values, duplicates, outliers) so we can practice cleaning!\n",
    "\n",
    "**Why create dirty data?** Real datasets have these problems! We need to learn how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Added 2 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# Add some duplicate rows to demonstrate duplicate removal\n",
    "# Why add duplicates? Real data often has duplicates from data entry errors or merging issues\n",
    "df = pd.concat([df, df.iloc[[0, 1]]], ignore_index=True)\n",
    "print(\"   - Added 2 duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Original Data Shape: (22, 6)\n",
      "Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: (22, 6)\n",
      "   - Added 1 extreme salary outlier\n",
      "   - Added 1 impossible age value\n",
      "\n",
      "ğŸ“„ Original Data (first 10 rows):\n",
      "   id       name   age  salary department  experience_years\n",
      "0   1   Person_1  25.0   50000         IT               2.0\n",
      "1   2   Person_2  30.0   60000         HR               5.0\n",
      "2   3   Person_3   NaN   55000         IT               3.0\n",
      "3   4   Person_4  35.0   70000    Finance               8.0\n",
      "4   5   Person_5  40.0   80000         IT              10.0\n",
      "5   6   Person_6   NaN   65000         HR               4.0\n",
      "6   7   Person_7  28.0   58000         IT               2.5\n",
      "7   8   Person_8  32.0   72000    Finance               6.0\n",
      "8   9   Person_9  45.0   90000         IT              12.0\n",
      "9  10  Person_10  50.0   95000         HR              15.0\n"
     ]
    }
   ],
   "source": [
    "# Add some outliers to demonstrate outlier handling\n",
    "# Why add outliers? Real data has outliers from errors (typos, measurement mistakes) or rare events\n",
    "df.loc[18, 'salary'] = 500000  # Extreme outlier (10x normal salary - likely a typo)\n",
    "df.loc[19, 'age'] = 150  # Impossible value (no one lives to 150 - data entry error)\n",
    "\n",
    "print(\"\\nğŸ“Š Original Data Shape:\", df.shape)\n",
    "print(\"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\", df.shape)\n",
    "print(\"   - Added 1 extreme salary outlier\")\n",
    "print(\"   - Added 1 impossible age value\")\n",
    "print(\"\\nğŸ“„ Original Data (first 10 rows):\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. Handling Missing Values\n",
      "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
      "============================================================\n",
      "\n",
      "ğŸ” Missing values before cleaning:\n",
      "Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:\n",
      "id                  0\n",
      "name                0\n",
      "age                 4\n",
      "salary              0\n",
      "department          2\n",
      "experience_years    0\n",
      "dtype: int64\n",
      "\n",
      "   Total missing values: 6\n",
      "   Percentage missing: 4.5%\n"
     ]
    }
   ],
   "source": [
    "# First, let's see how many missing values we have\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Handling Missing Values\")\n",
    "print(\"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ” Missing values before cleaning:\")\n",
    "print(\"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:\")\n",
    "missing_before = df.isnull().sum()\n",
    "print(missing_before)\n",
    "print(f\"\\n   Total missing values: {missing_before.sum()}\")\n",
    "print(f\"   Percentage missing: {(missing_before.sum() / (df.shape[0] * df.shape[1]) * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Method 1: Remove rows with missing values ---\n",
      "--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\n",
      "âœ… Rows after removal: 17 (removed 5 rows)\n",
      "Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù: 17 (ØªÙ… Ø­Ø°Ù 5 ØµÙ)\n",
      "   Data loss: 22.7%\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Remove rows with missing values\n",
    "# .dropna() removes any row that has at least one missing value\n",
    "# Why use this? If missing values are rare, it's better to remove than guess\n",
    "\n",
    "print(\"\\n--- Method 1: Remove rows with missing values ---\")\n",
    "print(\"--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\")\n",
    "df_removed = df.dropna()\n",
    "rows_removed = df.shape[0] - df_removed.shape[0]\n",
    "print(f\"âœ… Rows after removal: {df_removed.shape[0]} (removed {rows_removed} rows)\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù: {df_removed.shape[0]} (ØªÙ… Ø­Ø°Ù {rows_removed} ØµÙ)\")\n",
    "print(f\"   Data loss: {(rows_removed / df.shape[0] * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Handling Missing Values | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
    "\n",
    "**BEFORE**: We have missing values (NaN/None) that will break our ML models.\n",
    "\n",
    "**AFTER**: We'll either remove rows with missing values OR fill them with reasonable estimates!\n",
    "\n",
    "**Why handle missing values?** \n",
    "- ML algorithms cannot work with missing data\n",
    "- Missing values indicate incomplete information\n",
    "- We must decide: **Remove** (if few missing) or **Impute** (if many missing)\n",
    "\n",
    "**Two main strategies:**\n",
    "1. **Remove**: Drop rows/columns with missing values (good if <5% missing)\n",
    "2. **Impute**: Fill missing values with mean/median/mode (good if >5% missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Method 2: Fill missing values ---\n",
      "--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Fill missing values (imputation)\n",
    "# We'll use a copy so we don't modify the original\n",
    "print(\"\\n--- Method 2: Fill missing values ---\")\n",
    "print(\"--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\")\n",
    "df_filled = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Method 1: Remove rows with missing values ---\n",
      "--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\n",
      "Rows after removal: 17 (removed 5 rows)\n",
      "Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù: 17 (ØªÙ… Ø­Ø°Ù 5 ØµÙ)\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Remove rows with missing values (if few missing values)\n",
    "# Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‚Ù„ÙŠÙ„Ø©)\n",
    "print(\"\\n--- Method 1: Remove rows with missing values ---\")\n",
    "print(\"--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\")\n",
    "df_removed = df.dropna()\n",
    "print(f\"Rows after removal: {df_removed.shape[0]} (removed {df.shape[0] - df_removed.shape[0]} rows)\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù: {df_removed.shape[0]} (ØªÙ… Ø­Ø°Ù {df.shape[0] - df_removed.shape[0]} ØµÙ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Method 2: Fill missing values ---\n",
      "--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Fill missing values (imputation)\n",
    "# Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© (Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„)\n",
    "print(\"\\n--- Method 2: Fill missing values ---\")\n",
    "print(\"--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\")\n",
    "df_filled = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numeric columns with mean\n",
    "# Ù…Ù„Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ù„Ù…ØªÙˆØ³Ø·\n",
    "df_filled['age'].fillna(df_filled['age'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. Removing Duplicates\n",
      "Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
      "============================================================\n",
      "\n",
      "ğŸ” Number of duplicates: 2\n",
      "Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: 2\n",
      "\n",
      "âœ… Rows after removing duplicates: 20\n",
      "Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: 20\n",
      "   Removed 2 duplicate row(s)\n"
     ]
    }
   ],
   "source": [
    "# Check for and remove duplicate rows\n",
    "# .duplicated() finds rows that are exact duplicates\n",
    "# .drop_duplicates() removes them, keeping the first occurrence\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. Removing Duplicates\")\n",
    "print(\"Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "num_duplicates = df_filled.duplicated().sum()\n",
    "print(f\"\\nğŸ” Number of duplicates: {num_duplicates}\")\n",
    "print(f\"Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {num_duplicates}\")\n",
    "\n",
    "df_no_duplicates = df_filled.drop_duplicates()\n",
    "print(f\"\\nâœ… Rows after removing duplicates: {df_no_duplicates.shape[0]}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {df_no_duplicates.shape[0]}\")\n",
    "print(f\"   Removed {num_duplicates} duplicate row(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after filling:\n",
      "Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ù„Ø¡:\n",
      "id                  0\n",
      "name                0\n",
      "age                 0\n",
      "salary              0\n",
      "department          0\n",
      "experience_years    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill categorical columns with mode\n",
    "# Ù…Ù„Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ÙØ¦ÙˆÙŠØ© Ø¨Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹\n",
    "df_filled['department'].fillna(df_filled['department'].mode()[0], inplace=True)\n",
    "print(\"\\nMissing values after filling:\")\n",
    "print(\"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ù„Ø¡:\")\n",
    "print(df_filled.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. Handling Outliers\n",
      "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Handling Outliers\")\n",
    "print(\"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. Removing Duplicates\n",
      "Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
      "============================================================\n",
      "\n",
      "Number of duplicates: 2\n",
      "Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: 2\n",
      "Rows after removing duplicates: 20\n",
      "Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: 20\n"
     ]
    }
   ],
   "source": [
    "# 3. Removing Duplicates\n",
    "# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. Removing Duplicates\")\n",
    "print(\"Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNumber of duplicates: {df_filled.duplicated().sum()}\")\n",
    "print(f\"Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {df_filled.duplicated().sum()}\")\n",
    "df_no_duplicates = df_filled.drop_duplicates()\n",
    "print(f\"Rows after removing duplicates: {df_no_duplicates.shape[0]}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {df_no_duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- IQR Method for Outlier Detection ---\n",
      "--- Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø±Ø¨ÙŠØ¹ÙŠ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ---\n",
      "   âœ… IQR function defined\n",
      "   This will identify values that are too far from the median\n"
     ]
    }
   ],
   "source": [
    "# IQR (Interquartile Range) Method for outlier detection\n",
    "# This is a statistical method that identifies values far from the median\n",
    "\n",
    "print(\"\\n--- IQR Method for Outlier Detection ---\")\n",
    "print(\"--- Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø±Ø¨ÙŠØ¹ÙŠ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ---\")\n",
    "\n",
    "def detect_outliers_iqr(series):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method.\n",
    "    Returns True for outliers, False for normal values.\n",
    "    \"\"\"\n",
    "    Q1 = series.quantile(0.25)  # 25th percentile\n",
    "    Q3 = series.quantile(0.75)  # 75th percentile\n",
    "    IQR = Q3 - Q1  # Interquartile Range\n",
    "    lower_bound = Q1 - 1.5 * IQR  # Lower fence\n",
    "    upper_bound = Q3 + 1.5 * IQR  # Upper fence\n",
    "    \n",
    "    # Values outside the fences are outliers\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "print(\"   âœ… IQR function defined\")\n",
    "print(\"   This will identify values that are too far from the median\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. Handling Outliers\n",
      "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 4. Handling Outliers\n",
    "# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Handling Outliers\")\n",
    "print(\"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- IQR Method for Outlier Detection ---\n",
      "--- Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø±Ø¨ÙŠØ¹ÙŠ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ---\n"
     ]
    }
   ],
   "source": [
    "# Method 1: IQR (Interquartile Range) Method\n",
    "# Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø±Ø¨ÙŠØ¹ÙŠ\n",
    "print(\"\\n--- IQR Method for Outlier Detection ---\")\n",
    "print(\"--- Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø±Ø¨ÙŠØ¹ÙŠ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ---\")\n",
    "def detect_outliers_iqr(series):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (series < lower_bound) | (series > upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outliers in salary column:\n",
      "Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ÙÙŠ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø±Ø§ØªØ¨:\n",
      "Number of outliers: 1\n",
      "         name  salary\n",
      "18  Person_19  500000\n"
     ]
    }
   ],
   "source": [
    "# Check for outliers in salary\n",
    "print(\"\\nOutliers in salary column:\")\n",
    "print(\"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ÙÙŠ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø±Ø§ØªØ¨:\")\n",
    "salary_outliers = detect_outliers_iqr(df_no_duplicates['salary'])\n",
    "print(f\"Number of outliers: {salary_outliers.sum()}\")\n",
    "print(df_no_duplicates[salary_outliers][['name', 'salary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "df_clean = df_no_duplicates[~salary_outliers].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. Data Type Conversion\n",
      "ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Data types before conversion:\n",
      "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\n",
      "id                    int64\n",
      "name                 object\n",
      "age                 float64\n",
      "salary                int64\n",
      "department           object\n",
      "experience_years    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. Data Type Conversion\")\n",
    "print(\"ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ“Š Data types before conversion:\")\n",
    "print(\"Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\")\n",
    "print(df_clean.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows after removing outliers: 18\n",
      "Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©: 18\n"
     ]
    }
   ],
   "source": [
    "# Also remove impossible age values\n",
    "df_clean = df_clean[df_clean['age'] <= 100].copy()\n",
    "print(f\"\\nRows after removing outliers: {df_clean.shape[0]}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©: {df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. Data Type Conversion\n",
      "ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
      "============================================================\n",
      "\n",
      "Data types before conversion:\n",
      "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\n",
      "id                    int64\n",
      "name                 object\n",
      "age                 float64\n",
      "salary                int64\n",
      "department           object\n",
      "experience_years    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 5. Data Type Conversion\n",
    "# ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. Data Type Conversion\")\n",
    "print(\"ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nData types before conversion:\")\n",
    "print(\"Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\")\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert experience_years to int (rounding)\n",
    "df_clean['experience_years'] = df_clean['experience_years'].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. Cleaning Summary\n",
      "Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Original rows: 22\n",
      "Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ØµÙ„ÙŠØ©: 22\n",
      "âœ… Final cleaned rows: 18\n",
      "Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: 18\n",
      "\n",
      "ğŸ—‘ï¸  Rows removed: 4 (18.2%)\n",
      "Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: 4\n",
      "\n",
      "ğŸ“„ Cleaned Data (first 10 rows):\n",
      "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©:\n",
      "   id       name   age  salary department  experience_years\n",
      "0   1   Person_1  25.0   50000         IT                 2\n",
      "1   2   Person_2  30.0   60000         HR                 5\n",
      "2   3   Person_3  40.5   55000         IT                 3\n",
      "3   4   Person_4  35.0   70000    Finance                 8\n",
      "4   5   Person_5  40.0   80000         IT                10\n",
      "5   6   Person_6  40.5   65000         HR                 4\n",
      "6   7   Person_7  28.0   58000         IT                 2\n",
      "7   8   Person_8  32.0   72000    Finance                 6\n",
      "8   9   Person_9  45.0   90000         IT                12\n",
      "9  10  Person_10  50.0   95000         HR                15\n",
      "\n",
      "============================================================\n",
      "âœ… Example 2 Complete! âœ“\n",
      "Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 2! âœ“\n",
      "============================================================\n",
      "\n",
      "ğŸ“ What you accomplished:\n",
      "   âœ… Handled missing values (imputed with mean/mode)\n",
      "   âœ… Removed duplicate rows\n",
      "   âœ… Detected and removed outliers\n",
      "   âœ… Converted data types correctly\n",
      "   âœ… Created clean, model-ready data!\n"
     ]
    }
   ],
   "source": [
    "# Final summary of cleaning process\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. Cleaning Summary\")\n",
    "print(\"Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "original_rows = df.shape[0]\n",
    "final_rows = df_clean.shape[0]\n",
    "rows_removed = original_rows - final_rows\n",
    "\n",
    "print(f\"\\nğŸ“Š Original rows: {original_rows}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ØµÙ„ÙŠØ©: {original_rows}\")\n",
    "print(f\"âœ… Final cleaned rows: {final_rows}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {final_rows}\")\n",
    "print(f\"\\nğŸ—‘ï¸  Rows removed: {rows_removed} ({(rows_removed/original_rows*100):.1f}%)\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {rows_removed}\")\n",
    "\n",
    "print(\"\\nğŸ“„ Cleaned Data (first 10 rows):\")\n",
    "print(\"Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©:\")\n",
    "print(df_clean.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Example 2 Complete! âœ“\")\n",
    "print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 2! âœ“\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ“ What you accomplished:\")\n",
    "print(\"   âœ… Handled missing values (imputed with mean/mode)\")\n",
    "print(\"   âœ… Removed duplicate rows\")\n",
    "print(\"   âœ… Detected and removed outliers\")\n",
    "print(\"   âœ… Converted data types correctly\")\n",
    "print(\"   âœ… Created clean, model-ready data!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types after conversion:\n",
      "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\n",
      "id                   int64\n",
      "name                object\n",
      "age                  int64\n",
      "salary               int64\n",
      "department          object\n",
      "experience_years     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert age to int\n",
    "df_clean['age'] = df_clean['age'].round().astype(int)\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(\"Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\")\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. Cleaning Summary\n",
      "Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n",
      "============================================================\n",
      "\n",
      "Original rows: 22\n",
      "Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ØµÙ„ÙŠØ©: 22\n",
      "Final cleaned rows: 18\n",
      "Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: 18\n",
      "\n",
      "Rows removed: 4\n",
      "Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: 4\n",
      "\n",
      "Cleaned Data:\n",
      "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©:\n",
      "   id       name  age  salary department  experience_years\n",
      "0   1   Person_1   25   50000         IT                 2\n",
      "1   2   Person_2   30   60000         HR                 5\n",
      "2   3   Person_3   40   55000         IT                 3\n",
      "3   4   Person_4   35   70000    Finance                 8\n",
      "4   5   Person_5   40   80000         IT                10\n",
      "5   6   Person_6   40   65000         HR                 4\n",
      "6   7   Person_7   28   58000         IT                 2\n",
      "7   8   Person_8   32   72000    Finance                 6\n",
      "8   9   Person_9   45   90000         IT                12\n",
      "9  10  Person_10   50   95000         HR                15\n",
      "\n",
      "============================================================\n",
      "Example 2 Complete! âœ“\n",
      "Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 2! âœ“\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6. Summary\n",
    "# Ø§Ù„Ù…Ù„Ø®Øµ\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. Cleaning Summary\")\n",
    "print(\"Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOriginal rows: {df.shape[0]}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ØµÙ„ÙŠØ©: {df.shape[0]}\")\n",
    "print(f\"Final cleaned rows: {df_clean.shape[0]}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {df_clean.shape[0]}\")\n",
    "print(f\"\\nRows removed: {df.shape[0] - df_clean.shape[0]}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {df.shape[0] - df_clean.shape[0]}\")\n",
    "print(\"\\nCleaned Data:\")\n",
    "print(\"Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©:\")\n",
    "print(df_clean.head(10))\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 2 Complete! âœ“\")\n",
    "print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 2! âœ“\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
