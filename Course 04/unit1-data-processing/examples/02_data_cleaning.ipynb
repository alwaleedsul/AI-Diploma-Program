{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Data Cleaning | ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Data Loading and Exploration** - You need to know how to identify data quality issues first!\n",
    "- âœ… **Basic pandas knowledge**: DataFrames, indexing, filtering\n",
    "- âœ… **Understanding of data quality**: What are missing values, duplicates, outliers?\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why data cleaning is necessary\n",
    "- Knowing which cleaning method to use\n",
    "- Understanding the impact of cleaning on your data\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the SECOND example** - it fixes the problems we found in Example 1!\n",
    "\n",
    "**Why this example SECOND?**\n",
    "- **Before** you can preprocess data, you need to clean it\n",
    "- **Before** you can build models, you need clean data\n",
    "- **Before** you can make predictions, you need to fix data quality issues\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: Data Loading and Exploration (we found the problems, now we fix them!)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 3: Data Preprocessing (needs clean data to work with)\n",
    "- ğŸ““ Example 4: Linear Regression (needs clean, preprocessed data)\n",
    "- ğŸ““ All ML models (all need clean data!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Data cleaning fixes **quality issues** (needed before preprocessing)\n",
    "2. Data cleaning teaches you **when to remove vs. impute** (critical decision-making)\n",
    "3. Data cleaning shows you **the impact of outliers** (affects model accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Cleaning Before Cooking | Ø§Ù„Ù‚ØµØ©: Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ù‚Ø¨Ù„ Ø§Ù„Ø·Ø¨Ø®\n",
    "\n",
    "Imagine you're cooking a meal. **Before** you can cook, you need to clean your ingredients - remove spoiled items, wash vegetables, check for foreign objects. **After** cleaning everything, you can prepare a safe, delicious meal!\n",
    "\n",
    "Same with machine learning: **Before** building models, we clean our data - remove duplicates, handle missing values, fix outliers. **After** cleaning, we can build accurate, reliable models!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Cleaning Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ\n",
    "\n",
    "Data cleaning is essential for accurate models:\n",
    "- **Missing Values**: Break ML algorithms - must be handled\n",
    "- **Duplicates**: Bias your models (same data counted twice)\n",
    "- **Outliers**: Skew predictions and statistics\n",
    "- **Wrong Data Types**: Cause errors in calculations\n",
    "- **Dirty Data = Bad Models**: No amount of ML can fix fundamentally bad data\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Handle missing values (remove or impute)\n",
    "2. Remove duplicate rows\n",
    "3. Detect and handle outliers\n",
    "4. Convert data types correctly\n",
    "5. Understand trade-offs between different cleaning methods\n",
    "6. Know when to remove vs. when to fix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "\n",
      "ğŸ“š What each library does:\n",
      "   - pandas: Clean data (remove, fill, filter)\n",
      "   - numpy: Handle missing values (NaN operations)\n",
      "   - matplotlib: Visualize data quality issues\n",
      "   - seaborn: Create beautiful quality check plots\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us clean and analyze data\n",
    "\n",
    "import pandas as pd  # For data manipulation (cleaning operations)\n",
    "import numpy as np   # For numerical operations (handling NaN, calculations)\n",
    "import matplotlib.pyplot as plt  # For visualizations (seeing outliers)\n",
    "import seaborn as sns  # For statistical plots (data quality visualization)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each library does:\")\n",
    "print(\"   - pandas: Clean data (remove, fill, filter)\")\n",
    "print(\"   - numpy: Handle missing values (NaN operations)\")\n",
    "print(\"   - matplotlib: Visualize data quality issues\")\n",
    "print(\"   - seaborn: Create beautiful quality check plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We explored our data in Example 1 and found problems - missing values, duplicates, outliers.\n",
    "\n",
    "**AFTER**: We'll clean the data by fixing all these issues, making it ready for preprocessing and modeling!\n",
    "\n",
    "**Why this matters**: Dirty data produces unreliable models. Cleaning is non-negotiable for good ML results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Creating sample data with common issues...\n",
      "Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ© Ø¨Ù…Ø´Ø§ÙƒÙ„ Ø´Ø§Ø¦Ø¹Ø©...\n",
      "âœ… Sample data created with intentional issues:\n",
      "   - Missing values in 'age' and 'department'\n",
      "   - We'll add duplicates and outliers next\n"
     ]
    }
   ],
   "source": [
    "# Create sample data with common data quality issues\n",
    "# In real projects, you'd load this from a file and discover these issues during exploration\n",
    "\n",
    "print(\"\\n1. Creating sample data with common issues...\")\n",
    "print(\"Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ© Ø¨Ù…Ø´Ø§ÙƒÙ„ Ø´Ø§Ø¦Ø¹Ø©...\")\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data = {\n",
    "    'id': range(1, 21),\n",
    "    'name': [f'Person_{i}' for i in range(1, 21)],\n",
    "    'age': [25, 30, None, 35, 40, None, 28, 32, 45, 50,\n",
    "            22, None, 38, 42, 29, 33, 48, None, 27, 31],  # Missing values (None = NaN)\n",
    "    'salary': [50000, 60000, 55000, 70000, 80000, 65000, 58000,\n",
    "               72000, 90000, 95000, 52000, 68000, 75000, 85000,\n",
    "               57000, 73000, 92000, 62000, 54000, 71000],\n",
    "    'department': ['IT', 'HR', 'IT', 'Finance', 'IT', 'HR', 'IT',\n",
    "                   'Finance', 'IT', 'HR', 'IT', None, 'Finance', 'IT',\n",
    "                   'HR', 'IT', 'Finance', 'IT', None, 'HR'],  # Missing values\n",
    "    'experience_years': [2, 5, 3, 8, 10, 4, 2.5, 6, 12, 15,\n",
    "                         1, 5.5, 9, 11, 3, 7, 13, 4.5, 2, 6]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"âœ… Sample data created with intentional issues:\")\n",
    "print(\"   - Missing values in 'age' and 'department'\")\n",
    "print(\"   - We'll add duplicates and outliers next\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample Data with Issues | Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ© Ø¨Ù…Ø´Ø§ÙƒÙ„\n",
    "\n",
    "**BEFORE**: We need to learn cleaning techniques, but we need \"dirty\" data to practice on.\n",
    "\n",
    "**AFTER**: We'll create sample data with common real-world problems (missing values, duplicates, outliers) so we can practice cleaning!\n",
    "\n",
    "**Why create dirty data?** Real datasets have these problems! We need to learn how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Added 2 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "# Add some duplicate rows to demonstrate duplicate removal\n",
    "# Why add duplicates? Real data often has duplicates from data entry errors or merging issues\n",
    "\n",
    "# pd.concat([df, df.iloc[[0, 1]]], ignore_index=True)\n",
    "# - pd.concat(): Combines multiple DataFrames vertically (stacks them)\n",
    "# - [df, df.iloc[[0, 1]]]: List of DataFrames to combine\n",
    "#   - df: Original DataFrame\n",
    "#   - df.iloc[[0, 1]]: Selects rows 0 and 1 by integer position\n",
    "#     - iloc: Integer-location based indexing (selects by position, not label)\n",
    "#     - [[0, 1]]: Double brackets select multiple rows, returns DataFrame\n",
    "#     - This gets a copy of the first 2 rows\n",
    "# - ignore_index=True: Resets index after concatenation (0, 1, 2, ... instead of 0, 1, 0, 1)\n",
    "# Result: Original 20 rows + 2 duplicate rows = 22 rows total\n",
    "df = pd.concat([df, df.iloc[[0, 1]]], ignore_index=True)\n",
    "print(\"   - Added 2 duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Original Data Shape: (22, 6)\n",
      "Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: (22, 6)\n",
      "   - Added 1 extreme salary outlier\n",
      "   - Added 1 impossible age value\n",
      "\n",
      "ğŸ“„ Original Data (first 10 rows):\n",
      "   id       name   age  salary department  experience_years\n",
      "0   1   Person_1  25.0   50000         IT               2.0\n",
      "1   2   Person_2  30.0   60000         HR               5.0\n",
      "2   3   Person_3   NaN   55000         IT               3.0\n",
      "3   4   Person_4  35.0   70000    Finance               8.0\n",
      "4   5   Person_5  40.0   80000         IT              10.0\n",
      "5   6   Person_6   NaN   65000         HR               4.0\n",
      "6   7   Person_7  28.0   58000         IT               2.5\n",
      "7   8   Person_8  32.0   72000    Finance               6.0\n",
      "8   9   Person_9  45.0   90000         IT              12.0\n",
      "9  10  Person_10  50.0   95000         HR              15.0\n"
     ]
    }
   ],
   "source": [
    "# Add some outliers to demonstrate outlier handling\n",
    "# Why add outliers? Real data has outliers from errors (typos, measurement mistakes) or rare events\n",
    "\n",
    "# df.loc[18, 'salary'] = 500000\n",
    "# - df.loc[]: Label-based indexing (selects by index label, not position)\n",
    "# - [18, 'salary']: Row index 18, column 'salary'\n",
    "# - = 500000: Assigns new value to that cell\n",
    "# - loc uses labels: if index is [0,1,2,...], loc[18] means row with label 18\n",
    "df.loc[18, 'salary'] = 500000  # Extreme outlier (10x normal salary - likely a typo)\n",
    "\n",
    "# df.loc[19, 'age'] = 150\n",
    "# - Same as above, but for row 19, column 'age'\n",
    "# - Sets age to 150 (impossible value for demonstration)\n",
    "df.loc[19, 'age'] = 150  # Impossible value (no one lives to 150 - data entry error)\n",
    "\n",
    "# df.shape\n",
    "# - Returns tuple (rows, columns): (number_of_rows, number_of_columns)\n",
    "# - Example: (22, 6) means 22 rows and 6 columns\n",
    "print(\"\\nğŸ“Š Original Data Shape:\", df.shape)\n",
    "print(\"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\", df.shape)\n",
    "print(\"   - Added 1 extreme salary outlier\")\n",
    "print(\"   - Added 1 impossible age value\")\n",
    "\n",
    "# df.head(10)\n",
    "# - Returns first 10 rows of DataFrame\n",
    "# - head(n): Shows first n rows (default is 5)\n",
    "# - Useful for quick data inspection\n",
    "print(\"\\nğŸ“„ Original Data (first 10 rows):\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. Handling Missing Values\n",
      "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
      "============================================================\n",
      "\n",
      "ğŸ” Missing values before cleaning:\n",
      "Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:\n",
      "id                  0\n",
      "name                0\n",
      "age                 4\n",
      "salary              0\n",
      "department          2\n",
      "experience_years    0\n",
      "dtype: int64\n",
      "\n",
      "   Total missing values: 6\n",
      "   Percentage missing: 4.5%\n"
     ]
    }
   ],
   "source": [
    "# First, let's see how many missing values we have\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Handling Missing Values\")\n",
    "print(\"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ” Missing values before cleaning:\")\n",
    "print(\"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:\")\n",
    "\n",
    "# df.isnull().sum()\n",
    "# - df.isnull(): Returns DataFrame with True/False (True = missing value, False = not missing)\n",
    "# - .sum(): Sums True values (counts missing values) for each column\n",
    "# - Returns Series with column names and count of missing values\n",
    "# - Alternative: df.isna() does the same thing\n",
    "missing_before = df.isnull().sum()\n",
    "print(missing_before)\n",
    "\n",
    "# missing_before.sum()\n",
    "# - .sum() on Series: Adds up all values in the Series\n",
    "# - This gives total missing values across all columns\n",
    "print(f\"\\n   Total missing values: {missing_before.sum()}\")\n",
    "\n",
    "# df.shape[0] * df.shape[1]\n",
    "# - df.shape[0]: Number of rows\n",
    "# - df.shape[1]: Number of columns\n",
    "# - shape[0] * shape[1]: Total number of cells in DataFrame\n",
    "# - Used to calculate percentage of missing values\n",
    "print(f\"   Percentage missing: {(missing_before.sum() / (df.shape[0] * df.shape[1]) * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Method 1: Remove rows with missing values ---\n",
      "--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\n",
      "âœ… Rows after removal: 17 (removed 5 rows)\n",
      "Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù: 17 (ØªÙ… Ø­Ø°Ù 5 ØµÙ)\n",
      "   Data loss: 22.7%\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Remove rows with missing values\n",
    "# .dropna() removes any row that has at least one missing value\n",
    "# Why use this? If missing values are rare, it's better to remove than guess\n",
    "\n",
    "print(\"\\n--- Method 1: Remove rows with missing values ---\")\n",
    "print(\"--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\")\n",
    "\n",
    "# df.dropna()\n",
    "# - Removes rows that contain ANY missing values (NaN/None)\n",
    "# - Returns new DataFrame (doesn't modify original unless inplace=True)\n",
    "# - Parameters:\n",
    "#   - axis=0 (default): Drop rows (axis=1 would drop columns)\n",
    "#   - how='any' (default): Drop if ANY value is missing\n",
    "#   - subset=None: Check all columns (can specify columns to check)\n",
    "#   - inplace=False: Return new DataFrame (True modifies original)\n",
    "df_removed = df.dropna()\n",
    "\n",
    "# df.shape[0] - df_removed.shape[0]\n",
    "# - df.shape[0]: Original number of rows\n",
    "# - df_removed.shape[0]: Number of rows after removal\n",
    "# - Difference = number of rows removed\n",
    "rows_removed = df.shape[0] - df_removed.shape[0]\n",
    "print(f\"âœ… Rows after removal: {df_removed.shape[0]} (removed {rows_removed} rows)\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù: {df_removed.shape[0]} (ØªÙ… Ø­Ø°Ù {rows_removed} ØµÙ)\")\n",
    "print(f\"   Data loss: {(rows_removed / df.shape[0] * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Handling Missing Values | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
    "\n",
    "**BEFORE**: We have missing values (NaN/None) that will break our ML models.\n",
    "\n",
    "**AFTER**: We'll either remove rows with missing values OR fill them with reasonable estimates!\n",
    "\n",
    "**Why handle missing values?** \n",
    "- ML algorithms cannot work with missing data\n",
    "- Missing values indicate incomplete information\n",
    "- We must decide: **Remove** (if few missing) or **Impute** (if many missing)\n",
    "\n",
    "**Two main strategies:**\n",
    "1. **Remove**: Drop rows/columns with missing values (good if <5% missing)\n",
    "2. **Impute**: Fill missing values with mean/median/mode (good if >5% missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Method 2: Fill missing values ---\n",
      "--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Fill missing values (imputation)\n",
    "# We'll use a copy so we don't modify the original\n",
    "print(\"\\n--- Method 2: Fill missing values ---\")\n",
    "print(\"--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\")\n",
    "\n",
    "# df.copy()\n",
    "# - Creates a deep copy of DataFrame (independent copy, not a reference)\n",
    "# - Changes to copy don't affect original\n",
    "# - Important: Without copy(), both variables point to same data\n",
    "# - Alternative: df.copy(deep=True) is explicit (deep=True is default)\n",
    "df_filled = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numeric columns with mean\n",
    "# Ù…Ù„Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ù„Ù…ØªÙˆØ³Ø·\n",
    "\n",
    "# df_filled['age'].fillna(df_filled['age'].mean(), inplace=True)\n",
    "# - df_filled['age']: Selects 'age' column (returns Series)\n",
    "# - .fillna(): Fills missing values (NaN) with specified value\n",
    "#   - Parameter: Value to fill (here: mean of age column)\n",
    "#   - inplace=True: Modifies DataFrame directly (False returns new Series)\n",
    "# - df_filled['age'].mean(): Calculates mean (average) of age column\n",
    "#   - .mean(): Returns average of all non-missing values\n",
    "#   - Ignores NaN values automatically\n",
    "# Result: All missing ages replaced with average age\n",
    "df_filled['age'].fillna(df_filled['age'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. Removing Duplicates\n",
      "Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
      "============================================================\n",
      "\n",
      "ğŸ” Number of duplicates: 2\n",
      "Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: 2\n",
      "\n",
      "âœ… Rows after removing duplicates: 20\n",
      "Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: 20\n",
      "   Removed 2 duplicate row(s)\n"
     ]
    }
   ],
   "source": [
    "# Check for and remove duplicate rows\n",
    "# .duplicated() finds rows that are exact duplicates\n",
    "# .drop_duplicates() removes them, keeping the first occurrence\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. Removing Duplicates\")\n",
    "print(\"Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# df_filled.duplicated().sum()\n",
    "# - df_filled.duplicated(): Returns boolean Series (True = duplicate row, False = unique)\n",
    "#   - Checks if each row is identical to a previous row\n",
    "#   - First occurrence marked as False, duplicates as True\n",
    "#   - Parameters:\n",
    "#     - subset=None: Check all columns (can specify columns to check)\n",
    "#     - keep='first' (default): Mark first as False, rest as True\n",
    "# - .sum(): Counts True values (number of duplicate rows)\n",
    "num_duplicates = df_filled.duplicated().sum()\n",
    "print(f\"\\nğŸ” Number of duplicates: {num_duplicates}\")\n",
    "print(f\"Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {num_duplicates}\")\n",
    "\n",
    "# df_filled.drop_duplicates()\n",
    "# - Removes duplicate rows, keeps first occurrence\n",
    "# - Returns new DataFrame (doesn't modify original unless inplace=True)\n",
    "# - Parameters:\n",
    "#   - subset=None: Check all columns for duplicates\n",
    "#   - keep='first' (default): Keep first, remove rest ('last' keeps last, False removes all)\n",
    "#   - inplace=False: Return new DataFrame\n",
    "df_no_duplicates = df_filled.drop_duplicates()\n",
    "print(f\"\\nâœ… Rows after removing duplicates: {df_no_duplicates.shape[0]}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {df_no_duplicates.shape[0]}\")\n",
    "print(f\"   Removed {num_duplicates} duplicate row(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after filling:\n",
      "Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ù„Ø¡:\n",
      "id                  0\n",
      "name                0\n",
      "age                 0\n",
      "salary              0\n",
      "department          0\n",
      "experience_years    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill categorical columns with mode\n",
    "# Ù…Ù„Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ÙØ¦ÙˆÙŠØ© Ø¨Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹\n",
    "\n",
    "# df_filled['department'].fillna(df_filled['department'].mode()[0], inplace=True)\n",
    "# - df_filled['department']: Selects 'department' column\n",
    "# - .mode(): Returns Series with most frequent value(s)\n",
    "#   - Mode = most common value (for categorical data)\n",
    "#   - Returns Series (can have multiple modes if tie)\n",
    "# - [0]: Gets first mode value (if multiple modes, takes first)\n",
    "# - .fillna(): Fills missing values with mode\n",
    "# - inplace=True: Modifies DataFrame directly\n",
    "# Result: All missing departments replaced with most common department\n",
    "df_filled['department'].fillna(df_filled['department'].mode()[0], inplace=True)\n",
    "print(\"\\nMissing values after filling:\")\n",
    "print(\"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ù„Ø¡:\")\n",
    "print(df_filled.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- IQR Method for Outlier Detection ---\n",
      "--- Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø±Ø¨ÙŠØ¹ÙŠ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ---\n",
      "   âœ… IQR function defined\n",
      "   This will identify values that are too far from the median\n"
     ]
    }
   ],
   "source": [
    "# IQR (Interquartile Range) Method for outlier detection\n",
    "# This is a statistical method that identifies values far from the median\n",
    "\n",
    "print(\"\\n--- IQR Method for Outlier Detection ---\")\n",
    "print(\"--- Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø±Ø¨ÙŠØ¹ÙŠ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ---\")\n",
    "\n",
    "def detect_outliers_iqr(series):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method.\n",
    "    Returns True for outliers, False for normal values.\n",
    "    \"\"\"\n",
    "    # series.quantile(0.25)\n",
    "    # - Calculates 25th percentile (Q1) - value below which 25% of data falls\n",
    "    # - quantile(q): Returns value at quantile q (0.0 to 1.0)\n",
    "    # - 0.25 = 25th percentile, 0.5 = median, 0.75 = 75th percentile\n",
    "    Q1 = series.quantile(0.25)  # 25th percentile\n",
    "    \n",
    "    # series.quantile(0.75)\n",
    "    # - Calculates 75th percentile (Q3) - value below which 75% of data falls\n",
    "    Q3 = series.quantile(0.75)  # 75th percentile\n",
    "    \n",
    "    # IQR = Q3 - Q1\n",
    "    # - Interquartile Range: Spread of middle 50% of data\n",
    "    # - Measures variability, less sensitive to outliers than range\n",
    "    IQR = Q3 - Q1  # Interquartile Range\n",
    "    \n",
    "    # Q1 - 1.5 * IQR\n",
    "    # - Lower fence: Values below this are considered outliers\n",
    "    # - 1.5 is standard multiplier (can be adjusted)\n",
    "    lower_bound = Q1 - 1.5 * IQR  # Lower fence\n",
    "    \n",
    "    # Q3 + 1.5 * IQR\n",
    "    # - Upper fence: Values above this are considered outliers\n",
    "    upper_bound = Q3 + 1.5 * IQR  # Upper fence\n",
    "    \n",
    "    # (series < lower_bound) | (series > upper_bound)\n",
    "    # - Boolean indexing: Creates boolean Series (True = outlier, False = normal)\n",
    "    # - | : Logical OR operator (element-wise)\n",
    "    # - Returns True for values outside [lower_bound, upper_bound]\n",
    "    # Values outside the fences are outliers\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "print(\"   âœ… IQR function defined\")\n",
    "print(\"   This will identify values that are too far from the median\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. Handling Outliers\n",
      "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 4. Handling Outliers\n",
    "# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Handling Outliers\")\n",
    "print(\"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outliers in salary column:\n",
      "Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ÙÙŠ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø±Ø§ØªØ¨:\n",
      "Number of outliers: 1\n",
      "         name  salary\n",
      "18  Person_19  500000\n"
     ]
    }
   ],
   "source": [
    "# Check for outliers in salary\n",
    "print(\"\\nOutliers in salary column:\")\n",
    "print(\"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ÙÙŠ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø±Ø§ØªØ¨:\")\n",
    "\n",
    "# detect_outliers_iqr(df_no_duplicates['salary'])\n",
    "# - Calls function defined earlier\n",
    "# - df_no_duplicates['salary']: Passes salary column as Series\n",
    "# - Returns boolean Series (True = outlier, False = normal)\n",
    "salary_outliers = detect_outliers_iqr(df_no_duplicates['salary'])\n",
    "\n",
    "# salary_outliers.sum()\n",
    "# - Counts True values (number of outliers)\n",
    "print(f\"Number of outliers: {salary_outliers.sum()}\")\n",
    "\n",
    "# df_no_duplicates[salary_outliers][['name', 'salary']]\n",
    "# - df_no_duplicates[salary_outliers]: Boolean indexing - selects rows where salary_outliers is True\n",
    "#   - Boolean Series used as filter: True rows kept, False rows removed\n",
    "# - [['name', 'salary']]: Selects only 'name' and 'salary' columns\n",
    "#   - Double brackets [[]] returns DataFrame (single [] returns Series)\n",
    "# Result: Shows only outlier rows with name and salary columns\n",
    "print(df_no_duplicates[salary_outliers][['name', 'salary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "\n",
    "# df_no_duplicates[~salary_outliers].copy()\n",
    "# - ~salary_outliers: NOT operator (~) inverts boolean Series\n",
    "#   - True becomes False, False becomes True\n",
    "#   - Selects rows that are NOT outliers (keeps normal values)\n",
    "# - [~salary_outliers]: Boolean indexing - filters DataFrame\n",
    "# - .copy(): Creates independent copy (good practice)\n",
    "# Result: DataFrame with outliers removed\n",
    "df_clean = df_no_duplicates[~salary_outliers].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows after removing outliers: 18\n",
      "Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©: 18\n"
     ]
    }
   ],
   "source": [
    "# Also remove impossible age values\n",
    "\n",
    "# df_clean[df_clean['age'] <= 100].copy()\n",
    "# - df_clean['age'] <= 100: Creates boolean Series (True if age <= 100, False otherwise)\n",
    "#   - Comparison operator (<=) applied element-wise to all values\n",
    "# - df_clean[boolean_series]: Boolean indexing - keeps rows where condition is True\n",
    "# - .copy(): Creates independent copy\n",
    "# Result: Keeps only rows where age is 100 or less (removes impossible ages like 150)\n",
    "df_clean = df_clean[df_clean['age'] <= 100].copy()\n",
    "print(f\"\\nRows after removing outliers: {df_clean.shape[0]}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©: {df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. Data Type Conversion\n",
      "ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
      "============================================================\n",
      "\n",
      "Data types before conversion:\n",
      "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\n",
      "id                    int64\n",
      "name                 object\n",
      "age                 float64\n",
      "salary                int64\n",
      "department           object\n",
      "experience_years    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 5. Data Type Conversion\n",
    "# ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. Data Type Conversion\")\n",
    "print(\"ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nData types before conversion:\")\n",
    "print(\"Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\")\n",
    "\n",
    "# df_clean.dtypes\n",
    "# - Returns Series showing data type of each column\n",
    "# - Common types: int64, float64, object (string), bool, datetime64\n",
    "# - Useful for checking if types are correct (e.g., numbers stored as strings)\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert experience_years to int (rounding)\n",
    "\n",
    "# df_clean['experience_years'].round().astype(int)\n",
    "# - df_clean['experience_years']: Selects 'experience_years' column\n",
    "# - .round(): Rounds decimal values to nearest integer\n",
    "#   - No parameter = rounds to 0 decimal places\n",
    "#   - .round(2) would round to 2 decimal places\n",
    "# - .astype(int): Converts data type to integer\n",
    "#   - astype(): Changes data type of Series\n",
    "#   - int: Integer type (int64 in pandas)\n",
    "#   - Alternative: .astype('int64') or .astype(float) for float\n",
    "# Result: Converts float values like 2.5 to integer 3 (after rounding)\n",
    "df_clean['experience_years'] = df_clean['experience_years'].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. Cleaning Summary\n",
      "Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Original rows: 22\n",
      "Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ØµÙ„ÙŠØ©: 22\n",
      "âœ… Final cleaned rows: 18\n",
      "Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: 18\n",
      "\n",
      "ğŸ—‘ï¸  Rows removed: 4 (18.2%)\n",
      "Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: 4\n",
      "\n",
      "ğŸ“„ Cleaned Data (first 10 rows):\n",
      "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©:\n",
      "   id       name   age  salary department  experience_years\n",
      "0   1   Person_1  25.0   50000         IT                 2\n",
      "1   2   Person_2  30.0   60000         HR                 5\n",
      "2   3   Person_3  40.5   55000         IT                 3\n",
      "3   4   Person_4  35.0   70000    Finance                 8\n",
      "4   5   Person_5  40.0   80000         IT                10\n",
      "5   6   Person_6  40.5   65000         HR                 4\n",
      "6   7   Person_7  28.0   58000         IT                 2\n",
      "7   8   Person_8  32.0   72000    Finance                 6\n",
      "8   9   Person_9  45.0   90000         IT                12\n",
      "9  10  Person_10  50.0   95000         HR                15\n",
      "\n",
      "============================================================\n",
      "âœ… Example 2 Complete! âœ“\n",
      "Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 2! âœ“\n",
      "============================================================\n",
      "\n",
      "ğŸ“ What you accomplished:\n",
      "   âœ… Handled missing values (imputed with mean/mode)\n",
      "   âœ… Removed duplicate rows\n",
      "   âœ… Detected and removed outliers\n",
      "   âœ… Converted data types correctly\n",
      "   âœ… Created clean, model-ready data!\n"
     ]
    }
   ],
   "source": [
    "# Final summary of cleaning process\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. Cleaning Summary\")\n",
    "print(\"Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "original_rows = df.shape[0]\n",
    "final_rows = df_clean.shape[0]\n",
    "rows_removed = original_rows - final_rows\n",
    "\n",
    "print(f\"\\nğŸ“Š Original rows: {original_rows}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ØµÙ„ÙŠØ©: {original_rows}\")\n",
    "print(f\"âœ… Final cleaned rows: {final_rows}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {final_rows}\")\n",
    "print(f\"\\nğŸ—‘ï¸  Rows removed: {rows_removed} ({(rows_removed/original_rows*100):.1f}%)\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {rows_removed}\")\n",
    "\n",
    "print(\"\\nğŸ“„ Cleaned Data (first 10 rows):\")\n",
    "print(\"Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©:\")\n",
    "print(df_clean.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Example 2 Complete! âœ“\")\n",
    "print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 2! âœ“\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ“ What you accomplished:\")\n",
    "print(\"   âœ… Handled missing values (imputed with mean/mode)\")\n",
    "print(\"   âœ… Removed duplicate rows\")\n",
    "print(\"   âœ… Detected and removed outliers\")\n",
    "print(\"   âœ… Converted data types correctly\")\n",
    "print(\"   âœ… Created clean, model-ready data!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Decision Framework - When to Remove vs. When to Fix | Ø§Ù„Ø®Ø·ÙˆØ© 6: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ Ù†Ø­Ø°Ù ÙˆÙ…ØªÙ‰ Ù†ØµÙ„Ø­\n",
    "\n",
    "**BEFORE**: You've learned different cleaning methods, but when should you use each one?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose the right cleaning method for any situation!\n",
    "\n",
    "**Why this matters**: Making the wrong cleaning decision can:\n",
    "- **Remove too much data** â†’ Lose valuable information\n",
    "- **Keep bad data** â†’ Break your models\n",
    "- **Use wrong method** â†’ Introduce bias or errors\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Missing Values | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
    "\n",
    "**Key Question**: Should I **REMOVE** or **IMPUTE** missing values?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Is missing data < 5% of total?\n",
    "â”œâ”€ YES â†’ REMOVE (dropna)\n",
    "â”‚   â””â”€ Why? Small loss, keeps data \"pure\"\n",
    "â”‚\n",
    "â””â”€ NO â†’ Is missing data random or systematic?\n",
    "    â”œâ”€ RANDOM â†’ IMPUTE (fillna with mean/median/mode)\n",
    "    â”‚   â””â”€ Why? Random missing = no bias, safe to estimate\n",
    "    â”‚\n",
    "    â””â”€ SYSTEMATIC â†’ INVESTIGATE FIRST\n",
    "        â””â”€ Why? Systematic missing might indicate important pattern\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Remove** | < 5% missing, random | â€¢ No bias introduced<br>â€¢ Keeps data \"pure\"<br>â€¢ Simple | â€¢ Loses data<br>â€¢ Can't use if many missing | Age missing in 2% of records |\n",
    "| **Impute (Mean/Median)** | > 5% missing, numeric, random | â€¢ Keeps all rows<br>â€¢ Preserves sample size<br>â€¢ Works for numeric | â€¢ Can introduce bias<br>â€¢ Assumes normal distribution | Salary missing in 15% of records |\n",
    "| **Impute (Mode)** | > 5% missing, categorical, random | â€¢ Keeps all rows<br>â€¢ Preserves sample size<br>â€¢ Works for categories | â€¢ Can create artificial patterns<br>â€¢ May over-represent common values | Department missing in 10% of records |\n",
    "| **Investigate** | Systematic missing | â€¢ Finds root cause<br>â€¢ Prevents bias<br>â€¢ Better decisions | â€¢ Takes time<br>â€¢ Requires domain knowledge | All salaries missing for one department |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Outliers | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\n",
    "\n",
    "**Key Question**: Should I **REMOVE** or **KEEP** outliers?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Is the outlier a data entry error?\n",
    "â”œâ”€ YES â†’ REMOVE\n",
    "â”‚   â””â”€ Example: Age = 150, Salary = 500000 (typo)\n",
    "â”‚\n",
    "â””â”€ NO â†’ Is the outlier a rare but valid event?\n",
    "    â”œâ”€ YES â†’ KEEP (but handle separately)\n",
    "    â”‚   â””â”€ Example: CEO salary in employee dataset\n",
    "    â”‚\n",
    "    â””â”€ NO â†’ Does it affect model performance?\n",
    "        â”œâ”€ YES â†’ REMOVE or TRANSFORM\n",
    "        â”‚   â””â”€ Example: Extreme values breaking linear regression\n",
    "        â”‚\n",
    "        â””â”€ NO â†’ KEEP\n",
    "            â””â”€ Example: Outlier in non-critical feature\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Remove** | Data entry errors, impossible values | â€¢ Removes noise<br>â€¢ Improves model accuracy<br>â€¢ Simple | â€¢ Loses information<br>â€¢ May remove valid rare events | Age = 150, Salary = 500000 |\n",
    "| **Cap/Clip** | Valid but extreme values | â€¢ Keeps data<br>â€¢ Reduces impact<br>â€¢ Preserves distribution | â€¢ Arbitrary threshold<br>â€¢ May hide important patterns | Cap salary at 99th percentile |\n",
    "| **Transform** | Skewed distributions | â€¢ Normalizes data<br>â€¢ Keeps all values<br>â€¢ Better for ML | â€¢ Changes interpretation<br>â€¢ More complex | Log transform for income |\n",
    "| **Keep** | Rare but valid events | â€¢ Preserves reality<br>â€¢ No information loss<br>â€¢ Important for analysis | â€¢ Can skew models<br>â€¢ May need special handling | CEO in employee dataset |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Duplicates | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
    "\n",
    "**Key Question**: Should I **REMOVE** all duplicates or **INVESTIGATE** first?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Are duplicates exact copies?\n",
    "â”œâ”€ YES â†’ REMOVE (keep first)\n",
    "â”‚   â””â”€ Why? No new information, wastes space\n",
    "â”‚\n",
    "â””â”€ NO â†’ Are duplicates near-duplicates (typos)?\n",
    "    â”œâ”€ YES â†’ FIX (merge or correct)\n",
    "    â”‚   â””â”€ Why? Same entity, different spelling\n",
    "    â”‚\n",
    "    â””â”€ NO â†’ Are duplicates valid (same person, different records)?\n",
    "        â””â”€ YES â†’ KEEP (but flag for analysis)\n",
    "            â””â”€ Why? Important information (e.g., repeat customers)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: E-commerce Dataset\n",
    "- **Missing values in \"price\"**: 20% missing\n",
    "  - **Decision**: IMPUTE with median (too much to remove, random missing)\n",
    "  - **Reason**: Random missing prices, median preserves distribution\n",
    "\n",
    "#### Example 2: Medical Dataset\n",
    "- **Outlier in \"age\"**: One patient age = 200\n",
    "  - **Decision**: REMOVE (impossible value)\n",
    "  - **Reason**: Data entry error, no one lives to 200\n",
    "\n",
    "#### Example 3: Customer Dataset\n",
    "- **Missing values in \"email\"**: 3% missing\n",
    "  - **Decision**: REMOVE (small percentage)\n",
    "  - **Reason**: Email is critical, can't impute, small loss acceptable\n",
    "\n",
    "#### Example 4: Sales Dataset\n",
    "- **Outlier in \"revenue\"**: One sale = $1,000,000 (normal range: $10-$1000)\n",
    "  - **Decision**: INVESTIGATE FIRST\n",
    "  - **Reason**: Could be valid (enterprise sale) or error (extra zero)\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Always investigate first** - Understand WHY data is missing/outlier/duplicate\n",
    "2. **Consider data loss** - Removing >10% of data is usually too much\n",
    "3. **Consider bias** - Systematic missing/outliers may indicate important patterns\n",
    "4. **Test both approaches** - Sometimes try removing AND imputing, compare results\n",
    "5. **Document decisions** - Write down WHY you chose each method (for reproducibility)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario**: You have a dataset with:\n",
    "- 8% missing values in \"income\" column\n",
    "- 2 outliers in \"age\" (ages 0 and 200)\n",
    "- 5 duplicate rows\n",
    "\n",
    "**Your task**: Decide what to do for each issue and explain why!\n",
    "\n",
    "**Answers**:\n",
    "1. **Income (8% missing)**: IMPUTE with median (too much to remove, likely random)\n",
    "2. **Age outliers**: REMOVE (impossible values - data entry errors)\n",
    "3. **Duplicates**: REMOVE (exact copies, no new information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Practical Example: Decision-Making in Action\n",
      "Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ: Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø± ÙÙŠ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø©\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Scenario: Dataset with multiple issues\n",
      "   - 15% missing values in 'income' (too much to remove)\n",
      "   - 2% missing values in 'email' (can remove)\n",
      "   - 1 outlier: age = 200 (impossible, should remove)\n",
      "   - 3 duplicates (should remove)\n",
      "\n",
      "------------------------------------------------------------\n",
      "DECISION 1: Income missing (15%)\n",
      "------------------------------------------------------------\n",
      "   âŒ Can't remove: Would lose 15% of data (too much!)\n",
      "   âœ… Should impute: Use median (preserves distribution)\n",
      "   ğŸ“ Reason: Random missing, large percentage, numeric data\n",
      "\n",
      "------------------------------------------------------------\n",
      "DECISION 2: Email missing (2%)\n",
      "------------------------------------------------------------\n",
      "   âœ… Can remove: Only 2% loss (acceptable)\n",
      "   âŒ Can't impute: Email is unique, can't estimate\n",
      "   ğŸ“ Reason: Small percentage, critical field, can't fill\n",
      "\n",
      "------------------------------------------------------------\n",
      "DECISION 3: Age outlier (age = 200)\n",
      "------------------------------------------------------------\n",
      "   âœ… Should remove: Impossible value (data entry error)\n",
      "   âŒ Can't keep: Would break age-based models\n",
      "   ğŸ“ Reason: No human lives to 200, clearly an error\n",
      "\n",
      "------------------------------------------------------------\n",
      "DECISION 4: Duplicates (3 rows)\n",
      "------------------------------------------------------------\n",
      "   âœ… Should remove: Exact duplicates, no new information\n",
      "   ğŸ“ Reason: Wastes space, can bias models (same data counted twice)\n",
      "\n",
      "============================================================\n",
      "âœ… Decision Framework Applied Successfully!\n",
      "ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Practical Example: Comparing Different Cleaning Approaches\n",
    "# Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ: Ù…Ù‚Ø§Ø±Ù†Ø© Ø·Ø±Ù‚ Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Practical Example: Decision-Making in Action\")\n",
    "print(\"Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ: Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø± ÙÙŠ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a scenario with different data quality issues\n",
    "print(\"\\nğŸ“Š Scenario: Dataset with multiple issues\")\n",
    "print(\"   - 15% missing values in 'income' (too much to remove)\")\n",
    "print(\"   - 2% missing values in 'email' (can remove)\")\n",
    "print(\"   - 1 outlier: age = 200 (impossible, should remove)\")\n",
    "print(\"   - 3 duplicates (should remove)\")\n",
    "\n",
    "# Simulate the decision-making process\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 1: Income missing (15%)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   âŒ Can't remove: Would lose 15% of data (too much!)\")\n",
    "print(\"   âœ… Should impute: Use median (preserves distribution)\")\n",
    "print(\"   ğŸ“ Reason: Random missing, large percentage, numeric data\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 2: Email missing (2%)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   âœ… Can remove: Only 2% loss (acceptable)\")\n",
    "print(\"   âŒ Can't impute: Email is unique, can't estimate\")\n",
    "print(\"   ğŸ“ Reason: Small percentage, critical field, can't fill\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 3: Age outlier (age = 200)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   âœ… Should remove: Impossible value (data entry error)\")\n",
    "print(\"   âŒ Can't keep: Would break age-based models\")\n",
    "print(\"   ğŸ“ Reason: No human lives to 200, clearly an error\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 4: Duplicates (3 rows)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   âœ… Should remove: Exact duplicates, no new information\")\n",
    "print(\"   ğŸ“ Reason: Wastes space, can bias models (same data counted twice)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Decision Framework Applied Successfully!\")\n",
    "print(\"ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types after conversion:\n",
      "Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\n",
      "id                   int64\n",
      "name                object\n",
      "age                  int64\n",
      "salary               int64\n",
      "department          object\n",
      "experience_years     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert age to int\n",
    "\n",
    "# df_clean['age'].round().astype(int)\n",
    "# - df_clean['age']: Selects 'age' column\n",
    "# - .round(): Rounds decimal values (e.g., 40.5 â†’ 41)\n",
    "# - .astype(int): Converts to integer type\n",
    "# Result: Converts float ages to integer ages\n",
    "df_clean['age'] = df_clean['age'].round().astype(int)\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(\"Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\")\n",
    "\n",
    "# df_clean.dtypes\n",
    "# - Shows data types after conversion\n",
    "# - Should show int64 for age and experience_years now\n",
    "print(df_clean.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
