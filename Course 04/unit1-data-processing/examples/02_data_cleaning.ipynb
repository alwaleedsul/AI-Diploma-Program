{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Data Cleaning | ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Data Loading and Exploration** - You need to know how to identify data quality issues first!\n",
    "- âœ… **Basic pandas knowledge**: DataFrames, indexing, filtering\n",
    "- âœ… **Understanding of data quality**: What are missing values, duplicates, outliers?\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why data cleaning is necessary\n",
    "- Knowing which cleaning method to use\n",
    "- Understanding the impact of cleaning on your data\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the SECOND example** - it fixes the problems we found in Example 1!\n",
    "\n",
    "**Why this example SECOND?**\n",
    "- **Before** you can preprocess data, you need to clean it\n",
    "- **Before** you can build models, you need clean data\n",
    "- **Before** you can make predictions, you need to fix data quality issues\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: Data Loading and Exploration (we found the problems, now we fix them!)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 3: Data Preprocessing (needs clean data to work with)\n",
    "- ğŸ““ Example 4: Linear Regression (needs clean, preprocessed data)\n",
    "- ğŸ““ All ML models (all need clean data!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Data cleaning fixes **quality issues** (needed before preprocessing)\n",
    "2. Data cleaning teaches you **when to remove vs. impute** (critical decision-making)\n",
    "3. Data cleaning shows you **the impact of outliers** (affects model accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Cleaning Before Cooking | Ø§Ù„Ù‚ØµØ©: Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ù‚Ø¨Ù„ Ø§Ù„Ø·Ø¨Ø®\n",
    "\n",
    "Imagine you're cooking a meal. **Before** you can cook, you need to clean your ingredients - remove spoiled items, wash vegetables, check for foreign objects. **After** cleaning everything, you can prepare a safe, delicious meal!\n",
    "\n",
    "Same with machine learning: **Before** building models, we clean our data - remove duplicates, handle missing values, fix outliers. **After** cleaning, we can build accurate, reliable models!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Cleaning Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ\n",
    "\n",
    "Data cleaning is essential for accurate models:\n",
    "- **Missing Values**: Break ML algorithms - must be handled\n",
    "- **Duplicates**: Bias your models (same data counted twice)\n",
    "- **Outliers**: Skew predictions and statistics\n",
    "- **Wrong Data Types**: Cause errors in calculations\n",
    "- **Dirty Data = Bad Models**: No amount of ML can fix fundamentally bad data\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Handle missing values (remove or impute)\n",
    "2. Remove duplicate rows\n",
    "3. Detect and handle outliers\n",
    "4. Convert data types correctly\n",
    "5. Understand trade-offs between different cleaning methods\n",
    "6. Know when to remove vs. when to fix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us clean and analyze data\n",
    "\n",
    "import pandas as pd  # For data manipulation (cleaning operations)\n",
    "import numpy as np   # For numerical operations (handling NaN, calculations)\n",
    "import matplotlib.pyplot as plt  # For visualizations (seeing outliers)\n",
    "import seaborn as sns  # For statistical plots (data quality visualization)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each library does:\")\n",
    "print(\"   - pandas: Clean data (remove, fill, filter)\")\n",
    "print(\"   - numpy: Handle missing values (NaN operations)\")\n",
    "print(\"   - matplotlib: Visualize data quality issues\")\n",
    "print(\"   - seaborn: Create beautiful quality check plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We explored our data in Example 1 and found problems - missing values, duplicates, outliers.\n",
    "\n",
    "**AFTER**: We'll clean the data by fixing all these issues, making it ready for preprocessing and modeling!\n",
    "\n",
    "**Why this matters**: Dirty data produces unreliable models. Cleaning is non-negotiable for good ML results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real-world Titanic dataset with natural data quality issues\n",
    "# The Titanic dataset is famous for having missing values, which makes it perfect for learning data cleaning!\n",
    "# This is REAL data from the 1912 Titanic disaster\n",
    "\n",
    "print(\"\\nğŸ“¥ Loading Titanic dataset...\")\n",
    "print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ§ÙŠØªØ§Ù†ÙŠÙƒ...\")\n",
    "\n",
    "# Load from public URL (well-known dataset)\n",
    "titanic_url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "df = pd.read_csv(titanic_url)\n",
    "\n",
    "# pd.read_csv(url)\n",
    "# - pd.read_csv(): Reads CSV file from URL or local path\n",
    "# - This is REAL historical data from the Titanic passenger manifest\n",
    "# - Returns DataFrame with passenger information\n",
    "\n",
    "print(f\"\\nâœ… Real-world Titanic dataset loaded!\")\n",
    "print(f\"   ğŸ“Š This is REAL data from the 1912 Titanic passenger manifest\")\n",
    "print(f\"   ğŸ“ˆ Contains {len(df)} passengers with {len(df.columns)} features\")\n",
    "print(f\"\\nğŸ” Notice:\")\n",
    "print(\"   - This dataset naturally has missing values (Age, Cabin, Embarked)\")\n",
    "print(\"   - Real data often has data quality issues - this is normal!\")\n",
    "print(\"   - We'll learn to clean this real data in the following steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Understanding the Dataset | ÙÙ‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø¬Ø§Ù„\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (medical, financial, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: ~890 rows Ã— 12 columns (samples Ã— features)\n",
    "- **Feature Types**: Mixed (numerical: Age, Fare; categorical: Sex, Embarked, Cabin)\n",
    "- **Target Type**: Classification (predicting survival: 0 or 1)\n",
    "- **Task**: Predict passenger survival based on features\n",
    "- **Data Quality**: Has missing values (Age, Cabin, Embarked) - perfect for learning cleaning!\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Mixed feature types** â†’ Need different cleaning methods (numeric vs. categorical)\n",
    "- **Missing values present** â†’ Real-world scenario (most datasets have missing values)\n",
    "- **Classification task** â†’ We'll use classification metrics later (not regression)\n",
    "- **Historical data** â†’ Natural data quality issues (real-world scenario)\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ÙÙ‡Ù… Ù…Ø¬Ø§Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø§Ø®ØªØµØ§Ø±)\n",
    "\n",
    "**What is this data?** Titanic passenger manifest from 1912 disaster.\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For choosing cleaning methods**: Missing Age â†’ use mean/median (numeric), Missing Embarked â†’ use mode (categorical)\n",
    "- **For outlier detection**: Age should be 0-120, Fare should be positive\n",
    "- **For data types**: Age should be numeric (not text), Sex should be categorical\n",
    "\n",
    "**Domain Context** (Brief):\n",
    "- **Survived**: Target variable (0 = died, 1 = survived)\n",
    "- **Pclass**: Passenger class (1st, 2nd, 3rd) - affects survival chances\n",
    "- **Sex**: Gender (male/female) - affected survival (women and children first)\n",
    "- **Age**: Age in years - missing for many passengers\n",
    "- **Fare**: Ticket price - higher fare might indicate better survival chances\n",
    "- **Cabin**: Cabin number - many missing (not all passengers had cabins)\n",
    "\n",
    "**ğŸ’¡ Key Point for CS Students**: You don't need to be a history expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types, missing values)\n",
    "- Knowing the **cleaning methods** (remove vs. impute, mean vs. median vs. mode)\n",
    "- Choosing the right **cleaning approach** based on structure, not domain knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Real-World Data with Issues | Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ø¨Ù…Ø´Ø§ÙƒÙ„\n",
    "\n",
    "**BEFORE**: We need to learn cleaning techniques, but we need data with real-world problems to practice on.\n",
    "\n",
    "**AFTER**: We'll load the Titanic dataset - a famous real-world dataset that naturally has missing values, making it perfect for learning data cleaning!\n",
    "\n",
    "**Why use Titanic?** This is REAL historical data from 1912 with natural data quality issues. Real datasets have these problems! We need to learn how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows in the real data\n",
    "# Real datasets sometimes have duplicates from data entry errors or merging issues\n",
    "\n",
    "# First, let's see if there are any natural duplicates\n",
    "initial_count = len(df)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "\n",
    "print(f\"\\nğŸ“Š Checking for duplicates...\")\n",
    "print(f\"   Initial rows: {initial_count}\")\n",
    "print(f\"   Duplicate rows found: {duplicate_count}\")\n",
    "\n",
    "# For demonstration, we'll add a few duplicates to show the cleaning process\n",
    "# (In real projects, you'd check if duplicates should be removed or kept)\n",
    "if duplicate_count == 0:\n",
    "    # Add 2 duplicates for demonstration purposes\n",
    "    df = pd.concat([df, df.iloc[[0, 1]]], ignore_index=True)\n",
    "    print(\"   - Added 2 duplicate rows for demonstration\")\n",
    "else:\n",
    "    print(\"   - Dataset already contains duplicates (real-world scenario!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in the real data\n",
    "# Real datasets often have outliers from errors (typos, measurement mistakes) or rare events\n",
    "\n",
    "# df.shape\n",
    "# - Returns tuple (rows, columns): (number_of_rows, number_of_columns)\n",
    "print(\"\\nğŸ“Š Original Data Shape:\", df.shape)\n",
    "print(\"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\", df.shape)\n",
    "\n",
    "# Check for potential outliers in Age (should be reasonable, e.g., 0-120)\n",
    "if 'Age' in df.columns:\n",
    "    age_outliers = df[(df['Age'] > 100) | (df['Age'] < 0)]\n",
    "    print(f\"\\nğŸ” Checking for age outliers...\")\n",
    "    print(f\"   Age range: {df['Age'].min():.1f} to {df['Age'].max():.1f}\")\n",
    "    if len(age_outliers) > 0:\n",
    "        print(f\"   Found {len(age_outliers)} potential age outliers\")\n",
    "    else:\n",
    "        print(\"   No obvious age outliers found\")\n",
    "\n",
    "# For demonstration, we'll add one impossible age value to show outlier handling\n",
    "# (In real projects, check if outliers are errors or valid rare cases)\n",
    "if 'Age' in df.columns and df['Age'].max() < 120:\n",
    "    df.loc[0, 'Age'] = 150  # Add impossible age for demonstration\n",
    "    print(\"   - Added 1 impossible age value (150) for demonstration\")\n",
    "\n",
    "# df.head(10)\n",
    "# - Returns first 10 rows of DataFrame\n",
    "# - head(n): Shows first n rows (default is 5)\n",
    "# - Useful for quick data inspection\n",
    "print(\"\\nğŸ“„ Original Data (first 10 rows):\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nğŸ“‹ Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see how many missing values we have\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Handling Missing Values\")\n",
    "print(\"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ” Missing values before cleaning:\")\n",
    "print(\"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ:\")\n",
    "\n",
    "# df.isnull().sum()\n",
    "# - df.isnull(): Returns DataFrame with True/False (True = missing value, False = not missing)\n",
    "# - .sum(): Sums True values (counts missing values) for each column\n",
    "# - Returns Series with column names and count of missing values\n",
    "# - Alternative: df.isna() does the same thing\n",
    "missing_before = df.isnull().sum()\n",
    "print(missing_before)\n",
    "\n",
    "# missing_before.sum()\n",
    "# - .sum() on Series: Adds up all values in the Series\n",
    "# - This gives total missing values across all columns\n",
    "print(f\"\\n   Total missing values: {missing_before.sum()}\")\n",
    "\n",
    "# df.shape[0] * df.shape[1]\n",
    "# - df.shape[0]: Number of rows\n",
    "# - df.shape[1]: Number of columns\n",
    "# - shape[0] * shape[1]: Total number of cells in DataFrame\n",
    "# - Used to calculate percentage of missing values\n",
    "print(f\"   Percentage missing: {(missing_before.sum() / (df.shape[0] * df.shape[1]) * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Remove rows with missing values\n",
    "# .dropna() removes any row that has at least one missing value\n",
    "# Why use this? If missing values are rare, it's better to remove than guess\n",
    "\n",
    "print(\"\\n--- Method 1: Remove rows with missing values ---\")\n",
    "print(\"--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø°Ø§Øª Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\")\n",
    "\n",
    "# df.dropna()\n",
    "# - Removes rows that contain ANY missing values (NaN/None)\n",
    "# - Returns new DataFrame (doesn't modify original unless inplace=True)\n",
    "# - Parameters:\n",
    "#   - axis=0 (default): Drop rows (axis=1 would drop columns)\n",
    "#   - how='any' (default): Drop if ANY value is missing\n",
    "#   - subset=None: Check all columns (can specify columns to check)\n",
    "#   - inplace=False: Return new DataFrame (True modifies original)\n",
    "df_removed = df.dropna()\n",
    "\n",
    "# df.shape[0] - df_removed.shape[0]\n",
    "# - df.shape[0]: Original number of rows\n",
    "# - df_removed.shape[0]: Number of rows after removal\n",
    "# - Difference = number of rows removed\n",
    "rows_removed = df.shape[0] - df_removed.shape[0]\n",
    "print(f\"âœ… Rows after removal: {df_removed.shape[0]} (removed {rows_removed} rows)\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù: {df_removed.shape[0]} (ØªÙ… Ø­Ø°Ù {rows_removed} ØµÙ)\")\n",
    "print(f\"   Data loss: {(rows_removed / df.shape[0] * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Handling Missing Values | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
    "\n",
    "**BEFORE**: We have missing values (NaN/None) that will break our ML models.\n",
    "\n",
    "**AFTER**: We'll either remove rows with missing values OR fill them with reasonable estimates!\n",
    "\n",
    "**Why handle missing values?** \n",
    "- ML algorithms cannot work with missing data\n",
    "- Missing values indicate incomplete information\n",
    "- We must decide: **Remove** (if few missing) or **Impute** (if many missing)\n",
    "\n",
    "**Two main strategies:**\n",
    "1. **Remove**: Drop rows/columns with missing values (good if <5% missing)\n",
    "2. **Impute**: Fill missing values with mean/median/mode (good if >5% missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Fill missing values (imputation)\n",
    "# We'll use a copy so we don't modify the original\n",
    "print(\"\\n--- Method 2: Fill missing values ---\")\n",
    "print(\"--- Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ---\")\n",
    "\n",
    "# df.copy()\n",
    "# - Creates a deep copy of DataFrame (independent copy, not a reference)\n",
    "# - Changes to copy don't affect original\n",
    "# - Important: Without copy(), both variables point to same data\n",
    "# - Alternative: df.copy(deep=True) is explicit (deep=True is default)\n",
    "df_filled = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numeric columns with mean\n",
    "# Ù…Ù„Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¨Ø§Ù„Ù…ØªÙˆØ³Ø·\n",
    "\n",
    "# df_filled['Age'].fillna(df_filled['Age'].mean(), inplace=True)\n",
    "# - df_filled['Age']: Selects 'age' column (returns Series)\n",
    "# - .fillna(): Fills missing values (NaN) with specified value\n",
    "#   - Parameter: Value to fill (here: mean of age column)\n",
    "#   - inplace=True: Modifies DataFrame directly (False returns new Series)\n",
    "# - df_filled['Age'].mean(): Calculates mean (average) of age column\n",
    "#   - .mean(): Returns average of all non-missing values\n",
    "#   - Ignores NaN values automatically\n",
    "# Result: All missing ages replaced with average age\n",
    "df_filled['Age'].fillna(df_filled['Age'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and remove duplicate rows\n",
    "# .duplicated() finds rows that are exact duplicates\n",
    "# .drop_duplicates() removes them, keeping the first occurrence\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. Removing Duplicates\")\n",
    "print(\"Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# df_filled.duplicated().sum()\n",
    "# - df_filled.duplicated(): Returns boolean Series (True = duplicate row, False = unique)\n",
    "#   - Checks if each row is identical to a previous row\n",
    "#   - First occurrence marked as False, duplicates as True\n",
    "#   - Parameters:\n",
    "#     - subset=None: Check all columns (can specify columns to check)\n",
    "#     - keep='first' (default): Mark first as False, rest as True\n",
    "# - .sum(): Counts True values (number of duplicate rows)\n",
    "num_duplicates = df_filled.duplicated().sum()\n",
    "print(f\"\\nğŸ” Number of duplicates: {num_duplicates}\")\n",
    "print(f\"Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {num_duplicates}\")\n",
    "\n",
    "# df_filled.drop_duplicates()\n",
    "# - Removes duplicate rows, keeps first occurrence\n",
    "# - Returns new DataFrame (doesn't modify original unless inplace=True)\n",
    "# - Parameters:\n",
    "#   - subset=None: Check all columns for duplicates\n",
    "#   - keep='first' (default): Keep first, remove rest ('last' keeps last, False removes all)\n",
    "#   - inplace=False: Return new DataFrame\n",
    "df_no_duplicates = df_filled.drop_duplicates()\n",
    "print(f\"\\nâœ… Rows after removing duplicates: {df_no_duplicates.shape[0]}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {df_no_duplicates.shape[0]}\")\n",
    "print(f\"   Removed {num_duplicates} duplicate row(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill categorical columns with mode\n",
    "# Ù…Ù„Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ÙØ¦ÙˆÙŠØ© Ø¨Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹\n",
    "\n",
    "# df_filled['Embarked'].fillna(df_filled['Embarked'].mode()[0], inplace=True)\n",
    "# - df_filled['Embarked']: Selects 'department' column\n",
    "# - .mode(): Returns Series with most frequent value(s)\n",
    "#   - Mode = most common value (for categorical data)\n",
    "#   - Returns Series (can have multiple modes if tie)\n",
    "# - [0]: Gets first mode value (if multiple modes, takes first)\n",
    "# - .fillna(): Fills missing values with mode\n",
    "# - inplace=True: Modifies DataFrame directly\n",
    "# Result: All missing departments replaced with most common department\n",
    "df_filled['Embarked'].fillna(df_filled['Embarked'].mode()[0], inplace=True)\n",
    "print(\"\\nMissing values after filling:\")\n",
    "print(\"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ù„Ø¡:\")\n",
    "print(df_filled.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR (Interquartile Range) Method for outlier detection\n",
    "# This is a statistical method that identifies values far from the median\n",
    "\n",
    "print(\"\\n--- IQR Method for Outlier Detection ---\")\n",
    "print(\"--- Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø±Ø¨ÙŠØ¹ÙŠ Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ---\")\n",
    "\n",
    "def detect_outliers_iqr(series):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method.\n",
    "    Returns True for outliers, False for normal values.\n",
    "    \"\"\"\n",
    "    # series.quantile(0.25)\n",
    "    # - Calculates 25th percentile (Q1) - value below which 25% of data falls\n",
    "    # - quantile(q): Returns value at quantile q (0.0 to 1.0)\n",
    "    # - 0.25 = 25th percentile, 0.5 = median, 0.75 = 75th percentile\n",
    "    Q1 = series.quantile(0.25)  # 25th percentile\n",
    "    \n",
    "    # series.quantile(0.75)\n",
    "    # - Calculates 75th percentile (Q3) - value below which 75% of data falls\n",
    "    Q3 = series.quantile(0.75)  # 75th percentile\n",
    "    \n",
    "    # IQR = Q3 - Q1\n",
    "    # - Interquartile Range: Spread of middle 50% of data\n",
    "    # - Measures variability, less sensitive to outliers than range\n",
    "    IQR = Q3 - Q1  # Interquartile Range\n",
    "    \n",
    "    # Q1 - 1.5 * IQR\n",
    "    # - Lower fence: Values below this are considered outliers\n",
    "    # - 1.5 is standard multiplier (can be adjusted)\n",
    "    lower_bound = Q1 - 1.5 * IQR  # Lower fence\n",
    "    \n",
    "    # Q3 + 1.5 * IQR\n",
    "    # - Upper fence: Values above this are considered outliers\n",
    "    upper_bound = Q3 + 1.5 * IQR  # Upper fence\n",
    "    \n",
    "    # (series < lower_bound) | (series > upper_bound)\n",
    "    # - Boolean indexing: Creates boolean Series (True = outlier, False = normal)\n",
    "    # - | : Logical OR operator (element-wise)\n",
    "    # - Returns True for values outside [lower_bound, upper_bound]\n",
    "    # Values outside the fences are outliers\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "print(\"   âœ… IQR function defined\")\n",
    "print(\"   This will identify values that are too far from the median\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Handling Outliers\n",
    "# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Handling Outliers\")\n",
    "print(\"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in salary\n",
    "print(\"\\nOutliers in salary column:\")\n",
    "print(\"Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ÙÙŠ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø±Ø§ØªØ¨:\")\n",
    "\n",
    "# detect_outliers_iqr(df_no_duplicates['Fare'])\n",
    "# - Calls function defined earlier\n",
    "# - df_no_duplicates['Fare']: Passes salary column as Series\n",
    "# - Returns boolean Series (True = outlier, False = normal)\n",
    "fare_outliers = detect_outliers_iqr(df_no_duplicates['Fare'])\n",
    "\n",
    "# fare_outliers.sum()\n",
    "# - Counts True values (number of outliers)\n",
    "print(f\"Number of outliers: {fare_outliers.sum()}\")\n",
    "\n",
    "# df_no_duplicates[fare_outliers][['name', 'salary']]\n",
    "# - df_no_duplicates[fare_outliers]: Boolean indexing - selects rows where fare_outliers is True\n",
    "#   - Boolean Series used as filter: True rows kept, False rows removed\n",
    "# - [['name', 'salary']]: Selects only 'name' and 'salary' columns\n",
    "#   - Double brackets [[]] returns DataFrame (single [] returns Series)\n",
    "# Result: Shows only outlier rows with name and salary columns\n",
    "print(df_no_duplicates[fare_outliers][['name', 'salary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "\n",
    "# df_no_duplicates[~salary_outliers].copy()\n",
    "# - ~salary_outliers: NOT operator (~) inverts boolean Series\n",
    "#   - True becomes False, False becomes True\n",
    "#   - Selects rows that are NOT outliers (keeps normal values)\n",
    "# - [~salary_outliers]: Boolean indexing - filters DataFrame\n",
    "# - .copy(): Creates independent copy (good practice)\n",
    "# Result: DataFrame with outliers removed\n",
    "df_clean = df_no_duplicates[~salary_outliers].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also remove impossible age values\n",
    "\n",
    "# df_clean[df_clean['age'] <= 100].copy()\n",
    "# - df_clean['age'] <= 100: Creates boolean Series (True if age <= 100, False otherwise)\n",
    "#   - Comparison operator (<=) applied element-wise to all values\n",
    "# - df_clean[boolean_series]: Boolean indexing - keeps rows where condition is True\n",
    "# - .copy(): Creates independent copy\n",
    "# Result: Keeps only rows where age is 100 or less (removes impossible ages like 150)\n",
    "df_clean = df_clean[df_clean['age'] <= 100].copy()\n",
    "print(f\"\\nRows after removing outliers: {df_clean.shape[0]}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©: {df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Data Type Conversion\n",
    "# ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. Data Type Conversion\")\n",
    "print(\"ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nData types before conversion:\")\n",
    "print(\"Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\")\n",
    "\n",
    "# df_clean.dtypes\n",
    "# - Returns Series showing data type of each column\n",
    "# - Common types: int64, float64, object (string), bool, datetime64\n",
    "# - Useful for checking if types are correct (e.g., numbers stored as strings)\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert experience_years to int (rounding)\n",
    "\n",
    "# df_clean['experience_years'].round().astype(int)\n",
    "# - df_clean['experience_years']: Selects 'experience_years' column\n",
    "# - .round(): Rounds decimal values to nearest integer\n",
    "#   - No parameter = rounds to 0 decimal places\n",
    "#   - .round(2) would round to 2 decimal places\n",
    "# - .astype(int): Converts data type to integer\n",
    "#   - astype(): Changes data type of Series\n",
    "#   - int: Integer type (int64 in pandas)\n",
    "#   - Alternative: .astype('int64') or .astype(float) for float\n",
    "# Result: Converts float values like 2.5 to integer 3 (after rounding)\n",
    "df_clean['experience_years'] = df_clean['experience_years'].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary of cleaning process\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. Cleaning Summary\")\n",
    "print(\"Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "original_rows = df.shape[0]\n",
    "final_rows = df_clean.shape[0]\n",
    "rows_removed = original_rows - final_rows\n",
    "\n",
    "print(f\"\\nğŸ“Š Original rows: {original_rows}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ØµÙ„ÙŠØ©: {original_rows}\")\n",
    "print(f\"âœ… Final cleaned rows: {final_rows}\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {final_rows}\")\n",
    "print(f\"\\nğŸ—‘ï¸  Rows removed: {rows_removed} ({(rows_removed/original_rows*100):.1f}%)\")\n",
    "print(f\"Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {rows_removed}\")\n",
    "\n",
    "print(\"\\nğŸ“„ Cleaned Data (first 10 rows):\")\n",
    "print(\"Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©:\")\n",
    "print(df_clean.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Example 2 Complete! âœ“\")\n",
    "print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 2! âœ“\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ“ What you accomplished:\")\n",
    "print(\"   âœ… Handled missing values (imputed with mean/mode)\")\n",
    "print(\"   âœ… Removed duplicate rows\")\n",
    "print(\"   âœ… Detected and removed outliers\")\n",
    "print(\"   âœ… Converted data types correctly\")\n",
    "print(\"   âœ… Created clean, model-ready data!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Decision Framework - When to Remove vs. When to Fix | Ø§Ù„Ø®Ø·ÙˆØ© 6: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ Ù†Ø­Ø°Ù ÙˆÙ…ØªÙ‰ Ù†ØµÙ„Ø­\n",
    "\n",
    "**BEFORE**: You've learned different cleaning methods, but when should you use each one?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose the right cleaning method for any situation!\n",
    "\n",
    "**Why this matters**: Making the wrong cleaning decision can:\n",
    "- **Remove too much data** â†’ Lose valuable information\n",
    "- **Keep bad data** â†’ Break your models\n",
    "- **Use wrong method** â†’ Introduce bias or errors\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Missing Values | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
    "\n",
    "**Key Question**: Should I **REMOVE** or **IMPUTE** missing values?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Is missing data < 5% of total?\n",
    "â”œâ”€ YES â†’ REMOVE (dropna)\n",
    "â”‚   â””â”€ Why? Small loss, keeps data \"pure\"\n",
    "â”‚\n",
    "â””â”€ NO â†’ Is missing data random or systematic?\n",
    "    â”œâ”€ RANDOM â†’ IMPUTE (fillna with mean/median/mode)\n",
    "    â”‚   â””â”€ Why? Random missing = no bias, safe to estimate\n",
    "    â”‚\n",
    "    â””â”€ SYSTEMATIC â†’ INVESTIGATE FIRST\n",
    "        â””â”€ Why? Systematic missing might indicate important pattern\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Remove** | < 5% missing, random | â€¢ No bias introduced<br>â€¢ Keeps data \"pure\"<br>â€¢ Simple | â€¢ Loses data<br>â€¢ Can't use if many missing | Age missing in 2% of records |\n",
    "| **Impute (Mean/Median)** | > 5% missing, numeric, random | â€¢ Keeps all rows<br>â€¢ Preserves sample size<br>â€¢ Works for numeric | â€¢ Can introduce bias<br>â€¢ Assumes normal distribution | Salary missing in 15% of records |\n",
    "| **Impute (Mode)** | > 5% missing, categorical, random | â€¢ Keeps all rows<br>â€¢ Preserves sample size<br>â€¢ Works for categories | â€¢ Can create artificial patterns<br>â€¢ May over-represent common values | Department missing in 10% of records |\n",
    "| **Investigate** | Systematic missing | â€¢ Finds root cause<br>â€¢ Prevents bias<br>â€¢ Better decisions | â€¢ Takes time<br>â€¢ Requires domain knowledge | All salaries missing for one department |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Outliers | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©\n",
    "\n",
    "**Key Question**: Should I **REMOVE** or **KEEP** outliers?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Is the outlier a data entry error?\n",
    "â”œâ”€ YES â†’ REMOVE\n",
    "â”‚   â””â”€ Example: Age = 150, Salary = 500000 (typo)\n",
    "â”‚\n",
    "â””â”€ NO â†’ Is the outlier a rare but valid event?\n",
    "    â”œâ”€ YES â†’ KEEP (but handle separately)\n",
    "    â”‚   â””â”€ Example: CEO salary in employee dataset\n",
    "    â”‚\n",
    "    â””â”€ NO â†’ Does it affect model performance?\n",
    "        â”œâ”€ YES â†’ REMOVE or TRANSFORM\n",
    "        â”‚   â””â”€ Example: Extreme values breaking linear regression\n",
    "        â”‚\n",
    "        â””â”€ NO â†’ KEEP\n",
    "            â””â”€ Example: Outlier in non-critical feature\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Remove** | Data entry errors, impossible values | â€¢ Removes noise<br>â€¢ Improves model accuracy<br>â€¢ Simple | â€¢ Loses information<br>â€¢ May remove valid rare events | Age = 150, Salary = 500000 |\n",
    "| **Cap/Clip** | Valid but extreme values | â€¢ Keeps data<br>â€¢ Reduces impact<br>â€¢ Preserves distribution | â€¢ Arbitrary threshold<br>â€¢ May hide important patterns | Cap salary at 99th percentile |\n",
    "| **Transform** | Skewed distributions | â€¢ Normalizes data<br>â€¢ Keeps all values<br>â€¢ Better for ML | â€¢ Changes interpretation<br>â€¢ More complex | Log transform for income |\n",
    "| **Keep** | Rare but valid events | â€¢ Preserves reality<br>â€¢ No information loss<br>â€¢ Important for analysis | â€¢ Can skew models<br>â€¢ May need special handling | CEO in employee dataset |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Duplicates | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
    "\n",
    "**Key Question**: Should I **REMOVE** all duplicates or **INVESTIGATE** first?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Are duplicates exact copies?\n",
    "â”œâ”€ YES â†’ REMOVE (keep first)\n",
    "â”‚   â””â”€ Why? No new information, wastes space\n",
    "â”‚\n",
    "â””â”€ NO â†’ Are duplicates near-duplicates (typos)?\n",
    "    â”œâ”€ YES â†’ FIX (merge or correct)\n",
    "    â”‚   â””â”€ Why? Same entity, different spelling\n",
    "    â”‚\n",
    "    â””â”€ NO â†’ Are duplicates valid (same person, different records)?\n",
    "        â””â”€ YES â†’ KEEP (but flag for analysis)\n",
    "            â””â”€ Why? Important information (e.g., repeat customers)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: E-commerce Dataset\n",
    "- **Missing values in \"price\"**: 20% missing\n",
    "  - **Decision**: IMPUTE with median (too much to remove, random missing)\n",
    "  - **Reason**: Random missing prices, median preserves distribution\n",
    "\n",
    "#### Example 2: Medical Dataset\n",
    "- **Outlier in \"age\"**: One patient age = 200\n",
    "  - **Decision**: REMOVE (impossible value)\n",
    "  - **Reason**: Data entry error, no one lives to 200\n",
    "\n",
    "#### Example 3: Customer Dataset\n",
    "- **Missing values in \"email\"**: 3% missing\n",
    "  - **Decision**: REMOVE (small percentage)\n",
    "  - **Reason**: Email is critical, can't impute, small loss acceptable\n",
    "\n",
    "#### Example 4: Sales Dataset\n",
    "- **Outlier in \"revenue\"**: One sale = $1,000,000 (normal range: $10-$1000)\n",
    "  - **Decision**: INVESTIGATE FIRST\n",
    "  - **Reason**: Could be valid (enterprise sale) or error (extra zero)\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Always investigate first** - Understand WHY data is missing/outlier/duplicate\n",
    "2. **Consider data loss** - Removing >10% of data is usually too much\n",
    "3. **Consider bias** - Systematic missing/outliers may indicate important patterns\n",
    "4. **Test both approaches** - Sometimes try removing AND imputing, compare results\n",
    "5. **Document decisions** - Write down WHY you chose each method (for reproducibility)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario**: You have a dataset with:\n",
    "- 8% missing values in \"income\" column\n",
    "- 2 outliers in \"age\" (ages 0 and 200)\n",
    "- 5 duplicate rows\n",
    "\n",
    "**Your task**: Decide what to do for each issue and explain why!\n",
    "\n",
    "**Answers**:\n",
    "1. **Income (8% missing)**: IMPUTE with median (too much to remove, likely random)\n",
    "2. **Age outliers**: REMOVE (impossible values - data entry errors)\n",
    "3. **Duplicates**: REMOVE (exact copies, no new information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Example: Comparing Different Cleaning Approaches\n",
    "# Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ: Ù…Ù‚Ø§Ø±Ù†Ø© Ø·Ø±Ù‚ Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Practical Example: Decision-Making in Action\")\n",
    "print(\"Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ: Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø± ÙÙŠ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a scenario with different data quality issues\n",
    "print(\"\\nğŸ“Š Scenario: Dataset with multiple issues\")\n",
    "print(\"   - 15% missing values in 'income' (too much to remove)\")\n",
    "print(\"   - 2% missing values in 'email' (can remove)\")\n",
    "print(\"   - 1 outlier: age = 200 (impossible, should remove)\")\n",
    "print(\"   - 3 duplicates (should remove)\")\n",
    "\n",
    "# Simulate the decision-making process\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 1: Income missing (15%)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   âŒ Can't remove: Would lose 15% of data (too much!)\")\n",
    "print(\"   âœ… Should impute: Use median (preserves distribution)\")\n",
    "print(\"   ğŸ“ Reason: Random missing, large percentage, numeric data\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 2: Email missing (2%)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   âœ… Can remove: Only 2% loss (acceptable)\")\n",
    "print(\"   âŒ Can't impute: Email is unique, can't estimate\")\n",
    "print(\"   ğŸ“ Reason: Small percentage, critical field, can't fill\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 3: Age outlier (age = 200)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   âœ… Should remove: Impossible value (data entry error)\")\n",
    "print(\"   âŒ Can't keep: Would break age-based models\")\n",
    "print(\"   ğŸ“ Reason: No human lives to 200, clearly an error\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DECISION 4: Duplicates (3 rows)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   âœ… Should remove: Exact duplicates, no new information\")\n",
    "print(\"   ğŸ“ Reason: Wastes space, can bias models (same data counted twice)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Decision Framework Applied Successfully!\")\n",
    "print(\"ØªÙ… ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert age to int\n",
    "\n",
    "# df_clean['age'].round().astype(int)\n",
    "# - df_clean['age']: Selects 'age' column\n",
    "# - .round(): Rounds decimal values (e.g., 40.5 â†’ 41)\n",
    "# - .astype(int): Converts to integer type\n",
    "# Result: Converts float ages to integer ages\n",
    "df_clean['age'] = df_clean['age'].round().astype(int)\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(\"Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„:\")\n",
    "\n",
    "# df_clean.dtypes\n",
    "# - Shows data types after conversion\n",
    "# - Should show int64 for age and experience_years now\n",
    "print(df_clean.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
