{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Linear Regression | Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Data Loading and Exploration** - Know your data\n",
    "- âœ… **Example 2: Data Cleaning** - Have clean data\n",
    "- âœ… **Example 3: Data Preprocessing** - Have preprocessed data ready\n",
    "- âœ… **Basic math**: Understanding of lines, slopes, equations\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why we need regression\n",
    "- Knowing how to evaluate model performance\n",
    "- Understanding the difference between simple and multiple regression\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the FOURTH example** - it's your first machine learning model!\n",
    "\n",
    "**Why this example FOURTH?**\n",
    "- **Before** you can build ML models, you need clean, preprocessed data\n",
    "- **Before** you can predict, you need to understand the simplest model first\n",
    "- **Before** you can use complex models, you need to master the basics\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: Data Loading (we know our data)\n",
    "- ğŸ““ Example 2: Data Cleaning (we have clean data)\n",
    "- ğŸ““ Example 3: Data Preprocessing (we have ML-ready data)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 5: Polynomial Regression (extends linear regression)\n",
    "- ğŸ““ Unit 2: Advanced Regression (Ridge, Lasso)\n",
    "- ğŸ““ All ML models (linear regression is the foundation!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Linear regression is the **simplest ML model** (easy to understand)\n",
    "2. Linear regression teaches you **model evaluation** (essential for all ML)\n",
    "3. Linear regression shows you **the ML workflow** (fit, predict, evaluate)\n",
    "\n",
    "**ğŸ“š Related Content:**\n",
    "- **Course 02, Notebook 5**: For an introduction to ML concepts and how linear regression fits into the broader AI landscape, see `Course 02/NOTEBOOKS/05_AI_Learning_Models.ipynb`\n",
    "- **Why both exist**: Course 02 introduces ML concepts at a high level. This Course 04 example provides **detailed, hands-on implementation** with full ML pipeline (data processing, evaluation, visualization).\n",
    "- **ğŸ“– Course Navigation**: For a complete guide to navigating between courses and understanding duplications, see `COURSE_MAP.md` in the root directory.\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Finding the Best Line | Ø§Ù„Ù‚ØµØ©: Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙØ¶Ù„ Ø®Ø·\n",
    "\n",
    "Imagine you're trying to predict house prices. **Before** using linear regression, you guess randomly or use simple averages. **After** learning linear regression, you find the best line that predicts price based on size - much more accurate!\n",
    "\n",
    "Same with machine learning: **Before** building models, we have data but no predictions. **After** linear regression, we can predict continuous values (like prices) from features (like size)!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Linear Regression Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠØŸ\n",
    "\n",
    "Linear regression is the foundation of machine learning:\n",
    "- **Simplest ML Model**: Easy to understand and interpret\n",
    "- **Fast and Efficient**: Works quickly even on large datasets\n",
    "- **Interpretable**: You can see exactly how features affect predictions\n",
    "- **Foundation**: Many advanced models build on linear regression concepts\n",
    "- **Real-World Use**: Used in finance, healthcare, marketing, and more\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Build simple linear regression (one feature)\n",
    "2. Build multiple linear regression (multiple features)\n",
    "3. Evaluate models using MSE, MAE, and RÂ²\n",
    "4. Visualize regression results and residuals\n",
    "5. Understand feature importance from coefficients\n",
    "6. Know when linear regression is appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us build and evaluate linear regression models\n",
    "\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np   # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For visualizations\n",
    "import seaborn as sns  # For beautiful plots\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.linear_model import LinearRegression  # The regression model!\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,    # MSE - measures average squared error\n",
    "    mean_absolute_error,   # MAE - measures average absolute error\n",
    "    r2_score              # RÂ² - measures how well model fits (0-1, higher is better)\n",
    ")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each tool does:\")\n",
    "print(\"   - LinearRegression: Builds the regression model\")\n",
    "print(\"   - train_test_split: Splits data for training and testing\")\n",
    "print(\"   - MSE/MAE/RÂ²: Metrics to evaluate model performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We have clean, preprocessed data, but we can't make predictions yet.\n",
    "\n",
    "**AFTER**: We'll build our first ML model - linear regression - to predict continuous values (like prices) from features (like size)!\n",
    "\n",
    "**Why this matters**: Linear regression is the foundation of ML. Master this, and you understand how all ML models work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Example 4: Linear Regression\")\n",
    "print(\"Ù…Ø«Ø§Ù„ 4: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Simple Linear Regression | Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ·\n",
    "\n",
    "**BEFORE**: We have one feature (house size) and want to predict price, but we don't know the relationship.\n",
    "\n",
    "**AFTER**: We'll find the best line (y = mx + b) that predicts price from size!\n",
    "\n",
    "**Why start with simple regression?**\n",
    "- **One feature**: Easy to understand and visualize\n",
    "- **Linear relationship**: Price = slope Ã— size + intercept\n",
    "- **Foundation**: Once you understand this, multiple regression is easy\n",
    "- **Interpretable**: You can see exactly how size affects price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. Simple Linear Regression (One Feature)\")\n",
    "print(\"Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ· (Ù…ÙŠØ²Ø© ÙˆØ§Ø­Ø¯Ø©)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Split Data into Train and Test? | Ù„Ù…Ø§Ø°Ø§ Ù†Ù‚Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±ØŸ\n",
    "\n",
    "**BEFORE**: We have all our data (features X and target y), but we can't use all of it for training!\n",
    "\n",
    "**AFTER**: We'll split data into training set (80%) and test set (20%) to properly evaluate our model!\n",
    "\n",
    "**Why split data?**\n",
    "\n",
    "**The Problem**: If we train AND test on the same data, the model will \"memorize\" the data instead of learning patterns!\n",
    "- **Example**: Like a student who memorizes answers to specific questions\n",
    "- **Result**: Model gets perfect scores on training data, but fails on new data\n",
    "- **This is called \"Overfitting\"** - model memorizes instead of learning\n",
    "\n",
    "**The Solution**: Split data into TWO separate sets:\n",
    "1. **Training Set (X_train, y_train)** - 80% of data\n",
    "   - **Purpose**: Model LEARNS from this data\n",
    "   - **What happens**: Model sees features (X_train) and correct answers (y_train)\n",
    "   - **Process**: Model finds the best line that fits this data\n",
    "   - **Like**: Student studying from a textbook\n",
    "\n",
    "2. **Test Set (X_test, y_test)** - 20% of data\n",
    "   - **Purpose**: Model is EVALUATED on this data\n",
    "   - **What happens**: Model sees features (X_test) but NOT answers (y_test)\n",
    "   - **Process**: Model makes predictions, we compare with actual answers\n",
    "   - **Like**: Student taking an exam (unseen questions)\n",
    "\n",
    "**Why this works:**\n",
    "- Model learns patterns from training data (not memorizing)\n",
    "- Test data is \"unseen\" - model hasn't seen it during training\n",
    "- If model performs well on test data â†’ model learned general patterns!\n",
    "- If model performs poorly on test data â†’ model overfitted (memorized training data)\n",
    "\n",
    "**What are X_train, X_test, y_train, y_test?**\n",
    "- **X_train**: Training features (inputs) - what model learns from\n",
    "- **y_train**: Training targets (outputs) - correct answers for training\n",
    "- **X_test**: Test features (inputs) - what model predicts on\n",
    "- **y_test**: Test targets (outputs) - correct answers for evaluation\n",
    "\n",
    "**Additional Notes:**\n",
    "\n",
    "**Why 80/20 split?**\n",
    "- Good balance - need enough training data (80%) to learn, enough test data (20%) to evaluate\n",
    "- Can adjust: Small datasets (<1000) â†’ 70/30, Large datasets (>10k) â†’ 90/10\n",
    "\n",
    "**Can I use test data for training?**\n",
    "- NO! Never use test data for training - this defeats the purpose!\n",
    "- Problem: If model sees test data during training, it's not a fair test\n",
    "- Rule: Test data should be \"locked away\" until final evaluation\n",
    "\n",
    "**What if I need more training data?**\n",
    "- Use cross-validation (Unit 2) - splits data multiple ways without wasting test set\n",
    "\n",
    "**Why split X and y separately?**\n",
    "- X (features) and y (target) must stay together!\n",
    "- Each row in X_train corresponds to same row in y_train\n",
    "- train_test_split keeps them aligned automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real-world California Housing dataset for linear regression\n",
    "# This is REAL data from the 1990 California census\n",
    "# We'll use Median Income to predict Median House Value (clear linear relationship)\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "print(\"ğŸ“¥ Loading California Housing dataset...\")\n",
    "housing_data = fetch_california_housing()\n",
    "\n",
    "# Create DataFrame from the real data\n",
    "df_housing = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\n",
    "df_housing['MedHouseVal'] = housing_data.target\n",
    "\n",
    "# For simple linear regression, we'll use MedInc (Median Income) to predict MedHouseVal\n",
    "# This is a real-world scenario: income affects housing prices\n",
    "# We'll use a sample of 300 points for clearer visualization\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(df_housing), 300, replace=False)\n",
    "df_simple = df_housing.iloc[sample_indices][['MedInc', 'MedHouseVal']].copy()\n",
    "df_simple.columns = ['size', 'price']  # Rename for consistency with teaching flow\n",
    "# Note: 'size' = MedInc (Median Income), 'price' = MedHouseVal (Median House Value in $100,000s)\n",
    "\n",
    "# pd.DataFrame(data)\n",
    "# - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "# - This is REAL data from sklearn, not generated!\n",
    "# - Returns DataFrame with real housing data\n",
    "\n",
    "print(\"\\nâœ… Real-world California Housing data loaded!\")\n",
    "print(f\"   ğŸ“Š Using {len(df_simple)} samples from {len(df_housing):,} total housing districts\")\n",
    "print(f\"   ğŸ“ˆ Feature: MedInc (Median Income) - predictor variable\")\n",
    "print(f\"   ğŸ’° Target: MedHouseVal (Median House Value in $100,000s)\")\n",
    "print(f\"\\nğŸ“Š Sample Data:\")\n",
    "print(df_simple.head())\n",
    "print(f\"\\nğŸ“ Data Shape: {df_simple.shape}\")\n",
    "print(\"\\nğŸ” Notice:\")\n",
    "print(\"   - This is REAL data from 1990 California census\")\n",
    "print(\"   - As income increases, house values generally increase (linear relationship)\")\n",
    "print(\"   - Real data has natural noise and variation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "# X = features (what we use to predict) - in this case, just house size\n",
    "# y = target (what we want to predict) - house price\n",
    "X = df_simple[['size']]\n",
    "y = df_simple['price']\n",
    "print(f\"   Features (X) shape: {X.shape}\")\n",
    "print(f\"   Target (y) shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "# Why split? We train on training data, then evaluate on unseen test data\n",
    "# This tells us if our model will work on new data (generalization)\n",
    "# train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# - Splits data into training and testing sets\n",
    "# - X: Features (input variables), y: Target (output variable)\n",
    "# - test_size=0.2: 20% for testing, 80% for training\n",
    "# - random_state=42: Seed for reproducibility (same split every time)\n",
    "# - stratify=y: Maintains class distribution in train/test (for classification)\n",
    "# - Returns: X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"   Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the linear regression model\n",
    "# LinearRegression() creates an empty model\n",
    "# .fit() trains the model - it finds the best line (slope and intercept)\n",
    "\n",
    "# LinearRegression()\n",
    "# - Creates linear regression model object\n",
    "# - Linear regression: Finds best line (y = mx + b) to fit data\n",
    "#   - m = slope (coefficient)\n",
    "#   - b = intercept (bias)\n",
    "# - Model is empty until .fit() is called\n",
    "model_simple = LinearRegression()\n",
    "\n",
    "# model_simple.fit(X_train, y_train)\n",
    "# - .fit(): Trains the model on training data\n",
    "# - X_train: Training features (input variables)\n",
    "# - y_train: Training targets (output variables)\n",
    "# - Process:\n",
    "#   1. Model learns best slope and intercept\n",
    "#   2. Finds line that minimizes prediction errors\n",
    "#   3. Stores learned parameters in model object\n",
    "# - After fit: model.coef_ (slope) and model.intercept_ (bias) are set\n",
    "# - Returns: self (model object, for method chaining)\n",
    "model_simple.fit(X_train, y_train)\n",
    "print(\"   âœ… Model trained!\")\n",
    "print(\"   The model learned the best line to predict price from size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on both training and test sets\n",
    "# .predict() uses the learned line to predict prices for new sizes\n",
    "# Why predict on both? Compare training vs test performance to check for overfitting\n",
    "y_train_pred = model_simple.predict(X_train)\n",
    "y_test_pred = model_simple.predict(X_test)\n",
    "print(\"   âœ… Predictions made!\")\n",
    "print(f\"   Training predictions: {len(y_train_pred)}\")\n",
    "print(f\"   Test predictions: {len(y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters - the learned line equation\n",
    "# Intercept (bias): The base price when size = 0 (not realistic, but part of the line)\n",
    "# Coefficient (slope): How much price increases per unit of size\n",
    "print(\"\\nğŸ“Š Model Parameters (The Learned Line):\")\n",
    "print(\"Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø§Ù„Ø®Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…):\")\n",
    "# model_simple.intercept_\n",
    "# - intercept_: The y-intercept (bias term) of the regression line\n",
    "#   - Value when X = 0 (base price in this case)\n",
    "#   - Part of equation: y = coef_ * X + intercept_\n",
    "# - Access as attribute (not method, no parentheses)\n",
    "print(f\"   Intercept (bias): ${model_simple.intercept_:,.2f}\")\n",
    "\n",
    "# model_simple.coef_[0]\n",
    "# - coef_: Array of coefficients (slopes) for each feature\n",
    "#   - For simple regression: one coefficient (slope)\n",
    "#   - For multiple regression: one coefficient per feature\n",
    "# - [0]: Gets first coefficient (for simple regression, there's only one)\n",
    "# - Interpretation: How much y changes when X increases by 1 unit\n",
    "# - Access as attribute (not method)\n",
    "print(f\"   Coefficient (slope): ${model_simple.coef_[0]:.4f} per sq ft\")\n",
    "print(f\"\\n   Equation: Price = {model_simple.coef_[0]:.2f} Ã— Size + {model_simple.intercept_:,.2f}\")\n",
    "print(f\"   Interpretation: For every 1 sq ft increase, price increases by ${model_simple.coef_[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance using multiple metrics\n",
    "# Why multiple metrics? Each tells us something different:\n",
    "# - MSE: Penalizes large errors more (squared)\n",
    "# - MAE: Average error in dollars (easier to interpret)\n",
    "# - RÂ²: How well model fits (0-1, 1 = perfect, 0 = no better than average)\n",
    "\n",
    "\n",
    "# mean_squared_error(y_true, y_pred)\n",
    "# - Measures Mean Squared Error (MSE) - average squared error\n",
    "# - Formula: average of (actual - predicted)Â²\n",
    "# - Penalizes large errors more (squared term)\n",
    "# - Lower is better (0 = perfect predictions)\n",
    "# - Units: squared units of target (e.g., $Â² for prices)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# mean_absolute_error(y_true, y_pred)\n",
    "# - Calculates Mean Absolute Error (MAE)\n",
    "# - Formula: average of |actual - predicted|\n",
    "# - Easier to interpret than MSE (same units as target)\n",
    "# - Less sensitive to outliers than MSE\n",
    "# - Lower is better (0 = perfect predictions)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# r2_score(y_true, y_pred)\n",
    "# - Calculates RÂ² (R-squared) score\n",
    "# - Measures how well model fits data\n",
    "# - Range: -âˆ to 1 (1 = perfect, 0 = no better than average, negative = worse)\n",
    "# - Formula: 1 - (sum of squared errors) / (sum of squared deviations from mean)\n",
    "# - Higher is better\n",
    "# - Interpretation: % of variance explained by model\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nğŸ“Š Training Metrics (How well model learned):\")\n",
    "print(\"Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
    "print(f\"   MSE: ${train_mse:,.2f} (lower is better)\")\n",
    "print(f\"   MAE: ${train_mae:,.2f} (average error in dollars)\")\n",
    "print(f\"   RÂ² Score: {train_r2:.4f} (closer to 1 is better)\")\n",
    "\n",
    "print(\"\\nğŸ“Š Test Metrics (How well model generalizes):\")\n",
    "print(\"Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
    "print(f\"   MSE: ${test_mse:,.2f} (lower is better)\")\n",
    "print(f\"   MAE: ${test_mae:,.2f} (average error in dollars)\")\n",
    "print(f\"   RÂ² Score: {test_r2:.4f} (closer to 1 is better)\")\n",
    "\n",
    "# Check for overfitting\n",
    "if test_r2 > train_r2 * 0.95:\n",
    "    print(\"\\n   âœ… Good! Test RÂ² is close to training RÂ² - model generalizes well!\")\n",
    "else:\n",
    "    print(\"\\n   âš ï¸  Warning: Test RÂ² is much lower - possible overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Simple Linear Regression | ØªØµÙˆØ± Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ·\n",
    "\n",
    "**BEFORE**: We've built the model and made predictions, but we need to see how well it fits the data!\n",
    "\n",
    "**AFTER**: We'll create plots showing the actual data points and the regression line!\n",
    "\n",
    "**What you'll see in the plots:**\n",
    "- **Points (scatter)**: The actual data points (real house sizes and prices)\n",
    "  - Blue points = Training data (what the model learned from)\n",
    "  - Green points = Test data (what the model predicts on)\n",
    "- **Red Line**: The regression line (model's predictions)\n",
    "  - Shows how the model predicts price from size\n",
    "  - If points are close to the line = good predictions!\n",
    "  - If points are far from the line = poor predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "# axes[0].scatter() - Creates POINTS showing actual data\n",
    "#   - X_train: House sizes (x-axis)\n",
    "#   - y_train: Actual prices (y-axis)\n",
    "#   - Points = Real data points (what actually happened)\n",
    "#\n",
    "# axes[0].plot() - Creates LINE showing model predictions\n",
    "#   - X_train: House sizes (x-axis)\n",
    "#   - y_train_pred: Predicted prices (y-axis)\n",
    "#   - Red line = Model's predictions (what model thinks prices should be)\n",
    "#   - If points are close to line = good model!\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training data\n",
    "axes[0].scatter(X_train, y_train, alpha=0.6, label='Training Data')  # POINTS = actual data\n",
    "axes[0].plot(X_train, y_train_pred, 'r-', linewidth=2, label='Regression Line')  # LINE = predictions\n",
    "axes[0].set_xlabel('House Size (sq ft)')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].set_title('Simple Linear Regression - Training Data')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test data\n",
    "axes[1].scatter(X_test, y_test, alpha=0.6, label='Test Data', color='green')\n",
    "axes[1].plot(X_test, y_test_pred, 'r-', linewidth=2, label='Regression Line')\n",
    "axes[1].set_xlabel('House Size (sq ft)')\n",
    "axes[1].set_ylabel('Price ($)')\n",
    "axes[1].set_title('Simple Linear Regression - Test Data')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('simple_linear_regression.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nâœ“ Plot saved as 'simple_linear_regression.png'\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Multiple Linear Regression | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯\n",
    "\n",
    "**BEFORE**: We used one feature (size) to predict price, but real predictions use multiple features.\n",
    "\n",
    "**AFTER**: We'll use multiple features (size, bedrooms, age, location) to predict price - more accurate!\n",
    "\n",
    "**Why multiple regression?**\n",
    "- **More features = Better predictions**: Real-world problems have many factors\n",
    "- **Same concept**: Still finding a line, but in higher dimensions\n",
    "- **Feature importance**: We can see which features matter most\n",
    "- **Real-world use**: Most ML problems use multiple features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Multiple Linear Regression (Multiple Features)\")\n",
    "print(\"Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ (Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use real-world California Housing data with multiple features\n",
    "# Real house prices depend on many factors - income, age, rooms, population!\n",
    "# This is REAL data from the 1990 California census\n",
    "\n",
    "# We'll use the full California Housing dataset for multiple regression\n",
    "# Select 4 meaningful features for demonstration\n",
    "# Using a sample of 500 points for better performance and visualization\n",
    "np.random.seed(42)\n",
    "sample_indices_m = np.random.choice(len(df_housing), 500, replace=False)\n",
    "df_multiple = df_housing.iloc[sample_indices_m][['MedInc', 'HouseAge', 'AveRooms', 'Population', 'MedHouseVal']].copy()\n",
    "\n",
    "# Rename columns for consistency with teaching flow\n",
    "df_multiple.columns = ['size', 'age', 'rooms', 'population', 'price']\n",
    "# Note: 'size' = MedInc (Median Income), 'age' = HouseAge, 'rooms' = AveRooms, \n",
    "#       'population' = Population, 'price' = MedHouseVal\n",
    "\n",
    "# pd.DataFrame(data)\n",
    "# - This is REAL data from sklearn, not generated!\n",
    "# - Returns DataFrame with real housing data\n",
    "\n",
    "print(f\"\\nâœ… Real-world California Housing data loaded for multiple regression!\")\n",
    "print(f\"   ğŸ“Š Using {len(df_multiple)} samples from {len(df_housing):,} total housing districts\")\n",
    "print(f\"   ğŸ“ˆ Features: MedInc (Income), HouseAge, AveRooms, Population\")\n",
    "print(f\"   ğŸ’° Target: MedHouseVal (Median House Value in $100,000s)\")\n",
    "print(f\"\\nğŸ“„ First 5 rows:\")\n",
    "print(df_multiple.head())\n",
    "print(f\"\\nğŸ” Notice:\")\n",
    "print(\"   - This is REAL data from 1990 California census\")\n",
    "print(\"   - Multiple features affect house value (income, age, rooms, population)\")\n",
    "print(\"   - Real-world scenario: House prices depend on many factors!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is already loaded from California Housing dataset above\n",
    "# The 'price' column (MedHouseVal) is already included in df_multiple\n",
    "# No need to generate synthetic data - we have real data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Features (X) and Target (y) | Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª (X) ÙˆØ§Ù„Ù‡Ø¯Ù (y)\n",
    "\n",
    "**BEFORE**: We have data with multiple columns, but we need to separate what we use to predict (features) from what we want to predict (target).\n",
    "\n",
    "**AFTER**: We'll prepare X (features) and y (target) for the model!\n",
    "\n",
    "**Why 4 features for X and 1 feature for y?**\n",
    "\n",
    "- **X (Features) = Multiple inputs** that affect house value:\n",
    "  - `size` (MedInc): Median income affects house value (higher income areas = more expensive)\n",
    "  - `age` (HouseAge): House age affects value (newer houses = more expensive)\n",
    "  - `rooms` (AveRooms): Average rooms affects value (more rooms = more expensive)\n",
    "  - `population`: Population affects value (densely populated areas may differ)\n",
    "  - We use **MULTIPLE features** because house value depends on **MANY factors**!\n",
    "\n",
    "- **y (Target) = Single output** we want to predict:\n",
    "  - `price` (MedHouseVal): The **ONE thing** we're trying to predict (median house value)\n",
    "  - We predict **ONE value** (house value) from **MULTIPLE inputs** (features)\n",
    "  - This is the standard ML pattern: **Multiple inputs â†’ Single output**\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "MedHouseVal = intercept + (coef1 Ã— MedInc) + (coef2 Ã— HouseAge)\n",
    " + (coef3 Ã— AveRooms) + (coef4 Ã— Population)\n",
    "y = bâ‚€ + (bâ‚ Ã— xâ‚) + (bâ‚‚ Ã— xâ‚‚) + (bâ‚ƒ Ã— xâ‚ƒ) + (bâ‚„ Ã— xâ‚„)\n",
    "```\n",
    "\n",
    "**What if I want to predict MORE than one target?**\n",
    "\n",
    "You have **TWO options**:\n",
    "\n",
    "1. **Build SEPARATE models** (one for each target) - **Recommended for beginners**\n",
    "   - Model 1: Predict `price` from features â†’ yâ‚ = price\n",
    "   - Model 2: Predict `days_on_market` from features â†’ yâ‚‚ = days_on_market\n",
    "   - **Why?** Each target might depend on features differently\n",
    "   - **Example**: Price might depend more on size, days_on_market might depend more on location\n",
    "   - This is called \"Multi-Output Regression\" (separate models)\n",
    "\n",
    "2. **Use Multi-Output Regression** (one model, multiple outputs)\n",
    "   - `sklearn.linear_model.LinearRegression()` can handle multiple targets\n",
    "   - `y = [[price1, days1], [price2, days2], ...]` (2D array with multiple columns)\n",
    "   - Model learns separate coefficients for each target\n",
    "   - **Formula**:\n",
    "     ```\n",
    "     price = bâ‚€â‚ + bâ‚â‚Ã—size + bâ‚‚â‚Ã—bedrooms + ...\n",
    "     days  = bâ‚€â‚‚ + bâ‚â‚‚Ã—size + bâ‚‚â‚‚Ã—bedrooms + ...\n",
    "     ```\n",
    "   - **Use when**: Targets are related and you want one model\n",
    "\n",
    "**Recommendation**: Start with **separate models** (easier to understand and interpret). Use multi-output only if targets are strongly related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data for Multiple Regression | ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯\n",
    "\n",
    "**BEFORE**: We have features (X_multiple) and target (y_multiple), but need to split them!\n",
    "\n",
    "**AFTER**: We'll split into training and test sets - same concept as simple regression!\n",
    "\n",
    "**Why split?**\n",
    "- **Same reason as before**: Need separate training and test sets\n",
    "- **Training set**: Model learns from X_train_m and y_train_m\n",
    "- **Test set**: Model is evaluated on X_test_m and y_test_m\n",
    "- **Purpose**: Check if model generalizes to new data (not just memorizes)\n",
    "\n",
    "**What we get:**\n",
    "- **X_train_m**: Training features (size/MedInc, age/HouseAge, rooms/AveRooms, population) - 80% of data\n",
    "- **y_train_m**: Training targets (price) - 80% of data\n",
    "- **X_test_m**: Test features - 20% of data\n",
    "- **y_test_m**: Test targets - 20% of data\n",
    "\n",
    "**Note**: The `_m` suffix means \"multiple\" (multiple features), to distinguish from simple regression variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "# X = Features (input variables) - what we use to PREDICT\n",
    "# y = Target (output variable) - what we want to PREDICT\n",
    "\n",
    "X_multiple = df_multiple[['size', 'age', 'rooms', 'population']]  # 4 features: Income, HouseAge, AveRooms, Population\n",
    "y_multiple = df_multiple['price']  # 1 target: MedHouseVal\n",
    "\n",
    "# Show what we prepared\n",
    "print(\"\\nğŸ“Š Features (X) and Target (y) Prepared:\")\n",
    "print(\"Ø§Ù„Ù…ÙŠØ²Ø§Øª (X) ÙˆØ§Ù„Ù‡Ø¯Ù (y) Ø¬Ø§Ù‡Ø²Ø©:\")\n",
    "print(f\"\\n   Features (X) shape: {X_multiple.shape}\")\n",
    "print(f\"   - {X_multiple.shape[0]} samples, {X_multiple.shape[1]} features\")\n",
    "print(f\"   - Features: {list(X_multiple.columns)}\")\n",
    "print(f\"\\n   Target (y) shape: {y_multiple.shape}\")\n",
    "print(f\"   - {y_multiple.shape[0]} samples, 1 target\")\n",
    "print(f\"   - Target: price\")\n",
    "print(f\"\\n   âœ… Ready for model training!\")\n",
    "print(f\"   - Model will use {X_multiple.shape[1]} features to predict 1 target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "# train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# - Splits data into training and testing sets\n",
    "# - X: Features (input variables), y: Target (output variable)\n",
    "# - test_size=0.2: 20% for testing, 80% for training\n",
    "# - random_state=42: Seed for reproducibility (same split every time)\n",
    "# - stratify=y: Maintains class distribution in train/test (for classification)\n",
    "# - Returns: X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_multiple, y_multiple, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "model_multiple = LinearRegression()\n",
    "model_multiple.fit(X_train_m, y_train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred_m = model_multiple.predict(X_train_m)\n",
    "y_test_pred_m = model_multiple.predict(X_test_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "print(\"\\nModel Parameters:\")\n",
    "print(\"Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\")\n",
    "print(f\"Intercept: {model_multiple.intercept_:.2f}\")\n",
    "print(\"\\nCoefficients:\")\n",
    "print(\"Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:\")\n",
    "for feature, coef in zip(X_multiple.columns, model_multiple.coef_):\n",
    "    print(f\"  {feature}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "train_mse_m = mean_squared_error(y_train_m, y_train_pred_m)\n",
    "test_mse_m = mean_squared_error(y_test_m, y_test_pred_m)\n",
    "train_r2_m = r2_score(y_train_m, y_train_pred_m)\n",
    "test_r2_m = r2_score(y_test_m, y_test_pred_m)\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(\"Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
    "print(f\"  MSE: {train_mse_m:,.2f}\")\n",
    "print(f\"  RMSE: {np.sqrt(train_mse_m):,.2f}\")\n",
    "print(f\"  RÂ² Score: {train_r2_m:.4f}\")\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(\"Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
    "print(f\"  MSE: {test_mse_m:,.2f}\")\n",
    "print(f\"  RMSE: {np.sqrt(test_mse_m):,.2f}\")\n",
    "print(f\"  RÂ² Score: {test_r2_m:.4f}\")\n",
    "\n",
    "# Add interpretation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ’¡ Interpreting the Metrics | ØªÙØ³ÙŠØ± Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nğŸ“Š RÂ² Score (R-squared):\")\n",
    "print(f\"   - Training: {train_r2_m:.2%} | Test: {test_r2_m:.2%}\")\n",
    "print(f\"   - Range: -âˆ to 1.0 (higher is better)\")\n",
    "print(f\"   - 1.0 = Perfect predictions (all variance explained)\")\n",
    "print(f\"   - 0.0 = Model is as good as predicting the mean\")\n",
    "print(f\"   - <0.0 = Model is worse than just predicting the mean\")\n",
    "if test_r2_m >= 0.9:\n",
    "    print(f\"   - âœ… EXCELLENT! (>0.9 means model explains >90% of variance)\")\n",
    "elif test_r2_m >= 0.7:\n",
    "    print(f\"   - âœ… GOOD! (>0.7 means model explains >70% of variance)\")\n",
    "elif test_r2_m >= 0.5:\n",
    "    print(f\"   - âš ï¸  FAIR (>0.5 means model explains >50% of variance)\")\n",
    "else:\n",
    "    print(f\"   - âš ï¸  POOR (<0.5 means model explains <50% of variance)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š RMSE (Root Mean Squared Error):\")\n",
    "print(f\"   - Training: ${np.sqrt(train_mse_m):,.2f} | Test: ${np.sqrt(test_mse_m):,.2f}\")\n",
    "print(f\"   - Lower is better (measures average prediction error)\")\n",
    "print(f\"   - In same units as target (price in dollars)\")\n",
    "print(f\"   - Average prediction error: ${np.sqrt(test_mse_m):,.2f}\")\n",
    "print(f\"   - This means predictions are typically off by ${np.sqrt(test_mse_m):,.0f}\")\n",
    "\n",
    "print(f\"\\nğŸ” Comparing Train vs Test:\")\n",
    "if abs(train_r2_m - test_r2_m) < 0.05:\n",
    "    print(f\"   - âœ… Similar RÂ² ({train_r2_m:.2%} vs {test_r2_m:.2%}) - Good generalization!\")\n",
    "    print(f\"   - Model performs similarly on new data (not overfitting)\")\n",
    "else:\n",
    "    print(f\"   - âš ï¸  Different RÂ² ({train_r2_m:.2%} vs {test_r2_m:.2%})\")\n",
    "    if train_r2_m > test_r2_m:\n",
    "        print(f\"   - Training RÂ² is higher - possible overfitting!\")\n",
    "    else:\n",
    "        print(f\"   - Test RÂ² is higher - unusual but possible\")\n",
    "\n",
    "print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "print(f\"   - RÂ² shows how much variance the model explains\")\n",
    "print(f\"   - RMSE shows actual prediction error in dollars\")\n",
    "print(f\"   - Compare train vs test to check for overfitting\")\n",
    "print(f\"   - Good models have high RÂ² and low RMSE\")\n",
    "print(f\"   - RÂ² > 0.7 is generally considered good for real-world problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Multiple Linear Regression | ØªØµÙˆØ± Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯\n",
    "\n",
    "**BEFORE**: We've built a multiple regression model, but how do we visualize it with multiple features?\n",
    "\n",
    "**AFTER**: We'll plot predicted vs actual prices to see how well the model performs!\n",
    "\n",
    "**What you'll see in the plots:**\n",
    "- **Points (scatter)**: Each point = one house\n",
    "  - X-axis: Actual price (what the house actually sold for)\n",
    "  - Y-axis: Predicted price (what the model predicted)\n",
    "  - If points are close to the diagonal line = good predictions!\n",
    "  - If points are far from the diagonal line = poor predictions\n",
    "- **Red Dashed Line**: Perfect prediction line (y = x)\n",
    "  - If predictions were perfect, all points would be on this line\n",
    "  - Points above line = model over-predicted (predicted higher than actual)\n",
    "  - Points below line = model under-predicted (predicted lower than actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training predictions\n",
    "axes[0].scatter(y_train_m, y_train_pred_m, alpha=0.6)\n",
    "axes[0].plot([y_train_m.min(), y_train_m.max()],\n",
    "             [y_train_m.min(), y_train_m.max()], 'r--', linewidth=2)\n",
    "axes[0].set_xlabel('Actual Price ($)')\n",
    "axes[0].set_ylabel('Predicted Price ($)')\n",
    "axes[0].set_title(f'Training Set (RÂ² = {train_r2_m:.4f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test predictions\n",
    "axes[1].scatter(y_test_m, y_test_pred_m, alpha=0.6, color='green')\n",
    "axes[1].plot([y_test_m.min(), y_test_m.max()],\n",
    "             [y_test_m.min(), y_test_m.max()], 'r--', linewidth=2)\n",
    "axes[1].set_xlabel('Actual Price ($)')\n",
    "axes[1].set_ylabel('Predicted Price ($)')\n",
    "axes[1].set_title(f'Test Set (RÂ² = {test_r2_m:.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('multiple_linear_regression.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nâœ“ Plot saved as 'multiple_linear_regression.png'\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Residuals? | Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠØŸ\n",
    "\n",
    "**BEFORE**: We've evaluated our model with metrics (MSE, MAE, RÂ²), but there's another important way to check model quality!\n",
    "\n",
    "**AFTER**: We'll learn what residuals are and why they're important for diagnosing model problems!\n",
    "\n",
    "**What are Residuals? (Simple Explanation)**\n",
    "\n",
    "**Residuals = Prediction Errors = The Mistake the Model Made**\n",
    "\n",
    "Think of it like this:\n",
    "- You take a **test** (actual exam)\n",
    "- You **predict** what score you'll get\n",
    "- The **residual** = How wrong your prediction was!\n",
    "\n",
    "**In Machine Learning:**\n",
    "- **Actual Value** = What really happened (real house price)\n",
    "- **Predicted Value** = What the model guessed (predicted house price)\n",
    "- **Residual** = The difference (how wrong the model was)\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Residual = Actual Value - Predicted Value\n",
    "         = y_actual - y_predicted\n",
    "```\n",
    "\n",
    "**Real-World Example:**\n",
    "\n",
    "Imagine predicting house prices:\n",
    "- **House 1**: \n",
    "  - Actual price: $300,000\n",
    "  - Model predicted: $280,000\n",
    "  - **Residual**: $300,000 - $280,000 = **+$20,000**\n",
    "  - Meaning: Model **under-predicted** by $20,000 (predicted too low)\n",
    "\n",
    "- **House 2**:\n",
    "  - Actual price: $250,000\n",
    "  - Model predicted: $270,000\n",
    "  - **Residual**: $250,000 - $270,000 = **-$20,000**\n",
    "  - Meaning: Model **over-predicted** by $20,000 (predicted too high)\n",
    "\n",
    "- **House 3**:\n",
    "  - Actual price: $200,000\n",
    "  - Model predicted: $200,000\n",
    "  - **Residual**: $200,000 - $200,000 = **$0**\n",
    "  - Meaning: **Perfect prediction!** No error!\n",
    "\n",
    "**Key Points:**\n",
    "- **Positive residual** = Model predicted **too low** (actual > predicted)\n",
    "- **Negative residual** = Model predicted **too high** (actual < predicted)\n",
    "- **Residual = 0** = **Perfect prediction!**\n",
    "- **Small residual** = Good prediction âœ…\n",
    "- **Large residual** = Bad prediction âŒ\n",
    "\n",
    "**Why Check Residuals? (Why This Matters)**\n",
    "\n",
    "1. **See Individual Mistakes**: \n",
    "   - MSE/MAE/RÂ² give us **average** performance\n",
    "   - Residuals show us **each individual** prediction error\n",
    "   - Example: \"House 1 was off by $20k, House 2 was perfect, House 3 was off by $5k\"\n",
    "\n",
    "2. **Find Patterns**:\n",
    "   - If residuals are **random** = Good model âœ…\n",
    "   - If residuals show **patterns** (curves, trends) = Model has problems âŒ\n",
    "   - Patterns tell us **what's wrong** with the model\n",
    "\n",
    "3. **Diagnose Problems**:\n",
    "   - **Curved pattern** = Model can't capture non-linear relationships\n",
    "   - **Funnel shape** = Model errors get bigger for certain values\n",
    "   - **Outliers** = Some predictions are way off (need to check data)\n",
    "\n",
    "**What's the Difference Between Residuals and Other Metrics?**\n",
    "\n",
    "| Aspect | Residuals | MSE/MAE/RÂ² |\n",
    "|--------|-----------|------------|\n",
    "| **What they show** | **Individual errors** for each prediction | **Average/summary** of all errors |\n",
    "| **Example** | House 1: -$5k, House 2: +$20k, House 3: $0 | Average error: $10k |\n",
    "| **Use for** | **Diagnosing problems** (find patterns) | **Overall performance** (how good is model?) |\n",
    "| **Shows patterns?** | âœ… YES - can see curves, funnels, trends | âŒ NO - just one number |\n",
    "| **Shows outliers?** | âœ… YES - can see which predictions are way off | âŒ NO - outliers averaged out |\n",
    "| **Interpretation** | \"This house was off by $20k\" | \"Average error is $10k\" |\n",
    "\n",
    "**Real Example:**\n",
    "\n",
    "Imagine you predicted prices for 3 houses:\n",
    "- **House 1**: Actual $300k, Predicted $295k â†’ Residual = +$5k\n",
    "- **House 2**: Actual $250k, Predicted $270k â†’ Residual = -$20k  \n",
    "- **House 3**: Actual $200k, Predicted $200k â†’ Residual = $0\n",
    "\n",
    "**Metrics tell you:**\n",
    "- **MSE**: Average squared error = (5Â² + 20Â² + 0Â²)/3 = 141.67\n",
    "- **MAE**: Average absolute error = (5 + 20 + 0)/3 = $8.33k\n",
    "- **RÂ²**: How well model fits = 0.85 (85% variance explained)\n",
    "\n",
    "**Residuals tell you:**\n",
    "- House 1: Small error ($5k) âœ…\n",
    "- House 2: Large error ($20k) âŒ - **Why?** Need to investigate!\n",
    "- House 3: Perfect ($0) âœ…\n",
    "\n",
    "**Key Difference:**\n",
    "- **Metrics (MSE/MAE/RÂ²)**: Give you **one number** summarizing all predictions\n",
    "- **Residuals**: Give you **individual errors** for each prediction\n",
    "\n",
    "**Why Both Matter:**\n",
    "- **Use Metrics**: To answer \"Is my model good overall?\" (RÂ² = 0.85 â†’ good!)\n",
    "- **Use Residuals**: To answer \"Which predictions are wrong and why?\" (House 2 is way off â†’ check data!)\n",
    "- **Together**: Metrics show overall quality, residuals show where problems are!\n",
    "\n",
    "**Additional Notes:**\n",
    "\n",
    "**What does \"residual\" mean in simple words?**\n",
    "- **\"Residual\" = leftover error** = what the model got wrong\n",
    "- Like: You predict you'll get 90% on a test, but you actually got 85%\n",
    "- Residual = 85% - 90% = -5% (you over-predicted by 5%)\n",
    "- It's the \"leftover\" mistake after the model makes its prediction\n",
    "\n",
    "**Why check residuals if we already have MSE/MAE/RÂ²?**\n",
    "- Metrics give us **numbers** (average performance)\n",
    "- Residuals show us **patterns** (what's wrong)\n",
    "- Example: MSE might be low, but residuals show a curve â†’ model has problems\n",
    "- Residuals help diagnose **WHAT'S wrong**, not just **HOW wrong**\n",
    "\n",
    "**Should residuals be positive or negative?**\n",
    "- **Both!** Good models have residuals randomly scattered around 0\n",
    "- Some predictions too high (negative), some too low (positive)\n",
    "- Mean of residuals should be close to 0 (no systematic bias)\n",
    "- If all residuals are positive â†’ model always predicts too low\n",
    "- If all residuals are negative â†’ model always predicts too high\n",
    "\n",
    "**What's the difference between residuals and errors?**\n",
    "- They're **the same thing!** \n",
    "- \"Residual\" = technical term used in statistics\n",
    "- \"Error\" = more general term\n",
    "- Both mean: difference between actual and predicted\n",
    "\n",
    "**How do I know if residuals are good or bad?**\n",
    "- Good residuals:\n",
    "  - **Mean close to 0** (no bias)\n",
    "  - **Small spread** (consistent predictions)\n",
    "  - **Random pattern** (no curves or trends)\n",
    "- Bad residuals:\n",
    "  - **Mean far from 0** (systematic bias)\n",
    "  - **Large spread** (inconsistent predictions)\n",
    "  - **Patterns** (curves, funnels, trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Residuals | Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ\n",
    "\n",
    "**BEFORE**: We know what residuals are, now let's calculate them!\n",
    "\n",
    "**AFTER**: We'll compute residuals and check their statistics to see if our model is good!\n",
    "\n",
    "**What we'll calculate:**\n",
    "- **Residuals**: `residuals = y_test - y_test_pred`\n",
    "  - For each test sample: actual price - predicted price\n",
    "  - Positive = model under-predicted, Negative = model over-predicted\n",
    "- **Statistics**: Mean, standard deviation, min, max\n",
    "  - Mean close to 0 = no bias âœ…\n",
    "  - Small std = consistent predictions âœ…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Residuals | ØªØµÙˆØ± Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ\n",
    "\n",
    "**BEFORE**: We've calculated residual statistics, but we need to visualize them to see patterns!\n",
    "\n",
    "**AFTER**: We'll create plots to check if residuals are randomly distributed (good) or have patterns (indicates problems)!\n",
    "\n",
    "**Why visualize residuals?**\n",
    "- **Check assumptions**: Linear regression assumes residuals are random and normally distributed\n",
    "- **Detect patterns**: Patterns in residuals indicate model problems (non-linearity, heteroscedasticity)\n",
    "- **Diagnose issues**: Visual inspection helps identify what's wrong with the model\n",
    "- **Validate model**: Good models have randomly scattered residuals around zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residuals distribution\n",
    "axes[0].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Residuals')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Residuals Distribution')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[1].scatter(y_test_pred_m, residuals, alpha=0.6)\n",
    "axes[1].axhline(0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Residuals vs Predicted Values')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('residuals_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nâœ“ Plot saved as 'residuals_analysis.png'\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 4 Complete! âœ“\")\n",
    "print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 4! âœ“\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to Do When Predictions Are Too Low (or Too High) | Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ù‹Ø§ (Ø£Ùˆ Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ù‹Ø§)\n",
    "\n",
    "**BEFORE**: You've analyzed residuals and found that your model systematically under-predicts (or over-predicts). Now what?\n",
    "\n",
    "**AFTER**: You'll learn practical solutions to fix systematic bias in your predictions!\n",
    "\n",
    "**The Problem We Found:**\n",
    "- **Residual Mean**: 5748.12 (not close to 0)\n",
    "- **Interpretation**: Model tends to **UNDER-predict** (predictions are too low)\n",
    "- **Meaning**: On average, the model predicts prices that are $5,748 lower than actual prices\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Systematic bias** = Model consistently makes the same type of error\n",
    "- **Good models** should have residual mean close to 0 (no bias)\n",
    "- **Biased models** = Poor predictions, even if RÂ² is high\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Step 1: Diagnose the Problem | Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ´Ø®ÙŠØµ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©\n",
    "\n",
    "**Check Your Residuals:**\n",
    "\n",
    "1. **Residual Mean**:\n",
    "   - **Close to 0** (Â±small value) â†’ âœ… No bias, model is good\n",
    "   - **Positive mean** â†’ âš ï¸ Model UNDER-predicts (predictions too low)\n",
    "   - **Negative mean** â†’ âš ï¸ Model OVER-predicts (predictions too high)\n",
    "\n",
    "2. **Residual Patterns**:\n",
    "   - **Random scatter** â†’ âœ… Good (no patterns)\n",
    "   - **Curved pattern** â†’ âš ï¸ Non-linear relationship (need polynomial regression)\n",
    "   - **Funnel shape** â†’ âš ï¸ Heteroscedasticity (variance changes with predictions)\n",
    "\n",
    "3. **Residual Distribution**:\n",
    "   - **Normal distribution** â†’ âœ… Good (assumption met)\n",
    "   - **Skewed distribution** â†’ âš ï¸ Model bias or outliers\n",
    "\n",
    "**In Our Case:**\n",
    "- Mean = 5748.12 (positive) â†’ Model UNDER-predicts\n",
    "- Need to investigate WHY and fix it\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ› ï¸ Step 2: Solutions to Fix Systematic Bias | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø­Ù„ÙˆÙ„ Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ­ÙŠØ² Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠ\n",
    "\n",
    "#### Solution 1: Check for Missing Features | Ø§Ù„Ø­Ù„ 1: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
    "\n",
    "**Problem**: Model might be missing important features that affect the target.\n",
    "\n",
    "**Example:**\n",
    "- Predicting house price from size only\n",
    "- But price also depends on location, age, condition\n",
    "- Missing features â†’ Model can't capture full relationship â†’ Under-predicts\n",
    "\n",
    "**What to Do:**\n",
    "1. **Review your data**: Are there other features that affect the target?\n",
    "2. **Add relevant features**: Include features that logically affect predictions\n",
    "3. **Check feature importance**: Use domain knowledge to identify missing factors\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Before: Only using size\n",
    "X = df[['size']]\n",
    "\n",
    "# After: Using multiple relevant features\n",
    "X = df[['size', 'bedrooms', 'age', 'location_score', 'condition']]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Solution 2: Check for Data Quality Issues | Ø§Ù„Ø­Ù„ 2: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø´Ø§ÙƒÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**Problem**: Data issues can cause bias:\n",
    "- **Outliers**: Extreme values skew the model\n",
    "- **Missing values**: Incorrectly handled missing data\n",
    "- **Data leakage**: Using future information\n",
    "- **Wrong target values**: Incorrect labels in training data\n",
    "\n",
    "**What to Do:**\n",
    "1. **Check for outliers**: Plot data, look for extreme values\n",
    "2. **Handle missing values**: Impute or remove missing data properly\n",
    "3. **Verify target values**: Make sure y values are correct\n",
    "4. **Check data distribution**: Ensure training and test data are similar\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Check for outliers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(y_train)\n",
    "plt.title('Check for Outliers in Target')\n",
    "plt.show()\n",
    "\n",
    "# Remove outliers (if appropriate)\n",
    "Q1 = y_train.quantile(0.25)\n",
    "Q3 = y_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = (y_train < (Q1 - 1.5 * IQR)) | (y_train > (Q3 + 1.5 * IQR))\n",
    "X_train_clean = X_train[~outliers]\n",
    "y_train_clean = y_train[~outliers]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Solution 3: Try Feature Engineering | Ø§Ù„Ø­Ù„ 3: ØªØ¬Ø±Ø¨Ø© Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
    "\n",
    "**Problem**: Raw features might not capture relationships well.\n",
    "\n",
    "**What to Do:**\n",
    "1. **Create interaction features**: Multiply features together (e.g., size Ã— bedrooms)\n",
    "2. **Transform features**: Log, square, or other transformations\n",
    "3. **Create polynomial features**: Add squared or cubed terms\n",
    "4. **Normalize/scale features**: Ensure all features are on similar scales\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Create interaction features\n",
    "df['size_bedrooms'] = df['size'] * df['bedrooms']\n",
    "df['size_location'] = df['size'] * df['location_score']\n",
    "\n",
    "# Transform features (if needed)\n",
    "df['log_size'] = np.log(df['size'])\n",
    "\n",
    "# Use new features\n",
    "X = df[['size', 'bedrooms', 'size_bedrooms', 'size_location']]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Solution 4: Try Different Models | Ø§Ù„Ø­Ù„ 4: ØªØ¬Ø±Ø¨Ø© Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØªÙ„ÙØ©\n",
    "\n",
    "**Problem**: Linear regression might not be appropriate for your data.\n",
    "\n",
    "**What to Do:**\n",
    "1. **Polynomial Regression**: If relationship is curved (non-linear)\n",
    "2. **Ridge/Lasso Regression**: If you have many features (regularization)\n",
    "3. **Random Forest**: If relationship is complex and non-linear\n",
    "4. **XGBoost**: For best performance on complex patterns\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Try Polynomial Regression (for non-linear relationships)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Solution 5: Adjust the Intercept (Last Resort) | Ø§Ù„Ø­Ù„ 5: ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªÙ‚Ø§Ø·Ø¹ (Ø­Ù„ Ø£Ø®ÙŠØ±)\n",
    "\n",
    "**Problem**: Model intercept might be systematically wrong.\n",
    "\n",
    "**What to Do:**\n",
    "1. **Calculate mean residual**: This is the systematic bias\n",
    "2. **Adjust predictions**: Add mean residual to all predictions\n",
    "3. **Note**: This is a \"band-aid\" solution - better to fix the root cause!\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Calculate mean residual (systematic bias)\n",
    "mean_residual = residuals.mean()\n",
    "print(f\"Systematic bias: {mean_residual:.2f}\")\n",
    "\n",
    "# Adjust predictions\n",
    "y_test_pred_adjusted = y_test_pred + mean_residual\n",
    "\n",
    "# Recalculate metrics\n",
    "mse_adjusted = mean_squared_error(y_test, y_test_pred_adjusted)\n",
    "print(f\"Original MSE: {test_mse:.2f}\")\n",
    "print(f\"Adjusted MSE: {mse_adjusted:.2f}\")\n",
    "```\n",
    "\n",
    "**âš ï¸ Warning**: This doesn't fix the model - it just shifts predictions. Better to fix the root cause!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Decision Tree: Which Solution to Try? | Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±: Ø£ÙŠ Ø­Ù„ ØªØ¬Ø±Ø¨ØŸ\n",
    "\n",
    "**Start Here:**\n",
    "\n",
    "1. **Check residual mean**:\n",
    "   - If close to 0 â†’ âœ… Model is good, no action needed\n",
    "   - If far from 0 â†’ Continue to step 2\n",
    "\n",
    "2. **Check residual patterns**:\n",
    "   - **Curved pattern** â†’ Try Solution 4 (Polynomial Regression)\n",
    "   - **Random scatter** â†’ Continue to step 3\n",
    "\n",
    "3. **Check your features**:\n",
    "   - **Few features (<5)** â†’ Try Solution 1 (Add more features)\n",
    "   - **Many features (>10)** â†’ Try Solution 3 (Feature engineering)\n",
    "\n",
    "4. **Check data quality**:\n",
    "   - **Outliers present** â†’ Try Solution 2 (Fix data quality)\n",
    "   - **No outliers** â†’ Continue to step 5\n",
    "\n",
    "5. **Try different models**:\n",
    "   - **Linear relationship** â†’ Current model should work (check features)\n",
    "   - **Non-linear relationship** â†’ Try Solution 4 (Different models)\n",
    "\n",
    "6. **Last resort**:\n",
    "   - If nothing works â†’ Try Solution 5 (Adjust intercept)\n",
    "   - But remember: This is a band-aid, not a real fix!\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Residual mean â‰  0** = Systematic bias (model consistently wrong)\n",
    "2. **Positive mean** = Under-prediction (predictions too low)\n",
    "3. **Negative mean** = Over-prediction (predictions too high)\n",
    "4. **Fix root cause** = Better than adjusting predictions\n",
    "5. **Check features first** = Most common cause of bias\n",
    "6. **Try different models** = If linear regression isn't appropriate\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Next Steps | Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©\n",
    "\n",
    "**After fixing bias:**\n",
    "1. **Re-train model** with improved features/data\n",
    "2. **Re-evaluate** residuals (should be close to 0 now)\n",
    "3. **Check metrics** (MSE, MAE, RÂ² should improve)\n",
    "4. **Validate** on new data to ensure fix works\n",
    "\n",
    "**If bias persists:**\n",
    "- Consider that linear regression might not be appropriate\n",
    "- Try polynomial regression or other non-linear models\n",
    "- Consult domain experts about missing features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Example: Diagnosing and Addressing Systematic Bias\n",
    "# This demonstrates how to check for bias and try solutions\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. Fixing Systematic Bias in Predictions\")\n",
    "print(\"Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ­ÙŠØ² Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠ ÙÙŠ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Diagnose the bias\n",
    "print(\"\\nğŸ“Š Step 1: Diagnosing the Bias\")\n",
    "print(\"Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ´Ø®ÙŠØµ Ø§Ù„ØªØ­ÙŠØ²\")\n",
    "mean_residual = residuals.mean()\n",
    "std_residual = residuals.std()\n",
    "\n",
    "print(f\"\\n   Residual Mean: {mean_residual:.2f}\")\n",
    "print(f\"   Residual Std: {std_residual:.2f}\")\n",
    "\n",
    "# Determine if bias is significant\n",
    "bias_threshold = std_residual * 0.1  # 10% of std deviation\n",
    "if abs(mean_residual) > bias_threshold:\n",
    "    print(f\"\\n   âš ï¸  SIGNIFICANT BIAS DETECTED!\")\n",
    "    if mean_residual > 0:\n",
    "        print(f\"   - Model UNDER-predicts by ${mean_residual:,.2f} on average\")\n",
    "        print(f\"   - Predictions are consistently too LOW\")\n",
    "    else:\n",
    "        print(f\"   - Model OVER-predicts by ${abs(mean_residual):,.2f} on average\")\n",
    "        print(f\"   - Predictions are consistently too HIGH\")\n",
    "else:\n",
    "    print(f\"\\n   âœ… No significant bias (mean close to 0)\")\n",
    "\n",
    "# Step 2: Try Solution 5 - Adjust predictions (demonstration only)\n",
    "# NOTE: This is a band-aid solution - better to fix root cause!\n",
    "print(\"\\nğŸ“Š Step 2: Adjusting Predictions (Band-aid Solution)\")\n",
    "print(\"Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª (Ø­Ù„ Ù…Ø¤Ù‚Øª)\")\n",
    "print(\"\\n   âš ï¸  WARNING: This doesn't fix the model, just shifts predictions!\")\n",
    "print(\"   Better to fix root cause (missing features, data quality, etc.)\")\n",
    "\n",
    "# Calculate adjusted predictions\n",
    "y_test_pred_adjusted = y_test_pred_m + mean_residual\n",
    "\n",
    "# Calculate original MAE (if not already calculated)\n",
    "test_mae_m = mean_absolute_error(y_test_m, y_test_pred_m)\n",
    "\n",
    "# Recalculate metrics\n",
    "mse_adjusted = mean_squared_error(y_test_m, y_test_pred_adjusted)\n",
    "mae_adjusted = mean_absolute_error(y_test_m, y_test_pred_adjusted)\n",
    "r2_adjusted = r2_score(y_test_m, y_test_pred_adjusted)\n",
    "\n",
    "print(f\"\\n   Original Metrics:\")\n",
    "print(f\"     MSE: {test_mse_m:,.2f}\")\n",
    "print(f\"     MAE: {test_mae_m:,.2f}\")\n",
    "print(f\"     RÂ²: {test_r2_m:.4f}\")\n",
    "print(f\"\\n   Adjusted Metrics (after adding bias correction):\")\n",
    "print(f\"     MSE: {mse_adjusted:,.2f}\")\n",
    "print(f\"     MAE: {mae_adjusted:,.2f}\")\n",
    "print(f\"     RÂ²: {r2_adjusted:.4f}\")\n",
    "\n",
    "# Check if adjustment helped\n",
    "if mse_adjusted < test_mse_m:\n",
    "    print(f\"\\n   âœ… MSE improved by {test_mse_m - mse_adjusted:,.2f}\")\n",
    "else:\n",
    "    print(f\"\\n   âš ï¸  MSE didn't improve (this is expected - bias correction doesn't always help)\")\n",
    "\n",
    "# Step 3: Check residual patterns\n",
    "print(\"\\nğŸ“Š Step 3: Checking Residual Patterns\")\n",
    "print(\"Ø§Ù„Ø®Ø·ÙˆØ© 3: ÙØ­Øµ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ\")\n",
    "\n",
    "# Calculate residuals after adjustment\n",
    "residuals_adjusted = y_test_m - y_test_pred_adjusted\n",
    "mean_residual_adjusted = residuals_adjusted.mean()\n",
    "\n",
    "print(f\"\\n   Original Residual Mean: {mean_residual:.2f}\")\n",
    "print(f\"   Adjusted Residual Mean: {mean_residual_adjusted:.2f}\")\n",
    "\n",
    "if abs(mean_residual_adjusted) < abs(mean_residual):\n",
    "    print(f\"   âœ… Bias reduced! (closer to 0)\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Bias not fully corrected\")\n",
    "\n",
    "# Step 4: Recommendations\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ’¡ Recommendations | Ø§Ù„ØªÙˆØµÙŠØ§Øª\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n   To FIX the root cause (not just adjust predictions):\")\n",
    "print(\"   1. âœ… Check for missing features (most common cause)\")\n",
    "print(\"      - Review your data: Are there other features that affect price?\")\n",
    "print(\"      - Add relevant features: location, condition, year_built, etc.\")\n",
    "print(\"\\n   2. âœ… Check data quality\")\n",
    "print(\"      - Look for outliers in target variable\")\n",
    "print(\"      - Verify target values are correct\")\n",
    "print(\"      - Check for missing values\")\n",
    "print(\"\\n   3. âœ… Try feature engineering\")\n",
    "print(\"      - Create interaction features (size Ã— bedrooms)\")\n",
    "print(\"      - Transform features (log, square)\")\n",
    "print(\"\\n   4. âœ… Consider different models\")\n",
    "print(\"      - If relationship is non-linear â†’ Try Polynomial Regression\")\n",
    "print(\"      - If many features â†’ Try Ridge/Lasso Regression\")\n",
    "print(\"\\n   âš ï¸  Remember: Adjusting predictions is a BAND-AID, not a real fix!\")\n",
    "print(\"   Always try to fix the root cause first!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Decision Framework - When to Use Linear Regression | Ø§Ù„Ø®Ø·ÙˆØ© 6: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ\n",
    "\n",
    "**BEFORE**: You've learned how to build linear regression models, but when should you use them?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to determine if linear regression is the right choice for your problem!\n",
    "\n",
    "**Why this matters**: Using linear regression when it's not appropriate leads to:\n",
    "- **Poor predictions** â†’ Model can't capture non-linear patterns\n",
    "- **Wasted time** â†’ Trying to force linear relationships that don't exist\n",
    "- **Wrong conclusions** â†’ Making decisions based on inaccurate models\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework: Is Linear Regression Appropriate? | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±: Ù‡Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ù…Ù†Ø§Ø³Ø¨ØŸ\n",
    "\n",
    "**Key Question**: Should I use **LINEAR REGRESSION** or a different method?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "**Step 1: What type of problem?**\n",
    "- **Classification** (predicting categories) â†’ âŒ Use Logistic Regression or other classifiers\n",
    "  - Why? Linear regression predicts continuous values, not categories\n",
    "- **Regression** (predicting numbers) â†’ Continue to Step 2\n",
    "\n",
    "**Step 2: What is the relationship?**\n",
    "- **Linear relationship** â†’ âœ… Use Linear Regression\n",
    "  - Why? Linear regression assumes linear relationships\n",
    "- **Non-linear relationship** â†’ âš ï¸ Use Polynomial Regression or other methods\n",
    "  - Why? Linear regression can't capture curves\n",
    "\n",
    "**Step 3: Data characteristics?**\n",
    "- **Many features (>100)** â†’ âš ï¸ Use Ridge or Lasso Regression\n",
    "- **Multicollinearity present** â†’ âš ï¸ Use Ridge Regression\n",
    "- **Need feature selection** â†’ âš ï¸ Use Lasso Regression\n",
    "- **Slightly curved** â†’ Try Polynomial Regression\n",
    "- **Highly non-linear** â†’ Use Random Forest, XGBoost, Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Comparison Table: Linear Regression vs Alternatives | Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Linear Regression** | Linear relationships, continuous target, interpretable | â€¢ Simple & fast<br>â€¢ Interpretable<br>â€¢ No hyperparameters<br>â€¢ Works well with linear data | â€¢ Can't handle non-linear<br>â€¢ Assumes linearity<br>â€¢ Sensitive to outliers | House price vs size (linear) |\n",
    "| **Polynomial Regression** | Slightly curved relationships | â€¢ Handles curves<br>â€¢ Still interpretable<br>â€¢ Extends linear regression | â€¢ Can overfit<br>â€¢ More complex | House price vs size (curved) |\n",
    "| **Ridge/Lasso** | Many features, multicollinearity | â€¢ Prevents overfitting<br>â€¢ Handles many features<br>â€¢ Regularization | â€¢ More complex<br>â€¢ Hyperparameter tuning | 100+ features, correlated features |\n",
    "| **Random Forest** | Non-linear, complex patterns | â€¢ Handles non-linear<br>â€¢ Feature importance<br>â€¢ Robust | â€¢ Less interpretable<br>â€¢ More complex | Complex relationships |\n",
    "| **XGBoost** | Non-linear, best performance | â€¢ State-of-the-art<br>â€¢ Handles complex patterns | â€¢ Less interpretable<br>â€¢ Complex | Competition-level performance |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When Linear Regression IS Appropriate | Ù…ØªÙ‰ ÙŠÙƒÙˆÙ† Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§\n",
    "\n",
    "**Use Linear Regression when:**\n",
    "\n",
    "1. **Linear Relationship** âœ…\n",
    "   - Scatter plot shows a straight line pattern\n",
    "   - Example: House size vs price (larger = more expensive, linear)\n",
    "\n",
    "2. **Continuous Target Variable** âœ…\n",
    "   - Predicting numbers (price, temperature, sales)\n",
    "   - NOT categories (sick/healthy, yes/no)\n",
    "\n",
    "3. **Interpretability Important** âœ…\n",
    "   - Need to understand feature coefficients\n",
    "   - Example: \"Each bedroom adds $30,000 to price\"\n",
    "\n",
    "4. **Fast Predictions Needed** âœ…\n",
    "   - Simple model, fast training and prediction\n",
    "   - Good for real-time systems\n",
    "\n",
    "5. **Baseline Model** âœ…\n",
    "   - Start with linear regression as baseline\n",
    "   - Compare with more complex models\n",
    "\n",
    "6. **Small to Medium Datasets** âœ…\n",
    "   - Works well with limited data\n",
    "   - Doesn't require huge datasets\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ When Linear Regression IS NOT Appropriate | Ù…ØªÙ‰ Ù„Ø§ ÙŠÙƒÙˆÙ† Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§\n",
    "\n",
    "**Don't use Linear Regression when:**\n",
    "\n",
    "1. **Non-Linear Relationship** âŒ\n",
    "   - Scatter plot shows curves, exponential patterns\n",
    "   - Example: Growth patterns, decay curves\n",
    "   - **Use Instead**: Polynomial Regression, Random Forest, XGBoost\n",
    "\n",
    "2. **Classification Problem** âŒ\n",
    "   - Predicting categories (yes/no, A/B/C)\n",
    "   - **Use Instead**: Logistic Regression, Decision Trees, SVM\n",
    "\n",
    "3. **Many Features with Multicollinearity** âŒ\n",
    "   - Features are highly correlated\n",
    "   - **Use Instead**: Ridge Regression (handles multicollinearity)\n",
    "\n",
    "4. **Need Feature Selection** âŒ\n",
    "   - Want to automatically select important features\n",
    "   - **Use Instead**: Lasso Regression (automatic feature selection)\n",
    "\n",
    "5. **Complex Non-Linear Patterns** âŒ\n",
    "   - Multiple interactions, complex relationships\n",
    "   - **Use Instead**: Random Forest, XGBoost, Neural Networks\n",
    "\n",
    "6. **Outliers Present** âŒ\n",
    "   - Many extreme values that affect the line\n",
    "   - **Use Instead**: Robust regression methods, or clean outliers first\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: House Price Prediction âœ… APPROPRIATE\n",
    "- **Problem**: Predict house price from size\n",
    "- **Relationship**: Linear (larger houses = higher prices, roughly linear)\n",
    "- **Target**: Continuous (price in dollars)\n",
    "- **Decision**: âœ… Use Linear Regression\n",
    "- **Reasoning**: Linear relationship, continuous target, interpretable\n",
    "\n",
    "#### Example 2: Stock Price Prediction âš ï¸ MAY NOT BE APPROPRIATE\n",
    "- **Problem**: Predict stock price from time\n",
    "- **Relationship**: Non-linear (volatile, trends, cycles)\n",
    "- **Target**: Continuous (price)\n",
    "- **Decision**: âš ï¸ Try Linear first, but may need Polynomial or other methods\n",
    "- **Reasoning**: Stock prices have complex patterns, linear may not capture well\n",
    "\n",
    "#### Example 3: Customer Churn Prediction âŒ NOT APPROPRIATE\n",
    "- **Problem**: Predict if customer will leave (yes/no)\n",
    "- **Relationship**: Classification problem\n",
    "- **Target**: Categorical (churn/not churn)\n",
    "- **Decision**: âŒ Use Logistic Regression instead\n",
    "- **Reasoning**: Classification problem, not regression\n",
    "\n",
    "#### Example 4: Sales Prediction with Many Features âš ï¸ MAY NEED REGULARIZATION\n",
    "- **Problem**: Predict sales from 50+ features\n",
    "- **Relationship**: Likely linear, but many features\n",
    "- **Target**: Continuous (sales amount)\n",
    "- **Decision**: âš ï¸ Use Ridge or Lasso Regression\n",
    "- **Reasoning**: Many features may cause overfitting, regularization helps\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Check relationship first** - Plot data to see if it's linear\n",
    "2. **Classification â‰  Regression** - Use logistic regression for categories\n",
    "3. **Start simple** - Linear regression is a good baseline\n",
    "4. **Check assumptions** - Linearity, independence, homoscedasticity\n",
    "5. **Many features?** - Consider Ridge/Lasso for regularization\n",
    "6. **Non-linear?** - Try Polynomial Regression or other methods\n",
    "7. **Always visualize** - Scatter plots reveal relationship type\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario 1**: Predicting student test scores from study hours\n",
    "- **Relationship**: More hours = higher scores (roughly linear)\n",
    "- **Target**: Continuous (test score 0-100)\n",
    "- **Decision**: âœ… Linear Regression appropriate\n",
    "\n",
    "**Scenario 2**: Predicting customer satisfaction (satisfied/not satisfied)\n",
    "- **Relationship**: Classification problem\n",
    "- **Target**: Categorical (satisfied/not satisfied)\n",
    "- **Decision**: âŒ Use Logistic Regression, not Linear Regression\n",
    "\n",
    "**Scenario 3**: Predicting house price from 100+ features\n",
    "- **Relationship**: Likely linear, but many features\n",
    "- **Target**: Continuous (price)\n",
    "- **Decision**: âš ï¸ Use Ridge or Lasso Regression (regularization needed)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Example 5: Polynomial Regression** - For non-linear relationships\n",
    "- ğŸ““ **Unit 2, Example 1: Ridge/Lasso** - For many features and regularization\n",
    "- ğŸ““ **Unit 3: Classification** - For predicting categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Summary | Ø§Ù„Ù…Ù„Ø®Øµ\n",
    "\n",
    "Great job completing this example!\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "**Linear Regression - Your First ML Model:**\n",
    "- **Simple Linear Regression**: Predicts a continuous target using one feature. Finds the best line (y = mx + b) that minimizes prediction error.\n",
    "\n",
    "- **Multiple Linear Regression**: Extends to multiple features. Predicts target using multiple inputs simultaneously. Formula: y = bâ‚€ + bâ‚xâ‚ + bâ‚‚xâ‚‚ + ... + bâ‚™xâ‚™\n",
    "\n",
    "- **Model Evaluation Metrics**:\n",
    "  - **MSE (Mean Squared Error)**: Average squared difference between predictions and actual values (lower is better)\n",
    "  - **MAE (Mean Absolute Error)**: Average absolute difference (more interpretable than MSE)\n",
    "  - **RÂ² (R-squared)**: Proportion of variance explained (0 to 1, higher is better, 1 = perfect fit)\n",
    "\n",
    "- **Residual Analysis**: Residuals (errors) should be randomly distributed around zero. Patterns in residuals indicate model problems.\n",
    "\n",
    "- **Feature Importance**: Coefficients show how much each feature affects the prediction. Larger absolute coefficients = more important features.\n",
    "\n",
    "### ğŸ”— How This Connects:\n",
    "\n",
    "**This example builds on and leads to:**\n",
    "- **Example 1-3**: Data loading, cleaning, and preprocessing prepare data for ML models\n",
    "- **Example 5: Polynomial Regression** - Extends linear regression to capture non-linear relationships\n",
    "- **Unit 2: Advanced Regression** - Ridge and Lasso regression improve on basic linear regression\n",
    "- **Unit 3: Classification** - Similar workflow (fit, predict, evaluate) but for categorical targets\n",
    "- **All ML Models**: Linear regression teaches the fundamental ML workflow used by all models\n",
    "\n",
    "**Why this example is important:**\n",
    "1. **First ML model**: Simplest model, easiest to understand\n",
    "2. **ML workflow**: Teaches the standard process: prepare data â†’ train model â†’ evaluate â†’ interpret\n",
    "3. **Foundation**: Many advanced models build on linear regression concepts\n",
    "4. **Interpretability**: Linear models are highly interpretable - you can see exactly how features affect predictions\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Linear regression finds the best line**: Minimizes prediction error using least squares method\n",
    "2. **Multiple features improve predictions**: More relevant features generally lead to better models\n",
    "3. **Evaluation metrics matter**: MSE, MAE, and RÂ² tell you how good your model is\n",
    "4. **Residuals reveal problems**: Check residuals to diagnose model issues\n",
    "5. **Coefficients show feature importance**: Understand which features matter most\n",
    "\n",
    "### Next Steps:\n",
    "- Complete exercises in `exercises/` folder to practice building linear regression models\n",
    "- Review quiz materials to test your understanding\n",
    "- Proceed to **Example 5: Polynomial Regression** to handle non-linear relationships\n",
    "- Then move to **Unit 2: Advanced Regression** for Ridge and Lasso techniques\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Tips for Success:**\n",
    "- **Data quality matters**: Clean, preprocessed data leads to better models\n",
    "- **Feature selection**: Not all features help - remove irrelevant ones\n",
    "- **Check assumptions**: Linear regression assumes linear relationships - verify this!\n",
    "- **Visualize**: Always plot your data and residuals to understand what's happening"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
