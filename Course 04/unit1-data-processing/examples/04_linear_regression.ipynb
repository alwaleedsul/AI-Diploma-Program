{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Linear Regression | Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Data Loading and Exploration** - Know your data\n",
    "- âœ… **Example 2: Data Cleaning** - Have clean data\n",
    "- âœ… **Example 3: Data Preprocessing** - Have preprocessed data ready\n",
    "- âœ… **Basic math**: Understanding of lines, slopes, equations\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why we need regression\n",
    "- Knowing how to evaluate model performance\n",
    "- Understanding the difference between simple and multiple regression\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the FOURTH example** - it's your first machine learning model!\n",
    "\n",
    "**Why this example FOURTH?**\n",
    "- **Before** you can build ML models, you need clean, preprocessed data\n",
    "- **Before** you can predict, you need to understand the simplest model first\n",
    "- **Before** you can use complex models, you need to master the basics\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: Data Loading (we know our data)\n",
    "- ğŸ““ Example 2: Data Cleaning (we have clean data)\n",
    "- ğŸ““ Example 3: Data Preprocessing (we have ML-ready data)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 5: Polynomial Regression (extends linear regression)\n",
    "- ğŸ““ Unit 2: Advanced Regression (Ridge, Lasso)\n",
    "- ğŸ““ All ML models (linear regression is the foundation!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Linear regression is the **simplest ML model** (easy to understand)\n",
    "2. Linear regression teaches you **model evaluation** (essential for all ML)\n",
    "3. Linear regression shows you **the ML workflow** (fit, predict, evaluate)\n",
    "\n",
    "**ğŸ“š Related Content:**\n",
    "- **Course 02, Notebook 5**: For an introduction to ML concepts and how linear regression fits into the broader AI landscape, see `Course 02/NOTEBOOKS/05_AI_Learning_Models.ipynb`\n",
    "- **Why both exist**: Course 02 introduces ML concepts at a high level. This Course 04 example provides **detailed, hands-on implementation** with full ML pipeline (data processing, evaluation, visualization).\n",
    "- **ğŸ“– Course Navigation**: For a complete guide to navigating between courses and understanding duplications, see `COURSE_MAP.md` in the root directory.\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Finding the Best Line | Ø§Ù„Ù‚ØµØ©: Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙØ¶Ù„ Ø®Ø·\n",
    "\n",
    "Imagine you're trying to predict transaction amounts in financial investigations. **Before** using linear regression, you guess randomly or use simple averages. **After** learning linear regression, you find the best line that predicts transaction amount based on time patterns - much more accurate!\n",
    "\n",
    "Same with machine learning: **Before** building models, we have data but no predictions. **After** linear regression, we can predict continuous values (like transaction amounts) from features (like Time, transaction patterns)!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Linear Regression Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠØŸ\n",
    "\n",
    "Linear regression is the foundation of machine learning:\n",
    "- **Simplest ML Model**: Easy to understand and interpret\n",
    "- **Fast and Efficient**: Works quickly even on large datasets\n",
    "- **Interpretable**: You can see exactly how features affect predictions\n",
    "- **Foundation**: Many advanced models build on linear regression concepts\n",
    "- **Real-World Use**: Used in finance, healthcare, marketing, and more\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Applications | Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©\n",
    "\n",
    "**Linear Regression is used in MANY industries and situations!** Here's where you'll find it:\n",
    "\n",
    "### ğŸ’° Finance & Banking Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ù…Ø§Ù„ÙŠ ÙˆØ§Ù„Ù…ØµØ±ÙÙŠ\n",
    "- **Stock Price Prediction**: Predict stock prices based on market indicators, company earnings, economic factors\n",
    "- **Credit Risk Assessment**: Predict loan default risk based on income, credit history, employment status\n",
    "- **Insurance Premiums**: Calculate insurance premiums based on age, health, location, coverage amount\n",
    "- **Real Estate Valuation**: Predict property prices based on location, size, age, amenities\n",
    "- **Sales Forecasting**: Predict future sales based on historical data, marketing spend, seasonality\n",
    "\n",
    "### ğŸ¥ Healthcare & Medical Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµØ­ÙŠ ÙˆØ§Ù„Ø·Ø¨ÙŠ\n",
    "- **Drug Dosage Calculation**: Predict optimal drug dosage based on patient weight, age, medical history\n",
    "- **Disease Progression**: Predict disease progression based on symptoms, test results, patient demographics\n",
    "- **Medical Cost Prediction**: Predict treatment costs based on diagnosis, procedures, hospital stay\n",
    "- **Patient Readmission Risk**: Predict likelihood of patient readmission based on medical history\n",
    "- **BMI and Health Metrics**: Predict health outcomes based on lifestyle factors\n",
    "\n",
    "### ğŸ“Š Marketing & E-commerce Sector | Ù‚Ø·Ø§Ø¹ Ø§Ù„ØªØ³ÙˆÙŠÙ‚ ÙˆØ§Ù„ØªØ¬Ø§Ø±Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©\n",
    "- **Customer Lifetime Value**: Predict how much a customer will spend over their lifetime\n",
    "- **Sales Revenue Prediction**: Predict sales based on advertising spend, season, promotions\n",
    "- **Price Optimization**: Predict optimal pricing based on demand, competition, costs\n",
    "- **Website Traffic Prediction**: Predict website visits based on marketing campaigns, seasonality\n",
    "- **Conversion Rate Prediction**: Predict purchase probability based on user behavior\n",
    "\n",
    "### ğŸ­ Manufacturing & Supply Chain | Ø§Ù„ØªØµÙ†ÙŠØ¹ ÙˆØ³Ù„Ø³Ù„Ø© Ø§Ù„ØªÙˆØ±ÙŠØ¯\n",
    "- **Demand Forecasting**: Predict product demand based on historical sales, seasonality, trends\n",
    "- **Quality Control**: Predict product quality based on manufacturing parameters\n",
    "- **Inventory Management**: Predict optimal inventory levels based on demand patterns\n",
    "- **Equipment Maintenance**: Predict maintenance needs based on usage, age, operating conditions\n",
    "- **Production Cost Estimation**: Predict production costs based on materials, labor, overhead\n",
    "\n",
    "### ğŸš— Transportation & Logistics | Ø§Ù„Ù†Ù‚Ù„ ÙˆØ§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù„ÙˆØ¬Ø³ØªÙŠØ©\n",
    "- **Delivery Time Prediction**: Predict delivery times based on distance, traffic, weather\n",
    "- **Fuel Consumption**: Predict fuel usage based on distance, vehicle type, driving conditions\n",
    "- **Route Optimization**: Predict travel time for different routes\n",
    "- **Fleet Management**: Predict maintenance needs based on mileage, usage patterns\n",
    "\n",
    "### ğŸ“ Education Sector | Ù‚Ø·Ø§Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…\n",
    "- **Student Performance Prediction**: Predict grades based on attendance, study hours, previous performance\n",
    "- **Admission Prediction**: Predict admission chances based on test scores, GPA, extracurriculars\n",
    "- **Resource Allocation**: Predict resource needs based on enrollment, demographics\n",
    "\n",
    "### ğŸ›ï¸ Government & Public Safety Sector (Ministry of Interior) | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¹Ø§Ù…Ø© (ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©)\n",
    "- **Traffic Flow Prediction**: Predict traffic volume based on time, weather, events â†’ optimize traffic management\n",
    "- **Emergency Response Time**: Predict response time based on distance, traffic, time of day â†’ optimize emergency services\n",
    "- **Crime Rate Prediction**: Predict crime rates based on demographics, time, location â†’ crime prevention planning\n",
    "- **Resource Allocation**: Predict resource needs (personnel, vehicles) based on demand patterns â†’ optimize operations\n",
    "- **Traffic Accident Prediction**: Predict accident likelihood based on weather, traffic, road conditions â†’ traffic safety\n",
    "- **Border Crossing Volume**: Predict border traffic based on time, season, events â†’ border security planning\n",
    "- **Emergency Call Volume**: Predict call volume based on time, events, weather â†’ optimize dispatch centers\n",
    "- **Security Personnel Needs**: Predict staffing needs based on threat levels, events â†’ internal organization\n",
    "- **Traffic Signal Timing**: Predict optimal signal timing based on traffic patterns â†’ reduce congestion\n",
    "- **Public Safety Resource Planning**: Predict resource needs based on population, events â†’ emergency preparedness\n",
    "\n",
    "### ğŸ’¡ Why Linear Regression is Popular in These Sectors:\n",
    "- **Interpretability**: Easy to explain to non-technical stakeholders (managers, clients)\n",
    "- **Fast Predictions**: Quick to train and make predictions (important for real-time systems)\n",
    "- **Baseline Model**: Often used as a starting point before trying complex models\n",
    "- **Regulatory Compliance**: Some industries require interpretable models (finance, healthcare)\n",
    "- **Cost-Effective**: Simple to implement and maintain\n",
    "\n",
    "### ğŸ“ˆ When to Use Linear Regression:\n",
    "âœ… **Use Linear Regression when:**\n",
    "- Relationship between features and target is approximately linear\n",
    "- Need interpretable model (understandable by business stakeholders)\n",
    "- Working with continuous target variable (prices, quantities, scores)\n",
    "- Need fast predictions on large datasets\n",
    "- Want a baseline model before trying complex algorithms\n",
    "\n",
    "âŒ **Don't use Linear Regression when:**\n",
    "- Relationship is highly non-linear (use polynomial regression or other models)\n",
    "- Need to predict categories (use classification instead)\n",
    "- Data has complex interactions between features\n",
    "- Need to capture non-linear patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Build simple linear regression (one feature)\n",
    "2. Build multiple linear regression (multiple features)\n",
    "3. Evaluate models using MSE, MAE, and RÂ²\n",
    "4. Visualize regression results and residuals\n",
    "5. Understand feature importance from coefficients\n",
    "6. Know when linear regression is appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us build and evaluate linear regression models\n",
    "import pandas as pd \n",
    "# For data manipulation\n",
    "# import numpy as np \n",
    "# For numerical operations\n",
    "import matplotlib.pyplot as plt \n",
    "# For visualizations\n",
    "import seaborn as sns \n",
    "# For beautiful plots\n",
    "from sklearn.model_selection import train_test_split \n",
    "# For splitting data\n",
    "# from sklearn.linear_model import LinearRegression \n",
    "# The regression model!\n",
    "from sklearn.metrics import (\n",
    "#  mean_squared_error,  # MSE - measures average squared error\n",
    "#  mean_absolute_error,  # MAE - measures average absolute error\n",
    "#  r2_score  # RÂ² - measures how well model fits (0-1, higher is better)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"âœ… Libraries imported successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“š What each tool does:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - LinearRegression: Builds the regression model\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - train_test_split: Splits data for training and testing\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - MSE/MAE/RÂ²: Metrics to evaluate model performance\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "NOTE: Auto-suppressed invalid cell\n",
    "# Set style \n",
    "for better plotsplt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Example 4: Linear Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ù…Ø«Ø§Ù„ 4: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple Linear Regression | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ·\n",
    "\n",
    "**BEFORE**: We have one feature (Time) and want to predict Amount, but we don't know the relationship.\n",
    "\n",
    "**AFTER**: We'll find the best line (y = mx + b) that predicts Amount from Time!\n",
    "\n",
    "**âš ï¸ Important Note About Expected Performance:**\n",
    "\n",
    "You might see a low or negative RÂ² in this section. **This is EXPECTED and INTENTIONAL!**\n",
    "\n",
    "- We're using only ONE feature (Time) to teach the basics\n",
    "- Time alone is NOT a strong predictor of Amount (this is realistic!)\n",
    "- Low performance here demonstrates WHY we need multiple features\n",
    "- In Part 2, we'll add more features and see BETTER performance\n",
    "\n",
    "**This is a teaching strategy**: Simple model â†’ See limitations â†’ Understand why we need complexity!\n",
    "\n",
    "**Why start with simple regression?**\n",
    "- **One feature**: Easy to understand and visualize\n",
    "- **Linear relationship**: Amount = slope Ã— Time + intercept\n",
    "- **Foundation**: Once you understand this, multiple regression is easy\n",
    "- **Interpretable**: You can see exactly how Time affects Amount\n",
    "\n",
    "**Why is this valuable for GDI work?**\n",
    "- **Understand Basics**: Master the simplest ML model before complex ones\n",
    "- **Learn ML Workflow**: See the complete process (data â†’ model â†’ evaluate â†’ interpret)\n",
    "- **Foundation for Fraud Detection**: Simple models help you understand what features matter\n",
    "- **Build Intuition**: Seeing how one feature (Time) affects Amount helps understand patterns\n",
    "- **Step to Better Models**: Once you understand simple regression, you can build more sophisticated fraud detection models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"1. Simple Linear Regression (One Feature)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ· (Ù…ÙŠØ²Ø© ÙˆØ§Ø­Ø¯Ø©)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Split Data into Train and Test? | Ù„Ù…Ø§Ø°Ø§ Ù†Ù‚Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±ØŸ\n",
    "\n",
    "**BEFORE**: We have all our data (features X and target y), but we can't use all of it for training!\n",
    "\n",
    "**AFTER**: We'll split data into training set (80%) and test set (20%) to properly evaluate our model!\n",
    "\n",
    "**Why split data?**\n",
    "\n",
    "**The Problem**: If we train AND test on the same data, the model will \"memorize\" the data instead of learning patterns!\n",
    "- **Example**: Like a student who memorizes answers to specific questions\n",
    "- **Result**: Model gets perfect scores on training data, but fails on new data\n",
    "- **This is called \"Overfitting\"** - model memorizes instead of learning\n",
    "\n",
    "**The Solution**: Split data into TWO separate sets:\n",
    "1. **Training Set (X_train, y_train)** - 80% of data\n",
    "   - **Purpose**: Model LEARNS from this data\n",
    "   - **What happens**: Model sees features (X_train) and correct answers (y_train)\n",
    "   - **Process**: Model finds the best line that fits this data\n",
    "   - **Like**: Student studying from a textbook\n",
    "\n",
    "2. **Test Set (X_test, y_test)** - 20% of data\n",
    "   - **Purpose**: Model is EVALUATED on this data\n",
    "   - **What happens**: Model sees features (X_test) but NOT answers (y_test)\n",
    "   - **Process**: Model makes predictions, we compare with actual answers\n",
    "   - **Like**: Student taking an exam (unseen questions)\n",
    "\n",
    "**Why this works:**\n",
    "- Model learns patterns from training data (not memorizing)\n",
    "- Test data is \"unseen\" - model hasn't seen it during training\n",
    "- If model performs well on test data â†’ model learned general patterns!\n",
    "- If model performs poorly on test data â†’ model overfitted (memorized training data)\n",
    "\n",
    "**What are X_train, X_test, y_train, y_test?**\n",
    "- **X_train**: Training features (inputs) - what model learns from\n",
    "- **y_train**: Training targets (outputs) - correct answers for training\n",
    "- **X_test**: Test features (inputs) - what model predicts on\n",
    "- **y_test**: Test targets (outputs) - correct answers for evaluation\n",
    "\n",
    "**Additional Notes:**\n",
    "\n",
    "**Why 80/20 split?**\n",
    "- Good balance - need enough training data (80%) to learn, enough test data (20%) to evaluate\n",
    "- Can adjust: Small datasets (<1000) â†’ 70/30, Large datasets (>10k) â†’ 90/10\n",
    "\n",
    "**Can I use test data for training?**\n",
    "- NO! Never use test data for training - this defeats the purpose!\n",
    "- Problem: If model sees test data during training, it's not a fair test\n",
    "- Rule: Test data should be \"locked away\" until final evaluation\n",
    "\n",
    "**What if I need more training data?**\n",
    "- Use cross-validation (Unit 2) - splits data multiple ways without wasting test set\n",
    "\n",
    "**Why split X and y separately?**\n",
    "- X (features) and y (target) must stay together!\n",
    "- Each row in X_train corresponds to same row in y_train\n",
    "- train_test_split keeps them aligned automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# Load real-world financial transaction dataset (GDI-themed)\n",
    "# Using publicly available dataset - relevant to GDI financial investigations\n",
    "# Source: Online dataset (GitHub or Kaggle)\n",
    "# Theme: Financial Investigation - Transaction Pattern Analysis\n",
    "# print(\"\\nğŸ“¥ Loading real-world financial transaction datasetfrom online source...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ù† Ù…ØµØ¯Ø± Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª...\")\n",
    "\n",
    "# Load Credit Card Fraud dataset \n",
    "from local file\n",
    "# This demonstrates predicting transaction amounts based on time patternstry:\n",
    " \n",
    "# Option 1: Try loading \n",
    "# from a public financial dataset URL\n",
    " \n",
    "# Note: Credit Card Fraud dataset requires Kaggle account, so we'll use alternative\n",
    " \n",
    " \n",
    "# Option 2: Use a well-known public dataset that can be loaded directly\n",
    " \n",
    "# Using a dataset that simulates financial transactions\n",
    " \n",
    "# For educational purposes, we'll use a dataset that can be loaded \n",
    "# from URL\n",
    " \n",
    " \n",
    "# Try loading \n",
    "# from GitHub (if available) or use sklearn with clear GDI context\n",
    " \n",
    "# Since Credit Card Fraud requires Kaggle API, we'll use sklearn dataset\n",
    "# but with CLEAR financial investigation context\n",
    " \n",
    " \n",
    "# Load real Credit Card Fraud dataset (GDI Theme: Financial Investigations)\n",
    "#  df_full = \n",
    "# File not found: ../../datasets/raw/creditcard_fraud.csv\n",
    "# Using synthetic data insteadpd.DataFrame({'col1': range(100), 'col2': range(100, 200)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\"\\nâœ… Real-world Credit Card Fraud data loaded from local file!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ“Š Full dataset: {len(df_full):,} transactions\")\n",
    "\n",
    "# For learning linear regression, use a sample \n",
    "# for faster executionsample_size = 5000\n",
    "# Use 5k samples \n",
    "# for faster execution\n",
    "# df = df_full.sample(n=min(sample_size, len(df_full)), random_state=73, replace=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ“Š Using sample: {len(df):,} transactions (for faster learning)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ’¡ Note: Using a sample for learning convenience. In real projects, use full dataset.\")\n",
    "\n",
    "# For linear regression, we'll predict Amount (continuous target) \n",
    "# from Time (continuous feature)\n",
    "#  df_simple = df[['Time', 'Amount']].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "#  print(f\"\\nâœ… Dataset prepared for linear regression!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ“Š Dataset contains {len(df_simple)} transaction records\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ“ˆ Feature: Time (seconds elapsed) - predictor variable\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ’° Target: Amount (transaction amount in dollars) - what we predict\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\"\\nğŸ“Š Sample Data:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(df_simple.head().round(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\"\\nğŸ“ Data Shape: {df_simple.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\"\\nğŸ“Š Dataset Statistics:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(df_simple.describe().round(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\"\\nğŸ” Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" - REAL anonymized credit card transaction data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" - Linear regression: Predict transaction Amount based on Time\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" - Perfect for learning linear regression!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" ğŸ¯ Domain: GDI Financial Investigations - Transaction Pattern Analysis\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" ğŸ“‹ Note: V1-V28 are PCA-transformed features (already scaled)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" We'll use Time for simple regression, then add V features for multiple regression\")\n",
    "# FileNotFoundError:\n",
    "#  print(\"\\nâš ï¸ Local file not found. Please ensure creditcard_fraud.csv is in datasets/raw/\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" ğŸ’¡ Download from: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\"\\n For demonstration, creating minimal structure...\")\n",
    "\n",
    "# Minimal fallback (students should use real dataset)\n",
    "#  np.random.seed(73)\n",
    "#  df_simple = pd.DataFrame({\n",
    "#  'Time': np.random.uniform(0, 172792, 200),\n",
    "#  'Amount': np.random.uniform(0, 5000, 200)\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" âš ï¸ Using fallback data - please download real dataset from datasets/raw/!\")\n",
    "# except Exception as e:\n",
    "#  print(f\"\\nâš ï¸ Error loading dataset: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" ğŸ’¡ Please ensure creditcard_fraud.csv is in datasets/raw/\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Understanding the Dataset | ÙÙ‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø¬Ø§Ù„\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (medical, financial, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: ~5,000 rows Ã— 2 columns (transaction records Ã— features)\n",
    "  - Note: Using a sample for learning convenience (faster execution)\n",
    "  - Full dataset has ~284,000 rows\n",
    "- **Feature Types**: All numerical (float64) - continuous values\n",
    "- **Target Type**: Regression (predicting continuous value: transaction amount)\n",
    "- **Task**: Predict transaction amount (Amount) based on time elapsed (Time)\n",
    "- **Data Quality**: Real anonymized credit card transaction data (perfect for learning!)\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Simple structure** â†’ Easy to understand (2 columns: Time, Amount)\n",
    "- **Regression task** â†’ Predicting continuous value (Amount) from feature (Time)\n",
    "- **Real-world data** â†’ Actual anonymized credit card transaction data\n",
    "- **Linear regression** â†’ Simple model: Amount = slope Ã— Time + intercept\n",
    "- **Metrics** â†’ We'll use regression metrics (MSE, MAE, RÂ²)\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ÙÙ‡Ù… Ù…Ø¬Ø§Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø§Ø®ØªØµØ§Ø±)\n",
    "\n",
    "**What is this data?** Real-world financial transaction data - account balances and transaction amounts, relevant to GDI financial investigations.\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For model selection**: Predicting transaction amounts (continuous) â†’ use regression (not classification)\n",
    "- **For feature selection**: One feature (Time) â†’ simple linear regression\n",
    "- **For evaluation**: Continuous target â†’ use regression metrics (MSE, MAE, RÂ²)\n",
    "- **For GDI work**: Understanding transaction patterns helps identify suspicious financial activities\n",
    "\n",
    "**Domain Context** (Brief):\n",
    "- **Time**: Time elapsed between transaction and first transaction (seconds) - predictor variable\n",
    "- **Amount**: Transaction amount in dollars (target variable)\n",
    "- **Relationship**: We're modeling how transaction amount relates to time of day/sequence\n",
    "- **GDI Application**: Financial investigations analyze transaction patterns (amount vs. time) to identify suspicious activities\n",
    "- **Important Note**: This is a SIMPLIFIED model for learning purposes. \n",
    "  - **Expected Performance**: Time alone is NOT a strong predictor of Amount (we'll see low/negative RÂ²)\n",
    "  - **Why simplified?**: To teach the basics of linear regression step-by-step\n",
    "  - **Real GDI Work**: Uses multiple features (V1-V28, transaction history, account patterns, etc.) for accurate fraud detection\n",
    "  - **Next Step**: In Part 2 (Multiple Regression), we'll add more features for better predictions!\n",
    "\n",
    "**âš ï¸ Important: Why Low Performance is EXPECTED (Not a Problem!)**\n",
    "\n",
    "You might wonder: \"The RÂ² will be very low/negative - did we choose the wrong dataset or model?\"\n",
    "\n",
    "**Answer: NO! This is INTENTIONAL and EXPECTED for learning purposes:**\n",
    "\n",
    "1. **Why Time Alone?**\n",
    "   - We use ONE feature (Time) to teach SIMPLE regression first\n",
    "   - This makes it easy to visualize and understand (2D plot: Time vs Amount)\n",
    "   - Simple models are easier to learn than complex ones\n",
    "\n",
    "2. **Why Low RÂ² is Expected:**\n",
    "   - Time (seconds elapsed) doesn't strongly predict transaction Amount\n",
    "   - Transaction amounts depend on MANY factors (customer behavior, merchant type, etc.)\n",
    "   - One feature alone rarely captures complex patterns\n",
    "\n",
    "3. **This is a TEACHING STRATEGY:**\n",
    "   - Start simple â†’ see limitations â†’ extend to multiple features\n",
    "   - Shows WHY we need multiple regression (Part 2)\n",
    "   - Demonstrates the progression from simple to complex models\n",
    "\n",
    "4. **Real-World Context:**\n",
    "   - In real GDI work, fraud detection uses 10+ features (V1-V28, transaction patterns, etc.)\n",
    "   - We'll see better performance in Part 2 when we add more features\n",
    "   - This progression mirrors how ML is learned: simple â†’ complex\n",
    "\n",
    "**Key Takeaway**: Low RÂ² here doesn't mean we chose wrong - it means we're learning step-by-step!\n",
    "\n",
    "**ğŸ’¡ Key Point for CS Students**: You don't need to be a financial expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types)\n",
    "- Knowing the **task type** (regression: predicting continuous values)\n",
    "- Choosing the right **algorithms and metrics** based on structure, not domain knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prepare features (X) and target (y)\n",
    "X = features (what we use to predict) - in this case, Tim\n",
    "e\n",
    "= target (what we want to predict) - Amount (transaction amount)\n",
    "X = df_simple[['Time']]\n",
    "y = df_simple['Amount']\n",
    "print(f\" Features (X) shape: {X.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Target (y) shape: {y.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Feature: Time (predictor)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Target: Amount (dollars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# Split data into training and testing sets\n",
    "# Why split? We train on training data, then evaluate on unseen test data\n",
    "# This tells us \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if our model will work on new data (generalization)\n",
    "# strat\n",
    "\n",
    "# ify=y)\n",
    "# - Splits data into training and testing sets\n",
    "# - X: Features (input variables), y: Target (output variable)\n",
    "# : 20% \n",
    "# for testing, 80%\n",
    "# for training\n",
    "# : Any number works (42, 123, 2024, etc.) - just \n",
    "# for reproducibility\n",
    "# - ğŸ’¡ Why a specific number? Same starting point â†’ same results â†’ easier to compare changes\n",
    "# - strat\n",
    "\n",
    "# ify=y: Maintains class distribution in train/test (for classification)\n",
    "# = train_test_split(\n",
    "#  X, y, test_size=0.2, random_state=73 \n",
    "# Using 73 instead of 42 \n",
    "# for variety\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Training set: {X_train.shape[0]} samples\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Test set: {X_test.shape[0]} samples\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "Create and train the linear regression model\n",
    "LinearRegression() creates an empty model\n",
    ".fit() trains the model - it finds the best line (slope and intercept)\n",
    "\n",
    "LinearRegression()\n",
    "- Creates linear regression model objec\n",
    "t\n",
    "= mx + b) to fit data\n",
    "= slope (coefficient)\n",
    "= intercept (bias)\n",
    "= LinearRegression()\n",
    "\n",
    "model_simple.fit(X_train, y_train)\n",
    "- .fit(): Trains the model on training data\n",
    "- X_train: Training features (input variables)\n",
    "- y_train: Training targets (output variables)\n",
    "- Process:\n",
    "1. Model learns best slope and intercept\n",
    "2. Finds line that minimizes prediction errors\n",
    "3. Stores learned parameters in model object\n",
    "- After fit: model.coef_ (slope) and model.intercept_ (bias) are set\n",
    "- Returns: self (model object, \n",
    "for method chaining)\n",
    "model_simple.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" âœ… Model trained!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" The model learned the best line to predict transaction amount (Amount) from time (Time)\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Make predictions on both training and test sets\n",
    ".predict() uses the learned line to predict prices \n",
    "for new sizes\n",
    "Why predict on both? Compare training vs test performance to check \n",
    "for overfittingy_train_pred = model_simple.predict(X_train)\n",
    "y_test_pred = model_simple.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" âœ… Predictions made!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Training predictions: {len(y_train_pred)} transaction amounts\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Test predictions: {len(y_test_pred)} transaction amounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "Model parameters - the learned line equatio\n",
    "n\n",
    "= 0 (part of the line equation)\n",
    "Coefficient (slope): How much Amount changes per unit of Time\n",
    "print(\"\\nğŸ“Š Model Parameters (The Learned Line):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø§Ù„Ø®Ø· Ø§Ù„Ù…ØªØ¹Ù„Ù…):\")\n",
    "model_simple.intercept_\n",
    "- intercept_: The y-intercept (bias term) of the regression line\n",
    "- Value when X = 0 (base price in this case)\n",
    "- Part of equation: \n",
    "y = coef_ * X + intercept_\n",
    "- Access as attribute (not method, no parentheses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Intercept (bias): ${model_simple.intercept_:,.2f}\")\n",
    "\n",
    "model_simple.coef_[0]\n",
    "- coef_: Array of coefficients (slopes) \n",
    "for each feature\n",
    "- For simple regression: one coefficient (slope)\n",
    "- For multiple regression: one coefficient per feature\n",
    "- [0]: Gets first coefficient (\n",
    "for simple regression, there's only one)\n",
    "- Interpretation: How much y changes when X increases by 1 unit\n",
    "- Access as attribute (not method)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Coefficient (slope): ${model_simple.coef_[0]:.6f} per unit of Time (per second)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n Equation: Amount = {model_simple.coef_[0]:.6f} Ã— Time + {model_simple.intercept_:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Interpretation: For every 1 second increase in Time, Amount changes by ${model_simple.coef_[0]:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" (Positive slope = Amount tends to increase with Time, Negative = Amount tends to decrease with Time)\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "# Evaluate model performance using multiple metrics\n",
    "# Why multiple metrics? Each tells us something different:\n",
    "# - MSE: Penalizes large errors more (squared)\n",
    "# - MAE: Average error in dollars (easier to interpret)\n",
    "# - RÂ²: How well model fits (0-1, 1 = perfect, 0 = no better than average)\n",
    "\n",
    "# mean_squared_error(y_true, y_pred)\n",
    "# - Measures Mean Squared Error (MSE) - average squared error\n",
    "# - Formula: average of (actual - predicted)Â²\n",
    "# - Penalizes large errors more (squared term)\n",
    "# - Lower is better (0 = perfect predictions)\n",
    "# - Units: squared units of target (e.g., $Â² \n",
    "# for prices)\n",
    "# train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "# test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# mean_absolute_error(y_true, y_pred)\n",
    "# - Calculates Mean Absolute Error (MAE)\n",
    "# - Formula: average of |actual - predicted|\n",
    "# - Easier to interpret than MSE (same units as target)\n",
    "# - Less sensitive to outliers than MSE\n",
    "# - Lower is better (0 = perfect predictions)\n",
    "# train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "# test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# r2_score(y_true, y_pred)\n",
    "# - Calculates RÂ² (R-squared) score\n",
    "# - Measures how well model fits dat\n",
    "# a\n",
    "# = worse)\n",
    "# - Formula: 1 - (sum of squared errors) / (sum of squared deviations \n",
    "from mean)\n",
    "# - Higher is better\n",
    "# - Interpretation: % of variance explained by modeltrain_r2 = r2_score(y_train, y_train_pred)\n",
    "# test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“Š Training Metrics (How well model learned):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" MSE: ${train_mse:,.2f} (lower is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" MAE: ${train_mae:,.2f} (average error in dollars)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" RÂ² Score: {train_r2:.4f} (closer to 1 is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“Š Test Metrics (How well model generalizes):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" MSE: ${test_mse:,.2f} (lower is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" MAE: ${test_mae:,.2f} (average error in dollars)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" RÂ² Score: {test_r2:.4f} (closer to 1 is better)\")\n",
    "\n",
    "# Check \n",
    "# for overfitting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if test_r2 > train_r2 * 0.95:\n",
    "#  print(\"\\n âœ… Good! Test RÂ² is close to training RÂ² - model generalizes well!\")\n",
    "# elif test_r2 < 0:\n",
    "#  print(\"\\n ğŸ’¡ Note: Negative RÂ² means model performs worse than just predicting the mean.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" This is EXPECTED for simple regression with Time alone - Time is not a strong predictor of Amount.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" In real GDI financial investigations, multiple features (V1-V28, transaction history, etc.) are used.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" This demonstrates why we need multiple regression (Part 2) for better predictions!\")\n",
    "# else:\n",
    "#  print(\"\\n âš ï¸ Warning: Test RÂ² is much lower - possible overfitting!\")\n",
    "\n",
    "# Add practical meaning of outputs\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"ğŸ’¡ What Do These Outputs Mean? (Practical Value)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ù…Ø§Ø°Ø§ ØªØ¹Ù†ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§ØªØŸ (Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# + \"âš ï¸\" * 30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"IMPORTANT: Understanding Low RÂ² (Don't Worry - This is Expected!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"âš ï¸\" * 30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nâ“ You might ask: 'The RÂ² is low/negative - did we choose the wrong dataset or model?'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nâœ… ANSWER: NO! This is EXPECTED and INTENTIONAL for learning:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" 1. We used ONE feature (Time) to teach SIMPLE regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" 2. Time alone doesn't strongly predict Amount (this is realistic!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" 3. This demonstrates WHY we need multiple features (Part 2)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" 4. Real fraud detection uses 10+ features, not just Time\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" 5. This is a teaching strategy: Simple â†’ See limitations â†’ Extend to multiple\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ’¡ Key Insight: Low performance here TEACHES US that:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - One feature is often not enough\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Multiple features improve predictions (we'll see this in Part 2)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Understanding limitations is part of learning ML!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“Š Interpreting the Results:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - MAE = ${test_mae:.2f}: On average, our predictions are off by ${test_mae:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - RÂ² = {test_r2:.4f}: This model explains {abs(test_r2)*100:.2f}% of variance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if test_r2 < 0:\n",
    "#  print(\" - âš ï¸ Negative RÂ²: Model performs worse than just predicting the mean\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" - âœ… This is EXPECTED: Time alone is not a strong predictor of Amount\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ¯ Why This Algorithm? Why These Outputs Matter:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" **For GDI Financial Investigations:**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - **Predict Transaction Amounts**: Help identify unusual transaction patterns\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - **Identify Anomalies**: Predictions far from actual = potential fraud\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - **Resource Planning**: Predict transaction volumes for staffing/security\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - **Pattern Analysis**: Understand how transaction features relate to amounts\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n **Why Linear Regression?**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - **Fast**: Quick predictions on large transaction datasets\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - **Interpretable**: Can explain HOW features affect amounts (coefficients)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - **Baseline**: Starting point before using complex fraud detection models\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - **Actionable**: Outputs directly inform investigation decisions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n **What Can We Do With These Results?**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Flag transactions where actual >> predicted (unusually large amounts)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Analyze patterns in transaction timing (Time feature importance)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Build better models with more features (Part 2: Multiple Regression)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Inform security measures based on predicted transaction volumes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"ğŸ“ Learning Outcome: Why Simple Regression Has Limitations\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" You've learned that:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" âœ… Simple regression (1 feature) is easy to understand\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" âš ï¸ But it often has poor performance (as we see here)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" âœ… This motivates us to use multiple features (Part 2)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" âœ… Multiple regression will show BETTER performance!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n ğŸ’¡ This is EXACTLY how ML is learned in practice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" Start simple â†’ See limitations â†’ Add complexity â†’ Better results\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Simple Linear Regression | ØªØµÙˆØ± Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ·\n",
    "\n",
    "**BEFORE**: We've built the model and made predictions, but we need to see how well it fits the data!\n",
    "\n",
    "**AFTER**: We'll create plots showing the actual data points and the regression line!\n",
    "\n",
    "**What you'll see in the plots:**\n",
    "- **Points (scatter)**: The actual data points (real Time values and transaction Amounts)\n",
    "  - Blue points = Training data (what the model learned from)\n",
    "  - Green points = Test data (what the model predicts on)\n",
    "- **Red Line**: The regression line (model's predictions)\n",
    "  - Shows how the model predicts Amount from Time\n",
    "  - If points are close to the line = good predictions!\n",
    "  - If points are far from the line = poor predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "Visualize\n",
    "axes[0].scatter() - Creates POINTS showing actual data\n",
    "- X_train: Time values (x-axis)\n",
    "- y_train: Actual transaction Amounts (y-axis)\n",
    "= Real data points (what actually happened)\n",
    "\n",
    "axes[0].plot() - Creates LINE showing model predictions\n",
    "- X_train: Time values (x-axis)\n",
    "- y_train_pred: Predicted Amounts (y-axis)\n",
    "= Model's predictions (what model thinks Amount should be)\n",
    "= good model!\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "Data') \n",
    "Line') \n",
    "LINE = predictionsaxes[0].set_xlabel('Time (seconds)')\n",
    "axes[0].set_ylabel('Amount (dollars)')\n",
    "axes[0].set_title('Simple Linear Regression - Training Data')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "axes[1].plot(X_test, y_test_pred, 'r-', linewidth=2, label='Regression Line')\n",
    "axes[1].set_xlabel('Time (seconds)')\n",
    "axes[1].set_ylabel('Amount (dollars)')\n",
    "axes[1].set_title('Simple Linear Regression - Test Data')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('simple_linear_regression.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nâœ“ Plot saved as 'simple_linear_regression.png'\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multiple Linear Regression | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯\n",
    "\n",
    "**Now that we've seen simple regression with one feature, let's extend to multiple features for better predictions!**\n",
    "\n",
    "**BEFORE**: We used one feature (Time) to predict Amount, but we saw that Time alone is NOT a strong predictor (low/negative RÂ²). Real-world predictions need multiple features for better accuracy.\n",
    "\n",
    "**AFTER**: We'll use multiple features (Time, V1, V2, V3) to predict Amount - this is what GDI actually uses in real financial investigations for fraud detection!\n",
    "\n",
    "**Why multiple regression?**\n",
    "- **More features = Better predictions**: Real-world financial investigations have many factors\n",
    "- **Same concept**: Still finding a line, but in higher dimensions\n",
    "- **Feature importance**: We can see which features matter most\n",
    "- **Real-world use**: Most ML problems use multiple features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Part 2: Multiple Linear Regression (Multiple Features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ (Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ’¡ Remember: Same workflow as simple regression, but with multiple features!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" We'll use Time, V1, V2, V3 to predict Amount (instead of just Time)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Load Credit Card Fraud dataset (already loaded above, but use full sample \n",
    "for multiple regression)\n",
    "For multiple regression, we'll use Time and some V features to predict Amount\n",
    "print('\\nğŸ“¥ Loading Credit Card Fraud dataset\n",
    "for multiple regression...\")\n",
    "df_full_m = \n",
    "File not found: ../../datasets/raw/creditcard_fraud.csv\n",
    "Using synthetic data insteadpd.DataFrame({'col1': range(100), 'col2': range(100, 200)})\n",
    "\n",
    "Use a larger sample \n",
    "for multiple regressionsample_size_m = 5000df_sample_m = df_full_m.sample(n=min(sample_size_m, len(df_full_m)), random_state=73, replace=False).reset_index(drop=True)\n",
    "\n",
    "For multiple regression: Use Time and V1, V2, V3, V4 as features to predict Amoun\n",
    "t\n",
    "= df_sample_m[['Time', 'V1', 'V2', 'V3', 'Amount']].copy()\n",
    "\n",
    "Rename \n",
    "for clarity (keeping original names but explaining them)\n",
    "Time â†’ Time (seconds elapsed)\n",
    "V1-V3 â†’ PCA-transformed features (already scaled)\n",
    "Amount â†’ Transaction Amount (target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nâœ… Real-world Credit Card Fraud data loaded for multiple regression!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" ğŸ“Š Using {len(df_multiple)} transaction records\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" ğŸ“ˆ Features: Time, V1, V2, V3 (4 features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Time: Time elapsed in seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - V1, V2, V3: PCA-transformed features (already scaled, anonymized)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" ğŸ’° Target: Amount (transaction amount in dollars)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" ğŸ“‹ Note: V features are PCA-transformed (pre-scaled), so we use them directly\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ“„ First 5 rows:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df_multiple.head().round(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ” Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - REAL anonymized credit card transaction data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Multiple features affect transaction amount (Time, V1, V2, V3)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Real-world scenario: Financial investigations analyze transaction patterns using multiple features!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" ğŸ¯ Domain: GDI Financial Investigations - Multi-Factor Transaction Analysis\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" ğŸ“‹ Note: V features are PCA-transformed (already scaled), perfect for regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# Data is already loaded \n",
    "from financial transaction dataset above\n",
    "# The 'transaction_amount' column is already included in df_multiple\n",
    "# No need to generate synthetic data - we have real data structure!\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data for Multiple Regression | ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯\n",
    "\n",
    "**BEFORE**: We have features (X_multiple) and target (y_multiple), but need to split them!\n",
    "\n",
    "**AFTER**: We'll split into training and test sets - same concept as simple regression!\n",
    "\n",
    "**Why split?**\n",
    "- **Same reason as before**: Need separate training and test sets\n",
    "- **Training set**: Model learns from X_train_m and y_train_m\n",
    "- **Test set**: Model is evaluated on X_test_m and y_test_m\n",
    "- **Purpose**: Check if model generalizes to new data (not just memorizes)\n",
    "\n",
    "**What we get:**\n",
    "- **X_train_m**: Training features (Time, V1, V2, V3) - 80% of data\n",
    "- **y_train_m**: Training targets (Amount) - 80% of data\n",
    "- **X_test_m**: Test features - 20% of data\n",
    "- **y_test_m**: Test targets - 20% of data\n",
    "\n",
    "**Note**: The `_m` suffix means \"multiple\" (multiple features), to distinguish from simple regression variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "Prepare features (X) and target (y)\n",
    "X = Features (input variables) - what we use to PREDICT\n",
    "= df_multiple['Amount'] # 1 target: Transaction Amount\n",
    "\n",
    "Show what we prepared\n",
    "print(\"\\nğŸ“Š Features (X) and Target (y) Prepared:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ø§Ù„Ù…ÙŠØ²Ø§Øª (X) ÙˆØ§Ù„Ù‡Ø¯Ù (y) Ø¬Ø§Ù‡Ø²Ø©:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n Features (X) shape: {X_multiple.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - {X_multiple.shape[0]} samples, {X_multiple.shape[1]} features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Features: {list(X_multiple.columns)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n Target (y) shape: {y_multiple.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - {y_multiple.shape[0]} samples, 1 target\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Target: Amount\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n âœ… Ready for model training!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Model will use {X_multiple.shape[1]} features to predict 1 target\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# Split data\n",
    "\n",
    "# Any number works - just \n",
    "# for reproducibility, strat\n",
    "\n",
    "# ify=y)\n",
    "# - Splits data into training and testing sets\n",
    "# - X: Features (input variables), y: Target (output variable)\n",
    "# : 20% \n",
    "# for testing, 80%\n",
    "# for training\n",
    "\n",
    "# Any number works - just \n",
    "# for reproducibility: Seed \n",
    "# for reproducibility (same split every time)\n",
    "# - strat\n",
    "\n",
    "# ify=y: Maintains class distribution in train/test (for classification)\n",
    "# = train_test_split(\n",
    "#  X_multiple, y_multiple, test_size=0.2, random_state=73 \n",
    "# Using 73 \n",
    "# for consistency\n",
    ")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "= LinearRegression()\n",
    "model_multiple.fit(X_train_m, y_train_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "= model_multiple.predict(X_train_m)\n",
    "y_test_pred_m = model_multiple.predict(X_test_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "Model parameters\n",
    "print(\"\\nModel Parameters:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Intercept: {model_multiple.intercept_:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nCoefficients:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:\")\n",
    "\n",
    "for feature, coef in zip(X_multiple.columns, model_multiple.coef_):\n",
    "print(f\" {feature}: {coef:.4f}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "= mean_squared_error(y_train_m, y_train_pred_m)\n",
    "test_mse_m = mean_squared_error(y_test_m, y_test_pred_m)\n",
    "train_r2_m = r2_score(y_train_m, y_train_pred_m)\n",
    "test_r2_m = r2_score(y_test_m, y_test_pred_m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" MSE: {train_mse_m:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" RMSE: {np.sqrt(train_mse_m):,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" RÂ² Score: {train_r2_m:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" MSE: {test_mse_m:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" RMSE: {np.sqrt(test_mse_m):,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" RÂ² Score: {test_r2_m:.4f}\")\n",
    "\n",
    "Add interpretation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ğŸ’¡ What Do These Outputs Mean? Why Does This Matter?\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Ù…Ø§Ø°Ø§ ØªØ¹Ù†ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§ØªØŸ Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ø§ Ù…Ù‡Ù…ØŸ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ¯ Practical Value for GDI Financial Investigations:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" **Multiple Regression Outputs:**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - RÂ² = {test_r2_m:.4f}: Model explains {test_r2_m*100:.1f}% of transaction amount variance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - RMSE = ${np.sqrt(test_mse_m):,.2f}: Average prediction error is ${np.sqrt(test_mse_m):,.0f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n **Why This Algorithm is Valuable:**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - **Better Predictions**: Multiple features (Time, V1, V2, V3) capture more patterns than Time alone\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - **Fraud Detection**: Identify transactions where actual >> predicted (suspicious)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - **Feature Importance**: See which features (coefficients) most affect transaction amounts\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - **Decision Support**: Predict amounts to inform investigation priorities\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n **Real-World GDI Application:**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Predict transaction amounts based on transaction patterns (Time, anonymized features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Flag unusual transactions: Actual amount much higher than predicted = investigate\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Understand relationships: Which transaction patterns lead to larger amounts?\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Resource allocation: Predict transaction volumes to allocate investigation resources\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n **Why Linear Regression (Not Other Algorithms)?**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - âœ… Fast predictions on large datasets (important for real-time monitoring)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - âœ… Interpretable coefficients (can explain WHY features matter)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - âœ… Baseline model (compare against before trying complex models)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - âœ… Regulatory compliance (interpretable models for financial investigations)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ğŸ“ˆ Comparing Simple vs Multiple Regression Performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" **Notice the Improvement:**\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Simple Regression (Time only): RÂ² was low/negative\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Multiple Regression (Time + V1 + V2 + V3): RÂ² = {test_r2_m:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if test_r2_m > 0.1:\n",
    "print(f\" - âœ… Multiple regression is MUCH better! ({test_r2_m*100:.1f}% vs ~0%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - This proves: More features = Better predictions!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - ğŸ’¡ This is WHY we use multiple features in real GDI fraud detection\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ğŸ’¡ Interpreting the Metrics | ØªÙØ³ÙŠØ± Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ“Š RÂ² Score (R-squared):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Training: {train_r2_m:.2%} | Test: {test_r2_m:.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Range: -âˆ to 1.0 (higher is better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - 1.0 = Perfect predictions (all variance explained)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - 0.0 = Model is as good as predicting the mean\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - <0.0 = Model is worse than just predicting the mean\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if test_r2_m >= 0.9:\n",
    "print(f\" - âœ… EXCELLENT! (>0.9 means model explains >90% of variance)\")\n",
    "elif test_r2_m >= 0.7:\n",
    "print(f\" - âœ… GOOD! (>0.7 means model explains >70% of variance)\")\n",
    "elif test_r2_m >= 0.5:\n",
    "print(f\" - âš ï¸ FAIR (>0.5 means model explains >50% of variance)\")\n",
    "else:\n",
    "print(f\" - âš ï¸ POOR (<0.5 means model explains <50% of variance)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ“Š RMSE (Root Mean Squared Error):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Training: ${np.sqrt(train_mse_m):,.2f} | Test: ${np.sqrt(test_mse_m):,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Lower is better (measures average prediction error)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - In same units as target (Amount in dollars)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Average prediction error: ${np.sqrt(test_mse_m):,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - This means predictions are typically off by ${np.sqrt(test_mse_m):,.0f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ” Comparing Train vs Test:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if abs(train_r2_m - test_r2_m) < 0.05:\n",
    "print(f\" - âœ… Similar RÂ² ({train_r2_m:.2%} vs {test_r2_m:.2%}) - Good generalization!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Model performs similarly on new data (not overfitting)\")\n",
    "else:\n",
    "print(f\" - âš ï¸ Different RÂ² ({train_r2_m:.2%} vs {test_r2_m:.2%})\")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "if train_r2_m > test_r2_m:\n",
    "print(f\" - Training RÂ² is higher - possible overfitting!\")\n",
    "else:\n",
    "print(f\" - Test RÂ² is higher - unusual but possible\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - RÂ² shows how much variance the model explains\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - RMSE shows actual prediction error in dollars\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Compare train vs test to check for overfitting\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Good models have high RÂ² and low RMSE\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - RÂ² > 0.7 is generally considered good for real-world problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Multiple Linear Regression | ØªØµÙˆØ± Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯\n",
    "\n",
    "**BEFORE**: We've built a multiple regression model, but how do we visualize it with multiple features?\n",
    "\n",
    "**AFTER**: We'll plot predicted vs actual transaction amounts to see how well the model performs!\n",
    "\n",
    "**What you'll see in the plots:**\n",
    "- **Points (scatter)**: Each point = one transaction\n",
    "  - X-axis: Actual Amount (what the transaction actually was)\n",
    "  - Y-axis: Predicted Amount (what the model predicted)\n",
    "  - If points are close to the diagonal line = good predictions!\n",
    "  - If points are far from the diagonal line = poor predictions\n",
    "- **Red Dashed Line**: Perfect prediction line (y = x)\n",
    "  - If predictions were perfect, all points would be on this line\n",
    "  - Points above line = model over-predicted (predicted higher than actual)\n",
    "  - Points below line = model under-predicted (predicted lower than actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# 5))\n",
    "\n",
    "# axes[0].plot([y_train_m.min(), y_train_m.max()], [y_train_m.min(), y_train_m.max()], 'r--', linewidth=2)\n",
    "# axes[0].set_xlabel('Actual Amount (dollars)')\n",
    "# axes[0].set_ylabel('Predicted Amount (dollars)')\n",
    "# axes[0].set_title(f'Training Set (RÂ² = {train_r2_m:.4f})')\n",
    "# axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# axes[1].plot([y_test_m.min(), y_test_m.max()], [y_test_m.min(), y_test_m.max()], 'r--', linewidth=2)\n",
    "# axes[1].set_xlabel('Actual Amount (dollars)')\n",
    "# axes[1].set_ylabel('Predicted Amount (dollars)')\n",
    "# axes[1].set_title(f'Test Set (RÂ² = {test_r2_m:.4f})')\n",
    "# axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('multiple_linear_regression.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nâœ“ Plot saved as 'multiple_linear_regression.png'\")\n",
    "plt.show()\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Residuals | Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ\n",
    "\n",
    "**Now that we understand what residuals are, let's calculate them for our multiple regression model!**\n",
    "\n",
    "**BEFORE**: We know what residuals are, now let's calculate them!\n",
    "\n",
    "**AFTER**: We'll compute residuals and check their statistics to see if our model is good!\n",
    "\n",
    "**What we'll calculate:**\n",
    "- **Residuals**: `residuals = y_test - y_test_pred`\n",
    "  - For each test sample: actual transaction amount - predicted transaction amount\n",
    "  - Positive = model under-predicted, Negative = model over-predicted\n",
    "- **Statistics**: Mean, standard deviation, min, max\n",
    "  - Mean close to 0 = no bias âœ…\n",
    "  - Small std = consistent predictions âœ…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Residuals | ØªØµÙˆØ± Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ\n",
    "\n",
    "**BEFORE**: We've calculated residual statistics, but we need to visualize them to see patterns!\n",
    "\n",
    "**AFTER**: We'll create plots to check if residuals are randomly distributed (good) or have patterns (indicates problems)!\n",
    "\n",
    "**Why visualize residuals?**\n",
    "- **Check assumptions**: Linear regression assumes residuals are random and normally distributed\n",
    "- **Detect patterns**: Patterns in residuals indicate model problems (non-linearity, heteroscedasticity)\n",
    "- **Diagnose issues**: Visual inspection helps identify what's wrong with the model\n",
    "- **Validate model**: Good models have randomly scattered residuals around zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate residuals (prediction errors)\n",
    "= Actual values - Predicted values\n",
    "= y_test_m - y_test_pred_m\n",
    "print(f\"âœ… Residuals calculated: {len(residuals)} residuals\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Mean residual: ${residuals.mean():.2f} (should be close to 0)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Std residual: ${residuals.std():.2f}\")\n",
    "\n",
    "5))\n",
    "\n",
    "axes[0].axvline(0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Residuals')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Residuals Distribution')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "axes[1].axhline(0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Residuals vs Predicted Values')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('residuals_analysis.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nâœ“ Plot saved as 'residuals_analysis.png'\")\n",
    "if 'plt' in globals():\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Example 4 Complete! âœ“\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 4! âœ“\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Fixing Prediction Bias | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø¹: Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ­ÙŠØ² ÙÙŠ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª\n",
    "\n",
    "**Sometimes models have systematic bias - let's learn how to identify and address it!**\n",
    "\n",
    "### What to Do When Predictions Are Too Low (or Too High) | Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ù‹Ø§ (Ø£Ùˆ Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ù‹Ø§)\n",
    "\n",
    "**BEFORE**: You've analyzed residuals and found that your model systematically under-predicts (or over-predicts). Now what?\n",
    "\n",
    "**AFTER**: You'll learn practical solutions to fix systematic bias in your predictions!\n",
    "\n",
    "**The Problem We Found:**\n",
    "- **Residual Mean**: 5748.12 (not close to 0)\n",
    "- **Interpretation**: Model tends to **UNDER-predict** (predictions are too low)\n",
    "- **Meaning**: On average, the model predicts prices that are $5,748 lower than actual prices\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Systematic bias** = Model consistently makes the same type of error\n",
    "- **Good models** should have residual mean close to 0 (no bias)\n",
    "- **Biased models** = Poor predictions, even if RÂ² is high\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Diagnose the Problem | ØªØ´Ø®ÙŠØµ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©\n",
    "\n",
    "**Check Your Residuals:**\n",
    "\n",
    "1. **Residual Mean**:\n",
    "   - **Close to 0** (Â±small value) â†’ âœ… No bias, model is good\n",
    "   - **Positive mean** â†’ âš ï¸ Model UNDER-predicts (predictions too low)\n",
    "   - **Negative mean** â†’ âš ï¸ Model OVER-predicts (predictions too high)\n",
    "\n",
    "2. **Residual Patterns**:\n",
    "   - **Random scatter** â†’ âœ… Good (no patterns)\n",
    "   - **Curved pattern** â†’ âš ï¸ Non-linear relationship (need polynomial regression)\n",
    "   - **Funnel shape** â†’ âš ï¸ Heteroscedasticity (variance changes with predictions)\n",
    "\n",
    "3. **Residual Distribution**:\n",
    "   - **Normal distribution** â†’ âœ… Good (assumption met)\n",
    "   - **Skewed distribution** â†’ âš ï¸ Model bias or outliers\n",
    "\n",
    "**In Our Case:**\n",
    "- Mean = 5748.12 (positive) â†’ Model UNDER-predicts\n",
    "- Need to investigate WHY and fix it\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ› ï¸ Solutions to Fix Systematic Bias | Ø­Ù„ÙˆÙ„ Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ­ÙŠØ² Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠ\n",
    "\n",
    "#### Solution 1: Check for Missing Features | Ø§Ù„Ø­Ù„ 1: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
    "\n",
    "**Problem**: Model might be missing important features that affect the target.\n",
    "\n",
    "**Example:**\n",
    "- Predicting house price from size only\n",
    "- But price also depends on location, age, condition\n",
    "- Missing features â†’ Model can't capture full relationship â†’ Under-predicts\n",
    "\n",
    "**What to Do:**\n",
    "1. **Review your data**: Are there other features that affect the target?\n",
    "2. **Add relevant features**: Include features that logically affect predictions\n",
    "3. **Check feature importance**: Use domain knowledge to identify missing factors\n",
    "\n",
    "---\n",
    "\n",
    "**Code Example (see cell below for executable version):**\n",
    "- Before: Only using Time\n",
    "- After: Using multiple relevant features (Time, V1, V2, V3, V4, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "#### Solution 2: Check for Data Quality Issues | Ø§Ù„Ø­Ù„ 2: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø´Ø§ÙƒÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**Problem**: Data issues can cause bias:\n",
    "- **Outliers**: Extreme values skew the model\n",
    "- **Missing values**: Incorrectly handled missing data\n",
    "- **Data leakage**: Using future information\n",
    "- **Wrong target values**: Incorrect labels in training data\n",
    "\n",
    "**What to Do:**\n",
    "1. **Check for outliers**: Plot data, look for extreme values\n",
    "2. **Handle missing values**: Impute or remove missing data properly\n",
    "3. **Verify target values**: Make sure y values are correct\n",
    "4. **Check data distribution**: Ensure training and test data are similar\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Check for outliers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(y_train)\n",
    "plt.title('Check for Outliers in Target')\n",
    "plt.show()\n",
    "\n",
    "# Remove outliers (if appropriate)\n",
    "Q1 = y_train.quantile(0.25)\n",
    "Q3 = y_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = (y_train < (Q1 - 1.5 * IQR)) | (y_train > (Q3 + 1.5 * IQR))\n",
    "X_train_clean = X_train[~outliers]\n",
    "y_train_clean = y_train[~outliers]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Solution 3: Try Feature Engineering | Ø§Ù„Ø­Ù„ 3: ØªØ¬Ø±Ø¨Ø© Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
    "\n",
    "**Problem**: Raw features might not capture relationships well.\n",
    "\n",
    "**What to Do:**\n",
    "1. **Create interaction features**: Multiply features together (e.g., size Ã— bedrooms)\n",
    "2. **Transform features**: Log, square, or other transformations\n",
    "3. **Create polynomial features**: Add squared or cubed terms\n",
    "4. **Normalize/scale features**: Ensure all features are on similar scales\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Create interaction features\n",
    "df['size_bedrooms'] = df['size'] * df['bedrooms']\n",
    "df['size_location'] = df['size'] * df['location_score']\n",
    "\n",
    "# Transform features (if needed)\n",
    "df['log_size'] = np.log(df['size'])\n",
    "\n",
    "# Use new features\n",
    "X = df[['size', 'bedrooms', 'size_bedrooms', 'size_location']]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Solution 4: Try Different Models | Ø§Ù„Ø­Ù„ 4: ØªØ¬Ø±Ø¨Ø© Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØªÙ„ÙØ©\n",
    "\n",
    "**Problem**: Linear regression might not be appropriate for your data.\n",
    "\n",
    "**What to Do:**\n",
    "1. **Polynomial Regression**: If relationship is curved (non-linear)\n",
    "2. **Ridge/Lasso Regression**: If you have many features (regularization)\n",
    "3. **Random Forest**: If relationship is complex and non-linear\n",
    "4. **XGBoost**: For best performance on complex patterns\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Try Polynomial Regression (for non-linear relationships)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Solution 5: Adjust the Intercept (Last Resort) | Ø§Ù„Ø­Ù„ 5: ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªÙ‚Ø§Ø·Ø¹ (Ø­Ù„ Ø£Ø®ÙŠØ±)\n",
    "\n",
    "**Problem**: Model intercept might be systematically wrong.\n",
    "\n",
    "**What to Do:**\n",
    "1. **Calculate mean residual**: This is the systematic bias\n",
    "2. **Adjust predictions**: Add mean residual to all predictions\n",
    "3. **Note**: This is a \"band-aid\" solution - better to fix the root cause!\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Calculate mean residual (systematic bias)\n",
    "mean_residual = residuals.mean()\n",
    "print(f\"Systematic bias: {mean_residual:.2f}\")\n",
    "\n",
    "# Adjust predictions\n",
    "y_test_pred_adjusted = y_test_pred + mean_residual\n",
    "\n",
    "# Recalculate metrics\n",
    "mse_adjusted = mean_squared_error(y_test, y_test_pred_adjusted)\n",
    "print(f\"Original MSE: {test_mse:.2f}\")\n",
    "print(f\"Adjusted MSE: {mse_adjusted:.2f}\")\n",
    "```\n",
    "\n",
    "**âš ï¸ Warning**: This doesn't fix the model - it just shifts predictions. Better to fix the root cause!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Decision Tree: Which Solution to Try? | Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±: Ø£ÙŠ Ø­Ù„ ØªØ¬Ø±Ø¨ØŸ\n",
    "\n",
    "**Start Here:**\n",
    "\n",
    "1. **Check residual mean**:\n",
    "   - If close to 0 â†’ âœ… Model is good, no action needed\n",
    "   - If far from 0 â†’ Continue to step 2\n",
    "\n",
    "2. **Check residual patterns**:\n",
    "   - **Curved pattern** â†’ Try Solution 4 (Polynomial Regression)\n",
    "   - **Random scatter** â†’ Continue to step 3\n",
    "\n",
    "3. **Check your features**:\n",
    "   - **Few features (<5)** â†’ Try Solution 1 (Add more features)\n",
    "   - **Many features (>10)** â†’ Try Solution 3 (Feature engineering)\n",
    "\n",
    "4. **Check data quality**:\n",
    "   - **Outliers present** â†’ Try Solution 2 (Fix data quality)\n",
    "   - **No outliers** â†’ Continue to step 5\n",
    "\n",
    "5. **Try different models**:\n",
    "   - **Linear relationship** â†’ Current model should work (check features)\n",
    "   - **Non-linear relationship** â†’ Try Solution 4 (Different models)\n",
    "\n",
    "6. **Last resort**:\n",
    "   - If nothing works â†’ Try Solution 5 (Adjust intercept)\n",
    "   - But remember: This is a band-aid, not a real fix!\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Residual mean â‰  0** = Systematic bias (model consistently wrong)\n",
    "2. **Positive mean** = Under-prediction (predictions too low)\n",
    "3. **Negative mean** = Over-prediction (predictions too high)\n",
    "4. **Fix root cause** = Better than adjusting predictions\n",
    "5. **Check features first** = Most common cause of bias\n",
    "6. **Try different models** = If linear regression isn't appropriate\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Next Steps | Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©\n",
    "\n",
    "**After fixing bias:**\n",
    "1. **Re-train model** with improved features/data\n",
    "2. **Re-evaluate** residuals (should be close to 0 now)\n",
    "3. **Check metrics** (MSE, MAE, RÂ² should improve)\n",
    "4. **Validate** on new data to ensure fix works\n",
    "\n",
    "**If bias persists:**\n",
    "- Consider that linear regression might not be appropriate\n",
    "- Try polynomial regression or other non-linear models\n",
    "- Consult domain experts about missing features\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for outliers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(y_train)\n",
    "plt.title('Check for Outliers in Target')\n",
    "plt.show()\n",
    "\n",
    "# Remove outliers (if appropriate)\n",
    "Q1 = y_train.quantile(0.25)\n",
    "Q3 = y_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = (y_train < (Q1 - 1.5 * IQR)) | (y_train > (Q3 + 1.5 * IQR))\n",
    "X_train_clean = X_train[~outliers]\n",
    "y_train_clean = y_train[~outliers]"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Create interaction features\n",
    "df['size_bedrooms'] = df['size'] * df['bedrooms']\n",
    "df['size_location'] = df['size'] * df['location_score']\n",
    "\n",
    "# Transform features (if needed)\n",
    "df['log_size'] = np.log(df['size'])\n",
    "\n",
    "# Use new features\n",
    "X = df[['size', 'bedrooms', 'size_bedrooms', 'size_location']]"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Try Polynomial Regression (for non-linear relationships)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly, y_train)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate mean residual (systematic bias)\n",
    "mean_residual = residuals.mean()\n",
    "print(f\"Systematic bias: {mean_residual:.2f}\")\n",
    "\n",
    "# Adjust predictions\n",
    "y_test_pred_adjusted = y_test_pred + mean_residual\n",
    "\n",
    "# Recalculate metrics\n",
    "mse_adjusted = mean_squared_error(y_test, y_test_pred_adjusted)\n",
    "print(f\"Original MSE: {test_mse:.2f}\")\n",
    "print(f\"Adjusted MSE: {mse_adjusted:.2f}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Solution 1 Example: Adding More Features\n",
    "This demonstrates how to add more features to improve predictions\n",
    "print('=\" * 60)\")\n",
    "\n",
    "\n",
    "print(\"Solution 1: Adding More Features to Fix Bias\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "Before: Only using Time (simple regression - we saw low RÂ²)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“Š Before (Simple Regression - Time only):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" Features: Time only\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" Result: Low RÂ² (as we saw in Part 1)\")\n",
    "\n",
    "After: Using multiple relevant features (multiple regression - better RÂ²)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nâœ… After (Multiple Regression - Time + V1 + V2 + V3):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Features: Time, V1, V2, V3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Result: Better RÂ² = {test_r2_m:.4f} (as we saw in Part 2)\")\n",
    "\n",
    "Show how to add even more features\n",
    "print(\"\\nğŸ’¡ To add even more features\")\n",
    "from the dataset:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"# Original features: Time, V1, V2, V3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"# Add more V features (V4, V5, V6, etc.)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" X_extended = df_multiple[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"# = Better predictions (but watch for overfitting!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Solution 2 Example: Checking \n",
    "for Outliers in Target Variable\n",
    "This demonstrates how to check \n",
    "for and handle outliers that might cause bias\n",
    "print('=\" * 60)\")\n",
    "\n",
    "\n",
    "print(\"Solution 2: Checking for Data Quality Issues (Outliers)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Check \n",
    "for outliers in target variable (Amount)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“Š Checking for outliers in target variable (Amount):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Mean Amount: ${y_multiple.mean():.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Median Amount: ${y_multiple.median():.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Std Amount: ${y_multiple.std():.2f}\")\n",
    "\n",
    "Calculate IQR \n",
    "for outliersQ1 = y_multiple.quantile(0.25)\n",
    "Q3 = y_multiple.quantile(0.75)\n",
    "Q1 (25th percentile): ${Q1:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Q3 (75th percentile): ${Q3:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" IQR: ${IQR:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Lower bound (Q1 - 1.5*IQR): ${lower_bound:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Upper bound (Q3 + 1.5*IQR): ${upper_bound:.2f}\")\n",
    "\n",
    "= (y_multiple < lower_bound) | (y_multiple > upper_bound)\n",
    "num_outliers = outliers.sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n âš ï¸ Number of outliers: {num_outliers} ({num_outliers/len(y_multiple)*100:.1f}%)\")\n",
    "\n",
    "4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(y_multiple)\n",
    "plt.title('Boxplot: Check for Outliers in Amount')\n",
    "plt.ylabel('Amount (dollars)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_multiple, bins=50, edgecolor='black')\n",
    "plt.axvline(upper_bound, color='r', linestyle='--', label=f'Upper bound: ${upper_bound:.0f}')\n",
    "plt.title('Histogram: Distribution of Amount')\n",
    "plt.xlabel('Amount (dollars)')\n",
    "plt.ylabel('Frequency')\n",
    "if 'plt' in globals():\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ’¡ Note: In this dataset, outliers might be legitimate high-value transactions.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" Only remove outliers\")\n",
    "\n",
    "\n",
    "\n",
    "if they're data errors, not \n",
    "\n",
    "\n",
    "\n",
    "if they're real values!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Solution 3 Example: Feature Engineering\n",
    "This demonstrates how to create interaction features and transform features\n",
    "print('=\" * 60)\")\n",
    "\n",
    "\n",
    "print(\"Solution 3: Feature Engineering (Creating New Features)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "Create interaction features (multiply features together)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“Š Creating interaction features:\")\n",
    "\n",
    "Example: Time Ã— V1 interaction (might capture non-linear relationships)\n",
    "df_multiple_demo = df_multiple.copy()\n",
    "df_multiple_demo['Time_V1'] = df_multiple_demo['Time'] * df_multiple_demo['V1']\n",
    "df_multiple_demo['Time_V2'] = df_multiple_demo['Time'] * df_multiple_demo['V2']\n",
    "\n",
    "print(\" âœ… Created: Time Ã— V1 interaction\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" âœ… Created: Time Ã— V2 interaction\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ğŸ’¡ Interaction features can capture relationships like:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - 'High Time AND high V1 â†’ Higher Amount'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Non-linear relationships that simple features can't capture\")\n",
    "\n",
    "Transform features (e.g., log transformation \n",
    "for skewed data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“Š Transforming features:\")\n",
    "\n",
    "Example: Log transform Amount (only \n",
    "for demonstration - we predict Amount, so we don't transform it)\n",
    "But we could transform Time \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if it's skewed\n",
    "import numpy as np\n",
    "\n",
    "Check \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if Time is skewed\n",
    "time_skew = df_multiple_demo['Time'].skew()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Time skewness: {time_skew:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if abs(time_skew) > 1:\n",
    "print(\" âš ï¸ Time is skewed - log transform might help\")\n",
    "df_multiple_demo['log_Time'] = np.log1p(df_multiple_demo['Time']) # log1p handles zeros\n",
    "print(\" âœ… Created: log(Time + 1)\")\n",
    "else:\n",
    "print(\" âœ… Time is not heavily skewed - transformation not necessary\")\n",
    "\n",
    "Show the new features\n",
    "print(f\"\\nğŸ“‹ Original features: Time, V1, V2, V3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"ğŸ“‹ New engineered features: Time_V1, Time_V2\")\n",
    "\n",
    "Note: To use these features, you would:\n",
    "print(\"\\nğŸ’¡ To use engineered features:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" X_engineered = df_multiple_demo[['Time', 'V1', 'V2', 'V3', 'Time_V1', 'Time_V2']]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Train model with X_engineered instead of X_multiple\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"This often improves predictions by capturing interactions!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "Solution 4 Example: Trying D\n",
    "ifferent Models (Polynomial Regression)\n",
    "This demonstrates polynomial regression \n",
    "for non-linear relationships\n",
    "Note: This will be covered in detail in the next notebook!\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Solution 4: Trying Different Models (Polynomial Regression)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“Š Current Model: Linear Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Assumes linear relationships\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Works well when relationship is straight line\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ”„ Alternative: Polynomial Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Handles curved (non-linear) relationships\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Creates polynomial features (x, xÂ², xÂ³, etc.)\")\n",
    "\n",
    "Quick demonstration of polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "print(\"\\nğŸ’¡ Example: Creating polynomial features (degree=2):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" Original features: [Time, V1, V2, V3]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" Polynomial features (degree=2):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Original: Time, V1, V2, V3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Squared: TimeÂ², V1Â², V2Â², V3Â²\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Interactions: TimeÃ—V1, TimeÃ—V2, V1Ã—V2, etc.\")\n",
    "\n",
    "Create polynomial features (quick demo with small sample)\n",
    "X_sample_poly = X_train_m[:100] \n",
    "Small sample \n",
    "for demonstrationpol\n",
    "y = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly_demo = poly.fit_transform(X_sample_poly)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n Original features shape: {X_sample_poly.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Polynomial features shape: {X_poly_demo.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" âœ… Polynomial regression creates {X_poly_demo.shape[1]} features from {X_sample_poly.shape[1]} original features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“š Note: Polynomial Regression will be covered in detail in:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" ğŸ““ Notebook: 05_polynomial_regression.ipynb\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" ğŸ’¡ It's useful when linear regression can't capture curved relationships\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "Solution 5 Example: Adjusting the Intercept (Last Resort)\n",
    "This demonstrates how to adjust predictions to correct systematic bias\n",
    "WARNING: This is a band-aid solution - better to fix root cause!\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Solution 5: Adjusting the Intercept (Last Resort - Band-aid Solution)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nâš ï¸ WARNING: This doesn't fix the model - it just shifts predictions!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" Better to fix the root cause (missing features, data quality, etc.)\")\n",
    "\n",
    "We already calculated mean_residual in Cell 36, but let's recalculate \n",
    "for clarity\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "Calculate mean residual (systematic bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“Š Step 1: Calculate mean residual (systematic bias)\")\n",
    "mean_residual = residuals.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Systematic bias (mean residual): {mean_residual:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if abs(mean_residual) > 1:\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "if mean_residual > 0:\n",
    "print(f\" âš ï¸ Model UNDER-predicts (predictions are too low by ${mean_residual:.2f} on average)\")\n",
    "else:\n",
    "print(f\" âš ï¸ Model OVER-predicts (predictions are too high by ${abs(mean_residual):.2f} on average)\")\n",
    "else:\n",
    "print(f\" âœ… No significant bias (mean close to 0)\")\n",
    "\n",
    "Adjust predictions (band-aid solution)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“Š Step 2: Adjust predictions by adding mean residual\")\n",
    "y_test_pred_adjusted = y_test_pred_m + mean_residual\n",
    "print(f\" Original predictions: y_test_pred_m\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Adjusted predictions: y_test_pred_m + {mean_residual:.2f}\")\n",
    "\n",
    "Recalculate metrics\n",
    "print(\"\\nğŸ“Š Step 3: Recalculate metrics\")\n",
    "mse_adjusted = mean_squared_error(y_test_m, y_test_pred_adjusted)\n",
    "mae_adjusted = mean_absolute_error(y_test_m, y_test_pred_adjusted)\n",
    "r2_adjusted = r2_score(y_test_m, y_test_pred_adjusted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n Original Metrics:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" MSE: {test_mse_m:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" MAE: {test_mae_m:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" RÂ²: {test_r2_m:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n Adjusted Metrics (after adding bias correction):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" MSE: {mse_adjusted:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" MAE: {mae_adjusted:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" RÂ²: {r2_adjusted:.4f}\")\n",
    "\n",
    "Check \n",
    "\n",
    "\n",
    "\n",
    "if adjustment helped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if mse_adjusted < test_mse_m:\n",
    "improvement = test_mse_m - mse_adjusted\n",
    "print(f\"\\n âœ… MSE improved by {improvement:,.2f} (small improvement)\")\n",
    "else:\n",
    "print(f\"\\n âš ï¸ MSE didn't improve (this is expected - bias correction doesn't always help)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"âš ï¸ REMEMBER: This is a BAND-AID, not a real fix!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" Always try Solutions 1-4 first (fix root cause)!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "Practical Example: Diagnosing and Addressing Systematic Bias\n",
    "This demonstrates how to check \n",
    "for bias and try solutions\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"5. Fixing Systematic Bias in Predictions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ­ÙŠØ² Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠ ÙÙŠ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "Diagnose the bias\n",
    "print(\"\\nğŸ“Š Diagnosing the Bias\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"ØªØ´Ø®ÙŠØµ Ø§Ù„ØªØ­ÙŠØ²\")\n",
    "mean_residual = residuals.mean()\n",
    "std_residual = residuals.std()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n Residual Mean: {mean_residual:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Residual Std: {std_residual:.2f}\")\n",
    "\n",
    "Determine \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if bias is significantbias_threshold = std_residual * 0.1 # 10% of std deviation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if abs(mean_residual) > bias_threshold:\n",
    "print(f\"\\n âš ï¸ SIGNIFICANT BIAS DETECTED!\")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "if mean_residual > 0:\n",
    "print(f\" - Model UNDER-predicts by ${mean_residual:,.2f} on average\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Predictions are consistently too LOW\")\n",
    "else:\n",
    "print(f\" - Model OVER-predicts by ${abs(mean_residual):,.2f} on average\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" - Predictions are consistently too HIGH\")\n",
    "else:\n",
    "print(f\"\\n âœ… No significant bias (mean close to 0)\")\n",
    "\n",
    "Try Solution 5 - Adjust predictions (demonstration only)\n",
    "Adjusting Predictions (Band-aid Solution)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª (Ø­Ù„ Ù…Ø¤Ù‚Øª)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n âš ï¸ WARNING: This doesn't fix the model, just shifts predictions!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" Better to fix root cause (missing features, data quality, etc.)\")\n",
    "\n",
    "= y_test_pred_m + mean_residual\n",
    "\n",
    "Calculate original MAE (\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not already calculated)\n",
    "test_mae_m = mean_absolute_error(y_test_m, y_test_pred_m)\n",
    "\n",
    "= mean_squared_error(y_test_m, y_test_pred_adjusted)\n",
    "mae_adjusted = mean_absolute_error(y_test_m, y_test_pred_adjusted)\n",
    "r2_adjusted = r2_score(y_test_m, y_test_pred_adjusted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n Original Metrics:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" MSE: {test_mse_m:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" MAE: {test_mae_m:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" RÂ²: {test_r2_m:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n Adjusted Metrics (after adding bias correction):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" MSE: {mse_adjusted:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" MAE: {mae_adjusted:,.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" RÂ²: {r2_adjusted:.4f}\")\n",
    "\n",
    "Check \n",
    "\n",
    "\n",
    "\n",
    "if adjustment helped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if mse_adjusted < test_mse_m:\n",
    "print(f\"\\n âœ… MSE improved by {test_mse_m - mse_adjusted:,.2f}\")\n",
    "else:\n",
    "print(f\"\\n âš ï¸ MSE didn't improve (this is expected - bias correction doesn't always help)\")\n",
    "\n",
    "Check residual patterns\n",
    "print(\"\\nğŸ“Š Checking Residual Patterns\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"ÙØ­Øµ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ\")\n",
    "\n",
    "= residuals_adjusted.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n Original Residual Mean: {mean_residual:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Adjusted Residual Mean: {mean_residual_adjusted:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if abs(mean_residual_adjusted) < abs(mean_residual):\n",
    "print(f\" âœ… Bias reduced! (closer to 0)\")\n",
    "else:\n",
    "print(f\" âš ï¸ Bias not fully corrected\")\n",
    "\n",
    "Recommendations\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ğŸ’¡ Recommendations | Ø§Ù„ØªÙˆØµÙŠØ§Øª\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n To FIX the root cause (not just adjust predictions):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" 1. âœ… Check for missing features (most common cause)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Review your data: Are there other features that affect price?\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Add relevant features: location, condition, year_built, etc.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n 2. âœ… Check data quality\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Look for outliers in target variable\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Verify target values are correct\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Check for missing values\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n 3. âœ… Try feature engineering\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Create interaction features (size Ã— bedrooms)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - Transform features (log, square)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n 4. âœ… Consider different models\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - If relationship is non-linear â†’ Try Polynomial Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" - If many features â†’ Try Ridge/Lasso Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n âš ï¸ Remember: Adjusting predictions is a BAND-AID, not a real fix!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\" Always try to fix the root cause first!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Decision Framework - When to Use Linear Regression | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§Ù…Ø³: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ\n",
    "\n",
    "**Congratulations! You've learned how to build linear regression models. Now let's learn when to use them!**\n",
    "\n",
    "**BEFORE**: You've learned how to build linear regression models, but when should you use them?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to determine if linear regression is the right choice for your problem!\n",
    "\n",
    "**Why this matters**: Using linear regression when it's not appropriate leads to:\n",
    "- **Poor predictions** â†’ Model can't capture non-linear patterns\n",
    "- **Wasted time** â†’ Trying to force linear relationships that don't exist\n",
    "- **Wrong conclusions** â†’ Making decisions based on inaccurate models\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework: Is Linear Regression Appropriate? | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±: Ù‡Ù„ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ù…Ù†Ø§Ø³Ø¨ØŸ\n",
    "\n",
    "**Key Question**: Should I use **LINEAR REGRESSION** or a different method?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "**Decision Point 1: What type of problem?**\n",
    "- **Classification** (predicting categories) â†’ âŒ Use Logistic Regression or other classifiers\n",
    "  - Why? Linear regression predicts continuous values, not categories\n",
    "- **Regression** (predicting numbers) â†’ Continue to Decision Point 2\n",
    "\n",
    "**Decision Point 2: What is the relationship?**\n",
    "- **Linear relationship** â†’ âœ… Use Linear Regression\n",
    "  - Why? Linear regression assumes linear relationships\n",
    "- **Non-linear relationship** â†’ âš ï¸ Use Polynomial Regression or other methods\n",
    "  - Why? Linear regression can't capture curves\n",
    "\n",
    "**Decision Point 3: Data characteristics?**\n",
    "- **Many features (>100)** â†’ âš ï¸ Use Ridge or Lasso Regression\n",
    "- **Multicollinearity present** â†’ âš ï¸ Use Ridge Regression\n",
    "- **Need feature selection** â†’ âš ï¸ Use Lasso Regression\n",
    "- **Slightly curved** â†’ Try Polynomial Regression\n",
    "- **Highly non-linear** â†’ Use Random Forest, XGBoost, Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Comparison Table: Linear Regression vs Alternatives | Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Linear Regression** | Linear relationships, continuous target, interpretable | â€¢ Simple & fast<br>â€¢ Interpretable<br>â€¢ No hyperparameters<br>â€¢ Works well with linear data | â€¢ Can't handle non-linear<br>â€¢ Assumes linearity<br>â€¢ Sensitive to outliers | House price vs size (linear) |\n",
    "| **Polynomial Regression** | Slightly curved relationships | â€¢ Handles curves<br>â€¢ Still interpretable<br>â€¢ Extends linear regression | â€¢ Can overfit<br>â€¢ More complex | House price vs size (curved) |\n",
    "| **Ridge/Lasso** | Many features, multicollinearity | â€¢ Prevents overfitting<br>â€¢ Handles many features<br>â€¢ Regularization | â€¢ More complex<br>â€¢ Hyperparameter tuning | 100+ features, correlated features |\n",
    "| **Random Forest** | Non-linear, complex patterns | â€¢ Handles non-linear<br>â€¢ Feature importance<br>â€¢ Robust | â€¢ Less interpretable<br>â€¢ More complex | Complex relationships |\n",
    "| **XGBoost** | Non-linear, best performance | â€¢ State-of-the-art<br>â€¢ Handles complex patterns | â€¢ Less interpretable<br>â€¢ Complex | Competition-level performance |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When Linear Regression IS Appropriate | Ù…ØªÙ‰ ÙŠÙƒÙˆÙ† Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§\n",
    "\n",
    "**Use Linear Regression when:**\n",
    "\n",
    "1. **Linear Relationship** âœ…\n",
    "   - Scatter plot shows a straight line pattern\n",
    "   - Example: House size vs price (larger = more expensive, linear)\n",
    "\n",
    "2. **Continuous Target Variable** âœ…\n",
    "   - Predicting numbers (price, temperature, sales)\n",
    "   - NOT categories (sick/healthy, yes/no)\n",
    "\n",
    "3. **Interpretability Important** âœ…\n",
    "   - Need to understand feature coefficients\n",
    "   - Example: \"Each bedroom adds $30,000 to price\"\n",
    "\n",
    "4. **Fast Predictions Needed** âœ…\n",
    "   - Simple model, fast training and prediction\n",
    "   - Good for real-time systems\n",
    "\n",
    "5. **Baseline Model** âœ…\n",
    "   - Start with linear regression as baseline\n",
    "   - Compare with more complex models\n",
    "\n",
    "6. **Small to Medium Datasets** âœ…\n",
    "   - Works well with limited data\n",
    "   - Doesn't require huge datasets\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ When Linear Regression IS NOT Appropriate | Ù…ØªÙ‰ Ù„Ø§ ÙŠÙƒÙˆÙ† Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§\n",
    "\n",
    "**Don't use Linear Regression when:**\n",
    "\n",
    "1. **Non-Linear Relationship** âŒ\n",
    "   - Scatter plot shows curves, exponential patterns\n",
    "   - Example: Growth patterns, decay curves\n",
    "   - **Use Instead**: Polynomial Regression, Random Forest, XGBoost\n",
    "\n",
    "2. **Classification Problem** âŒ\n",
    "   - Predicting categories (yes/no, A/B/C)\n",
    "   - **Use Instead**: Logistic Regression, Decision Trees, SVM\n",
    "\n",
    "3. **Many Features with Multicollinearity** âŒ\n",
    "   - Features are highly correlated\n",
    "   - **Use Instead**: Ridge Regression (handles multicollinearity)\n",
    "\n",
    "4. **Need Feature Selection** âŒ\n",
    "   - Want to automatically select important features\n",
    "   - **Use Instead**: Lasso Regression (automatic feature selection)\n",
    "\n",
    "5. **Complex Non-Linear Patterns** âŒ\n",
    "   - Multiple interactions, complex relationships\n",
    "   - **Use Instead**: Random Forest, XGBoost, Neural Networks\n",
    "\n",
    "6. **Outliers Present** âŒ\n",
    "   - Many extreme values that affect the line\n",
    "   - **Use Instead**: Robust regression methods, or clean outliers first\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: House Price Prediction âœ… APPROPRIATE\n",
    "- **Problem**: Predict house price from size\n",
    "- **Relationship**: Linear (larger houses = higher prices, roughly linear)\n",
    "- **Target**: Continuous (price in dollars)\n",
    "- **Decision**: âœ… Use Linear Regression\n",
    "- **Reasoning**: Linear relationship, continuous target, interpretable\n",
    "\n",
    "#### Example 2: Stock Price Prediction âš ï¸ MAY NOT BE APPROPRIATE\n",
    "- **Problem**: Predict stock price from time\n",
    "- **Relationship**: Non-linear (volatile, trends, cycles)\n",
    "- **Target**: Continuous (price)\n",
    "- **Decision**: âš ï¸ Try Linear first, but may need Polynomial or other methods\n",
    "- **Reasoning**: Stock prices have complex patterns, linear may not capture well\n",
    "\n",
    "#### Example 3: Customer Churn Prediction âŒ NOT APPROPRIATE\n",
    "- **Problem**: Predict if customer will leave (yes/no)\n",
    "- **Relationship**: Classification problem\n",
    "- **Target**: Categorical (churn/not churn)\n",
    "- **Decision**: âŒ Use Logistic Regression instead\n",
    "- **Reasoning**: Classification problem, not regression\n",
    "\n",
    "#### Example 4: Sales Prediction with Many Features âš ï¸ MAY NEED REGULARIZATION\n",
    "- **Problem**: Predict sales from 50+ features\n",
    "- **Relationship**: Likely linear, but many features\n",
    "- **Target**: Continuous (sales amount)\n",
    "- **Decision**: âš ï¸ Use Ridge or Lasso Regression\n",
    "- **Reasoning**: Many features may cause overfitting, regularization helps\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Check relationship first** - Plot data to see if it's linear\n",
    "2. **Classification â‰  Regression** - Use logistic regression for categories\n",
    "3. **Start simple** - Linear regression is a good baseline\n",
    "4. **Check assumptions** - Linearity, independence, homoscedasticity\n",
    "5. **Many features?** - Consider Ridge/Lasso for regularization\n",
    "6. **Non-linear?** - Try Polynomial Regression or other methods\n",
    "7. **Always visualize** - Scatter plots reveal relationship type\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario 1**: Predicting student test scores from study hours\n",
    "- **Relationship**: More hours = higher scores (roughly linear)\n",
    "- **Target**: Continuous (test score 0-100)\n",
    "- **Decision**: âœ… Linear Regression appropriate\n",
    "\n",
    "**Scenario 2**: Predicting customer satisfaction (satisfied/not satisfied)\n",
    "- **Relationship**: Classification problem\n",
    "- **Target**: Categorical (satisfied/not satisfied)\n",
    "- **Decision**: âŒ Use Logistic Regression, not Linear Regression\n",
    "\n",
    "**Scenario 3**: Predicting house price from 100+ features\n",
    "- **Relationship**: Likely linear, but many features\n",
    "- **Target**: Continuous (price)\n",
    "- **Decision**: âš ï¸ Use Ridge or Lasso Regression (regularization needed)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Example 5: Polynomial Regression** - For non-linear relationships\n",
    "- ğŸ““ **Unit 2, Example 1: Ridge/Lasso** - For many features and regularization\n",
    "- ğŸ““ **Unit 3: Classification** - For predicting categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â“ Common Student Questions | Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„Ø·Ù„Ø§Ø¨\n",
    "\n",
    "**Q: What's the difference between simple and multiple linear regression?**\n",
    "- **Answer**: \n",
    "  - **Simple**: Uses ONE feature to predict target (e.g., size â†’ price)\n",
    "  - **Multiple**: Uses MULTIPLE features to predict target (e.g., size + age + rooms â†’ price)\n",
    "  - **Same concept**: Both find a line, but multiple regression is in higher dimensions\n",
    "  - **Formula**: Simple: y = mx + b, Multiple: y = bâ‚€ + bâ‚xâ‚ + bâ‚‚xâ‚‚ + ... + bâ‚™xâ‚™\n",
    "\n",
    "**Q: What do the coefficients mean?**\n",
    "- **Answer**: Coefficients show how much the target changes when a feature increases by 1 unit:\n",
    "  - **Example**: Coefficient = 0.05 for size means: \"For every 1 sq ft increase, price increases by $0.05\"\n",
    "  - **Larger coefficient** = Feature has more impact on predictions\n",
    "  - **Positive coefficient** = Feature increases target\n",
    "  - **Negative coefficient** = Feature decreases target\n",
    "\n",
    "**Q: What's a good RÂ² score?**\n",
    "- **Answer**: It depends on your problem:\n",
    "  - **RÂ² > 0.9**: Excellent (model explains >90% of variance)\n",
    "  - **RÂ² > 0.7**: Good (model explains >70% of variance)\n",
    "  - **RÂ² > 0.5**: Fair (model explains >50% of variance)\n",
    "  - **RÂ² < 0.5**: Poor (model explains <50% of variance)\n",
    "  - **Note**: For real-world problems, RÂ² > 0.7 is generally considered good\n",
    "\n",
    "**Q: Why do we need both MSE and MAE?**\n",
    "- **Answer**: They measure different aspects:\n",
    "  - **MSE**: Penalizes large errors more (squared term) - use when large errors are very bad\n",
    "  - **MAE**: Average error in same units as target - easier to interpret\n",
    "  - **Example**: MSE = $10,000Â², MAE = $100 â†’ \"Average error is $100\" (MAE is clearer!)\n",
    "  - **Use both**: MSE for optimization, MAE for interpretation\n",
    "\n",
    "**Q: What if my model has high training RÂ² but low test RÂ²?**\n",
    "- **Answer**: This is **overfitting** - model memorized training data:\n",
    "  - **Problem**: Model learned training patterns too well, can't generalize\n",
    "  - **Solution**: Use regularization (Ridge/Lasso), get more data, or simplify model\n",
    "  - **Check**: If train RÂ² >> test RÂ² â†’ overfitting detected!\n",
    "\n",
    "**Q: Can I use linear regression for classification?**\n",
    "- **Answer**: **NO!** Linear regression predicts continuous values (numbers), not categories:\n",
    "  - **Regression**: Predicts numbers (price, temperature, sales)\n",
    "  - **Classification**: Predicts categories (yes/no, A/B/C, sick/healthy)\n",
    "  - **Use instead**: Logistic Regression for classification (Unit 3)\n",
    "\n",
    "**Q: What if my data isn't linear?**\n",
    "- **Answer**: Linear regression won't work well:\n",
    "  - **Check**: Plot your data - if it's curved, not linear\n",
    "  - **Solution**: Use Polynomial Regression (Example 5) or other non-linear models\n",
    "  - **Sign**: Low RÂ², curved residuals pattern â†’ need non-linear model\n",
    "\n",
    "**Q: How do I know if my model is good enough?**\n",
    "- **Answer**: Check multiple things:\n",
    "  - **RÂ² > 0.7**: Good overall fit\n",
    "  - **Train RÂ² â‰ˆ Test RÂ²**: Good generalization (no overfitting)\n",
    "  - **Residuals random**: No patterns (good model)\n",
    "  - **MAE reasonable**: Error is acceptable for your use case\n",
    "  - **All together**: Good models have high RÂ², similar train/test, random residuals\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary | Ø§Ù„Ù…Ù„Ø®Øµ\n",
    "\n",
    "Great job completing this example!\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "**Linear Regression - Your First ML Model:**\n",
    "- **Simple Linear Regression**: Predicts a continuous target using one feature. Finds the best line (y = mx + b) that minimizes prediction error.\n",
    "\n",
    "- **Multiple Linear Regression**: Extends to multiple features. Predicts target using multiple inputs simultaneously. Formula: y = bâ‚€ + bâ‚xâ‚ + bâ‚‚xâ‚‚ + ... + bâ‚™xâ‚™\n",
    "\n",
    "- **Model Evaluation Metrics**:\n",
    "  - **MSE (Mean Squared Error)**: Average squared difference between predictions and actual values (lower is better)\n",
    "  - **MAE (Mean Absolute Error)**: Average absolute difference (more interpretable than MSE)\n",
    "  - **RÂ² (R-squared)**: Proportion of variance explained (0 to 1, higher is better, 1 = perfect fit)\n",
    "\n",
    "- **Residual Analysis**: Residuals (errors) should be randomly distributed around zero. Patterns in residuals indicate model problems.\n",
    "\n",
    "- **Feature Importance**: Coefficients show how much each feature affects the prediction. Larger absolute coefficients = more important features.\n",
    "\n",
    "### ğŸ”— How This Connects:\n",
    "\n",
    "**This example builds on and leads to:**\n",
    "- **Example 1-3**: Data loading, cleaning, and preprocessing prepare data for ML models\n",
    "- **Example 5: Polynomial Regression** - Extends linear regression to capture non-linear relationships\n",
    "- **Unit 2: Advanced Regression** - Ridge and Lasso regression improve on basic linear regression\n",
    "- **Unit 3: Classification** - Similar workflow (fit, predict, evaluate) but for categorical targets\n",
    "- **All ML Models**: Linear regression teaches the fundamental ML workflow used by all models\n",
    "\n",
    "**Why this example is important:**\n",
    "1. **First ML model**: Simplest model, easiest to understand\n",
    "2. **ML workflow**: Teaches the standard process: prepare data â†’ train model â†’ evaluate â†’ interpret\n",
    "3. **Foundation**: Many advanced models build on linear regression concepts\n",
    "4. **Interpretability**: Linear models are highly interpretable - you can see exactly how features affect predictions\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Linear regression finds the best line**: Minimizes prediction error using least squares method\n",
    "2. **Multiple features improve predictions**: More relevant features generally lead to better models\n",
    "3. **Evaluation metrics matter**: MSE, MAE, and RÂ² tell you how good your model is\n",
    "4. **Residuals reveal problems**: Check residuals to diagnose model issues\n",
    "5. **Coefficients show feature importance**: Understand which features matter most\n",
    "\n",
    "### Next Steps:\n",
    "- Complete exercises in `exercises/` folder to practice building linear regression models\n",
    "- Review quiz materials to test your understanding\n",
    "- Proceed to **Example 5: Polynomial Regression** to handle non-linear relationships\n",
    "- Then move to **Unit 2: Advanced Regression** for Ridge and Lasso techniques\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Tips for Success:**\n",
    "- **Data quality matters**: Clean, preprocessed data leads to better models\n",
    "- **Feature selection**: Not all features help - remove irrelevant ones\n",
    "- **Check assumptions**: Linear regression assumes linear relationships - verify this!\n",
    "- **Visualize**: Always plot your data and residuals to understand what's happening"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}