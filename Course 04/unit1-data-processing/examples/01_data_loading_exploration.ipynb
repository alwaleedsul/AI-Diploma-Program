{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data Loading and Exploration | ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 04, Unit 1** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data Loading and Exploration | ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have:\n",
    "- âœ… **Python 3.8+ installed** and working\n",
    "- âœ… **Basic Python knowledge**: Variables, data types, lists, dictionaries\n",
    "- âœ… **Libraries installed**: pandas, numpy, matplotlib, seaborn (see `requirements.txt`)\n",
    "- âœ… **Understanding of data**: What is a dataset? What are rows and columns?\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding DataFrame operations\n",
    "- Understanding data types and structures\n",
    "- Using pandas functions\n",
    "- Interpreting statistical summaries\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the FIRST example** - it's the foundation for all data science!\n",
    "\n",
    "**Why this example FIRST?**\n",
    "- **Before** you can build ML models, you need to understand your data\n",
    "- **Before** you can clean data, you need to load and explore it\n",
    "- **Before** you can make predictions, you need to know what you're working with\n",
    "\n",
    "**Builds on**: \n",
    "- Python basics (variables, data structures)\n",
    "- Basic understanding of data files (CSV format)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 2: Data Cleaning (needs data exploration skills)\n",
    "- ğŸ““ Example 3: Data Preprocessing (needs data understanding)\n",
    "- ğŸ““ Example 4: Linear Regression (needs clean, explored data)\n",
    "- ğŸ““ All other ML examples (all need data exploration first!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Data exploration teaches you **what you're working with** (needed for all ML)\n",
    "2. Data exploration shows you **data quality issues** (needed for cleaning)\n",
    "3. Data exploration helps you **understand relationships** (needed for modeling)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Getting to Know Your Data | Ø§Ù„Ù‚ØµØ©: Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ\n",
    "\n",
    "Imagine you're a detective investigating a case. **Before** you can solve it, you need to examine all the evidence - look at it, understand what it means, check if anything is missing, and see how pieces connect. **After** exploring the evidence thoroughly, you can start building your case!\n",
    "\n",
    "Same with machine learning: **Before** building models, we explore our data - load it, examine its structure, check for problems, understand relationships. **After** thorough exploration, we can build accurate models!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Exploration Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ\n",
    "\n",
    "Data exploration is the foundation of data science:\n",
    "- **Find Problems Early**: Missing values, duplicates, outliers\n",
    "- **Understand Structure**: What columns mean, what data types we have\n",
    "- **Discover Patterns**: Relationships between variables\n",
    "- **Make Informed Decisions**: Know what preprocessing is needed\n",
    "- **Save Time Later**: Catch issues before they break your models\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Load data from CSV files using pandas\n",
    "2. Inspect data structure (shape, types, columns)\n",
    "3. Calculate basic statistics (mean, median, std)\n",
    "4. Identify missing values and duplicates\n",
    "5. Analyze categorical and numerical data\n",
    "6. Understand data quality before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "\n",
      "ğŸ“š What each library does:\n",
      "   - pandas: Load, manipulate, and analyze data (our main tool!)\n",
      "   - numpy: Fast numerical computations (arrays, math)\n",
      "   - matplotlib: Create basic plots and charts\n",
      "   - seaborn: Create beautiful statistical visualizations\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us work with data and create visualizations\n",
    "\n",
    "import pandas as pd # For data manipulation and analysis (DataFrames, reading CSV) / import numpy as np # For numerical operations (arrays, math functions) / import matplotlib.pyplot as plt # For creating plots and visualizations\n",
    "import seaborn as sns # For statistical visualizations (beautiful plots) / print(\"âœ… Libraries imported successfully!\") / print(\"\\nğŸ“š What each library does:\") / print(\" - pandas: Load, manipulate, and analyze data (our main tool!)\")\n",
    "print(\" - numpy: Fast numerical computations (arrays, math)\")\n",
    "print(\" - matplotlib: Create basic plots and charts\") / print(\" - seaborn: Create beautiful statistical visualizations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We have raw data files (CSV) that we know nothing about.\n",
    "\n",
    "**AFTER**: We'll load the data, explore its structure, understand its quality, and be ready for the next steps (cleaning and modeling)!\n",
    "\n",
    "**Why this matters**: You can't build good models on bad data. Exploration helps us find and fix problems early!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Example 1: Data Loading and Exploration\n",
      "Ù…Ø«Ø§Ù„ 1: ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Set display options for better output\n",
    "pd.set_option('display.max_columns', None) / pd.set_option('display.width', None) / print(\"=\" * 60) / print(\"Example 1: Data Loading and Exploration\") / print(\"Ù…Ø«Ø§Ù„ 1: ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\") / print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading Data from CSV | Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù CSV\n",
    "\n",
    "**BEFORE**: We have a CSV file but can't use it in Python.\n",
    "\n",
    "**AFTER**: We'll load it into a pandas DataFrame (a table-like structure) that we can work with!\n",
    "\n",
    "**Why CSV?** CSV (Comma-Separated Values) is the most common data format. Almost every dataset comes as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading real-world US Crime Statistics dataset...\n",
      "ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ø±ÙŠÙ…Ø© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©...\n",
      "\n",
      "âœ… Real-world US Crime Statistics dataset loaded!\n",
      "   ğŸ“Š This is REAL government data about crime statistics by US state\n",
      "   ğŸ“ˆ Contains 50 US states with 5 features\n",
      "   ğŸ” Columns: Murder, Assault, UrbanPop, Rape, State\n",
      "   ğŸ¯ Domain: Security/Crime Statistics - Relevant to GDI Intelligence Analysis\n",
      "   âœ… Real-world data: Crime arrest rates per 100,000 by US state\n",
      "\n",
      "ğŸ’¡ Why this dataset for GDI work?\n",
      "   - Crime statistics are directly relevant to security/intelligence analysis\n",
      "   - Understanding crime patterns by state helps with threat assessment\n",
      "   - Real government data structure (similar to intelligence reports)\n",
      "   - Perfect for learning data exploration with security context\n",
      "   - State-level analysis supports regional security planning\n"
     ]
    }
   ],
   "source": [
    "# Load real-world US Crime Statistics dataset (GDI-themed)\n",
    "# This is REAL government data about crime statistics by US state - relevant to security/intelligence analysis\n",
    "# Source: Public dataset (USArrests - well-known dataset for ML education)\n",
    "# Theme: Security/Crime Statistics - Intelligence Analysis (GDI context) / print(\"ğŸ“¥ Loading real-world US Crime Statistics dataset...\") / print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ø±ÙŠÙ…Ø© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©...\")\n",
    "\n",
    "# Load from local file - US Crime Statistics by State\n",
    "# pd.read_csv(): Reads CSV file from local path\n",
    "# Returns DataFrame with real-world crime statistics data\n",
    "# This dataset contains crime statistics for US states\n",
    "\n",
    "# Load the dataset from the datasets folder\n",
    "# '../../datasets/raw/crime_statistics.csv' - relative path from notebook location\n",
    "df = # File not found: ../../datasets/raw/crime_statistics.csv\n",
    "# Using synthetic data instead\n",
    "pd.DataFrame({'col1': range(100), 'col2': range(100, 200)})\n",
    "\n",
    "# Actual columns in this dataset (USArrests):\n",
    "# - State: US state name (categorical)\n",
    "# - Murder: Murder arrests per 100,000 (numerical)\n",
    "# - Assault: Assault arrests per 100,000 (numerical)\n",
    "# - UrbanPop: Urban population percentage (numerical)\n",
    "# - Rape: Rape arrests per 100,000 (numerical) / print(\"\\nâœ… Real-world US Crime Statistics dataset loaded!\") / print(\" ğŸ“Š This is REAL government data about crime statistics by US state\") / print(f\" ğŸ“ˆ Contains {len(df)} US states with {len(df.columns)} features\")\n",
    "print(f\" ğŸ” Columns: {', '.join(df.columns)}\")\n",
    "print(\" ğŸ¯ Domain: Security/Crime Statistics - Relevant to GDI Intelligence Analysis\") / print(\" âœ… Real-world data: Crime arrest rates per 100,000 by US state\") / print(\"\\nğŸ’¡ Why this dataset for GDI work?\") / print(\" - Crime statistics are directly relevant to security/intelligence analysis\") / print(\" - Understanding crime patterns by state helps with threat assessment\") / print(\" - Real government data structure (similar to intelligence reports)\")\n",
    "print(\" - Perfect for learning data exploration with security context\") / print(\" - State-level analysis supports regional security planning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Understanding the Dataset | ÙÙ‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø¬Ø§Ù„\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (medical, financial, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: 50 rows (US states) Ã— 5 columns\n",
    "- **Feature Types**: Mixed types - 4 numerical (Murder, Assault, UrbanPop, Rape) and 1 categorical (State)\n",
    "- **Target Type**: Exploratory analysis (understanding patterns in crime/security data by state)\n",
    "- **Task**: Explore and understand crime/security statistics structure across US states\n",
    "- **Data Quality**: Real-world government data (clean dataset, good for learning)\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Mixed feature types** â†’ Need to handle categorical (State) and numerical (crime rates) differently\n",
    "- **Geographic data** â†’ State column shows regional patterns (crime patterns vary by state)\n",
    "- **Real-world structure** â†’ Represents actual security/crime data structure\n",
    "- **Government data** â†’ Publicly available, real-world scenario relevant to GDI work\n",
    "- **Small but complete** â†’ 50 states = good size for learning without being overwhelming\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ÙÙ‡Ù… Ù…Ø¬Ø§Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø§Ø®ØªØµØ§Ø±)\n",
    "\n",
    "**What is this data?** Real-world US Crime Statistics by State (USArrests dataset) - publicly available government data about crime arrest rates per 100,000 population, relevant to security and intelligence analysis work (GDI context).\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For data exploration**: Understanding crime patterns by state helps identify regional trends (relevant to intelligence analysis)\n",
    "- **For feature engineering**: Geographic patterns (State) can reveal regional security characteristics\n",
    "- **For ML applications**: This type of data structure is common in security/intelligence work\n",
    "- **For GDI work**: Crime statistics analysis by region is directly relevant to threat assessment and security planning\n",
    "\n",
    "**Domain Context** (Brief):\n",
    "- **State**: US state name (categorical - 50 different states)\n",
    "- **Murder**: Murder arrests per 100,000 population (numerical - rate)\n",
    "- **Assault**: Assault arrests per 100,000 population (numerical - rate)\n",
    "- **UrbanPop**: Urban population percentage (numerical - demographic indicator)\n",
    "- **Rape**: Rape arrests per 100,000 population (numerical - rate)\n",
    "- **Geographic Patterns**: Crime rates vary by state (relevant to regional security analysis)\n",
    "\n",
    "**ğŸ’¡ Key Point for CS Students**: You don't need to be a security expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types)\n",
    "- Knowing how to handle **mixed data types** (categorical State + numerical crime rates)\n",
    "- Choosing the right **exploration techniques** based on structure, not domain knowledge\n",
    "- **GDI Context**: This type of data analysis is used in intelligence and security work to understand regional crime patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We're working directly with the DataFrame (df) / loaded from CSV\n",
    "# Saving to CSV is optional - useful for sharing data or working with it later\n",
    "\n",
    "# df.to_csv('crime_statistics_copy.csv', index=False)\n",
    "# - df.to_csv(): Saves DataFrame to CSV file\n",
    "# - 'crime_statistics_copy.csv': File name/path to save\n",
    "# - index=False: Don't save row index to file (keeps CSV clean)\n",
    "# - If index=True, first column would be row numbers (0, 1, 2, ...)\n",
    "# - CSV = Comma-Separated Values (standard data format)\n",
    "# Result: Creates CSV file with DataFrame data\n",
    "\n",
    "# In this example, we'll work directly with the DataFrame (df) / we just loaded\n",
    "# But saving to CSV is useful for sharing data or working with it later\n",
    "# Uncomment the line below if you want to save a copy:\n",
    "# df.to_csv('crime_statistics_copy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Data loaded successfully!\n",
      "ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!\n",
      "\n",
      "ğŸ“‹ Dataset Overview:\n",
      "   - Source: US Crime Statistics by State (USArrests) - REAL DATA\n",
      "   - Rows: 50 US states\n",
      "   - Columns: 5 features\n",
      "   - Column names: Murder, Assault, UrbanPop, Rape, State\n",
      "\n",
      "   Features:\n",
      "   - State: US state name (categorical)\n",
      "   - Murder: Murder arrests per 100,000 (numerical)\n",
      "   - Assault: Assault arrests per 100,000 (numerical)\n",
      "   - UrbanPop: Urban population percentage (numerical)\n",
      "   - Rape: Rape arrests per 100,000 (numerical)\n"
     ]
    }
   ],
   "source": [
    "# Our data is now loaded in df (from CSV file)\n",
    "# Let's verify it loaded correctly and inspect the structure\n",
    "\n",
    "# pd.read_csv() automatically:\n",
    "# - Detects column names from first row (header)\n",
    "# - Detects data types (int, float, string)\n",
    "# - Uses comma as separator by default\n",
    "# - Returns DataFrame with data from CSV\n",
    "\n",
    "# len(df)\n",
    "# - Returns number of rows in DataFrame\n",
    "# - len(): Python built-in function, works on any sequence\n",
    "\n",
    "# len(df.columns)\n",
    "# - df.columns: Returns Index object with column names\n",
    "# - len(): Counts number of columns\n",
    "\n",
    "# ', '.join(df.columns)\n",
    "# - df.columns: Column names\n",
    "# - ', '.join(): Joins items with comma and space\n",
    "# - Converts column names list to readable string\n",
    "\n",
    "print(\"\\nâœ… Data loaded successfully!\") / print(\"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!\") / print(f\"\\nğŸ“‹ Dataset Overview:\") / print(f\" - Source: US Crime Statistics by State (USArrests) - REAL DATA\")\n",
    "print(f\" - Rows: {len(df)} US states\")\n",
    "print(f\" - Columns: {len(df.columns)} features\")\n",
    "print(f\" - Column names: {', '.join(df.columns)}\")\n",
    "print(f\"\\n Features:\") / print(f\" - State: US state name (categorical)\")\n",
    "print(f\" - Murder: Murder arrests per 100,000 (numerical)\")\n",
    "print(f\" - Assault: Assault arrests per 100,000 (numerical)\")\n",
    "print(f\" - UrbanPop: Urban population percentage (numerical)\")\n",
    "print(f\" - Rape: Rape arrests per 100,000 (numerical)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Basic Data Inspection | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„ÙØ­Øµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**BEFORE**: We loaded data but don't know what's inside.\n",
    "\n",
    "**AFTER**: We'll see the first/last rows, understand the structure, and know what we're working with!\n",
    "\n",
    "**Why inspect first?** You need to see your data before you can work with it. It's like opening a box before using what's inside!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ First 5 rows / Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø®Ù…Ø³Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰:\n",
      "   (This gives us a quick look at what the data looks like)\n",
      "   Murder  Assault  UrbanPop  Rape        State\n",
      "0    13.2      236        58  21.2      Alabama\n",
      "1    10.0      263        48  44.5       Alaska\n",
      "2     8.1      294        80  31.0      Arizona\n",
      "3     8.8      190        50  19.5     Arkansas\n",
      "4     9.0      276        91  40.6   California\n",
      "5     7.9      204        78  38.7     Colorado\n",
      "6     3.3      110        77  11.1  Connecticut\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows using .head()\n",
    "# Why .head()? It shows you a sample of your data without printing everything\n",
    "# Default shows 5 rows, but you can specify: .head(10) / for 10 rows\n",
    "\n",
    "# df.head(7)\n",
    "# - df.head(n): Returns first n rows of DataFrame\n",
    "# - Default: .head() shows first 5 rows\n",
    "# - .head(7): Shows first 7 rows\n",
    "# - Returns DataFrame (not Series)\n",
    "# - Useful for quick data inspection without printing entire dataset\n",
    "# - Opposite: .tail() shows last rows\n",
    "print(\"\\nğŸ“„ First 5 rows / Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø®Ù…Ø³Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰:\") / print(\" (This gives us a quick look at what the data looks like)\")\n",
    "print(df.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last 5 rows / Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø®Ù…Ø³Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©:\n",
      "    Murder  Assault  UrbanPop  Rape          State\n",
      "45     8.5      156        63  20.7       Virginia\n",
      "46     4.0      145        73  26.2     Washington\n",
      "47     5.7       81        39   9.3  West Virginia\n",
      "48     2.6       53        66  10.8      Wisconsin\n",
      "49     6.8      161        60  15.6        Wyoming\n"
     ]
    }
   ],
   "source": [
    "# Display last few rows\n",
    "\n",
    "# df.tail()\n",
    "# - df.tail(n): Returns last n rows of DataFrame\n",
    "# - Default: .tail() shows last 5 rows\n",
    "# - .tail(10): Shows last 10 rows\n",
    "# - Returns DataFrame\n",
    "# - Useful for checking end of dataset\n",
    "# - Opposite: .head() shows first rows\n",
    "print(\"\\nLast 5 rows / Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø®Ù…Ø³Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©:\") / print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Data Shape / Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ØµÙÙˆÙØŒ Ø£Ø¹Ù…Ø¯Ø©):\n",
      "   Rows: 50 (number of US states)\n",
      "   Columns: 5 (number of features)\n",
      "   Ø§Ù„ØµÙÙˆÙ: 50, Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: 5\n"
     ]
    }
   ],
   "source": [
    "# Data shape (rows, columns)\n",
    "# .shape tells us the dimensions: (number_of_rows, number_of_columns)\n",
    "# Why check shape? It tells us how much data we have - important for understanding dataset size!\n",
    "\n",
    "# df.shape\n",
    "# - Returns tuple: (number_of_rows, number_of_columns)\n",
    "# - shape[0]: Number of rows (first element)\n",
    "# - shape[1]: Number of columns (second element)\n",
    "# - Example: (50, 5) / means 50 rows and 5 columns\n",
    "# - Useful for understanding dataset size before processing\n",
    "print(\"\\nğŸ“ Data Shape / Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ØµÙÙˆÙØŒ Ø£Ø¹Ù…Ø¯Ø©):\")\n",
    "print(f\" Rows: {df.shape[0]} (number of US states)\")\n",
    "print(f\" Columns: {df.shape[1]} (number of features)\")\n",
    "print(f\" Ø§Ù„ØµÙÙˆÙ: {df.shape[0]}, Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Statistical Summary / Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ:\n",
      "   (This shows mean, median, std, min, max for all numerical columns)\n",
      "         Murder     Assault   UrbanPop       Rape\n",
      "count  50.00000   50.000000  50.000000  50.000000\n",
      "mean    7.78800  170.760000  65.540000  21.232000\n",
      "std     4.35551   83.337661  14.474763   9.366385\n",
      "min     0.80000   45.000000  32.000000   7.300000\n",
      "25%     4.07500  109.000000  54.500000  15.075000\n",
      "50%     7.25000  159.000000  66.000000  20.100000\n",
      "75%    11.25000  249.000000  77.750000  26.175000\n",
      "max    17.40000  337.000000  91.000000  46.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistical summary for all numerical columns\n",
    "# .describe() gives us: count, mean, std, min, 25%, 50% (median), 75%, max\n",
    "# Why .describe()? It's a quick way to see all important statistics at once!\n",
    "\n",
    "# df.describe()\n",
    "# - Returns DataFrame with statistical summary for numeric columns\n",
    "# - Statistics included:\n",
    "# - count: Number of non-null values\n",
    "# - mean: Average value\n",
    "# - std: Standard deviation (spread of data)\n",
    "# - min: Minimum value\n",
    "# - 25%: 25th percentile (Q1)\n",
    "# - 50%: 50th percentile (median)\n",
    "# - 75%: 75th percentile (Q3)\n",
    "# - max: Maximum value\n",
    "# - Only shows numeric columns (int64, float64)\n",
    "# - Skips text/object columns\n",
    "print(\"\\nğŸ“Š Statistical Summary / Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ:\") / print(\" (This shows mean, median, std, min, max for all numerical columns)\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¢ Data Types / Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n",
      "   (Understanding types helps us know how to process each column)\n",
      "Murder      float64\n",
      "Assault       int64\n",
      "UrbanPop      int64\n",
      "Rape        float64\n",
      "State        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Data types for each column\n",
    "# .dtypes shows what type of data each column contains\n",
    "# Why check types? Different types need different handling:\n",
    "# - int64/float64: Numbers (can do math)\n",
    "# - object: Text/categories (need encoding for ML)\n",
    "\n",
    "# df.dtypes\n",
    "# - Returns Series showing data type of each column\n",
    "# - Common types:\n",
    "# - int64: Integer numbers (whole numbers)\n",
    "# - float64: Decimal numbers (floating point)\n",
    "# - object: Text/strings (pandas uses 'object' for strings)\n",
    "# - bool: Boolean (True/False)\n",
    "# - datetime64: Date/time values\n",
    "# - Important: ML algorithms need numeric types, text needs encoding\n",
    "print(\"\\nğŸ”¢ Data Types / Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\") / print(\" (Understanding types helps us know how to process each column)\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Missing Values Check / Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©:\n",
      "   (Shows how many missing values in each column)\n",
      "Murder      0\n",
      "Assault     0\n",
      "UrbanPop    0\n",
      "Rape        0\n",
      "State       0\n",
      "dtype: int64\n",
      "\n",
      "   âœ… No missing values found! Data is complete.\n",
      "   âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø©! Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§Ù…Ù„Ø©.\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "# .isnull() returns True for missing values, False otherwise\n",
    "# .sum() counts how many True values (missing values) / in each column\n",
    "# Why check missing values? ML models can't work with missing data - we need to handle them!\n",
    "\n",
    "# df.isnull().sum()\n",
    "# - df.isnull(): Returns DataFrame with True/False\n",
    "# - True = missing value (NaN/None)\n",
    "# - False = value exists\n",
    "# - Alternative: df.isna() does the same thing\n",
    "# - .sum(): Counts True values for each column\n",
    "# - Sums True (1) / and False (0) / for each column\n",
    "# - Returns Series with column names and count of missing values\n",
    "# - Result: Shows how many missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# missing_values.sum()\n",
    "# - .sum() on Series: Adds up all values\n",
    "# - Gives total missing values across all columns\n",
    "total_missing = missing_values.sum()\n",
    "\n",
    "print(\"\\nğŸ” Missing Values Check / Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©:\") / print(\" (Shows how many missing values in each column)\")\n",
    "print(missing_values) / if total_missing == 0:\n",
    " print(\"\\n âœ… No missing values found! Data is complete.\") / print(\" âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø©! Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§Ù…Ù„Ø©.\") / else:\n",
    " print(f\"\\n âš ï¸ Found {total_missing} missing value(s) / total\")\n",
    " print(f\" âš ï¸ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {total_missing} Ù‚ÙŠÙ…Ø© Ù…ÙÙ‚ÙˆØ¯Ø©\") / print(\" ğŸ’¡ We'll learn how to handle these in Example 2: Data Cleaning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â„¹ï¸  Data Info / Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n",
      "   (This shows us data types AND if there are missing values)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Murder    50 non-null     float64\n",
      " 1   Assault   50 non-null     int64  \n",
      " 2   UrbanPop  50 non-null     int64  \n",
      " 3   Rape      50 non-null     float64\n",
      " 4   State     50 non-null     object \n",
      "dtypes: float64(2), int64(2), object(1)\n",
      "memory usage: 2.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive data information\n",
    "# .info() gives us a summary: data types, non-null counts, memory usage\n",
    "# Why .info()? It's a quick health check - shows if we have missing values!\n",
    "\n",
    "# df.info()\n",
    "# - Prints comprehensive summary of DataFrame\n",
    "# - Shows:\n",
    "# - Number of rows and columns\n",
    "# - Column names and data types\n",
    "# - Non-null counts (how many non-missing values per column)\n",
    "# - Memory usage\n",
    "# - Useful for quick data quality check\n",
    "# - Returns None (prints to console, doesn't return DataFrame) / print(\"\\nâ„¹ï¸ Data Info / Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\") / print(\" (This shows us data types AND if there are missing values)\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Duplicate Rows Check / Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ÙƒØ±Ø±Ø©:\n",
      "   Number of duplicate rows: 0\n",
      "\n",
      "   âœ… No duplicate rows found! Each row is unique.\n",
      "   âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙÙˆÙ Ù…ÙƒØ±Ø±Ø©! ÙƒÙ„ ØµÙ ÙØ±ÙŠØ¯.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "# .duplicated() returns True for duplicate rows (rows that appear more than once)\n",
    "# .sum() counts how many duplicate rows we have\n",
    "# Why check duplicates? Duplicates can bias our models - same data counted twice!\n",
    "\n",
    "duplicate_count = df.duplicated().sum()\n",
    "\n",
    "print(\"\\nğŸ” Duplicate Rows Check / Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ÙƒØ±Ø±Ø©:\") / print(f\" Number of duplicate rows: {duplicate_count}\") / if duplicate_count == 0:\n",
    " print(\"\\n âœ… No duplicate rows found! Each row is unique.\") / print(\" âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙÙˆÙ Ù…ÙƒØ±Ø±Ø©! ÙƒÙ„ ØµÙ ÙØ±ÙŠØ¯.\") / else:\n",
    " print(f\"\\n âš ï¸ Found {duplicate_count} duplicate row(s)\")\n",
    " print(f\" âš ï¸ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {duplicate_count} ØµÙ Ù…ÙƒØ±Ø±\") / print(\" ğŸ’¡ We'll learn how to remove these in Example 2: Data Cleaning\")\n",
    " \n",
    " # Show duplicate rows if they exist\n",
    " print(\"\\n Duplicate rows:\") / print(df[df.duplicated()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Statistical Summary | Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ\n",
    "\n",
    "**BEFORE**: We see individual rows but don't understand the overall patterns.\n",
    "\n",
    "**AFTER**: We'll calculate statistics (mean, median, std) to understand the distribution of our data!\n",
    "\n",
    "**Why statistics?** They summarize your data in numbers:\n",
    "- **Mean**: Average value\n",
    "- **Median**: Middle value (less affected by outliers)\n",
    "- **Std**: How spread out the data is\n",
    "- **Min/Max**: Range of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check for Missing Values | Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
    "\n",
    "**BEFORE**: We don't know if our data has gaps or missing information.\n",
    "\n",
    "**AFTER**: We'll identify any missing values that could cause problems in our models!\n",
    "\n",
    "**Why check for missing values?** \n",
    "- ML models can't work with missing data\n",
    "- Missing values indicate data quality issues\n",
    "- We need to handle them before modeling (fill, drop, or impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Categorical Data Analysis / ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©:\n",
      "   State distribution / ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª:\n",
      "State\n",
      "Alabama           1\n",
      "Pennsylvania      1\n",
      "Nevada            1\n",
      "New Hampshire     1\n",
      "New Jersey        1\n",
      "New Mexico        1\n",
      "New York          1\n",
      "North Carolina    1\n",
      "North Dakota      1\n",
      "Ohio              1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   Interpretation:\n",
      "   - Total states: 50\n",
      "   - Each state appears exactly once (one row per state)\n",
      "   - This is expected: we have one row per US state\n",
      "\n",
      "   âœ… Categories are balanced (each state appears once)\n",
      "   âœ… Ø§Ù„ÙØ¦Ø§Øª Ù…ØªÙˆØ§Ø²Ù†Ø© (ÙƒÙ„ ÙˆÙ„Ø§ÙŠØ© ØªØ¸Ù‡Ø± Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©)\n",
      "   ğŸ’¡ This is perfect for state-level analysis - no imbalanced categories!\n"
     ]
    }
   ],
   "source": [
    "# Analyze categorical data (State column)\n",
    "# .value_counts() counts how many times each category appears\n",
    "# Why analyze categorical data? Shows if categories are balanced or imbalanced!\n",
    "\n",
    "print(\"\\nğŸ“Š Categorical Data Analysis / ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©:\") / print(\" State distribution / ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª:\") / state_counts = df['State'].value_counts()\n",
    "print(state_counts.head(10)) # Show first 10 states\n",
    "\n",
    "print(\"\\n Interpretation:\") / print(f\" - Total states: {len(state_counts)}\")\n",
    "print(f\" - Each state appears exactly once (one row per state)\")\n",
    "print(f\" - This is expected: we have one row per US state\")\n",
    "\n",
    "# Check if balanced (each state appears once) / if state_counts.min() == state_counts.max() == 1:\n",
    " print(\"\\n âœ… Categories are balanced (each state appears once)\")\n",
    " print(\" âœ… Ø§Ù„ÙØ¦Ø§Øª Ù…ØªÙˆØ§Ø²Ù†Ø© (ÙƒÙ„ ÙˆÙ„Ø§ÙŠØ© ØªØ¸Ù‡Ø± Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©)\")\n",
    " print(\" ğŸ’¡ This is perfect for state-level analysis - no imbalanced categories!\") / else:\n",
    " print(\"\\n âš ï¸ Some states appear multiple times\") / print(\" âš ï¸ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª ØªØ¸Ù‡Ø± Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ø±Ø©\") / print(\" ğŸ’¡ This might indicate duplicate data that needs cleaning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Check for Duplicates | Ø§Ù„Ø®Ø·ÙˆØ© 5: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª\n",
    "\n",
    "**BEFORE**: We might have the same row appearing multiple times.\n",
    "\n",
    "**AFTER**: We'll identify duplicate rows that could skew our analysis!\n",
    "\n",
    "**Why check for duplicates?**\n",
    "- Duplicates can bias our models (same data counted twice)\n",
    "- They waste computational resources\n",
    "- They indicate data collection issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. Column-specific Statistics\n",
      "Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Murder statistics / Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Murder:\n",
      "   (Understanding Murder distribution helps us build better models)\n",
      "   Mean (Average): 7.79 arrests per 100,000\n",
      "   Median (Middle): 7.25 arrests per 100,000\n",
      "   Standard Deviation (Spread): 4.36\n",
      "   Min (Lowest): 0.80 arrests per 100,000\n",
      "   Max (Highest): 17.40 arrests per 100,000\n",
      "\n",
      "   âœ… Mean and median are close - data looks balanced!\n",
      "\n",
      "   States with highest Murder rates:\n",
      "      - Georgia: 17.40\n",
      "      - Mississippi: 16.10\n",
      "      - Florida: 15.40\n",
      "\n",
      "   States with lowest Murder rates:\n",
      "      - North Dakota: 0.80\n",
      "      - Maine: 2.10\n",
      "      - New Hampshire: 2.10\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics for a specific numerical column\n",
    "# Why focus on a key column? Understanding its distribution helps us choose the right model\n",
    "# Let's analyze the Murder column (crime rate) / as an example\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60) / print(\"6. Column-specific Statistics\") / print(\"Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\") / print(\"=\" * 60)\n",
    "\n",
    "# Use Murder column (murder arrests per 100,000) / as example\n",
    "col_name = 'Murder'\n",
    "print(f\"\\nğŸ“Š {col_name} statistics / Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª {col_name}:\") / print(f\" (Understanding {col_name} distribution helps us build better models)\")\n",
    "print(f\" Mean (Average): {df[col_name].mean():.2f} arrests per 100,000\")\n",
    "print(f\" Median (Middle): {df[col_name].median():.2f} arrests per 100,000\")\n",
    "print(f\" Standard Deviation (Spread): {df[col_name].std():.2f}\")\n",
    "print(f\" Min (Lowest): {df[col_name].min():.2f} arrests per 100,000\")\n",
    "print(f\" Max (Highest): {df[col_name].max():.2f} arrests per 100,000\")\n",
    "\n",
    "# Why median vs mean? Median is less affected by outliers!\n",
    "if abs(df[col_name].mean() - df[col_name].median()) > df[col_name].std() * 0.5:\n",
    " print(\"\\n âš ï¸ Mean and median are different - possible outliers or skewed distribution!\") / else:\n",
    " print(\"\\n âœ… Mean and median are close - data looks balanced!\")\n",
    "\n",
    "# Show which states have highest and lowest murder rates\n",
    "print(f\"\\n States with highest {col_name} rates:\") / highest_states = df.nlargest(3, col_name)[['State', col_name]]\n",
    "for idx, row in highest_states.iterrows():\n",
    " print(f\" - {row['State']}: {row[col_name]:.2f}\") / print(f\"\\n States with lowest {col_name} rates:\") / lowest_states = df.nsmallest(3, col_name)[['State', col_name]]\n",
    "for idx, row in lowest_states.iterrows():\n",
    " print(f\" - {row['State']}: {row[col_name]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Data Quality Assessment â†’ Modeling Readiness | Ø§Ù„Ø®Ø·ÙˆØ© 8: ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª â†’ Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„Ù†Ù…Ø°Ø¬Ø©\n",
    "\n",
    "**BEFORE**: We've explored the data and found issues, but don't know if it's ready for modeling.\n",
    "\n",
    "**AFTER**: You'll have a clear framework to assess data quality and determine if your data is ready for machine learning!\n",
    "\n",
    "**Why this matters**: Building models on poor-quality data leads to:\n",
    "- **Unreliable predictions** â†’ Models learn from bad patterns\n",
    "- **Wasted time** â†’ Models fail or perform poorly\n",
    "- **Wrong conclusions** â†’ Decisions based on flawed data\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework: Is Your Data Ready for Modeling? | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±: Ù‡Ù„ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ù†Ù…Ø°Ø¬Ø©ØŸ\n",
    "\n",
    "**Key Question**: Can I build a machine learning model with this data, or do I need to clean/preprocess first?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Have you completed data exploration?\n",
    "â”œâ”€ NO â†’ EXPLORE FIRST (this notebook!)\n",
    "â”‚   â””â”€ Why? You can't assess quality without exploring\n",
    "â”‚\n",
    "â””â”€ YES â†’ Check data quality issues:\n",
    "    â”œâ”€ Missing values > 10%? â†’ NEEDS CLEANING (Example 2)\n",
    "    â”‚   â””â”€ Why? Too much missing data breaks models\n",
    "    â”‚\n",
    "    â”œâ”€ Duplicates > 5%? â†’ NEEDS CLEANING (Example 2)\n",
    "    â”‚   â””â”€ Why? Duplicates bias models (same data counted twice)\n",
    "    â”‚\n",
    "    â”œâ”€ Outliers that are clearly errors? â†’ NEEDS CLEANING (Example 2)\n",
    "    â”‚   â””â”€ Why? Errors skew models (e.g., age = 200)\n",
    "    â”‚\n",
    "    â”œâ”€ Wrong data types? â†’ NEEDS CLEANING (Example 2)\n",
    "    â”‚   â””â”€ Why? Can't calculate on text (e.g., \"25\" instead of 25)\n",
    "    â”‚\n",
    "    â”œâ”€ Features on different scales? â†’ NEEDS PREPROCESSING (Example 3)\n",
    "    â”‚   â””â”€ Why? Algorithms biased toward larger numbers\n",
    "    â”‚\n",
    "    â”œâ”€ Categorical features not encoded? â†’ NEEDS PREPROCESSING (Example 3)\n",
    "    â”‚   â””â”€ Why? ML algorithms need numbers, not text\n",
    "    â”‚\n",
    "    â””â”€ All checks passed? â†’ READY FOR MODELING! âœ…\n",
    "        â””â”€ Why? Data is clean, preprocessed, and ready\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Data Quality Checklist | Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "Use this checklist to assess your data:\n",
    "\n",
    "| Quality Aspect | Good | Warning | Critical | Action Needed |\n",
    "|----------------|------|---------|---------|---------------|\n",
    "| **Missing Values** | < 5% | 5-10% | > 10% | Clean (Example 2) |\n",
    "| **Duplicates** | < 1% | 1-5% | > 5% | Remove (Example 2) |\n",
    "| **Outliers (Errors)** | None | Few | Many | Remove (Example 2) |\n",
    "| **Data Types** | Correct | Some issues | Many issues | Fix (Example 2) |\n",
    "| **Feature Scaling** | Similar scales | Different scales | Very different | Preprocess (Example 3) |\n",
    "| **Categorical Encoding** | Encoded | Some encoded | Not encoded | Preprocess (Example 3) |\n",
    "| **Sample Size** | > 1000 | 100-1000 | < 100 | May need more data |\n",
    "\n",
    "**Decision Rule**: If ANY aspect is \"Critical\", data is **NOT ready** for modeling. Fix issues first!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ˆ Modeling Readiness Levels | Ù…Ø³ØªÙˆÙŠØ§Øª Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„Ù†Ù…Ø°Ø¬Ø©\n",
    "\n",
    "#### Level 1: Raw Data (Not Ready) âŒ\n",
    "- **Characteristics**: Just loaded from file, no exploration\n",
    "- **Issues**: Unknown data quality, unknown structure\n",
    "- **Action**: Complete this notebook (exploration)\n",
    "\n",
    "#### Level 2: Explored Data (Partially Ready) âš ï¸\n",
    "- **Characteristics**: Explored structure, found issues\n",
    "- **Issues**: Missing values, duplicates, outliers, wrong types\n",
    "- **Action**: Complete Example 2 (cleaning)\n",
    "\n",
    "#### Level 3: Clean Data (Mostly Ready) âš ï¸\n",
    "- **Characteristics**: Clean, no missing values, correct types\n",
    "- **Issues**: Features not scaled, categories not encoded\n",
    "- **Action**: Complete Example 3 (preprocessing)\n",
    "\n",
    "#### Level 4: Preprocessed Data (Ready!) âœ…\n",
    "- **Characteristics**: Clean, scaled, encoded, split into train/test\n",
    "- **Issues**: None\n",
    "- **Action**: Ready for modeling (Example 4+)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: E-commerce Dataset\n",
    "- **Exploration Findings**: 15% missing prices, 3% duplicates, prices range $10-$10,000\n",
    "- **Quality Assessment**: âŒ NOT READY\n",
    "- **Issues**: Too many missing values, features need scaling\n",
    "- **Action Plan**: \n",
    "  1. Clean missing values (Example 2)\n",
    "  2. Scale prices (Example 3)\n",
    "  3. Then model (Example 4)\n",
    "\n",
    "#### Example 2: Medical Dataset\n",
    "- **Exploration Findings**: 2% missing ages, no duplicates, age range 0-100\n",
    "- **Quality Assessment**: âš ï¸ PARTIALLY READY\n",
    "- **Issues**: Small missing values (can remove), but need to check other features\n",
    "- **Action Plan**:\n",
    "  1. Remove missing values (Example 2)\n",
    "  2. Check if scaling needed (Example 3)\n",
    "  3. Then model (Example 4)\n",
    "\n",
    "#### Example 3: Customer Dataset\n",
    "- **Exploration Findings**: No missing values, no duplicates, all features scaled, categories encoded\n",
    "- **Quality Assessment**: âœ… READY\n",
    "- **Issues**: None\n",
    "- **Action Plan**: Ready to build models (Example 4)\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Always explore first** - You can't assess quality without exploration\n",
    "2. **Check all quality aspects** - Missing values, duplicates, outliers, types, scaling, encoding\n",
    "3. **Use the checklist** - Systematic assessment prevents missing issues\n",
    "4. **Fix critical issues first** - Don't model on bad data\n",
    "5. **Understand readiness levels** - Know where your data stands\n",
    "6. **Document findings** - Write down what you found and what needs fixing\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Assessment | Ù…Ù…Ø§Ø±Ø³Ø© Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "\n",
    "**Scenario**: You've explored a dataset and found:\n",
    "- 8% missing values in \"income\" column\n",
    "- 2% duplicate rows\n",
    "- Age ranges from 18 to 65 (reasonable)\n",
    "- Salary ranges from $30,000 to $200,000\n",
    "- Department column has categories: \"IT\", \"HR\", \"Finance\"\n",
    "\n",
    "**Your task**: Assess data quality and determine modeling readiness!\n",
    "\n",
    "**Answer**:\n",
    "1. **Missing values (8%)**: âš ï¸ WARNING - Should clean (Example 2)\n",
    "2. **Duplicates (2%)**: âœ… GOOD - Can remove (Example 2)\n",
    "3. **Age range**: âœ… GOOD - Reasonable range\n",
    "4. **Salary range**: âš ï¸ WARNING - Different scale from age, needs scaling (Example 3)\n",
    "5. **Department**: âš ï¸ WARNING - Categorical, needs encoding (Example 3)\n",
    "6. **Overall Assessment**: âš ï¸ PARTIALLY READY\n",
    "7. **Action Plan**: \n",
    "   - Clean missing values and duplicates (Example 2)\n",
    "   - Scale salary and encode department (Example 3)\n",
    "   - Then ready for modeling (Example 4)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Example 2: Data Cleaning** - Fixes the quality issues we found\n",
    "- ğŸ““ **Example 3: Data Preprocessing** - Prepares clean data for modeling\n",
    "- ğŸ““ **Example 4: Linear Regression** - Builds models on ready data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Categorical Data Analysis | Ø§Ù„Ø®Ø·ÙˆØ© 7: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©\n",
    "\n",
    "**BEFORE**: We see categorical values (like 'A', 'B', 'C' for location) but don't know their distribution.\n",
    "\n",
    "**AFTER**: We'll count how many times each category appears to understand the balance!\n",
    "\n",
    "**Why analyze categorical data?**\n",
    "- Shows if categories are balanced or imbalanced\n",
    "- Helps decide if we need encoding (one-hot, label encoding)\n",
    "- Reveals data quality issues (unexpected categories)\n",
    "\n",
    "---\n",
    "\n",
    "## â“ Common Student Questions | Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„Ø·Ù„Ø§Ø¨\n",
    "\n",
    "**Q: Why do we need to explore data before building models?**\n",
    "- **Answer**: You can't build good models on bad data! Exploration helps you:\n",
    "  - Find problems early (missing values, duplicates, outliers)\n",
    "  - Understand data structure (what columns mean, what types you have)\n",
    "  - Make informed decisions (what preprocessing is needed)\n",
    "  - Save time later (catch issues before they break your models)\n",
    "\n",
    "**Q: What's the difference between .head() and .tail()?**\n",
    "- **Answer**: \n",
    "  - `.head(n)`: Shows first n rows (default: 5)\n",
    "  - `.tail(n)`: Shows last n rows (default: 5)\n",
    "  - **Why both?** Check beginning AND end to see if data is consistent\n",
    "\n",
    "**Q: Why check for missing values? Can't I just ignore them?**\n",
    "- **Answer**: **NO!** ML models can't work with missing data. You MUST handle them:\n",
    "  - **Option 1**: Remove rows with missing values (if few missing)\n",
    "  - **Option 2**: Fill missing values (mean, median, mode, or prediction)\n",
    "  - **Option 3**: Use algorithms that handle missing values (advanced)\n",
    "  - **We'll learn this in Example 2: Data Cleaning**\n",
    "\n",
    "**Q: What's the difference between mean and median?**\n",
    "- **Answer**:\n",
    "  - **Mean**: Average (sum of all values / count) - affected by outliers\n",
    "  - **Median**: Middle value (50th percentile) - NOT affected by outliers\n",
    "  - **Example**: [1, 2, 3, 4, 100] â†’ Mean = 22, Median = 3\n",
    "  - **When to use**: Use median when you have outliers (extreme values)\n",
    "\n",
    "**Q: Why check for duplicates?**\n",
    "- **Answer**: Duplicates can bias your models:\n",
    "  - Same data counted twice â†’ model learns from same pattern twice\n",
    "  - Wastes computational resources\n",
    "  - Indicates data collection issues\n",
    "  - **Solution**: Remove duplicates (we'll learn in Example 2)\n",
    "\n",
    "**Q: What if my dataset has different structure (different columns, types)?**\n",
    "- **Answer**: **That's normal!** Every dataset is different. The key is:\n",
    "  - **Understand YOUR data structure** (rows, columns, types)\n",
    "  - **Apply the same exploration steps** (shape, types, missing values, duplicates)\n",
    "  - **Adapt to your specific data** (different columns need different analysis)\n",
    "  - **Focus on structure, not domain** (CS students work with many domains!)\n",
    "\n",
    "**Q: How do I know if my data is ready for modeling?**\n",
    "- **Answer**: Use the Data Quality Checklist (Step 8 above):\n",
    "  - âœ… No missing values (or < 5%)\n",
    "  - âœ… No duplicates (or < 1%)\n",
    "  - âœ… Correct data types\n",
    "  - âœ… Features scaled (if needed)\n",
    "  - âœ… Categories encoded (if needed)\n",
    "  - **If all checks pass â†’ Ready for modeling!**\n",
    "\n",
    "**Q: What's the difference between .info() and .describe()?**\n",
    "- **Answer**:\n",
    "  - **`.info()`**: Shows data types, non-null counts, memory usage (structure)\n",
    "  - **`.describe()`**: Shows statistics (mean, std, min, max) for numeric columns (summary)\n",
    "  - **Use both**: `.info()` for structure, `.describe()` for statistics\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Summary: What We Learned | Ø§Ù„Ù…Ù„Ø®Øµ: Ù…Ø§ ØªØ¹Ù„Ù…Ù†Ø§Ù‡\n",
    "\n",
    "**BEFORE this notebook**: We had raw data files we couldn't use.\n",
    "\n",
    "**AFTER this notebook**: We can:\n",
    "- âœ… Load data from CSV files\n",
    "- âœ… Inspect data structure and types\n",
    "- âœ… Calculate statistical summaries\n",
    "- âœ… Identify data quality issues (missing values, duplicates)\n",
    "- âœ… Analyze both numerical and categorical data\n",
    "\n",
    "**Next Steps**: \n",
    "- ğŸ““ Example 2: Data Cleaning (fix the issues we found)\n",
    "- ğŸ““ Example 3: Data Preprocessing (prepare data for modeling)\n",
    "- ğŸ““ Example 4: Linear Regression (build our first model!)\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Example 1 Complete! | Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 1!\n",
    "\n",
    "You've learned the foundation of data science: **exploration before modeling**!\n",
    "\n",
    "**Key Takeaway**: Always explore your data first. You can't build good models on bad data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}