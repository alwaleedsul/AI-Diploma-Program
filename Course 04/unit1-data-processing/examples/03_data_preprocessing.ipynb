{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Data Preprocessing | Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 04, Unit 1** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Data Preprocessing | Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Data Loading and Exploration** - Know your data structure\n",
    "- âœ… **Example 2: Data Cleaning** - Have clean data to preprocess\n",
    "- âœ… **Basic ML concepts**: Features, targets, training vs testing\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why preprocessing is needed\n",
    "- Knowing which preprocessing method to use\n",
    "- Understanding the difference between scaling methods\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the THIRD example** - it prepares clean data for machine learning!\n",
    "\n",
    "**Why this example THIRD?**\n",
    "- **Before** you can build ML models, you need preprocessed data\n",
    "- **Before** you can train models, you need scaled features\n",
    "- **Before** you can make predictions, you need encoded categories\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: Data Loading (we know our data)\n",
    "- ğŸ““ Example 2: Data Cleaning (we have clean data)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 4: Linear Regression (needs preprocessed data)\n",
    "- ğŸ““ Example 5: Polynomial Regression (needs scaled features)\n",
    "- ğŸ““ All ML models (all need preprocessing!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Preprocessing transforms **clean data into ML-ready format**\n",
    "2. Preprocessing teaches you **scaling vs encoding** (different problems, different solutions)\n",
    "3. Preprocessing shows you **train-test split** (essential for evaluation)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Preparing Ingredients for Cooking | Ø§Ù„Ù‚ØµØ©: ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ù„Ù„Ø·Ø¨Ø®\n",
    "\n",
    "Imagine you're cooking. **Before** you can cook, you need to prepare ingredients - cut vegetables to the same size, marinate meat, measure spices. **After** preparing everything uniformly, you can cook a perfect meal!\n",
    "\n",
    "Same with machine learning: **Before** building models, we preprocess data - scale features to same range, encode categories to numbers, split into train/test. **After** preprocessing, we can build accurate models!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Preprocessing Matters | Ù„Ù…Ø§Ø°Ø§ ØªÙ‡Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ\n",
    "\n",
    "Preprocessing is essential for ML success:\n",
    "- **Feature Scaling**: Algorithms work better when features are on similar scales\n",
    "- **Encoding**: ML algorithms need numbers, not text categories\n",
    "- **Train-Test Split**: We need separate data to evaluate model performance\n",
    "- **Without Preprocessing**: Models fail or perform poorly\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Scale features using Standardization and Normalization\n",
    "2. Encode categorical variables (Label vs One-Hot)\n",
    "3. Split data into training and testing sets\n",
    "4. Understand when to use each preprocessing method\n",
    "5. Build a complete preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "NOTE: Auto-suppressed invalid cell\n",
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us preprocess data \n",
    "for machine learningimport pandas as pd \n",
    "# For data manipulation\n",
    "import numpy as np \n",
    "# For numerical operations\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, \n",
    "MinMaxScaler, \n",
    "# For normalization (range 0-1)\n",
    "LabelEncoder, \n",
    "# For ordinal encodingOneHotEncoder \n",
    "# For nominal encoding\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "# For splitting data\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each preprocessing tool does:\")\n",
    "print(\" - StandardScaler: Scale features to mean=0, std=1 (z-score)\")\n",
    "print(\" - MinMaxScaler: Scale features to range [0, 1]\")\n",
    "print(\" - LabelEncoder: Convert categories to numbers (ordinal)\")\n",
    "print(\" - OneHotEncoder: Convert categories to binary columns (nominal)\")\n",
    "print(\" - train_test_split: Split data into train/test sets\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We have clean data, but it's not ready for machine learning - features have different scales, categories are text, and we haven't split the data.\n",
    "\n",
    "**AFTER**: We'll preprocess the data - scale features, encode categories, split into train/test - making it ready for ML models!\n",
    "\n",
    "**Why this matters**: Most ML algorithms require preprocessed data. Without it, models fail or give poor results!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Load Real-World Data | Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "**BEFORE**: We need to learn preprocessing, but we need real data with different scales and categories.\n",
    "\n",
    "**AFTER**: We'll load the California Housing dataset - real data with features on different scales and we'll add categorical features to practice preprocessing techniques!\n",
    "\n",
    "**Why use California Housing?** This is REAL data from the 1990 census with features that need scaling (MedInc ranges differently than HouseAge, etc.). Real datasets have these characteristics - we need to learn how to handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# Load real-world Credit Card Fraud dataset \n",
    "# for preprocessing\n",
    "# GDI Theme: Financial Investigations - Fraud Detection\n",
    "# This dataset has features on different scales - perfect \n",
    "# for learning preprocessing techniques!\n",
    "\n",
    "# print(\"\\nğŸ“¥ Loading Credit Card Fraud dataset...\")\n",
    "\n",
    "\n",
    "print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø­ØªÙŠØ§Ù„ Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø§Ø¦ØªÙ…Ø§Ù†ÙŠØ©...\")\n",
    "\n",
    "# try:\n",
    " \n",
    "# Load \n",
    "# from local filedf_full = \n",
    "# File not found: ../../datasets/raw/creditcard_fraud.csv\n",
    "# Using synthetic data insteadpd.DataFrame({'col1': range(100), 'col2': range(100, 200)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\"\\nâœ… Real-world Credit Card Fraud data loaded from local file!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ“Š Full dataset: {len(df_full):,} transactions\")\n",
    "\n",
    "# For learning purposes, use a sample \n",
    "# for faster execution\n",
    " \n",
    "# This makes the notebook more convenient \n",
    "# for students while still demonstrating concepts\n",
    " \n",
    "# = 20000 \n",
    "# Sample 20k rows \n",
    "# for faster processing\n",
    "# df = df_full.sample(n=min(sample_size, len(df_full)), random_state=57, replace=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ“Š Using sample: {len(df):,} transactions (for faster learning)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" ğŸ’¡ Note: Using a sample for learning convenience. In real projects, use full dataset.\")\n",
    "# except FileNotFoundError:\n",
    " \n",
    "# Fallback: Load \n",
    "# from URL (if available) or create minimal structure\n",
    "# print(\"\\nâš ï¸ Local file not found. Please ensure creditcard_fraud.csv is in datasets/raw/\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(\" For this notebook, we'll use a sample structure...\")\n",
    "\n",
    "# = pd.DataFrame({\n",
    "#  'Time': [0] * 1000, 'Amount': [100.0] * 1000,\n",
    "#  'Class': [0] * 1000\n",
    "})\n",
    "\n",
    "# For preprocessing demonstration, we'll create a categorical column \n",
    "# from Amount\n",
    "# This helps demonstrate encoding techniques (transaction size categories)\n",
    "# df['transaction_size'] = pd.cut(df['Amount'], bins=[0, 50, 200, float('inf')], \n",
    "#  labels=['Small', 'Medium', 'Large'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" ğŸ“Š This is REAL anonymized credit card transaction data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" ğŸ“ˆ Contains {len(df)} transactions with {len(df.columns)} features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" ğŸ¯ Domain: Financial Investigations - Fraud Detection\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" ğŸ” Features have different scales (Time: seconds, Amount: dollars)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" ğŸ“‹ Note: V1-V28 are already PCA-transformed (pre-scaled), but Time and Amount need scaling\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" ğŸ“‹ Added 'transaction_size' category for encoding demonstration\")\n",
    "\n",
    "# Inspect the data to see why preprocessing is needed\n",
    "# print(\"\\nğŸ“Š Original Data (Credit Card Fraud Dataset):\")\n",
    "\n",
    "\n",
    "# print(df[['Time', 'Amount', 'V1', 'V2', 'V3', 'transaction_size', 'Class']].head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ“ Data Shape: {df.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ” Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Numeric features have VERY different scales:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Time ranges: {df['Time'].min()} to {df['Time'].max()} (seconds)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Amount ranges: ${df['Amount'].min():.2f} to ${df['Amount'].max():.2f} (dollars)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - V1 ranges: {df['V1'].min():.3f} to {df['V1'].max():.3f} (already PCA-scaled)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Categorical feature (transaction_size) is text: Small, Medium, Large\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - ML algorithms need same-scale numbers! This is why we preprocess!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - V1-V28 are already scaled (PCA-transformed), so we'll focus on Time and Amount\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Understanding the Dataset | ÙÙ‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø¬Ø§Ù„\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (medical, financial, security, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: ~20,000 rows Ã— 31 columns (sample of transactions Ã— features)\n",
    "  - Note: We use a sample for learning convenience (faster execution)\n",
    "  - Full dataset has ~284,000 rows - would be slower but more representative\n",
    "- **Feature Types**: Mixed (numerical: Time, Amount, V1-V28; categorical: transaction_size)\n",
    "- **Target Type**: Classification (binary: 0=Normal, 1=Fraud)\n",
    "- **Task**: Detect fraudulent transactions based on features\n",
    "- **Preprocessing Needs**: Features on different scales (Time, Amount need scaling), categorical feature (need encoding)\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Different feature scales** â†’ Need standardization/normalization (Time: 0-172792 seconds vs. Amount: $0-$25691)\n",
    "- **Categorical feature** â†’ Need encoding (transaction_size: Small/Medium/Large â†’ numbers)\n",
    "- **Classification task** â†’ Binary classification (fraud detection)\n",
    "- **Large dataset** â†’ Good for train/test split demonstration\n",
    "- **V1-V28 already scaled** â†’ These are PCA-transformed features (already standardized), so we focus on Time and Amount\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ÙÙ‡Ù… Ù…Ø¬Ø§Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø§Ø®ØªØµØ§Ø±)\n",
    "\n",
    "**What is this data?** Real anonymized credit card transaction data for fraud detection (GDI Theme: Financial Investigations).\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For scaling**: Features have very different scales (Time: seconds in thousands, Amount: dollars) â†’ need standardization/normalization\n",
    "- **For encoding**: transaction_size is categorical (Small/Medium/Large) â†’ need encoding for ML\n",
    "- **For train/test split**: Need separate data to evaluate model performance\n",
    "- **For fraud detection**: Financial investigation context - detecting suspicious transactions\n",
    "\n",
    "**Domain Context** (Brief):\n",
    "- **Time**: Time elapsed between transaction and first transaction (seconds) - helps detect patterns\n",
    "- **Amount**: Transaction amount (dollars) - important for fraud detection\n",
    "- **V1-V28**: PCA-transformed features (anonymized to protect privacy, already scaled)\n",
    "- **Class**: Target variable (0=Normal transaction, 1=Fraudulent transaction)\n",
    "- **transaction_size**: Categorical feature we created from Amount (Small/Medium/Large) - demonstrates encoding\n",
    "  - Note: This is created for learning encoding techniques only\n",
    "  - In real modeling, you wouldn't create categories from a feature you're already using\n",
    "  - It's just for demonstration purposes to show how encoding works!\n",
    "\n",
    "**ğŸ’¡ Key Point for CS Students**: You don't need to be a financial expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types, scales)\n",
    "- Knowing the **preprocessing methods** (scaling vs. encoding)\n",
    "- Choosing the right **preprocessing approach** based on structure, not domain knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"2. Feature Scaling - Standardization\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Scaling - Standardization | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ\n",
    "\n",
    "**BEFORE**: Features have very different scales (age: 20-60, salary: 30k-100k). Algorithms will be biased toward larger numbers!\n",
    "\n",
    "**AFTER**: We'll standardize features so they all have mean=0 and std=1, putting them on the same scale!\n",
    "\n",
    "**Why Standardization?**\n",
    "- **Problem**: Features have different scales (age: 20-60, salary: 30k-100k)\n",
    "- **Issue**: ML algorithms treat larger numbers as more important â†’ biased toward salary!\n",
    "- **Solution**: Standardization puts all features on same scale (mean=0, std=1)\n",
    "- **Result**: All features contribute equally to the model\n",
    "- **Formula**: `(x - mean) / std`\n",
    "\n",
    "**Why mean=0 and std=1? (Common Student Questions)**\n",
    "\n",
    "**Q: Why mean=0, not 1 or 100?**\n",
    "- **Answer**: We subtract the mean â†’ `(x - mean)` centers data at 0\n",
    "  - If mean=40: value=50 becomes (50-40)=10, value=30 becomes (30-40)=-10\n",
    "  - The average of all (x - mean) values is always 0 (by definition!)\n",
    "  - Mean=0 means data is centered (balanced around zero)\n",
    "  - Why not 1? Because subtracting mean naturally gives 0, not 1\n",
    "\n",
    "**Q: Why std=1, not 0 or 2?**\n",
    "- **Answer**: We divide by std â†’ `(x - mean) / std` normalizes spread to 1\n",
    "  - If std=10: value=50 becomes (50-40)/10=1.0, value=30 becomes (30-40)/10=-1.0\n",
    "  - Dividing by std makes the new std exactly 1 (by definition!)\n",
    "  - Std=1 means \"one unit of variation\" (standard unit)\n",
    "  - Why not 0? Std=0 means no variation (all values same) - not useful!\n",
    "  - Why not 2? We want standard unit (1), not arbitrary number\n",
    "\n",
    "**Q: Why these specific numbers (0 and 1)?**\n",
    "- **Answer**: They're the \"standard\" values that make comparison easy\n",
    "  - Mean=0: Easy to see if value is above/below average (positive/negative)\n",
    "  - Std=1: Easy to see how many standard deviations away (1.5 = 1.5 std away)\n",
    "  - Together: All features use same \"standard scale\" for fair comparison\n",
    "\n",
    "- **Use when**: Data is normally distributed, outliers are important\n",
    "- **Good for**: Linear models, neural networks, distance-based algorithms\n",
    "\n",
    "**Note**: Remove problematic outliers first (Example 2), then standardize. Standardization preserves outliers, so clean data first!"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# Create StandardScaler object\n",
    "# Standardization \n",
    "# formula: (x - mean) / std\n",
    "\n",
    "\n",
    "# StandardScaler()\n",
    "# - Creates scaler object \n",
    "# for standardization\n",
    "# - Standardization: (x - mean) / st\n",
    "# d\n",
    "# = 1\n",
    "# - Centers data around 0, scales to unit variance\n",
    "# - Two-step process:\n",
    "# 1. .fit(): Learn mean and std \n",
    "from data\n",
    "# 2. .trans\n",
    "# : Apply transformation\n",
    "# .fit_trans\n",
    "\n",
    "# - Two operations in one: .fit() then .trans\n",
    "\n",
    "# 1. .fit(): Learns parameters \n",
    "from data (mean/std, categories, etc.)\n",
    "# 2. .trans\n",
    "# : Applies transformation using learned parameters\n",
    "# - Use on training data\n",
    "# - For test data, use only .trans\n",
    "(don't refit!)\n",
    "\n",
    "# - Or use .fit_trans\n",
    "# : Do both in one step\n",
    "# = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" âœ… StandardScaler created\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" Formula: (x - mean) / std\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Select numeric columns \n",
    "for scaling\n",
    "Why only numeric? Categorical columns need encoding, not scaling!\n",
    "= ['Time', 'Amount']\n",
    "df_scaled_standard = df.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Scaling columns: {numeric_cols}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" (V1-V28 are already scaled via PCA, so we focus on Time and Amount)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# Fit and transform the data\n",
    "# .fit_translearns the mean/std \n",
    "from data, then transforms it\n",
    "# Why fit_transform? We learn parameters \n",
    "from training data, then apply to all data\n",
    "\n",
    "# scaler_standard.fit_trans\n",
    "\n",
    "# - .fit_trans\n",
    "# : Two operations in one\n",
    "# 1. .fit(): Learns mean and std \n",
    "from data\n",
    "# - Calculates mean and std \n",
    "# for each column\n",
    "# - Stores these values in scaler object\n",
    "# 2. .trans\n",
    "# : Applies standardization using learned parameters\n",
    "# - Formula: (x - learned_mean) / learned_std\n",
    "# - Transforms each value in column\n",
    "# - df[numeric_cols]: Selects only numeric columns to scale\n",
    "# - List of column names: ['age', 'salary', 'experience']\n",
    "# - Returns DataFrame with only those columns\n",
    "# - Result: All numeric columns now have meanâ‰ˆ0 and stdâ‰ˆ1\n",
    "# - Note: In real ML, fit on training data, transform on both train and testdf_scaled_standard[numeric_cols] = scaler_standard.fit_trans\n",
    "# print(\"\\nâœ… After Standardization (mean=0, std=1):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ (Ù…ØªÙˆØ³Ø·=0ØŒ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÙŠØ§Ø±ÙŠ=1):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Time - Mean: {df_scaled_standard['Time'].mean():.4f}, Std: {df_scaled_standard['Time'].std():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Amount - Mean: {df_scaled_standard['Amount'].mean():.4f}, Std: {df_scaled_standard['Amount'].std():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n âœ… All features now have meanâ‰ˆ0 and stdâ‰ˆ1!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n ğŸ“ Understanding the values:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Positive values (e.g., 1.67): Above average (1.67 std above mean)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Negative values (e.g., -1.36): Below average (1.36 std below mean)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Values near 0: Close to average\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Values > 2 or < -2: Far from average (potential outliers)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ“„ Scaled data (first 5 rows):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(df_scaled_standard[numeric_cols].head()\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Scaling - Normalization (Min-Max) | Ø§Ù„Ø®Ø·ÙˆØ© 3: Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªØ·Ø¨ÙŠØ¹\n",
    "\n",
    "**BEFORE**: Features have different ranges. We want them all in the same range [0, 1].\n",
    "\n",
    "**AFTER**: We'll normalize features so they all range from 0 to 1!\n",
    "\n",
    "**Why Normalization (Min-Max)?**\n",
    "- **Problem**: Same as standardization - different scales\n",
    "- **Solution**: Normalization puts all features in range [0, 1]\n",
    "- **Formula**: `(x - min) / (max - min)`\n",
    "- **Result**: Range [0, 1] (minimum becomes 0, maximum becomes 1)\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: Why range [0, 1], not [0, 100]?**\n",
    "  - Answer: [0, 1] is standard range (0% to 100% of the spread)\n",
    "  - Easy to interpret: 0 = minimum, 1 = maximum, 0.5 = middle\n",
    "- **Q: Why not use standardization?**\n",
    "  - Answer: Normalization gives bounded [0,1] range (good for neural networks)\n",
    "  - Standardization can give values outside [0,1] (e.g., -2.5, 3.1)\n",
    "- **Q: Which is better?**\n",
    "  - Answer: Depends on your algorithm!\n",
    "  - Standardization: Better for normal data, preserves outliers\n",
    "  - Normalization: Better for neural networks, bounded range needed\n",
    "\n",
    "- **Use when**: Data is not normally distributed, you want bounded values\n",
    "- **Good for**: Neural networks, algorithms that need [0,1] range"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# Normalization: (x - min) / (max - min) -> range [0, 1]\n",
    "# Ø§Ù„ØªØ·Ø¨ÙŠØ¹: (x - Ø§Ù„Ø£Ø¯Ù†Ù‰) / (Ø§Ù„Ø£Ø¹Ù„Ù‰ - Ø§Ù„Ø£Ø¯Ù†Ù‰) -> Ø§Ù„Ù†Ø·Ø§Ù‚ [0, 1]\n",
    "\n",
    "# MinMaxScaler()\n",
    "# - Creates scaler object \n",
    "# for min-max normalization\n",
    "# - Normalization: (x - min) / (max - min)\n",
    "# - Transforms data to range [0, 1]\n",
    "# - Minimum value becomes 0, maximum becomes 1\n",
    "# - Two-step process:\n",
    "# 1. .fit(): Learn min and max \n",
    "from data\n",
    "# 2. .trans\n",
    "# : Apply transformation\n",
    "# - Or use .fit_trans\n",
    "# : Do both in one step\n",
    "# = MinMaxScaler()\n",
    "# df_scaled_minmax = df.copy()\n",
    "# df_scaled_minmax[numeric_cols] = scaler_minmax.fit_transform(df[numeric_cols])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nAfter Min-Max Normalization (range 0-1):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¨Ø§Ù„Ø£Ø¯Ù†Ù‰-Ø§Ù„Ø£Ø¹Ù„Ù‰ (Ø§Ù„Ù†Ø·Ø§Ù‚ 0-1):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Time - Min: {df_scaled_minmax['Time'].min():.4f}, Max: {df_scaled_minmax['Time'].max():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Amount - Min: {df_scaled_minmax['Amount'].min():.4f}, Max: {df_scaled_minmax['Amount'].max():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(df_scaled_minmax[numeric_cols].head()\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Label Encoding | Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª\n",
    "\n",
    "**BEFORE**: We have categorical text data (e.g., 'Small', 'Medium', 'Large' transaction sizes), but ML algorithms need numbers!\n",
    "\n",
    "**AFTER**: We'll encode categories to numbers using Label Encoding - perfect for ordinal categories (categories with order)!\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: What is ordinal vs nominal?**\n",
    "  - Answer: Ordinal = categories have order (e.g., Small < Medium < Large, Low < Medium < High)\n",
    "  - Nominal = categories have no order (e.g., city names, colors, departments)\n",
    "  - Label encoding preserves order â†’ use for ordinal\n",
    "  - One-hot encoding treats all equally â†’ use for nominal\n",
    "- **Q: Why use Label Encoding instead of One-Hot?**\n",
    "  - Answer: Label encoding preserves the order relationship\n",
    "  - Example: Small=0, Medium=1, Large=2 â†’ algorithm knows Large > Medium > Small\n",
    "  - One-hot would create 3 separate columns â†’ loses the order information\n",
    "  - Use label encoding when order matters (transaction sizes, ratings, sizes)\n",
    "- **Q: Can I use Label Encoding for nominal categories?**\n",
    "  - Answer: Technically yes, but NOT recommended!\n",
    "  - Problem: Creates false order (e.g., 'Red'=0, 'Blue'=1, 'Green'=2 â†’ algorithm thinks Green > Blue > Red)\n",
    "  - Solution: Use One-Hot Encoding for nominal categories (no false order)\n",
    "- **Q: How does LabelEncoder assign numbers?**\n",
    "  - Answer: Assigns numbers in alphabetical order (or order found in data)\n",
    "  - Example: ['Large', 'Medium', 'Small'] â†’ Large=0, Medium=1, Small=2 (alphabetical)\n",
    "  - Important: Order might not match your desired order â†’ check `label_encoder.classes_`\n",
    "- **Q: What if I want a specific order?**\n",
    "  - Answer: You can manually map categories to numbers using a dictionary\n",
    "  - Example: `df['transaction_size'] = df['transaction_size'].map({'Small': 0, 'Medium': 1, 'Large': 2})`\n",
    "  - Or use pandas `Categorical` with `ordered=True` to control order\n",
    "\n",
    "**Use when**: Categories have inherent order (ordinal data)\n",
    "**Good for**: Transaction sizes, ratings, sizes, rankings (transaction_size: Small < Medium < Large)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LabelEncoder()\n",
    "- Creates encoder \n",
    "for ordinal categorical data\n",
    "- Ordinal: Categories have order (e.g., Bachelor < Master < PhD)\n",
    "- Converts text categories to integers (0, 1, 2, ...)\n",
    "- Two-step process:\n",
    "1. .fit(): Learn unique categories\n",
    "2. .trans\n",
    ": Convert categories to numbers\n",
    "- Or use .fit_trans\n",
    ": Do both in one step\n",
    "= LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder()\n",
    "# - Creates encoder \n",
    "# for ordinal categorical data\n",
    "# - Ordinal: Categories have order (e.g., Bachelor < Master < PhD)\n",
    "# - Converts text categories to integers (0, 1, 2, ...)\n",
    "# - Two-step process:\n",
    "# 1. .fit(): Learn unique categories\n",
    "# 2. .trans\n",
    "# : Convert categories to numbers\n",
    "# - Or use .fit_trans\n",
    "# : Do both in one step\n",
    "# = LabelEncoder()\n",
    "\n",
    "# Encode transaction_size (ordinal: Small < Medium < Large)\n",
    "# df_encoded = df.copy()\n",
    "\n",
    "# label_encoder.fit_trans\n",
    "\n",
    "# - .fit_trans\n",
    "# : Two operations in one\n",
    "# 1. .fit(): Learns unique categories \n",
    "from data\n",
    "# - Finds all unique values (e.g., 'Small', 'Medium', 'Large')\n",
    "# - Assigns numbers in alphabetical order (or order found)\n",
    "# 2. .trans\n",
    "# : Converts categories to integers\n",
    "# - 'Large' â†’ 0, 'Medium' â†’ 1, 'Small' â†’ 2 (alphabetical order)\n",
    "# - Note: Alphabetical order is Large, Medium, Small (not the natural order!)\n",
    "# - df['transaction_size']: Passes transaction_size column as Series\n",
    "# - Returns numpy array with encoded integers\n",
    "# - label_encoder.\n",
    "# classes_: Shows mapping (original categories)\n",
    "# df_encoded['transaction_size_encoded'] = label_encoder.fit_transform(df['transaction_size'])\n",
    "\n",
    "# label_encoder.\n",
    "classes_\n",
    "# - Returns array of unique categories in order they were encoded\n",
    "# - Used to see mapping: \n",
    "# classes_[0] = first category, etc.\n",
    "# print(\"\\nLabel Encoding for Transaction Size:\")\n",
    "\n",
    "\n",
    "# print(\"Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø©:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Mapping / Ø§Ù„ØªØ¹ÙŠÙŠÙ†:\")\n",
    "\n",
    "# for i, label in enumerate(label_encoder.classes_):\n",
    "#  print(f\" {label}: {i}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nNote: Encoded in alphabetical order (Large=0, Medium=1, Small=2)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" For correct order, consider manual mapping or use pandas Categorical with ordered=True\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nEncoded values:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(df_encoded[['transaction_size', 'transaction_size_encoded']].head(10)\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: One-Hot Encoding | Ø§Ù„Ø®Ø·ÙˆØ© 5: Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†\n",
    "\n",
    "**BEFORE**: We have nominal categories (no order, like city names), but ML algorithms need numbers!\n",
    "\n",
    "**AFTER**: We'll encode categories using One-Hot Encoding - creates separate binary columns for each category!\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: What is One-Hot Encoding?**\n",
    "  - Answer: Each category becomes a separate binary column (0 or 1)\n",
    "  - Example: City ['Riyadh', 'Jeddah', 'Dammam'] â†’ 3 columns: city_Riyadh, city_Jeddah, city_Dammam\n",
    "  - Each row has 1 in one column (the city), 0 in all others\n",
    "  - No false order â†’ perfect for nominal categories\n",
    "- **Q: Why use One-Hot instead of Label Encoding?**\n",
    "  - Answer: One-Hot treats all categories equally (no false order)\n",
    "  - Label encoding creates false order for nominal data (e.g., 'Red'=0, 'Blue'=1 â†’ algorithm thinks Blue > Red)\n",
    "  - One-Hot: Each category is independent â†’ no order assumptions\n",
    "  - Use One-Hot for: City names, colors, departments, any categories without order\n",
    "- **Q: Why does One-Hot create so many columns?**\n",
    "  - Answer: Each category needs its own column to represent it independently\n",
    "  - Example: 10 cities â†’ 10 columns (or 9 if using drop='first')\n",
    "  - Trade-off: More columns = more memory, but no false order\n",
    "  - Solution: For many categories (>50), consider Target Encoding or Frequency Encoding\n",
    "- **Q: What is drop='first' in sklearn OneHotEncoder?**\n",
    "  - Answer: Drops the first category column to avoid multicollinearity\n",
    "  - Problem: If you have N categories, you only need N-1 columns (one is redundant)\n",
    "  - Example: 4 cities â†’ 3 columns (can infer 4th from the other 3)\n",
    "  - Why: Prevents perfect correlation between columns (helps with linear models)\n",
    "- **Q: When should I use pandas get_dummies vs sklearn OneHotEncoder?**\n",
    "  - Answer: pandas get_dummies = easier, good for quick work\n",
    "  - sklearn OneHotEncoder = more control (drop='first', sparse matrices, better for pipelines)\n",
    "  - Use pandas: Quick prototyping, simple cases\n",
    "  - Use sklearn: Production code, need drop='first', building pipelines\n",
    "\n",
    "**Use when**: Categories have no inherent order (nominal data)\n",
    "**Good for**: City names, colors, departments, any categories without order"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# Using pandas get_dummies (easier)\n",
    "# Ø§Ø³ØªØ®Ø¯Ø§Ù… pandas get_dummies (Ø£Ø³Ù‡Ù„)\n",
    "\n",
    "# - pd.get_dummies(): Creates one-hot encoded columns \n",
    "# for categorical data\n",
    "# - df: DataFrame to encode\n",
    "# : Column to encode\n",
    "# : Prefix \n",
    "# for new column names\n",
    "# - 'transaction_size' column with values ['Small', 'Medium', 'Large'] â†’ 'txn_size_Small', 'txn_size_Medium', 'txn_size_Large'\n",
    "# - One-hot encoding: Each category becomes a binary column (0 or 1)\n",
    "\n",
    "# - Returns DataFrame with original columns + new binary columns\n",
    "# - Use \n",
    "# for: Nominal categories (no order), but can also use \n",
    "# for ordinal\n",
    "\n",
    "\n",
    "\n",
    "# if you want to avoid false order\n",
    "# Note: For transaction_size (Small/Medium/Large), One-Hot treats all equally (no order assumption)\n",
    "# df_onehot = pd.get_dummies(df, columns=['transaction_size'], prefix=['txn_size'])\n",
    "\n",
    "# df_onehot.columns.tolist()\n",
    "# - df_onehot.columns: Returns Index with column names\n",
    "# - .tolist(): Converts to Python list\n",
    "# - Shows all column names after encoding\n",
    "# print(\"\\nOne-Hot Encoded columns:\")\n",
    "\n",
    "\n",
    "# print(\"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø´ÙØ±Ø© Ø¨Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†:\")\n",
    "# = [col \n",
    "# for col in df_onehot.columns\n",
    "\n",
    "\n",
    "\n",
    "if 'txn_size_' in col]\n",
    "# print(f\"New columns: {onehot_cols}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nSample of One-Hot Encoded data:\")\n",
    "# sample_cols = ['Time', 'Amount'] + onehot_cols[:5] \n",
    "# Show Time, Amount, and first few encoded cols\n",
    "# print(df_onehot[sample_cols].head()\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE: Auto-suppressed invalid cell\n",
    "# Method 2: Using sklearn OneHotEncoder (\n",
    "# for more control)\n",
    "# Why sklearn? More control - can drop first column to avoid multicollinearityremoves one column (redundant - can be inferred \n",
    "from others)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n--- Using sklearn OneHotEncoder ---\")\n",
    "\n",
    "# - OneHotEncoder(): Creates one-hot encoder object\n",
    "# : Returns dense array (True returns sparse matrix)\n",
    "# : Drops first category column to avoid multicollinearity\n",
    "# - If 4 cities, creates 3 columns (not 4)\n",
    "# - One column is redundant (can be inferred \n",
    "from others)\n",
    "\n",
    "# onehot_encoder.fit_trans\n",
    "\n",
    "# - .fit_trans\n",
    "# : Learns categories and encodes\n",
    "# 1. .fit(): Learns unique categories\n",
    "# 2. .trans\n",
    "# : Creates binary columnsas DataFrame (not Series)\n",
    "# - OneHotEncoder expects 2D input (DataFrame or 2D array)\n",
    "# - Returns numpy array with binary columns\n",
    "# - Example: 3 transaction sizes (Small, Medium, Large) â†’ 2 columns (\n",
    "\n",
    "\n",
    "\n",
    "# if drop='first')\n",
    "# txn_size_encoded = onehot_encoder.fit_transform(df[['transaction_size']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" Transaction Size encoding classes:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" {onehot_encoder.categories_}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\n Encoded shape: {txn_size_encoded.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" (3 transaction sizes â†’ 2 columns, because drop='first' removes one)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n First 5 encoded values:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(txn_size_encoded[:5])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n âœ… Each row has 1 in one column (the transaction size), 0 in others\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train-Test Split | Ø§Ù„Ø®Ø·ÙˆØ© 6: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±\n",
    "\n",
    "**BEFORE**: We have all our data together. We can't evaluate model performance on data it has seen!\n",
    "\n",
    "**AFTER**: We'll split data into training (80%) and testing (20%) sets!\n",
    "\n",
    "**Why Train-Test Split?**\n",
    "- **Training set**: Used to train/learn the model\n",
    "- **Test set**: Used to evaluate model performance (unseen data)\n",
    "- **Why separate?** Testing on training data gives false high scores (overfitting)\n",
    "- **Standard split**: 80% train, 20% test (can vary based on dataset size)\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: Why split data into train and test?**\n",
    "  - Answer: Need unseen data to evaluate model performance honestly\n",
    "  - Problem: Testing on training data = model memorized data â†’ gives false high scores\n",
    "  - Solution: Train on 80%, test on 20% â†’ see how model performs on new data\n",
    "  - Real-world: Model must work on data it hasn't seen before!\n",
    "- **Q: Why 80/20 split?**\n",
    "  - Answer: Good balance between training (need enough to learn) and testing (need enough to evaluate)\n",
    "  - 80% training: Enough data for model to learn patterns\n",
    "  - 20% testing: Enough data for reliable performance evaluation\n",
    "  - Can adjust: Small datasets (<1000) â†’ 70/30, Large datasets (>10k) â†’ 90/10\n",
    "- **Q: What is random_state?**\n",
    "  - Answer: Seed for random number generator â†’ ensures same split every time\n",
    "  - Problem: Without seed, different split each run â†’ can't reproduce results\n",
    "  - Solution: Set random_state=57 â†’ same split every time (reproducibility)\n",
    "  - Note: We use 57 here (not 42) - different projects use different seeds for variety\n",
    "  - Important: Use same random_state within one project to compare fairly\n",
    "- **Q: What is stratify=y?**\n",
    "  - Answer: Maintains class distribution in train/test sets\n",
    "  - Problem: Random split might create imbalanced train/test (e.g., 60% class 0 in train, 40% in test)\n",
    "  - Solution: stratify=y â†’ train and test have same class distribution\n",
    "  - Example: If 60% class 0, 40% class 1 â†’ both train and test have 60/40 split\n",
    "  - Use when: Classification problems with imbalanced classes\n",
    "- **Q: Should I split before or after preprocessing?**\n",
    "  - Answer: Split FIRST, then preprocess train and test separately!\n",
    "  - Why: Fit scaler/encoder on training data only, then transform both train and test\n",
    "  - Problem: Preprocessing before split = data leakage (test data influences training)\n",
    "  - Correct order: Split â†’ Fit on train â†’ Transform train and test\n",
    "  - Note: In this notebook, we show preprocessing on full data for simplicity, but in real projects, split first!"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prepare features and target\n",
    "For fraud detection, we'll use Time and Amount as features (V1-V28 are already PCA-transformed)\n",
    "Target: Class (0=Normal, 1=Fraud)\n",
    "X = df[['Time', 'Amount']]\n",
    "y = df['Class']\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Split into train and test sets (80% train, 20% test)\n",
    "Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± (80% ØªØ¯Ø±ÙŠØ¨ØŒ 20% Ø§Ø®ØªØ¨Ø§Ø±)\n",
    "\n",
    "strat\n",
    "\n",
    "ify=y)\n",
    "(not 42) - different projects use different seeds \n",
    "for variety\n",
    "- train_test_split(): Splits data into training and testing sets\n",
    "- X: Features (input variables) - DataFrame or array\n",
    "- y: Target (output variable) - Series or array\n",
    ": 20% \n",
    "for testing, 80%\n",
    "for training (can use 0.25\n",
    "for 25%)\n",
    ": Seed \n",
    "for random number generator\n",
    "- Ensures same split every time (reproducibility)\n",
    "= different splits\n",
    "- We use 57 here (not 42) \n",
    "for variety across notebooks\n",
    "- strat\n",
    "\n",
    "ify=y: Maintains class distribution in train/test\n",
    "- If y has 60% \n",
    "class 0, 40% \n",
    "class 1, train/test will have same ratio\n",
    "- Important \n",
    "for imbalanced datasets\n",
    "- Returns 4 arrays: X_train, X_test, y_train, y_test\n",
    "- X_train: Training features (80% of X)\n",
    "- X_test: Testing features (20% of X)\n",
    "- y_train: Training targets (80% of y)\n",
    "- y_test: Testing targets (20% of y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y,\n",
    "test_size=0.2,\n",
    "random_state=57, \n",
    "Using 57 instead of 42 \n",
    "for variety across different notebooksstrat\n",
    "\n",
    "ify=y \n",
    "Maintain \n",
    "class distribution\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nOriginal data shape: {X.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: {X.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" X_train: {X_train.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" y_train: {y_train.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" X_test: {X_test.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\" y_test: {y_test.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nTrain percentage: {len(X_train) / len(X) * 100:.1f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Test percentage: {len(X_test) / len(X) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Complete Preprocessing Pipeline | Ø§Ù„Ø®Ø·ÙˆØ© 7: Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
    "\n",
    "**BEFORE**: We've learned individual preprocessing steps, but how do we combine them all together?\n",
    "\n",
    "**AFTER**: We'll create a complete preprocessing pipeline that combines all steps - encoding, scaling, and splitting - into one reusable function!\n",
    "\n",
    "**Why this matters**: In real projects, you need to apply the same preprocessing to training and test data consistently!"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "    pass\n",
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "+ \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"7. Complete Preprocessing Pipeline\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def preprocess_data(df, numeric_cols, categorical_cols, target_col, test_size=0.2):\n",
    "\"\"\"\n",
    "Complete preprocessing pipelineØ®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
    "\"\"\"\n",
    " \n",
    "Separate features and targetX = df[numeric_cols + categorical_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "\n",
    "\n",
    "= StandardScaler()\n",
    "X_encoded[numeric_cols] = scaler.fit_transform(X_encoded[numeric_cols])\n",
    "\n",
    "Split datastrat\n",
    "\n",
    "ify=y)\n",
    "- Splits data into training and testing sets\n",
    "- X: Features (input variables), y: Target (output variable)\n",
    ": 20% \n",
    "for testing, 80%\n",
    "for training\n",
    ": Seed \n",
    "for reproducibility (same split every time)\n",
    "- strat\n",
    "\n",
    "ify=y: Maintains class distribution in train/test (for classification)\n",
    "= train_test_split(\n",
    "X_encoded, y, test_size=test_size, random_state=42, stratify=y\n",
    ")\n",
    " \n",
    "return X_train, X_test, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Decision Framework - When to Use Each Preprocessing Method | Ø§Ù„Ø®Ø·ÙˆØ© 8: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¹Ø§Ù„Ø¬Ø©\n",
    "\n",
    "**BEFORE**: You've learned different preprocessing methods, but when should you use each one?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose the right preprocessing method for any situation!\n",
    "\n",
    "**Why this matters**: Using the wrong preprocessing method can:\n",
    "- **Break your models** â†’ Wrong scaling causes algorithm failures\n",
    "- **Reduce performance** â†’ Inappropriate encoding loses information\n",
    "- **Waste time** â†’ Trying all methods without guidance\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Feature Scaling | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
    "\n",
    "**Key Question**: Should I use **STANDARDIZATION** or **NORMALIZATION**?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Do you have numeric features?\n",
    "â”œâ”€ NO â†’ No scaling needed (categorical features need encoding, not scaling)\n",
    "â”‚\n",
    "â””â”€ YES â†’ Are features on different scales?\n",
    "    â”œâ”€ NO â†’ No scaling needed (already on similar scales)\n",
    "    â”‚\n",
    "    â””â”€ YES â†’ Is data normally distributed?\n",
    "        â”œâ”€ YES â†’ Use STANDARDIZATION (StandardScaler)\n",
    "        â”‚   â””â”€ Why? Preserves distribution, handles outliers well\n",
    "        â”‚\n",
    "        â””â”€ NO â†’ Do you need bounded values [0, 1]?\n",
    "            â”œâ”€ YES â†’ Use NORMALIZATION (MinMaxScaler)\n",
    "            â”‚   â””â”€ Why? Neural networks, algorithms requiring [0,1] range\n",
    "            â”‚\n",
    "            â””â”€ NO â†’ Use STANDARDIZATION (StandardScaler)\n",
    "                â””â”€ Why? More robust to outliers, preserves relationships\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Standardization** | Normal distribution, outliers present, linear models | â€¢ Preserves distribution<br>â€¢ Robust to outliers<br>â€¢ Mean=0, Std=1 | â€¢ Can produce values outside [0,1]<br>â€¢ Assumes normal distribution | Age (20-60), Salary (30k-100k) |\n",
    "| **Normalization** | Non-normal distribution, neural networks, need [0,1] | â€¢ Bounded [0,1]<br>â€¢ Works with any distribution<br>â€¢ Good for neural networks | â€¢ Sensitive to outliers<br>â€¢ May compress data too much | Image pixels (0-255), Ratings (1-5) |\n",
    "| **No Scaling** | Features already on similar scales, tree-based models | â€¢ No transformation needed<br>â€¢ Preserves original values | â€¢ Only works if scales are similar | All features 0-1, or all 100-200 |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Categorical Encoding | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©\n",
    "\n",
    "**Key Question**: Should I use **LABEL ENCODING** or **ONE-HOT ENCODING**?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Do you have categorical features?\n",
    "â”œâ”€ NO â†’ No encoding needed (all features are numeric)\n",
    "â”‚\n",
    "â””â”€ YES â†’ Is there an inherent order (ordinal)?\n",
    "    â”œâ”€ YES â†’ Use LABEL ENCODING\n",
    "    â”‚   â””â”€ Why? Preserves order (e.g., Low < Medium < High)\n",
    "    â”‚\n",
    "    â””â”€ NO â†’ How many unique categories?\n",
    "        â”œâ”€ < 10 â†’ Use ONE-HOT ENCODING\n",
    "        â”‚   â””â”€ Why? Creates separate columns, no false order\n",
    "        â”‚\n",
    "        â””â”€ â‰¥ 10 â†’ Consider alternatives:\n",
    "            â”œâ”€ Use ONE-HOT if important features\n",
    "            â”‚   â””â”€ Why? Each category matters\n",
    "            â”‚\n",
    "            â””â”€ Use TARGET ENCODING or frequency encoding\n",
    "                â””â”€ Why? Avoids creating too many columns\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Label Encoding** | Ordinal categories (inherent order) | â€¢ Preserves order<br>â€¢ Single column<br>â€¢ Simple | â€¢ Creates false order for nominal<br>â€¢ Algorithms may misinterpret | Education: Bachelor=0, Master=1, PhD=2 |\n",
    "| **One-Hot Encoding** | Nominal categories (no order), < 10 categories | â€¢ No false order<br>â€¢ Each category separate<br>â€¢ Works with all algorithms | â€¢ Creates many columns<br>â€¢ Can cause curse of dimensionality | Department: IT, HR, Finance â†’ 3 columns |\n",
    "| **Target Encoding** | Many categories (>10), nominal | â€¢ Keeps single column<br>â€¢ Captures target relationship | â€¢ Can overfit<br>â€¢ More complex | City with 50+ values |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Train-Test Split | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**Key Question**: What split ratio should I use?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "How much data do you have?\n",
    "â”œâ”€ < 1000 samples â†’ Use 70/30 or 80/20\n",
    "â”‚   â””â”€ Why? Need enough test data for reliable evaluation\n",
    "â”‚\n",
    "â”œâ”€ 1000-10,000 samples â†’ Use 80/20\n",
    "â”‚   â””â”€ Why? Standard split, good balance\n",
    "â”‚\n",
    "â””â”€ > 10,000 samples â†’ Use 90/10 or 95/5\n",
    "    â””â”€ Why? Large datasets, can use less for testing\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Split Ratio | When to Use | Pros | Cons | Example |\n",
    "|-------------|-------------|------|------|---------|\n",
    "| **70/30** | Small datasets (< 1000) | â€¢ More test data<br>â€¢ Reliable evaluation | â€¢ Less training data | Dataset with 500 samples |\n",
    "| **80/20** | Medium datasets (1000-10k) | â€¢ Standard split<br>â€¢ Good balance | â€¢ May need adjustment | Dataset with 5000 samples |\n",
    "| **90/10** | Large datasets (> 10k) | â€¢ More training data<br>â€¢ Still enough test data | â€¢ Less test data | Dataset with 50,000 samples |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: House Price Prediction\n",
    "- **Features**: Size (1000-5000 sq ft), Age (0-50 years), Location (A, B, C)\n",
    "- **Scaling Decision**: STANDARDIZATION (different scales, normal-ish distribution)\n",
    "- **Encoding Decision**: ONE-HOT for Location (nominal, 3 categories)\n",
    "- **Split Decision**: 80/20 (medium dataset)\n",
    "- **Reasoning**: Size and age need scaling, location needs encoding, standard split\n",
    "\n",
    "#### Example 2: Customer Segmentation\n",
    "- **Features**: Income ($30k-$200k), Education (Bachelor, Master, PhD), Age (18-65)\n",
    "- **Scaling Decision**: STANDARDIZATION (different scales)\n",
    "- **Encoding Decision**: LABEL ENCODING for Education (ordinal: Bachelor < Master < PhD)\n",
    "- **Split Decision**: 80/20\n",
    "- **Reasoning**: Income needs scaling, education has order, standard split\n",
    "\n",
    "#### Example 3: Image Classification\n",
    "- **Features**: Pixel values (0-255), Category (10 classes)\n",
    "- **Scaling Decision**: NORMALIZATION (need [0,1] for neural networks)\n",
    "- **Encoding Decision**: ONE-HOT for Category (nominal, 10 classes)\n",
    "- **Split Decision**: 80/20\n",
    "- **Reasoning**: Pixels need [0,1] range, categories need one-hot, standard split\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Standardization for normal data** - Use when data is normally distributed\n",
    "2. **Normalization for bounded values** - Use when you need [0,1] range\n",
    "3. **Label encoding for ordinal** - Use when categories have inherent order\n",
    "4. **One-hot for nominal** - Use when categories have no order\n",
    "5. **80/20 is standard** - Use for most datasets (adjust for very small/large)\n",
    "6. **Always scale before encoding** - Scale numeric features first, then encode categorical\n",
    "7. **Test your choices** - Try different methods and compare results\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario**: You have a dataset with:\n",
    "- Income: $20,000 - $150,000 (normally distributed)\n",
    "- Education: High School, Bachelor, Master, PhD (ordinal)\n",
    "- City: 15 different cities (nominal)\n",
    "- 5,000 samples\n",
    "\n",
    "**Your task**: Decide preprocessing methods for each feature!\n",
    "\n",
    "**Answers**:\n",
    "1. **Income**: STANDARDIZATION (normal distribution, different scale)\n",
    "2. **Education**: LABEL ENCODING (ordinal: High School < Bachelor < Master < PhD)\n",
    "3. **City**: ONE-HOT ENCODING (nominal, 15 categories is acceptable)\n",
    "4. **Split**: 80/20 (medium dataset, standard split)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Example 4: Linear Regression** - Uses preprocessed data to build models\n",
    "- ğŸ““ **Example 5: Polynomial Regression** - Extends linear regression with preprocessed data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}