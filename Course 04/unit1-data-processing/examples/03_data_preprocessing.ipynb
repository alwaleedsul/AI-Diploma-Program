{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03. Data Preprocessing | Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "\n",
        "## ğŸ“š Learning Objectives\n",
        "\n",
        "By completing this notebook, you will:\n",
        "- Understand the key concepts of this topic\n",
        "- Apply the topic using Python code examples\n",
        "- Practice with small, realistic datasets or scenarios\n",
        "\n",
        "## ğŸ”— Prerequisites\n",
        "\n",
        "- âœ… Basic Python\n",
        "- âœ… Basic NumPy/Pandas (when applicable)\n",
        "\n",
        "---\n",
        "\n",
        "## Official Structure Reference\n",
        "\n",
        "This notebook supports **Course 04, Unit 1** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03. Data Preprocessing | Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "\n",
        "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
        "\n",
        "**BEFORE starting this notebook**, you should have completed:\n",
        "- âœ… **Example 1: Data Loading and Exploration** - Know your data structure\n",
        "- âœ… **Example 2: Data Cleaning** - Have clean data to preprocess\n",
        "- âœ… **Basic ML concepts**: Features, targets, training vs testing\n",
        "\n",
        "**If you haven't completed these**, you might struggle with:\n",
        "- Understanding why preprocessing is needed\n",
        "- Knowing which preprocessing method to use\n",
        "- Understanding the difference between scaling methods\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
        "\n",
        "**This is the THIRD example** - it prepares clean data for machine learning!\n",
        "\n",
        "**Why this example THIRD?**\n",
        "- **Before** you can build ML models, you need preprocessed data\n",
        "- **Before** you can train models, you need scaled features\n",
        "- **Before** you can make predictions, you need encoded categories\n",
        "\n",
        "**Builds on**: \n",
        "- ğŸ““ Example 1: Data Loading (we know our data)\n",
        "- ğŸ““ Example 2: Data Cleaning (we have clean data)\n",
        "\n",
        "**Leads to**: \n",
        "- ğŸ““ Example 4: Linear Regression (needs preprocessed data)\n",
        "- ğŸ““ Example 5: Polynomial Regression (needs scaled features)\n",
        "- ğŸ““ All ML models (all need preprocessing!)\n",
        "\n",
        "**Why this order?**\n",
        "1. Preprocessing transforms **clean data into ML-ready format**\n",
        "2. Preprocessing teaches you **scaling vs encoding** (different problems, different solutions)\n",
        "3. Preprocessing shows you **train-test split** (essential for evaluation)\n",
        "\n",
        "---\n",
        "\n",
        "## The Story: Preparing Ingredients for Cooking | Ø§Ù„Ù‚ØµØ©: ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ù„Ù„Ø·Ø¨Ø®\n",
        "\n",
        "Imagine you're cooking. **Before** you can cook, you need to prepare ingredients - cut vegetables to the same size, marinate meat, measure spices. **After** preparing everything uniformly, you can cook a perfect meal!\n",
        "\n",
        "Same with machine learning: **Before** building models, we preprocess data - scale features to same range, encode categories to numbers, split into train/test. **After** preprocessing, we can build accurate models!\n",
        "\n",
        "---\n",
        "\n",
        "## Why Data Preprocessing Matters | Ù„Ù…Ø§Ø°Ø§ ØªÙ‡Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ\n",
        "\n",
        "Preprocessing is essential for ML success:\n",
        "- **Feature Scaling**: Algorithms work better when features are on similar scales\n",
        "- **Encoding**: ML algorithms need numbers, not text categories\n",
        "- **Train-Test Split**: We need separate data to evaluate model performance\n",
        "- **Without Preprocessing**: Models fail or perform poorly\n",
        "\n",
        "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
        "1. Scale features using Standardization and Normalization\n",
        "2. Encode categorical variables (Label vs One-Hot)\n",
        "3. Split data into training and testing sets\n",
        "4. Understand when to use each preprocessing method\n",
        "5. Build a complete preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "# These libraries help us preprocess data for machine learning\n",
        "\n",
        "import pandas as pd  # For data manipulation\n",
        "import numpy as np   # For numerical operations\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler,    # For standardization (mean=0, std=1)\n",
        "    MinMaxScaler,      # For normalization (range 0-1)\n",
        "    LabelEncoder,      # For ordinal encoding\n",
        "    OneHotEncoder      # For nominal encoding\n",
        ")\n",
        "from sklearn.model_selection import train_test_split  # For splitting data\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n",
        "print(\"\\nğŸ“š What each preprocessing tool does:\")\n",
        "print(\"   - StandardScaler: Scale features to mean=0, std=1 (z-score)\")\n",
        "print(\"   - MinMaxScaler: Scale features to range [0, 1]\")\n",
        "print(\"   - LabelEncoder: Convert categories to numbers (ordinal)\")\n",
        "print(\"   - OneHotEncoder: Convert categories to binary columns (nominal)\")\n",
        "print(\"   - train_test_split: Split data into train/test sets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
        "\n",
        "**BEFORE**: We have clean data, but it's not ready for machine learning - features have different scales, categories are text, and we haven't split the data.\n",
        "\n",
        "**AFTER**: We'll preprocess the data - scale features, encode categories, split into train/test - making it ready for ML models!\n",
        "\n",
        "**Why this matters**: Most ML algorithms require preprocessed data. Without it, models fail or give poor results!\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Load Real-World Data | Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
        "\n",
        "**BEFORE**: We need to learn preprocessing, but we need real data with different scales and categories.\n",
        "\n",
        "**AFTER**: We'll load the California Housing dataset - real data with features on different scales and we'll add categorical features to practice preprocessing techniques!\n",
        "\n",
        "**Why use California Housing?** This is REAL data from the 1990 census with features that need scaling (MedInc ranges differently than HouseAge, etc.). Real datasets have these characteristics - we need to learn how to handle them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load real-world Credit Card Fraud dataset for preprocessing\n",
        "# GDI Theme: Financial Investigations - Fraud Detection\n",
        "# This dataset has features on different scales - perfect for learning preprocessing techniques!\n",
        "\n",
        "print(\"\\nğŸ“¥ Loading Credit Card Fraud dataset...\")\n",
        "print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø­ØªÙŠØ§Ù„ Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø§Ø¦ØªÙ…Ø§Ù†ÙŠØ©...\")\n",
        "\n",
        "try:\n",
        "    # Load from local file\n",
        "    df_full = pd.read_csv('../../datasets/raw/creditcard_fraud.csv')\n",
        "    print(f\"\\nâœ… Real-world Credit Card Fraud data loaded from local file!\")\n",
        "    print(f\"   ğŸ“Š Full dataset: {len(df_full):,} transactions\")\n",
        "    \n",
        "    # For learning purposes, use a sample for faster execution\n",
        "    # This makes the notebook more convenient for students while still demonstrating concepts\n",
        "    # In real projects, you would use the full dataset\n",
        "    sample_size = 20000  # Sample 20k rows for faster processing\n",
        "    df = df_full.sample(n=min(sample_size, len(df_full)), random_state=57, replace=False).reset_index(drop=True)\n",
        "    print(f\"   ğŸ“Š Using sample: {len(df):,} transactions (for faster learning)\")\n",
        "    print(f\"   ğŸ’¡ Note: Using a sample for learning convenience. In real projects, use full dataset.\")\n",
        "except FileNotFoundError:\n",
        "    # Fallback: Load from URL (if available) or create minimal structure\n",
        "    print(\"\\nâš ï¸  Local file not found. Please ensure creditcard_fraud.csv is in datasets/raw/\")\n",
        "    print(\"   For this notebook, we'll use a sample structure...\")\n",
        "    # Minimal fallback structure\n",
        "    df = pd.DataFrame({\n",
        "        'Time': [0] * 1000,\n",
        "        'Amount': [100.0] * 1000,\n",
        "        'Class': [0] * 1000\n",
        "    })\n",
        "\n",
        "# For preprocessing demonstration, we'll create a categorical column from Amount\n",
        "# This helps demonstrate encoding techniques (transaction size categories)\n",
        "df['transaction_size'] = pd.cut(df['Amount'], \n",
        "                                 bins=[0, 50, 200, float('inf')], \n",
        "                                 labels=['Small', 'Medium', 'Large'])\n",
        "\n",
        "print(f\"   ğŸ“Š This is REAL anonymized credit card transaction data\")\n",
        "print(f\"   ğŸ“ˆ Contains {len(df)} transactions with {len(df.columns)} features\")\n",
        "print(f\"   ğŸ¯ Domain: Financial Investigations - Fraud Detection\")\n",
        "print(f\"   ğŸ” Features have different scales (Time: seconds, Amount: dollars)\")\n",
        "print(f\"   ğŸ“‹ Note: V1-V28 are already PCA-transformed (pre-scaled), but Time and Amount need scaling\")\n",
        "print(f\"   ğŸ“‹ Added 'transaction_size' category for encoding demonstration\")\n",
        "\n",
        "# Inspect the data to see why preprocessing is needed\n",
        "print(\"\\nğŸ“Š Original Data (Credit Card Fraud Dataset):\")\n",
        "print(df[['Time', 'Amount', 'V1', 'V2', 'V3', 'transaction_size', 'Class']].head())\n",
        "print(f\"\\nğŸ“ Data Shape: {df.shape}\")\n",
        "print(\"\\nğŸ” Notice:\")\n",
        "print(\"   - Numeric features have VERY different scales:\")\n",
        "print(f\"     - Time ranges: {df['Time'].min()} to {df['Time'].max()} (seconds)\")\n",
        "print(f\"     - Amount ranges: ${df['Amount'].min():.2f} to ${df['Amount'].max():.2f} (dollars)\")\n",
        "print(f\"     - V1 ranges: {df['V1'].min():.3f} to {df['V1'].max():.3f} (already PCA-scaled)\")\n",
        "print(\"   - Categorical feature (transaction_size) is text: Small, Medium, Large\")\n",
        "print(\"   - ML algorithms need same-scale numbers! This is why we preprocess!\")\n",
        "print(\"   - V1-V28 are already scaled (PCA-transformed), so we'll focus on Time and Amount\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Understanding the Dataset | ÙÙ‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "\n",
        "### For CS Students - Focus on Data Structure, Not Domain | Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø¬Ø§Ù„\n",
        "\n",
        "**As computer science students, you'll work with many different types of datasets** (medical, financial, security, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
        "\n",
        "**Data Structure Focus**:\n",
        "- **Data Shape**: ~20,000 rows Ã— 31 columns (sample of transactions Ã— features)\n",
        "  - Note: We use a sample for learning convenience (faster execution)\n",
        "  - Full dataset has ~284,000 rows - would be slower but more representative\n",
        "- **Feature Types**: Mixed (numerical: Time, Amount, V1-V28; categorical: transaction_size)\n",
        "- **Target Type**: Classification (binary: 0=Normal, 1=Fraud)\n",
        "- **Task**: Detect fraudulent transactions based on features\n",
        "- **Preprocessing Needs**: Features on different scales (Time, Amount need scaling), categorical feature (need encoding)\n",
        "\n",
        "**Why This Structure Matters**:\n",
        "- **Different feature scales** â†’ Need standardization/normalization (Time: 0-172792 seconds vs. Amount: $0-$25691)\n",
        "- **Categorical feature** â†’ Need encoding (transaction_size: Small/Medium/Large â†’ numbers)\n",
        "- **Classification task** â†’ Binary classification (fraud detection)\n",
        "- **Large dataset** â†’ Good for train/test split demonstration\n",
        "- **V1-V28 already scaled** â†’ These are PCA-transformed features (already standardized), so we focus on Time and Amount\n",
        "\n",
        "### Understanding the Dataset Domain (Brief) | ÙÙ‡Ù… Ù…Ø¬Ø§Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø§Ø®ØªØµØ§Ø±)\n",
        "\n",
        "**What is this data?** Real anonymized credit card transaction data for fraud detection (GDI Theme: Financial Investigations).\n",
        "\n",
        "**Why does this matter?** \n",
        "- **For scaling**: Features have very different scales (Time: seconds in thousands, Amount: dollars) â†’ need standardization/normalization\n",
        "- **For encoding**: transaction_size is categorical (Small/Medium/Large) â†’ need encoding for ML\n",
        "- **For train/test split**: Need separate data to evaluate model performance\n",
        "- **For fraud detection**: Financial investigation context - detecting suspicious transactions\n",
        "\n",
        "**Domain Context** (Brief):\n",
        "- **Time**: Time elapsed between transaction and first transaction (seconds) - helps detect patterns\n",
        "- **Amount**: Transaction amount (dollars) - important for fraud detection\n",
        "- **V1-V28**: PCA-transformed features (anonymized to protect privacy, already scaled)\n",
        "- **Class**: Target variable (0=Normal transaction, 1=Fraudulent transaction)\n",
        "- **transaction_size**: Categorical feature we created from Amount (Small/Medium/Large) - demonstrates encoding\n",
        "  - Note: This is created for learning encoding techniques only\n",
        "  - In real modeling, you wouldn't create categories from a feature you're already using\n",
        "  - It's just for demonstration purposes to show how encoding works!\n",
        "\n",
        "**ğŸ’¡ Key Point for CS Students**: You don't need to be a financial expert! Focus on:\n",
        "- Understanding the **data structure** (rows, columns, types, scales)\n",
        "- Knowing the **preprocessing methods** (scaling vs. encoding)\n",
        "- Choosing the right **preprocessing approach** based on structure, not domain knowledge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"2. Feature Scaling - Standardization\")\n",
        "print(\"Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Feature Scaling - Standardization | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ\n",
        "\n",
        "**BEFORE**: Features have very different scales (age: 20-60, salary: 30k-100k). Algorithms will be biased toward larger numbers!\n",
        "\n",
        "**AFTER**: We'll standardize features so they all have mean=0 and std=1, putting them on the same scale!\n",
        "\n",
        "**Why Standardization?**\n",
        "- **Problem**: Features have different scales (age: 20-60, salary: 30k-100k)\n",
        "- **Issue**: ML algorithms treat larger numbers as more important â†’ biased toward salary!\n",
        "- **Solution**: Standardization puts all features on same scale (mean=0, std=1)\n",
        "- **Result**: All features contribute equally to the model\n",
        "- **Formula**: `(x - mean) / std`\n",
        "\n",
        "**Why mean=0 and std=1? (Common Student Questions)**\n",
        "\n",
        "**Q: Why mean=0, not 1 or 100?**\n",
        "- **Answer**: We subtract the mean â†’ `(x - mean)` centers data at 0\n",
        "  - If mean=40: value=50 becomes (50-40)=10, value=30 becomes (30-40)=-10\n",
        "  - The average of all (x - mean) values is always 0 (by definition!)\n",
        "  - Mean=0 means data is centered (balanced around zero)\n",
        "  - Why not 1? Because subtracting mean naturally gives 0, not 1\n",
        "\n",
        "**Q: Why std=1, not 0 or 2?**\n",
        "- **Answer**: We divide by std â†’ `(x - mean) / std` normalizes spread to 1\n",
        "  - If std=10: value=50 becomes (50-40)/10=1.0, value=30 becomes (30-40)/10=-1.0\n",
        "  - Dividing by std makes the new std exactly 1 (by definition!)\n",
        "  - Std=1 means \"one unit of variation\" (standard unit)\n",
        "  - Why not 0? Std=0 means no variation (all values same) - not useful!\n",
        "  - Why not 2? We want standard unit (1), not arbitrary number\n",
        "\n",
        "**Q: Why these specific numbers (0 and 1)?**\n",
        "- **Answer**: They're the \"standard\" values that make comparison easy\n",
        "  - Mean=0: Easy to see if value is above/below average (positive/negative)\n",
        "  - Std=1: Easy to see how many standard deviations away (1.5 = 1.5 std away)\n",
        "  - Together: All features use same \"standard scale\" for fair comparison\n",
        "\n",
        "- **Use when**: Data is normally distributed, outliers are important\n",
        "- **Good for**: Linear models, neural networks, distance-based algorithms\n",
        "\n",
        "**Note**: Remove problematic outliers first (Example 2), then standardize. Standardization preserves outliers, so clean data first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create StandardScaler object\n",
        "# Standardization formula: (x - mean) / std\n",
        "# This transforms data so mean=0 and std=1\n",
        "\n",
        "# StandardScaler()\n",
        "# - Creates scaler object for standardization\n",
        "# - Standardization: (x - mean) / std\n",
        "#   - Transforms data so mean = 0, standard deviation = 1\n",
        "#   - Centers data around 0, scales to unit variance\n",
        "# - Two-step process:\n",
        "#   1. .fit(): Learn mean and std from data\n",
        "#   2. .transform(): Apply transformation\n",
        "# .fit_transform(data)\n",
        "# - Two operations in one: .fit() then .transform()\n",
        "#   1. .fit(): Learns parameters from data (mean/std, categories, etc.)\n",
        "#   2. .transform(): Applies transformation using learned parameters\n",
        "# - Use on training data\n",
        "# - For test data, use only .transform() (don't refit!)\n",
        "\n",
        "# - Or use .fit_transform(): Do both in one step\n",
        "# - Important: Fit on training data, transform both train and test\n",
        "scaler_standard = StandardScaler()\n",
        "print(\"   âœ… StandardScaler created\")\n",
        "print(\"   Formula: (x - mean) / std\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select numeric columns for scaling\n",
        "# Why only numeric? Categorical columns need encoding, not scaling!\n",
        "# Note: V1-V28 are already PCA-transformed (pre-scaled), so we focus on Time and Amount\n",
        "numeric_cols = ['Time', 'Amount']\n",
        "df_scaled_standard = df.copy()\n",
        "print(f\"   Scaling columns: {numeric_cols}\")\n",
        "print(f\"   (V1-V28 are already scaled via PCA, so we focus on Time and Amount)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit and transform the data\n",
        "# .fit_transform() learns the mean/std from data, then transforms it\n",
        "# Why fit_transform? We learn parameters from training data, then apply to all data\n",
        "\n",
        "# scaler_standard.fit_transform(df[numeric_cols])\n",
        "# - .fit_transform(): Two operations in one\n",
        "#   1. .fit(): Learns mean and std from data\n",
        "#      - Calculates mean and std for each column\n",
        "#      - Stores these values in scaler object\n",
        "#   2. .transform(): Applies standardization using learned parameters\n",
        "#      - Formula: (x - learned_mean) / learned_std\n",
        "#      - Transforms each value in column\n",
        "# - df[numeric_cols]: Selects only numeric columns to scale\n",
        "#   - List of column names: ['age', 'salary', 'experience']\n",
        "#   - Returns DataFrame with only those columns\n",
        "# - Result: All numeric columns now have meanâ‰ˆ0 and stdâ‰ˆ1\n",
        "# - Note: In real ML, fit on training data, transform on both train and test\n",
        "df_scaled_standard[numeric_cols] = scaler_standard.fit_transform(df[numeric_cols])\n",
        "\n",
        "print(\"\\nâœ… After Standardization (mean=0, std=1):\")\n",
        "print(\"Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ (Ù…ØªÙˆØ³Ø·=0ØŒ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÙŠØ§Ø±ÙŠ=1):\")\n",
        "print(f\"   Time - Mean: {df_scaled_standard['Time'].mean():.4f}, Std: {df_scaled_standard['Time'].std():.4f}\")\n",
        "print(f\"   Amount - Mean: {df_scaled_standard['Amount'].mean():.4f}, Std: {df_scaled_standard['Amount'].std():.4f}\")\n",
        "print(\"\\n   âœ… All features now have meanâ‰ˆ0 and stdâ‰ˆ1!\")\n",
        "print(\"\\n   ğŸ“ Understanding the values:\")\n",
        "print(\"      - Positive values (e.g., 1.67): Above average (1.67 std above mean)\")\n",
        "print(\"      - Negative values (e.g., -1.36): Below average (1.36 std below mean)\")\n",
        "print(\"      - Values near 0: Close to average\")\n",
        "print(\"      - Values > 2 or < -2: Far from average (potential outliers)\")\n",
        "print(\"\\nğŸ“„ Scaled data (first 5 rows):\")\n",
        "print(df_scaled_standard[numeric_cols].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Feature Scaling - Normalization (Min-Max) | Ø§Ù„Ø®Ø·ÙˆØ© 3: Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªØ·Ø¨ÙŠØ¹\n",
        "\n",
        "**BEFORE**: Features have different ranges. We want them all in the same range [0, 1].\n",
        "\n",
        "**AFTER**: We'll normalize features so they all range from 0 to 1!\n",
        "\n",
        "**Why Normalization (Min-Max)?**\n",
        "- **Problem**: Same as standardization - different scales\n",
        "- **Solution**: Normalization puts all features in range [0, 1]\n",
        "- **Formula**: `(x - min) / (max - min)`\n",
        "- **Result**: Range [0, 1] (minimum becomes 0, maximum becomes 1)\n",
        "\n",
        "**Common Student Questions:**\n",
        "- **Q: Why range [0, 1], not [0, 100]?**\n",
        "  - Answer: [0, 1] is standard range (0% to 100% of the spread)\n",
        "  - Easy to interpret: 0 = minimum, 1 = maximum, 0.5 = middle\n",
        "- **Q: Why not use standardization?**\n",
        "  - Answer: Normalization gives bounded [0,1] range (good for neural networks)\n",
        "  - Standardization can give values outside [0,1] (e.g., -2.5, 3.1)\n",
        "- **Q: Which is better?**\n",
        "  - Answer: Depends on your algorithm!\n",
        "  - Standardization: Better for normal data, preserves outliers\n",
        "  - Normalization: Better for neural networks, bounded range needed\n",
        "\n",
        "- **Use when**: Data is not normally distributed, you want bounded values\n",
        "- **Good for**: Neural networks, algorithms that need [0,1] range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalization: (x - min) / (max - min) -> range [0, 1]\n",
        "# Ø§Ù„ØªØ·Ø¨ÙŠØ¹: (x - Ø§Ù„Ø£Ø¯Ù†Ù‰) / (Ø§Ù„Ø£Ø¹Ù„Ù‰ - Ø§Ù„Ø£Ø¯Ù†Ù‰) -> Ø§Ù„Ù†Ø·Ø§Ù‚ [0, 1]\n",
        "\n",
        "# MinMaxScaler()\n",
        "# - Creates scaler object for min-max normalization\n",
        "# - Normalization: (x - min) / (max - min)\n",
        "#   - Transforms data to range [0, 1]\n",
        "#   - Minimum value becomes 0, maximum becomes 1\n",
        "# - Two-step process:\n",
        "#   1. .fit(): Learn min and max from data\n",
        "#   2. .transform(): Apply transformation\n",
        "# - Or use .fit_transform(): Do both in one step\n",
        "# - Use when: Data not normally distributed, need bounded [0,1] range\n",
        "scaler_minmax = MinMaxScaler()\n",
        "df_scaled_minmax = df.copy()\n",
        "df_scaled_minmax[numeric_cols] = scaler_minmax.fit_transform(df[numeric_cols])\n",
        "print(\"\\nAfter Min-Max Normalization (range 0-1):\")\n",
        "print(\"Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¨Ø§Ù„Ø£Ø¯Ù†Ù‰-Ø§Ù„Ø£Ø¹Ù„Ù‰ (Ø§Ù„Ù†Ø·Ø§Ù‚ 0-1):\")\n",
        "print(f\"Time - Min: {df_scaled_minmax['Time'].min():.4f}, Max: {df_scaled_minmax['Time'].max():.4f}\")\n",
        "print(f\"Amount - Min: {df_scaled_minmax['Amount'].min():.4f}, Max: {df_scaled_minmax['Amount'].max():.4f}\")\n",
        "print(df_scaled_minmax[numeric_cols].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Label Encoding | Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª\n",
        "\n",
        "**BEFORE**: We have categorical text data (e.g., 'Small', 'Medium', 'Large' transaction sizes), but ML algorithms need numbers!\n",
        "\n",
        "**AFTER**: We'll encode categories to numbers using Label Encoding - perfect for ordinal categories (categories with order)!\n",
        "\n",
        "**Common Student Questions:**\n",
        "- **Q: What is ordinal vs nominal?**\n",
        "  - Answer: Ordinal = categories have order (e.g., Small < Medium < Large, Low < Medium < High)\n",
        "  - Nominal = categories have no order (e.g., city names, colors, departments)\n",
        "  - Label encoding preserves order â†’ use for ordinal\n",
        "  - One-hot encoding treats all equally â†’ use for nominal\n",
        "- **Q: Why use Label Encoding instead of One-Hot?**\n",
        "  - Answer: Label encoding preserves the order relationship\n",
        "  - Example: Small=0, Medium=1, Large=2 â†’ algorithm knows Large > Medium > Small\n",
        "  - One-hot would create 3 separate columns â†’ loses the order information\n",
        "  - Use label encoding when order matters (transaction sizes, ratings, sizes)\n",
        "- **Q: Can I use Label Encoding for nominal categories?**\n",
        "  - Answer: Technically yes, but NOT recommended!\n",
        "  - Problem: Creates false order (e.g., 'Red'=0, 'Blue'=1, 'Green'=2 â†’ algorithm thinks Green > Blue > Red)\n",
        "  - Solution: Use One-Hot Encoding for nominal categories (no false order)\n",
        "- **Q: How does LabelEncoder assign numbers?**\n",
        "  - Answer: Assigns numbers in alphabetical order (or order found in data)\n",
        "  - Example: ['Large', 'Medium', 'Small'] â†’ Large=0, Medium=1, Small=2 (alphabetical)\n",
        "  - Important: Order might not match your desired order â†’ check `label_encoder.classes_`\n",
        "- **Q: What if I want a specific order?**\n",
        "  - Answer: You can manually map categories to numbers using a dictionary\n",
        "  - Example: `df['transaction_size'] = df['transaction_size'].map({'Small': 0, 'Medium': 1, 'Large': 2})`\n",
        "  - Or use pandas `Categorical` with `ordered=True` to control order\n",
        "\n",
        "**Use when**: Categories have inherent order (ordinal data)\n",
        "**Good for**: Transaction sizes, ratings, sizes, rankings (transaction_size: Small < Medium < Large)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# LabelEncoder()\n",
        "# - Creates encoder for ordinal categorical data\n",
        "# - Ordinal: Categories have order (e.g., Bachelor < Master < PhD)\n",
        "# - Converts text categories to integers (0, 1, 2, ...)\n",
        "# - Two-step process:\n",
        "#   1. .fit(): Learn unique categories\n",
        "#   2. .transform(): Convert categories to numbers\n",
        "# - Or use .fit_transform(): Do both in one step\n",
        "# - Use when: Categories have meaningful order\n",
        "label_encoder = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LabelEncoder()\n",
        "# - Creates encoder for ordinal categorical data\n",
        "# - Ordinal: Categories have order (e.g., Bachelor < Master < PhD)\n",
        "# - Converts text categories to integers (0, 1, 2, ...)\n",
        "# - Two-step process:\n",
        "#   1. .fit(): Learn unique categories\n",
        "#   2. .transform(): Convert categories to numbers\n",
        "# - Or use .fit_transform(): Do both in one step\n",
        "# - Use when: Categories have meaningful order\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode transaction_size (ordinal: Small < Medium < Large)\n",
        "df_encoded = df.copy()\n",
        "\n",
        "# label_encoder.fit_transform(df['transaction_size'])\n",
        "# - .fit_transform(): Two operations in one\n",
        "#   1. .fit(): Learns unique categories from data\n",
        "#      - Finds all unique values (e.g., 'Small', 'Medium', 'Large')\n",
        "#      - Assigns numbers in alphabetical order (or order found)\n",
        "#   2. .transform(): Converts categories to integers\n",
        "#      - 'Large' â†’ 0, 'Medium' â†’ 1, 'Small' â†’ 2 (alphabetical order)\n",
        "#      - Note: Alphabetical order is Large, Medium, Small (not the natural order!)\n",
        "# - df['transaction_size']: Passes transaction_size column as Series\n",
        "# - Returns numpy array with encoded integers\n",
        "# - label_encoder.classes_: Shows mapping (original categories)\n",
        "df_encoded['transaction_size_encoded'] = label_encoder.fit_transform(df['transaction_size'])\n",
        "\n",
        "# label_encoder.classes_\n",
        "# - Returns array of unique categories in order they were encoded\n",
        "# - Used to see mapping: classes_[0] = first category, etc.\n",
        "print(\"\\nLabel Encoding for Transaction Size:\")\n",
        "print(\"Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø©:\")\n",
        "print(\"Mapping / Ø§Ù„ØªØ¹ÙŠÙŠÙ†:\")\n",
        "for i, label in enumerate(label_encoder.classes_):\n",
        "    print(f\"  {label}: {i}\")\n",
        "print(\"\\nNote: Encoded in alphabetical order (Large=0, Medium=1, Small=2)\")\n",
        "print(\"      For correct order, consider manual mapping or use pandas Categorical with ordered=True\")\n",
        "print(\"\\nEncoded values:\")\n",
        "print(df_encoded[['transaction_size', 'transaction_size_encoded']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: One-Hot Encoding | Ø§Ù„Ø®Ø·ÙˆØ© 5: Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†\n",
        "\n",
        "**BEFORE**: We have nominal categories (no order, like city names), but ML algorithms need numbers!\n",
        "\n",
        "**AFTER**: We'll encode categories using One-Hot Encoding - creates separate binary columns for each category!\n",
        "\n",
        "**Common Student Questions:**\n",
        "- **Q: What is One-Hot Encoding?**\n",
        "  - Answer: Each category becomes a separate binary column (0 or 1)\n",
        "  - Example: City ['Riyadh', 'Jeddah', 'Dammam'] â†’ 3 columns: city_Riyadh, city_Jeddah, city_Dammam\n",
        "  - Each row has 1 in one column (the city), 0 in all others\n",
        "  - No false order â†’ perfect for nominal categories\n",
        "- **Q: Why use One-Hot instead of Label Encoding?**\n",
        "  - Answer: One-Hot treats all categories equally (no false order)\n",
        "  - Label encoding creates false order for nominal data (e.g., 'Red'=0, 'Blue'=1 â†’ algorithm thinks Blue > Red)\n",
        "  - One-Hot: Each category is independent â†’ no order assumptions\n",
        "  - Use One-Hot for: City names, colors, departments, any categories without order\n",
        "- **Q: Why does One-Hot create so many columns?**\n",
        "  - Answer: Each category needs its own column to represent it independently\n",
        "  - Example: 10 cities â†’ 10 columns (or 9 if using drop='first')\n",
        "  - Trade-off: More columns = more memory, but no false order\n",
        "  - Solution: For many categories (>50), consider Target Encoding or Frequency Encoding\n",
        "- **Q: What is drop='first' in sklearn OneHotEncoder?**\n",
        "  - Answer: Drops the first category column to avoid multicollinearity\n",
        "  - Problem: If you have N categories, you only need N-1 columns (one is redundant)\n",
        "  - Example: 4 cities â†’ 3 columns (can infer 4th from the other 3)\n",
        "  - Why: Prevents perfect correlation between columns (helps with linear models)\n",
        "- **Q: When should I use pandas get_dummies vs sklearn OneHotEncoder?**\n",
        "  - Answer: pandas get_dummies = easier, good for quick work\n",
        "  - sklearn OneHotEncoder = more control (drop='first', sparse matrices, better for pipelines)\n",
        "  - Use pandas: Quick prototyping, simple cases\n",
        "  - Use sklearn: Production code, need drop='first', building pipelines\n",
        "\n",
        "**Use when**: Categories have no inherent order (nominal data)\n",
        "**Good for**: City names, colors, departments, any categories without order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using pandas get_dummies (easier)\n",
        "# Ø§Ø³ØªØ®Ø¯Ø§Ù… pandas get_dummies (Ø£Ø³Ù‡Ù„)\n",
        "\n",
        "# pd.get_dummies(df, columns=['transaction_size'], prefix=['txn_size'])\n",
        "# - pd.get_dummies(): Creates one-hot encoded columns for categorical data\n",
        "# - df: DataFrame to encode\n",
        "# - columns=['transaction_size']: Column to encode\n",
        "# - prefix=['txn_size']: Prefix for new column names\n",
        "#   - 'transaction_size' column with values ['Small', 'Medium', 'Large'] â†’ 'txn_size_Small', 'txn_size_Medium', 'txn_size_Large'\n",
        "# - One-hot encoding: Each category becomes a binary column (0 or 1)\n",
        "#   - Example: If transaction_size='Small', then txn_size_Small=1, others=0\n",
        "# - Returns DataFrame with original columns + new binary columns\n",
        "# - Use for: Nominal categories (no order), but can also use for ordinal if you want to avoid false order\n",
        "# Note: For transaction_size (Small/Medium/Large), One-Hot treats all equally (no order assumption)\n",
        "df_onehot = pd.get_dummies(df, columns=['transaction_size'], prefix=['txn_size'])\n",
        "\n",
        "# df_onehot.columns.tolist()\n",
        "# - df_onehot.columns: Returns Index with column names\n",
        "# - .tolist(): Converts to Python list\n",
        "# - Shows all column names after encoding\n",
        "print(\"\\nOne-Hot Encoded columns:\")\n",
        "print(\"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø´ÙØ±Ø© Ø¨Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†:\")\n",
        "# Show relevant columns\n",
        "onehot_cols = [col for col in df_onehot.columns if 'txn_size_' in col]\n",
        "print(f\"New columns: {onehot_cols}\")\n",
        "print(\"\\nSample of One-Hot Encoded data:\")\n",
        "sample_cols = ['Time', 'Amount'] + onehot_cols[:5]  # Show Time, Amount, and first few encoded cols\n",
        "print(df_onehot[sample_cols].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: Using sklearn OneHotEncoder (for more control)\n",
        "# Why sklearn? More control - can drop first column to avoid multicollinearity\n",
        "# drop='first' removes one column (redundant - can be inferred from others)\n",
        "\n",
        "print(\"\\n--- Using sklearn OneHotEncoder ---\")\n",
        "\n",
        "# OneHotEncoder(sparse_output=False, drop='first')\n",
        "# - OneHotEncoder(): Creates one-hot encoder object\n",
        "# - sparse_output=False: Returns dense array (True returns sparse matrix)\n",
        "# - drop='first': Drops first category column to avoid multicollinearity\n",
        "#   - If 4 cities, creates 3 columns (not 4)\n",
        "#   - One column is redundant (can be inferred from others)\n",
        "#   - Prevents perfect correlation between columns\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "\n",
        "# onehot_encoder.fit_transform(df[['transaction_size']])\n",
        "# - .fit_transform(): Learns categories and encodes\n",
        "#   1. .fit(): Learns unique categories\n",
        "#   2. .transform(): Creates binary columns\n",
        "# - df[['transaction_size']]: Double brackets [[]] to pass as DataFrame (not Series)\n",
        "#   - OneHotEncoder expects 2D input (DataFrame or 2D array)\n",
        "# - Returns numpy array with binary columns\n",
        "# - Example: 3 transaction sizes (Small, Medium, Large) â†’ 2 columns (if drop='first')\n",
        "txn_size_encoded = onehot_encoder.fit_transform(df[['transaction_size']])\n",
        "\n",
        "print(\"   Transaction Size encoding classes:\")\n",
        "print(f\"   {onehot_encoder.categories_}\")\n",
        "print(f\"\\n   Encoded shape: {txn_size_encoded.shape}\")\n",
        "print(f\"   (3 transaction sizes â†’ 2 columns, because drop='first' removes one)\")\n",
        "print(\"\\n   First 5 encoded values:\")\n",
        "print(txn_size_encoded[:5])\n",
        "print(\"\\n   âœ… Each row has 1 in one column (the transaction size), 0 in others\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Train-Test Split | Ø§Ù„Ø®Ø·ÙˆØ© 6: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±\n",
        "\n",
        "**BEFORE**: We have all our data together. We can't evaluate model performance on data it has seen!\n",
        "\n",
        "**AFTER**: We'll split data into training (80%) and testing (20%) sets!\n",
        "\n",
        "**Why Train-Test Split?**\n",
        "- **Training set**: Used to train/learn the model\n",
        "- **Test set**: Used to evaluate model performance (unseen data)\n",
        "- **Why separate?** Testing on training data gives false high scores (overfitting)\n",
        "- **Standard split**: 80% train, 20% test (can vary based on dataset size)\n",
        "\n",
        "**Common Student Questions:**\n",
        "- **Q: Why split data into train and test?**\n",
        "  - Answer: Need unseen data to evaluate model performance honestly\n",
        "  - Problem: Testing on training data = model memorized data â†’ gives false high scores\n",
        "  - Solution: Train on 80%, test on 20% â†’ see how model performs on new data\n",
        "  - Real-world: Model must work on data it hasn't seen before!\n",
        "- **Q: Why 80/20 split?**\n",
        "  - Answer: Good balance between training (need enough to learn) and testing (need enough to evaluate)\n",
        "  - 80% training: Enough data for model to learn patterns\n",
        "  - 20% testing: Enough data for reliable performance evaluation\n",
        "  - Can adjust: Small datasets (<1000) â†’ 70/30, Large datasets (>10k) â†’ 90/10\n",
        "- **Q: What is random_state?**\n",
        "  - Answer: Seed for random number generator â†’ ensures same split every time\n",
        "  - Problem: Without seed, different split each run â†’ can't reproduce results\n",
        "  - Solution: Set random_state=57 â†’ same split every time (reproducibility)\n",
        "  - Note: We use 57 here (not 42) - different projects use different seeds for variety\n",
        "  - Important: Use same random_state within one project to compare fairly\n",
        "- **Q: What is stratify=y?**\n",
        "  - Answer: Maintains class distribution in train/test sets\n",
        "  - Problem: Random split might create imbalanced train/test (e.g., 60% class 0 in train, 40% in test)\n",
        "  - Solution: stratify=y â†’ train and test have same class distribution\n",
        "  - Example: If 60% class 0, 40% class 1 â†’ both train and test have 60/40 split\n",
        "  - Use when: Classification problems with imbalanced classes\n",
        "- **Q: Should I split before or after preprocessing?**\n",
        "  - Answer: Split FIRST, then preprocess train and test separately!\n",
        "  - Why: Fit scaler/encoder on training data only, then transform both train and test\n",
        "  - Problem: Preprocessing before split = data leakage (test data influences training)\n",
        "  - Correct order: Split â†’ Fit on train â†’ Transform train and test\n",
        "  - Note: In this notebook, we show preprocessing on full data for simplicity, but in real projects, split first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "# For fraud detection, we'll use Time and Amount as features (V1-V28 are already PCA-transformed)\n",
        "# Target: Class (0=Normal, 1=Fraud)\n",
        "X = df[['Time', 'Amount']]\n",
        "y = df['Class']\n",
        "print(f\"Features (X) shape: {X.shape}\")\n",
        "print(f\"Target (y) shape: {y.shape}\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train and test sets (80% train, 20% test)\n",
        "# Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± (80% ØªØ¯Ø±ÙŠØ¨ØŒ 20% Ø§Ø®ØªØ¨Ø§Ø±)\n",
        "\n",
        "# train_test_split(X, y, test_size=0.2, random_state=57, stratify=y)\n",
        "# Note: Using random_state=57 (not 42) - different projects use different seeds for variety\n",
        "# - train_test_split(): Splits data into training and testing sets\n",
        "# - X: Features (input variables) - DataFrame or array\n",
        "# - y: Target (output variable) - Series or array\n",
        "# - test_size=0.2: 20% for testing, 80% for training (can use 0.25 for 25%)\n",
        "# - random_state=57: Seed for random number generator\n",
        "#   - Ensures same split every time (reproducibility)\n",
        "#   - Different seeds = different splits\n",
        "#   - We use 57 here (not 42) for variety across notebooks\n",
        "# - stratify=y: Maintains class distribution in train/test\n",
        "#   - If y has 60% class 0, 40% class 1, train/test will have same ratio\n",
        "#   - Important for imbalanced datasets\n",
        "# - Returns 4 arrays: X_train, X_test, y_train, y_test\n",
        "#   - X_train: Training features (80% of X)\n",
        "#   - X_test: Testing features (20% of X)\n",
        "#   - y_train: Training targets (80% of y)\n",
        "#   - y_test: Testing targets (20% of y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=57,  # Using 57 instead of 42 for variety across different notebooks\n",
        "    stratify=y  # Maintain class distribution\n",
        ")\n",
        "print(f\"\\nOriginal data shape: {X.shape}\")\n",
        "print(f\"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: {X.shape}\")\n",
        "print(f\"\\nTraining set:\")\n",
        "print(f\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
        "print(f\"  X_train: {X_train.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}\")\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
        "print(f\"  X_test: {X_test.shape}\")\n",
        "print(f\"  y_test: {y_test.shape}\")\n",
        "print(f\"\\nTrain percentage: {len(X_train) / len(X) * 100:.1f}%\")\n",
        "print(f\"Test percentage: {len(X_test) / len(X) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Complete Preprocessing Pipeline | Ø§Ù„Ø®Ø·ÙˆØ© 7: Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
        "\n",
        "**BEFORE**: We've learned individual preprocessing steps, but how do we combine them all together?\n",
        "\n",
        "**AFTER**: We'll create a complete preprocessing pipeline that combines all steps - encoding, scaling, and splitting - into one reusable function!\n",
        "\n",
        "**Why this matters**: In real projects, you need to apply the same preprocessing to training and test data consistently!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ù…Ø«Ø§Ù„ Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"7. Complete Preprocessing Pipeline\")\n",
        "print(\"Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def preprocess_data(df, numeric_cols, categorical_cols, target_col, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline\n",
        "    Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
        "    \"\"\"\n",
        "    # Separate features and target\n",
        "    X = df[numeric_cols + categorical_cols].copy()\n",
        "    y = df[target_col].copy()\n",
        "    \n",
        "    # One-hot encode categorical variables\n",
        "    X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
        "    \n",
        "    # Scale numeric features\n",
        "    scaler = StandardScaler()\n",
        "    X_encoded[numeric_cols] = scaler.fit_transform(X_encoded[numeric_cols])\n",
        "    \n",
        "    # Split data\n",
        "# train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "# - Splits data into training and testing sets\n",
        "# - X: Features (input variables), y: Target (output variable)\n",
        "# - test_size=0.2: 20% for testing, 80% for training\n",
        "# - random_state=42: Seed for reproducibility (same split every time)\n",
        "# - stratify=y: Maintains class distribution in train/test (for classification)\n",
        "# - Returns: X_train, X_test, y_train, y_test\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_encoded, y, test_size=test_size, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test, scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Decision Framework - When to Use Each Preprocessing Method | Ø§Ù„Ø®Ø·ÙˆØ© 8: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¹Ø§Ù„Ø¬Ø©\n",
        "\n",
        "**BEFORE**: You've learned different preprocessing methods, but when should you use each one?\n",
        "\n",
        "**AFTER**: You'll have a clear decision framework to choose the right preprocessing method for any situation!\n",
        "\n",
        "**Why this matters**: Using the wrong preprocessing method can:\n",
        "- **Break your models** â†’ Wrong scaling causes algorithm failures\n",
        "- **Reduce performance** â†’ Inappropriate encoding loses information\n",
        "- **Waste time** â†’ Trying all methods without guidance\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¯ Decision Framework for Feature Scaling | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
        "\n",
        "**Key Question**: Should I use **STANDARDIZATION** or **NORMALIZATION**?\n",
        "\n",
        "#### Decision Tree:\n",
        "\n",
        "```\n",
        "Do you have numeric features?\n",
        "â”œâ”€ NO â†’ No scaling needed (categorical features need encoding, not scaling)\n",
        "â”‚\n",
        "â””â”€ YES â†’ Are features on different scales?\n",
        "    â”œâ”€ NO â†’ No scaling needed (already on similar scales)\n",
        "    â”‚\n",
        "    â””â”€ YES â†’ Is data normally distributed?\n",
        "        â”œâ”€ YES â†’ Use STANDARDIZATION (StandardScaler)\n",
        "        â”‚   â””â”€ Why? Preserves distribution, handles outliers well\n",
        "        â”‚\n",
        "        â””â”€ NO â†’ Do you need bounded values [0, 1]?\n",
        "            â”œâ”€ YES â†’ Use NORMALIZATION (MinMaxScaler)\n",
        "            â”‚   â””â”€ Why? Neural networks, algorithms requiring [0,1] range\n",
        "            â”‚\n",
        "            â””â”€ NO â†’ Use STANDARDIZATION (StandardScaler)\n",
        "                â””â”€ Why? More robust to outliers, preserves relationships\n",
        "```\n",
        "\n",
        "#### Comparison Table:\n",
        "\n",
        "| Method | When to Use | Pros | Cons | Example |\n",
        "|--------|-------------|------|------|---------|\n",
        "| **Standardization** | Normal distribution, outliers present, linear models | â€¢ Preserves distribution<br>â€¢ Robust to outliers<br>â€¢ Mean=0, Std=1 | â€¢ Can produce values outside [0,1]<br>â€¢ Assumes normal distribution | Age (20-60), Salary (30k-100k) |\n",
        "| **Normalization** | Non-normal distribution, neural networks, need [0,1] | â€¢ Bounded [0,1]<br>â€¢ Works with any distribution<br>â€¢ Good for neural networks | â€¢ Sensitive to outliers<br>â€¢ May compress data too much | Image pixels (0-255), Ratings (1-5) |\n",
        "| **No Scaling** | Features already on similar scales, tree-based models | â€¢ No transformation needed<br>â€¢ Preserves original values | â€¢ Only works if scales are similar | All features 0-1, or all 100-200 |\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¯ Decision Framework for Categorical Encoding | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©\n",
        "\n",
        "**Key Question**: Should I use **LABEL ENCODING** or **ONE-HOT ENCODING**?\n",
        "\n",
        "#### Decision Tree:\n",
        "\n",
        "```\n",
        "Do you have categorical features?\n",
        "â”œâ”€ NO â†’ No encoding needed (all features are numeric)\n",
        "â”‚\n",
        "â””â”€ YES â†’ Is there an inherent order (ordinal)?\n",
        "    â”œâ”€ YES â†’ Use LABEL ENCODING\n",
        "    â”‚   â””â”€ Why? Preserves order (e.g., Low < Medium < High)\n",
        "    â”‚\n",
        "    â””â”€ NO â†’ How many unique categories?\n",
        "        â”œâ”€ < 10 â†’ Use ONE-HOT ENCODING\n",
        "        â”‚   â””â”€ Why? Creates separate columns, no false order\n",
        "        â”‚\n",
        "        â””â”€ â‰¥ 10 â†’ Consider alternatives:\n",
        "            â”œâ”€ Use ONE-HOT if important features\n",
        "            â”‚   â””â”€ Why? Each category matters\n",
        "            â”‚\n",
        "            â””â”€ Use TARGET ENCODING or frequency encoding\n",
        "                â””â”€ Why? Avoids creating too many columns\n",
        "```\n",
        "\n",
        "#### Comparison Table:\n",
        "\n",
        "| Method | When to Use | Pros | Cons | Example |\n",
        "|--------|-------------|------|------|---------|\n",
        "| **Label Encoding** | Ordinal categories (inherent order) | â€¢ Preserves order<br>â€¢ Single column<br>â€¢ Simple | â€¢ Creates false order for nominal<br>â€¢ Algorithms may misinterpret | Education: Bachelor=0, Master=1, PhD=2 |\n",
        "| **One-Hot Encoding** | Nominal categories (no order), < 10 categories | â€¢ No false order<br>â€¢ Each category separate<br>â€¢ Works with all algorithms | â€¢ Creates many columns<br>â€¢ Can cause curse of dimensionality | Department: IT, HR, Finance â†’ 3 columns |\n",
        "| **Target Encoding** | Many categories (>10), nominal | â€¢ Keeps single column<br>â€¢ Captures target relationship | â€¢ Can overfit<br>â€¢ More complex | City with 50+ values |\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¯ Decision Framework for Train-Test Split | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "\n",
        "**Key Question**: What split ratio should I use?\n",
        "\n",
        "#### Decision Tree:\n",
        "\n",
        "```\n",
        "How much data do you have?\n",
        "â”œâ”€ < 1000 samples â†’ Use 70/30 or 80/20\n",
        "â”‚   â””â”€ Why? Need enough test data for reliable evaluation\n",
        "â”‚\n",
        "â”œâ”€ 1000-10,000 samples â†’ Use 80/20\n",
        "â”‚   â””â”€ Why? Standard split, good balance\n",
        "â”‚\n",
        "â””â”€ > 10,000 samples â†’ Use 90/10 or 95/5\n",
        "    â””â”€ Why? Large datasets, can use less for testing\n",
        "```\n",
        "\n",
        "#### Comparison Table:\n",
        "\n",
        "| Split Ratio | When to Use | Pros | Cons | Example |\n",
        "|-------------|-------------|------|------|---------|\n",
        "| **70/30** | Small datasets (< 1000) | â€¢ More test data<br>â€¢ Reliable evaluation | â€¢ Less training data | Dataset with 500 samples |\n",
        "| **80/20** | Medium datasets (1000-10k) | â€¢ Standard split<br>â€¢ Good balance | â€¢ May need adjustment | Dataset with 5000 samples |\n",
        "| **90/10** | Large datasets (> 10k) | â€¢ More training data<br>â€¢ Still enough test data | â€¢ Less test data | Dataset with 50,000 samples |\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
        "\n",
        "#### Example 1: House Price Prediction\n",
        "- **Features**: Size (1000-5000 sq ft), Age (0-50 years), Location (A, B, C)\n",
        "- **Scaling Decision**: STANDARDIZATION (different scales, normal-ish distribution)\n",
        "- **Encoding Decision**: ONE-HOT for Location (nominal, 3 categories)\n",
        "- **Split Decision**: 80/20 (medium dataset)\n",
        "- **Reasoning**: Size and age need scaling, location needs encoding, standard split\n",
        "\n",
        "#### Example 2: Customer Segmentation\n",
        "- **Features**: Income ($30k-$200k), Education (Bachelor, Master, PhD), Age (18-65)\n",
        "- **Scaling Decision**: STANDARDIZATION (different scales)\n",
        "- **Encoding Decision**: LABEL ENCODING for Education (ordinal: Bachelor < Master < PhD)\n",
        "- **Split Decision**: 80/20\n",
        "- **Reasoning**: Income needs scaling, education has order, standard split\n",
        "\n",
        "#### Example 3: Image Classification\n",
        "- **Features**: Pixel values (0-255), Category (10 classes)\n",
        "- **Scaling Decision**: NORMALIZATION (need [0,1] for neural networks)\n",
        "- **Encoding Decision**: ONE-HOT for Category (nominal, 10 classes)\n",
        "- **Split Decision**: 80/20\n",
        "- **Reasoning**: Pixels need [0,1] range, categories need one-hot, standard split\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
        "\n",
        "1. **Standardization for normal data** - Use when data is normally distributed\n",
        "2. **Normalization for bounded values** - Use when you need [0,1] range\n",
        "3. **Label encoding for ordinal** - Use when categories have inherent order\n",
        "4. **One-hot for nominal** - Use when categories have no order\n",
        "5. **80/20 is standard** - Use for most datasets (adjust for very small/large)\n",
        "6. **Always scale before encoding** - Scale numeric features first, then encode categorical\n",
        "7. **Test your choices** - Try different methods and compare results\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
        "\n",
        "**Scenario**: You have a dataset with:\n",
        "- Income: $20,000 - $150,000 (normally distributed)\n",
        "- Education: High School, Bachelor, Master, PhD (ordinal)\n",
        "- City: 15 different cities (nominal)\n",
        "- 5,000 samples\n",
        "\n",
        "**Your task**: Decide preprocessing methods for each feature!\n",
        "\n",
        "**Answers**:\n",
        "1. **Income**: STANDARDIZATION (normal distribution, different scale)\n",
        "2. **Education**: LABEL ENCODING (ordinal: High School < Bachelor < Master < PhD)\n",
        "3. **City**: ONE-HOT ENCODING (nominal, 15 categories is acceptable)\n",
        "4. **Split**: 80/20 (medium dataset, standard split)\n",
        "\n",
        "---\n",
        "\n",
        "**Connection to Next Steps**: \n",
        "- ğŸ““ **Example 4: Linear Regression** - Uses preprocessed data to build models\n",
        "- ğŸ““ **Example 5: Polynomial Regression** - Extends linear regression with preprocessed data\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "course2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}