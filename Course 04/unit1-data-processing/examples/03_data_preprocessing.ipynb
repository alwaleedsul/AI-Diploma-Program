{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Data Preprocessing | Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Data Loading and Exploration** - Know your data structure\n",
    "- âœ… **Example 2: Data Cleaning** - Have clean data to preprocess\n",
    "- âœ… **Basic ML concepts**: Features, targets, training vs testing\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why preprocessing is needed\n",
    "- Knowing which preprocessing method to use\n",
    "- Understanding the difference between scaling methods\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the THIRD example** - it prepares clean data for machine learning!\n",
    "\n",
    "**Why this example THIRD?**\n",
    "- **Before** you can build ML models, you need preprocessed data\n",
    "- **Before** you can train models, you need scaled features\n",
    "- **Before** you can make predictions, you need encoded categories\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: Data Loading (we know our data)\n",
    "- ğŸ““ Example 2: Data Cleaning (we have clean data)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 4: Linear Regression (needs preprocessed data)\n",
    "- ğŸ““ Example 5: Polynomial Regression (needs scaled features)\n",
    "- ğŸ““ All ML models (all need preprocessing!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Preprocessing transforms **clean data into ML-ready format**\n",
    "2. Preprocessing teaches you **scaling vs encoding** (different problems, different solutions)\n",
    "3. Preprocessing shows you **train-test split** (essential for evaluation)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Preparing Ingredients for Cooking | Ø§Ù„Ù‚ØµØ©: ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ù„Ù„Ø·Ø¨Ø®\n",
    "\n",
    "Imagine you're cooking. **Before** you can cook, you need to prepare ingredients - cut vegetables to the same size, marinate meat, measure spices. **After** preparing everything uniformly, you can cook a perfect meal!\n",
    "\n",
    "Same with machine learning: **Before** building models, we preprocess data - scale features to same range, encode categories to numbers, split into train/test. **After** preprocessing, we can build accurate models!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Preprocessing Matters | Ù„Ù…Ø§Ø°Ø§ ØªÙ‡Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ\n",
    "\n",
    "Preprocessing is essential for ML success:\n",
    "- **Feature Scaling**: Algorithms work better when features are on similar scales\n",
    "- **Encoding**: ML algorithms need numbers, not text categories\n",
    "- **Train-Test Split**: We need separate data to evaluate model performance\n",
    "- **Without Preprocessing**: Models fail or perform poorly\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Scale features using Standardization and Normalization\n",
    "2. Encode categorical variables (Label vs One-Hot)\n",
    "3. Split data into training and testing sets\n",
    "4. Understand when to use each preprocessing method\n",
    "5. Build a complete preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us preprocess data for machine learning\n",
    "\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np   # For numerical operations\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,    # For standardization (mean=0, std=1)\n",
    "    MinMaxScaler,      # For normalization (range 0-1)\n",
    "    LabelEncoder,      # For ordinal encoding\n",
    "    OneHotEncoder      # For nominal encoding\n",
    ")\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each preprocessing tool does:\")\n",
    "print(\"   - StandardScaler: Scale features to mean=0, std=1 (z-score)\")\n",
    "print(\"   - MinMaxScaler: Scale features to range [0, 1]\")\n",
    "print(\"   - LabelEncoder: Convert categories to numbers (ordinal)\")\n",
    "print(\"   - OneHotEncoder: Convert categories to binary columns (nominal)\")\n",
    "print(\"   - train_test_split: Split data into train/test sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We have clean data, but it's not ready for machine learning - features have different scales, categories are text, and we haven't split the data.\n",
    "\n",
    "**AFTER**: We'll preprocess the data - scale features, encode categories, split into train/test - making it ready for ML models!\n",
    "\n",
    "**Why this matters**: Most ML algorithms require preprocessed data. Without it, models fail or give poor results!\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Load Real-World Data | Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "**BEFORE**: We need to learn preprocessing, but we need real data with different scales and categories.\n",
    "\n",
    "**AFTER**: We'll load the California Housing dataset - real data with features on different scales and we'll add categorical features to practice preprocessing techniques!\n",
    "\n",
    "**Why use California Housing?** This is REAL data from the 1990 census with features that need scaling (MedInc ranges differently than HouseAge, etc.). Real datasets have these characteristics - we need to learn how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real-world California Housing dataset for preprocessing\n",
    "# This dataset has features on different scales and is perfect for learning preprocessing techniques!\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "print(\"\\nğŸ“¥ Loading California Housing dataset...\")\n",
    "housing_data = fetch_california_housing()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\n",
    "df['target'] = housing_data.target\n",
    "\n",
    "# For preprocessing demonstration, we'll add a categorical column (high/medium/low income areas)\n",
    "# This helps demonstrate encoding techniques\n",
    "df['income_level'] = pd.cut(df['MedInc'], bins=[0, 2, 4, 10], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(f\"\\nâœ… Real-world California Housing data loaded!\")\n",
    "print(f\"   ğŸ“Š This is REAL data from the 1990 California census\")\n",
    "print(f\"   ğŸ“ˆ Contains {len(df)} housing districts with {len(df.columns)} features\")\n",
    "print(f\"   ğŸ” Features have different scales (need standardization/normalization)\")\n",
    "print(f\"   ğŸ“‹ Added 'income_level' category for encoding demonstration\")\n",
    "\n",
    "# Data is already loaded above from California Housing\n",
    "# Let's inspect the data to see why preprocessing is needed\n",
    "\n",
    "print(\"\\nğŸ“Š Original Data (California Housing):\")\n",
    "print(df.head())\n",
    "print(f\"\\nğŸ“ Data Shape: {df.shape}\")\n",
    "print(\"\\nğŸ” Notice:\")\n",
    "print(\"   - Numeric features have VERY different scales:\")\n",
    "print(f\"     - MedInc ranges: {df['MedInc'].min():.2f} to {df['MedInc'].max():.2f}\")\n",
    "print(f\"     - HouseAge ranges: {df['HouseAge'].min():.1f} to {df['HouseAge'].max():.1f}\")\n",
    "print(f\"     - AveRooms ranges: {df['AveRooms'].min():.2f} to {df['AveRooms'].max():.2f}\")\n",
    "print(\"   - Categorical feature (income_level) is text: Low, Medium, High\")\n",
    "print(\"   - ML algorithms need same-scale numbers! This is why we preprocess!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Feature Scaling - Standardization\")\n",
    "print(\"Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Scaling - Standardization | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ\n",
    "\n",
    "**BEFORE**: Features have very different scales (age: 20-60, salary: 30k-100k). Algorithms will be biased toward larger numbers!\n",
    "\n",
    "**AFTER**: We'll standardize features so they all have mean=0 and std=1, putting them on the same scale!\n",
    "\n",
    "**Why Standardization?**\n",
    "- **Problem**: Features have different scales (age: 20-60, salary: 30k-100k)\n",
    "- **Issue**: ML algorithms treat larger numbers as more important â†’ biased toward salary!\n",
    "- **Solution**: Standardization puts all features on same scale (mean=0, std=1)\n",
    "- **Result**: All features contribute equally to the model\n",
    "- **Formula**: `(x - mean) / std`\n",
    "\n",
    "**Why mean=0 and std=1? (Common Student Questions)**\n",
    "\n",
    "**Q: Why mean=0, not 1 or 100?**\n",
    "- **Answer**: We subtract the mean â†’ `(x - mean)` centers data at 0\n",
    "  - If mean=40: value=50 becomes (50-40)=10, value=30 becomes (30-40)=-10\n",
    "  - The average of all (x - mean) values is always 0 (by definition!)\n",
    "  - Mean=0 means data is centered (balanced around zero)\n",
    "  - Why not 1? Because subtracting mean naturally gives 0, not 1\n",
    "\n",
    "**Q: Why std=1, not 0 or 2?**\n",
    "- **Answer**: We divide by std â†’ `(x - mean) / std` normalizes spread to 1\n",
    "  - If std=10: value=50 becomes (50-40)/10=1.0, value=30 becomes (30-40)/10=-1.0\n",
    "  - Dividing by std makes the new std exactly 1 (by definition!)\n",
    "  - Std=1 means \"one unit of variation\" (standard unit)\n",
    "  - Why not 0? Std=0 means no variation (all values same) - not useful!\n",
    "  - Why not 2? We want standard unit (1), not arbitrary number\n",
    "\n",
    "**Q: Why these specific numbers (0 and 1)?**\n",
    "- **Answer**: They're the \"standard\" values that make comparison easy\n",
    "  - Mean=0: Easy to see if value is above/below average (positive/negative)\n",
    "  - Std=1: Easy to see how many standard deviations away (1.5 = 1.5 std away)\n",
    "  - Together: All features use same \"standard scale\" for fair comparison\n",
    "\n",
    "- **Use when**: Data is normally distributed, outliers are important\n",
    "- **Good for**: Linear models, neural networks, distance-based algorithms\n",
    "\n",
    "**Note**: Remove problematic outliers first (Example 2), then standardize. Standardization preserves outliers, so clean data first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create StandardScaler object\n",
    "# Standardization formula: (x - mean) / std\n",
    "# This transforms data so mean=0 and std=1\n",
    "\n",
    "# StandardScaler()\n",
    "# - Creates scaler object for standardization\n",
    "# - Standardization: (x - mean) / std\n",
    "#   - Transforms data so mean = 0, standard deviation = 1\n",
    "#   - Centers data around 0, scales to unit variance\n",
    "# - Two-step process:\n",
    "#   1. .fit(): Learn mean and std from data\n",
    "#   2. .transform(): Apply transformation\n",
    "# .fit_transform(data)\n",
    "# - Two operations in one: .fit() then .transform()\n",
    "#   1. .fit(): Learns parameters from data (mean/std, categories, etc.)\n",
    "#   2. .transform(): Applies transformation using learned parameters\n",
    "# - Use on training data\n",
    "# - For test data, use only .transform() (don't refit!)\n",
    "\n",
    "# - Or use .fit_transform(): Do both in one step\n",
    "# - Important: Fit on training data, transform both train and test\n",
    "scaler_standard = StandardScaler()\n",
    "print(\"   âœ… StandardScaler created\")\n",
    "print(\"   Formula: (x - mean) / std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns for scaling\n",
    "# Why only numeric? Categorical columns need encoding, not scaling!\n",
    "numeric_cols = ['age', 'salary', 'experience']\n",
    "df_scaled_standard = df.copy()\n",
    "print(f\"   Scaling columns: {numeric_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the data\n",
    "# .fit_transform() learns the mean/std from data, then transforms it\n",
    "# Why fit_transform? We learn parameters from training data, then apply to all data\n",
    "\n",
    "# scaler_standard.fit_transform(df[numeric_cols])\n",
    "# - .fit_transform(): Two operations in one\n",
    "#   1. .fit(): Learns mean and std from data\n",
    "#      - Calculates mean and std for each column\n",
    "#      - Stores these values in scaler object\n",
    "#   2. .transform(): Applies standardization using learned parameters\n",
    "#      - Formula: (x - learned_mean) / learned_std\n",
    "#      - Transforms each value in column\n",
    "# - df[numeric_cols]: Selects only numeric columns to scale\n",
    "#   - List of column names: ['age', 'salary', 'experience']\n",
    "#   - Returns DataFrame with only those columns\n",
    "# - Result: All numeric columns now have meanâ‰ˆ0 and stdâ‰ˆ1\n",
    "# - Note: In real ML, fit on training data, transform on both train and test\n",
    "df_scaled_standard[numeric_cols] = scaler_standard.fit_transform(df[numeric_cols])\n",
    "\n",
    "print(\"\\nâœ… After Standardization (mean=0, std=1):\")\n",
    "print(\"Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ (Ù…ØªÙˆØ³Ø·=0ØŒ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÙŠØ§Ø±ÙŠ=1):\")\n",
    "print(f\"   Age - Mean: {df_scaled_standard['age'].mean():.4f}, Std: {df_scaled_standard['age'].std():.4f}\")\n",
    "print(f\"   Salary - Mean: {df_scaled_standard['salary'].mean():.4f}, Std: {df_scaled_standard['salary'].std():.4f}\")\n",
    "print(f\"   Experience - Mean: {df_scaled_standard['experience'].mean():.4f}, Std: {df_scaled_standard['experience'].std():.4f}\")\n",
    "print(\"\\n   âœ… All features now have meanâ‰ˆ0 and stdâ‰ˆ1!\")\n",
    "print(\"\\n   ğŸ“ Understanding the values:\")\n",
    "print(\"      - Positive values (e.g., 1.67): Above average (1.67 std above mean)\")\n",
    "print(\"      - Negative values (e.g., -1.36): Below average (1.36 std below mean)\")\n",
    "print(\"      - Values near 0: Close to average\")\n",
    "print(\"      - Values > 2 or < -2: Far from average (potential outliers)\")\n",
    "print(\"\\nğŸ“„ Scaled data (first 5 rows):\")\n",
    "print(df_scaled_standard[numeric_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Scaling - Normalization (Min-Max) | Ø§Ù„Ø®Ø·ÙˆØ© 3: Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªØ·Ø¨ÙŠØ¹\n",
    "\n",
    "**BEFORE**: Features have different ranges. We want them all in the same range [0, 1].\n",
    "\n",
    "**AFTER**: We'll normalize features so they all range from 0 to 1!\n",
    "\n",
    "**Why Normalization (Min-Max)?**\n",
    "- **Problem**: Same as standardization - different scales\n",
    "- **Solution**: Normalization puts all features in range [0, 1]\n",
    "- **Formula**: `(x - min) / (max - min)`\n",
    "- **Result**: Range [0, 1] (minimum becomes 0, maximum becomes 1)\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: Why range [0, 1], not [0, 100]?**\n",
    "  - Answer: [0, 1] is standard range (0% to 100% of the spread)\n",
    "  - Easy to interpret: 0 = minimum, 1 = maximum, 0.5 = middle\n",
    "- **Q: Why not use standardization?**\n",
    "  - Answer: Normalization gives bounded [0,1] range (good for neural networks)\n",
    "  - Standardization can give values outside [0,1] (e.g., -2.5, 3.1)\n",
    "- **Q: Which is better?**\n",
    "  - Answer: Depends on your algorithm!\n",
    "  - Standardization: Better for normal data, preserves outliers\n",
    "  - Normalization: Better for neural networks, bounded range needed\n",
    "\n",
    "- **Use when**: Data is not normally distributed, you want bounded values\n",
    "- **Good for**: Neural networks, algorithms that need [0,1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization: (x - min) / (max - min) -> range [0, 1]\n",
    "# Ø§Ù„ØªØ·Ø¨ÙŠØ¹: (x - Ø§Ù„Ø£Ø¯Ù†Ù‰) / (Ø§Ù„Ø£Ø¹Ù„Ù‰ - Ø§Ù„Ø£Ø¯Ù†Ù‰) -> Ø§Ù„Ù†Ø·Ø§Ù‚ [0, 1]\n",
    "\n",
    "# MinMaxScaler()\n",
    "# - Creates scaler object for min-max normalization\n",
    "# - Normalization: (x - min) / (max - min)\n",
    "#   - Transforms data to range [0, 1]\n",
    "#   - Minimum value becomes 0, maximum becomes 1\n",
    "# - Two-step process:\n",
    "#   1. .fit(): Learn min and max from data\n",
    "#   2. .transform(): Apply transformation\n",
    "# - Or use .fit_transform(): Do both in one step\n",
    "# - Use when: Data not normally distributed, need bounded [0,1] range\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_scaled_minmax = df.copy()\n",
    "df_scaled_minmax[numeric_cols] = scaler_minmax.fit_transform(df[numeric_cols])\n",
    "print(\"\\nAfter Min-Max Normalization (range 0-1):\")\n",
    "print(\"Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¨Ø§Ù„Ø£Ø¯Ù†Ù‰-Ø§Ù„Ø£Ø¹Ù„Ù‰ (Ø§Ù„Ù†Ø·Ø§Ù‚ 0-1):\")\n",
    "print(f\"Age - Min: {df_scaled_minmax['age'].min():.4f}, Max: {df_scaled_minmax['age'].max():.4f}\")\n",
    "print(f\"Salary - Min: {df_scaled_minmax['salary'].min():.4f}, Max: {df_scaled_minmax['salary'].max():.4f}\")\n",
    "print(df_scaled_minmax[numeric_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Label Encoding | Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª\n",
    "\n",
    "**BEFORE**: We have categorical text data (e.g., 'Bachelor', 'Master', 'PhD'), but ML algorithms need numbers!\n",
    "\n",
    "**AFTER**: We'll encode categories to numbers using Label Encoding - perfect for ordinal categories (categories with order)!\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: What is ordinal vs nominal?**\n",
    "  - Answer: Ordinal = categories have order (e.g., Low < Medium < High, Bachelor < Master < PhD)\n",
    "  - Nominal = categories have no order (e.g., city names, colors, departments)\n",
    "  - Label encoding preserves order â†’ use for ordinal\n",
    "  - One-hot encoding treats all equally â†’ use for nominal\n",
    "- **Q: Why use Label Encoding instead of One-Hot?**\n",
    "  - Answer: Label encoding preserves the order relationship\n",
    "  - Example: Bachelor=0, Master=1, PhD=2 â†’ algorithm knows PhD > Master > Bachelor\n",
    "  - One-hot would create 3 separate columns â†’ loses the order information\n",
    "  - Use label encoding when order matters (education levels, ratings, sizes)\n",
    "- **Q: Can I use Label Encoding for nominal categories?**\n",
    "  - Answer: Technically yes, but NOT recommended!\n",
    "  - Problem: Creates false order (e.g., 'Red'=0, 'Blue'=1, 'Green'=2 â†’ algorithm thinks Green > Blue > Red)\n",
    "  - Solution: Use One-Hot Encoding for nominal categories (no false order)\n",
    "- **Q: How does LabelEncoder assign numbers?**\n",
    "  - Answer: Assigns numbers in alphabetical order (or order found in data)\n",
    "  - Example: ['Bachelor', 'Master', 'PhD'] â†’ Bachelor=0, Master=1, PhD=2\n",
    "  - Important: Order might not match your desired order â†’ check `label_encoder.classes_`\n",
    "- **Q: What if I want a specific order?**\n",
    "  - Answer: You can manually map categories to numbers using a dictionary\n",
    "  - Example: `df['education'] = df['education'].map({'Bachelor': 0, 'Master': 1, 'PhD': 2})`\n",
    "  - Or use pandas `Categorical` with `ordered=True` to control order\n",
    "\n",
    "**Use when**: Categories have inherent order (ordinal data)\n",
    "**Good for**: Education levels, ratings, sizes, rankings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LabelEncoder()\n",
    "# - Creates encoder for ordinal categorical data\n",
    "# - Ordinal: Categories have order (e.g., Bachelor < Master < PhD)\n",
    "# - Converts text categories to integers (0, 1, 2, ...)\n",
    "# - Two-step process:\n",
    "#   1. .fit(): Learn unique categories\n",
    "#   2. .transform(): Convert categories to numbers\n",
    "# - Or use .fit_transform(): Do both in one step\n",
    "# - Use when: Categories have meaningful order\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder()\n",
    "# - Creates encoder for ordinal categorical data\n",
    "# - Ordinal: Categories have order (e.g., Bachelor < Master < PhD)\n",
    "# - Converts text categories to integers (0, 1, 2, ...)\n",
    "# - Two-step process:\n",
    "#   1. .fit(): Learn unique categories\n",
    "#   2. .transform(): Convert categories to numbers\n",
    "# - Or use .fit_transform(): Do both in one step\n",
    "# - Use when: Categories have meaningful order\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode education (if we assume ordinal: Bachelor < Master < PhD)\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# label_encoder.fit_transform(df['education'])\n",
    "# - .fit_transform(): Two operations in one\n",
    "#   1. .fit(): Learns unique categories from data\n",
    "#      - Finds all unique values (e.g., 'Bachelor', 'Master', 'PhD')\n",
    "#      - Assigns numbers in alphabetical order (or order found)\n",
    "#   2. .transform(): Converts categories to integers\n",
    "#      - 'Bachelor' â†’ 0, 'Master' â†’ 1, 'PhD' â†’ 2\n",
    "# - df['education']: Passes education column as Series\n",
    "# - Returns numpy array with encoded integers\n",
    "# - label_encoder.classes_: Shows mapping (original categories)\n",
    "df_encoded['education_encoded'] = label_encoder.fit_transform(df['education'])\n",
    "\n",
    "# label_encoder.classes_\n",
    "# - Returns array of unique categories in order they were encoded\n",
    "# - Used to see mapping: classes_[0] = first category, etc.\n",
    "print(\"\\nLabel Encoding for Education:\")\n",
    "print(\"Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ù„Ù„ØªØ¹Ù„ÙŠÙ…:\")\n",
    "print(\"Mapping / Ø§Ù„ØªØ¹ÙŠÙŠÙ†:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label}: {i}\")\n",
    "print(\"\\nEncoded values:\")\n",
    "print(df_encoded[['education', 'education_encoded']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: One-Hot Encoding | Ø§Ù„Ø®Ø·ÙˆØ© 5: Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†\n",
    "\n",
    "**BEFORE**: We have nominal categories (no order, like city names), but ML algorithms need numbers!\n",
    "\n",
    "**AFTER**: We'll encode categories using One-Hot Encoding - creates separate binary columns for each category!\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: What is One-Hot Encoding?**\n",
    "  - Answer: Each category becomes a separate binary column (0 or 1)\n",
    "  - Example: City ['Riyadh', 'Jeddah', 'Dammam'] â†’ 3 columns: city_Riyadh, city_Jeddah, city_Dammam\n",
    "  - Each row has 1 in one column (the city), 0 in all others\n",
    "  - No false order â†’ perfect for nominal categories\n",
    "- **Q: Why use One-Hot instead of Label Encoding?**\n",
    "  - Answer: One-Hot treats all categories equally (no false order)\n",
    "  - Label encoding creates false order for nominal data (e.g., 'Red'=0, 'Blue'=1 â†’ algorithm thinks Blue > Red)\n",
    "  - One-Hot: Each category is independent â†’ no order assumptions\n",
    "  - Use One-Hot for: City names, colors, departments, any categories without order\n",
    "- **Q: Why does One-Hot create so many columns?**\n",
    "  - Answer: Each category needs its own column to represent it independently\n",
    "  - Example: 10 cities â†’ 10 columns (or 9 if using drop='first')\n",
    "  - Trade-off: More columns = more memory, but no false order\n",
    "  - Solution: For many categories (>50), consider Target Encoding or Frequency Encoding\n",
    "- **Q: What is drop='first' in sklearn OneHotEncoder?**\n",
    "  - Answer: Drops the first category column to avoid multicollinearity\n",
    "  - Problem: If you have N categories, you only need N-1 columns (one is redundant)\n",
    "  - Example: 4 cities â†’ 3 columns (can infer 4th from the other 3)\n",
    "  - Why: Prevents perfect correlation between columns (helps with linear models)\n",
    "- **Q: When should I use pandas get_dummies vs sklearn OneHotEncoder?**\n",
    "  - Answer: pandas get_dummies = easier, good for quick work\n",
    "  - sklearn OneHotEncoder = more control (drop='first', sparse matrices, better for pipelines)\n",
    "  - Use pandas: Quick prototyping, simple cases\n",
    "  - Use sklearn: Production code, need drop='first', building pipelines\n",
    "\n",
    "**Use when**: Categories have no inherent order (nominal data)\n",
    "**Good for**: City names, colors, departments, any categories without order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas get_dummies (easier)\n",
    "# Ø§Ø³ØªØ®Ø¯Ø§Ù… pandas get_dummies (Ø£Ø³Ù‡Ù„)\n",
    "\n",
    "# pd.get_dummies(df, columns=['city', 'education'], prefix=['city', 'edu'])\n",
    "# - pd.get_dummies(): Creates one-hot encoded columns for categorical data\n",
    "# - df: DataFrame to encode\n",
    "# - columns=['city', 'education']: Columns to encode (list of column names)\n",
    "# - prefix=['city', 'edu']: Prefix for new column names\n",
    "#   - 'city' column with values ['Riyadh', 'Jeddah'] â†’ 'city_Riyadh', 'city_Jeddah'\n",
    "#   - 'education' column â†’ 'edu_Bachelor', 'edu_Master', 'edu_PhD'\n",
    "# - One-hot encoding: Each category becomes a binary column (0 or 1)\n",
    "#   - Example: If city='Riyadh', then city_Riyadh=1, others=0\n",
    "# - Returns DataFrame with original columns + new binary columns\n",
    "# - Use for: Nominal categories (no order, like city names)\n",
    "df_onehot = pd.get_dummies(df, columns=['city', 'education'], prefix=['city', 'edu'])\n",
    "\n",
    "# df_onehot.columns.tolist()\n",
    "# - df_onehot.columns: Returns Index with column names\n",
    "# - .tolist(): Converts to Python list\n",
    "# - Shows all column names after encoding\n",
    "print(\"\\nOne-Hot Encoded columns:\")\n",
    "print(\"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø´ÙØ±Ø© Ø¨Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†:\")\n",
    "print(df_onehot.columns.tolist())\n",
    "print(\"\\nSample of One-Hot Encoded data:\")\n",
    "print(df_onehot[['age', 'salary'] + [col for col in df_onehot.columns if 'city_' in col or 'edu_' in col]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using sklearn OneHotEncoder (for more control)\n",
    "# Why sklearn? More control - can drop first column to avoid multicollinearity\n",
    "# drop='first' removes one column (redundant - can be inferred from others)\n",
    "\n",
    "print(\"\\n--- Using sklearn OneHotEncoder ---\")\n",
    "\n",
    "# OneHotEncoder(sparse_output=False, drop='first')\n",
    "# - OneHotEncoder(): Creates one-hot encoder object\n",
    "# - sparse_output=False: Returns dense array (True returns sparse matrix)\n",
    "# - drop='first': Drops first category column to avoid multicollinearity\n",
    "#   - If 4 cities, creates 3 columns (not 4)\n",
    "#   - One column is redundant (can be inferred from others)\n",
    "#   - Prevents perfect correlation between columns\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "# onehot_encoder.fit_transform(df[['city']])\n",
    "# - .fit_transform(): Learns categories and encodes\n",
    "#   1. .fit(): Learns unique categories\n",
    "#   2. .transform(): Creates binary columns\n",
    "# - df[['city']]: Double brackets [[]] to pass as DataFrame (not Series)\n",
    "#   - OneHotEncoder expects 2D input (DataFrame or 2D array)\n",
    "# - Returns numpy array with binary columns\n",
    "# - Example: 4 cities â†’ 3 columns (if drop='first')\n",
    "city_encoded = onehot_encoder.fit_transform(df[['city']])\n",
    "\n",
    "print(\"   City encoding classes:\")\n",
    "print(f\"   {onehot_encoder.categories_}\")\n",
    "print(f\"\\n   Encoded shape: {city_encoded.shape}\")\n",
    "print(f\"   (4 cities â†’ 3 columns, because drop='first' removes one)\")\n",
    "print(\"\\n   First 5 encoded values:\")\n",
    "print(city_encoded[:5])\n",
    "print(\"\\n   âœ… Each row has 1 in one column (the city), 0 in others\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train-Test Split | Ø§Ù„Ø®Ø·ÙˆØ© 6: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±\n",
    "\n",
    "**BEFORE**: We have all our data together. We can't evaluate model performance on data it has seen!\n",
    "\n",
    "**AFTER**: We'll split data into training (80%) and testing (20%) sets!\n",
    "\n",
    "**Why Train-Test Split?**\n",
    "- **Training set**: Used to train/learn the model\n",
    "- **Test set**: Used to evaluate model performance (unseen data)\n",
    "- **Why separate?** Testing on training data gives false high scores (overfitting)\n",
    "- **Standard split**: 80% train, 20% test (can vary based on dataset size)\n",
    "\n",
    "**Common Student Questions:**\n",
    "- **Q: Why split data into train and test?**\n",
    "  - Answer: Need unseen data to evaluate model performance honestly\n",
    "  - Problem: Testing on training data = model memorized data â†’ gives false high scores\n",
    "  - Solution: Train on 80%, test on 20% â†’ see how model performs on new data\n",
    "  - Real-world: Model must work on data it hasn't seen before!\n",
    "- **Q: Why 80/20 split?**\n",
    "  - Answer: Good balance between training (need enough to learn) and testing (need enough to evaluate)\n",
    "  - 80% training: Enough data for model to learn patterns\n",
    "  - 20% testing: Enough data for reliable performance evaluation\n",
    "  - Can adjust: Small datasets (<1000) â†’ 70/30, Large datasets (>10k) â†’ 90/10\n",
    "- **Q: What is random_state?**\n",
    "  - Answer: Seed for random number generator â†’ ensures same split every time\n",
    "  - Problem: Without seed, different split each run â†’ can't reproduce results\n",
    "  - Solution: Set random_state=42 â†’ same split every time (reproducibility)\n",
    "  - Important: Use same random_state for all experiments to compare fairly\n",
    "- **Q: What is stratify=y?**\n",
    "  - Answer: Maintains class distribution in train/test sets\n",
    "  - Problem: Random split might create imbalanced train/test (e.g., 60% class 0 in train, 40% in test)\n",
    "  - Solution: stratify=y â†’ train and test have same class distribution\n",
    "  - Example: If 60% class 0, 40% class 1 â†’ both train and test have 60/40 split\n",
    "  - Use when: Classification problems with imbalanced classes\n",
    "- **Q: Should I split before or after preprocessing?**\n",
    "  - Answer: Split FIRST, then preprocess train and test separately!\n",
    "  - Why: Fit scaler/encoder on training data only, then transform both train and test\n",
    "  - Problem: Preprocessing before split = data leakage (test data influences training)\n",
    "  - Correct order: Split â†’ Fit on train â†’ Transform train and test\n",
    "  - Note: In this notebook, we show preprocessing on full data for simplicity, but in real projects, split first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[['age', 'salary', 'experience']]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets (80% train, 20% test)\n",
    "# Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± (80% ØªØ¯Ø±ÙŠØ¨ØŒ 20% Ø§Ø®ØªØ¨Ø§Ø±)\n",
    "\n",
    "# train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# - train_test_split(): Splits data into training and testing sets\n",
    "# - X: Features (input variables) - DataFrame or array\n",
    "# - y: Target (output variable) - Series or array\n",
    "# - test_size=0.2: 20% for testing, 80% for training (can use 0.25 for 25%)\n",
    "# - random_state=42: Seed for random number generator\n",
    "#   - Ensures same split every time (reproducibility)\n",
    "#   - Different seeds = different splits\n",
    "# - stratify=y: Maintains class distribution in train/test\n",
    "#   - If y has 60% class 0, 40% class 1, train/test will have same ratio\n",
    "#   - Important for imbalanced datasets\n",
    "# - Returns 4 arrays: X_train, X_test, y_train, y_test\n",
    "#   - X_train: Training features (80% of X)\n",
    "#   - X_test: Testing features (20% of X)\n",
    "#   - y_train: Training targets (80% of y)\n",
    "#   - y_test: Testing targets (20% of y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "print(f\"\\nOriginal data shape: {X.shape}\")\n",
    "print(f\"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: {X.shape}\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "print(f\"\\nTrain percentage: {len(X_train) / len(X) * 100:.1f}%\")\n",
    "print(f\"Test percentage: {len(X_test) / len(X) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Complete Preprocessing Pipeline | Ø§Ù„Ø®Ø·ÙˆØ© 7: Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
    "\n",
    "**BEFORE**: We've learned individual preprocessing steps, but how do we combine them all together?\n",
    "\n",
    "**AFTER**: We'll create a complete preprocessing pipeline that combines all steps - encoding, scaling, and splitting - into one reusable function!\n",
    "\n",
    "**Why this matters**: In real projects, you need to apply the same preprocessing to training and test data consistently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù…Ø«Ø§Ù„ Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"7. Complete Preprocessing Pipeline\")\n",
    "print(\"Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def preprocess_data(df, numeric_cols, categorical_cols, target_col, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline\n",
    "    Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df[numeric_cols + categorical_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_encoded[numeric_cols] = scaler.fit_transform(X_encoded[numeric_cols])\n",
    "    \n",
    "    # Split data\n",
    "# train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# - Splits data into training and testing sets\n",
    "# - X: Features (input variables), y: Target (output variable)\n",
    "# - test_size=0.2: 20% for testing, 80% for training\n",
    "# - random_state=42: Seed for reproducibility (same split every time)\n",
    "# - stratify=y: Maintains class distribution in train/test (for classification)\n",
    "# - Returns: X_train, X_test, y_train, y_test\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_encoded, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Decision Framework - When to Use Each Preprocessing Method | Ø§Ù„Ø®Ø·ÙˆØ© 8: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¹Ø§Ù„Ø¬Ø©\n",
    "\n",
    "**BEFORE**: You've learned different preprocessing methods, but when should you use each one?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose the right preprocessing method for any situation!\n",
    "\n",
    "**Why this matters**: Using the wrong preprocessing method can:\n",
    "- **Break your models** â†’ Wrong scaling causes algorithm failures\n",
    "- **Reduce performance** â†’ Inappropriate encoding loses information\n",
    "- **Waste time** â†’ Trying all methods without guidance\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Feature Scaling | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
    "\n",
    "**Key Question**: Should I use **STANDARDIZATION** or **NORMALIZATION**?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Do you have numeric features?\n",
    "â”œâ”€ NO â†’ No scaling needed (categorical features need encoding, not scaling)\n",
    "â”‚\n",
    "â””â”€ YES â†’ Are features on different scales?\n",
    "    â”œâ”€ NO â†’ No scaling needed (already on similar scales)\n",
    "    â”‚\n",
    "    â””â”€ YES â†’ Is data normally distributed?\n",
    "        â”œâ”€ YES â†’ Use STANDARDIZATION (StandardScaler)\n",
    "        â”‚   â””â”€ Why? Preserves distribution, handles outliers well\n",
    "        â”‚\n",
    "        â””â”€ NO â†’ Do you need bounded values [0, 1]?\n",
    "            â”œâ”€ YES â†’ Use NORMALIZATION (MinMaxScaler)\n",
    "            â”‚   â””â”€ Why? Neural networks, algorithms requiring [0,1] range\n",
    "            â”‚\n",
    "            â””â”€ NO â†’ Use STANDARDIZATION (StandardScaler)\n",
    "                â””â”€ Why? More robust to outliers, preserves relationships\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Standardization** | Normal distribution, outliers present, linear models | â€¢ Preserves distribution<br>â€¢ Robust to outliers<br>â€¢ Mean=0, Std=1 | â€¢ Can produce values outside [0,1]<br>â€¢ Assumes normal distribution | Age (20-60), Salary (30k-100k) |\n",
    "| **Normalization** | Non-normal distribution, neural networks, need [0,1] | â€¢ Bounded [0,1]<br>â€¢ Works with any distribution<br>â€¢ Good for neural networks | â€¢ Sensitive to outliers<br>â€¢ May compress data too much | Image pixels (0-255), Ratings (1-5) |\n",
    "| **No Scaling** | Features already on similar scales, tree-based models | â€¢ No transformation needed<br>â€¢ Preserves original values | â€¢ Only works if scales are similar | All features 0-1, or all 100-200 |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Categorical Encoding | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©\n",
    "\n",
    "**Key Question**: Should I use **LABEL ENCODING** or **ONE-HOT ENCODING**?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "Do you have categorical features?\n",
    "â”œâ”€ NO â†’ No encoding needed (all features are numeric)\n",
    "â”‚\n",
    "â””â”€ YES â†’ Is there an inherent order (ordinal)?\n",
    "    â”œâ”€ YES â†’ Use LABEL ENCODING\n",
    "    â”‚   â””â”€ Why? Preserves order (e.g., Low < Medium < High)\n",
    "    â”‚\n",
    "    â””â”€ NO â†’ How many unique categories?\n",
    "        â”œâ”€ < 10 â†’ Use ONE-HOT ENCODING\n",
    "        â”‚   â””â”€ Why? Creates separate columns, no false order\n",
    "        â”‚\n",
    "        â””â”€ â‰¥ 10 â†’ Consider alternatives:\n",
    "            â”œâ”€ Use ONE-HOT if important features\n",
    "            â”‚   â””â”€ Why? Each category matters\n",
    "            â”‚\n",
    "            â””â”€ Use TARGET ENCODING or frequency encoding\n",
    "                â””â”€ Why? Avoids creating too many columns\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Label Encoding** | Ordinal categories (inherent order) | â€¢ Preserves order<br>â€¢ Single column<br>â€¢ Simple | â€¢ Creates false order for nominal<br>â€¢ Algorithms may misinterpret | Education: Bachelor=0, Master=1, PhD=2 |\n",
    "| **One-Hot Encoding** | Nominal categories (no order), < 10 categories | â€¢ No false order<br>â€¢ Each category separate<br>â€¢ Works with all algorithms | â€¢ Creates many columns<br>â€¢ Can cause curse of dimensionality | Department: IT, HR, Finance â†’ 3 columns |\n",
    "| **Target Encoding** | Many categories (>10), nominal | â€¢ Keeps single column<br>â€¢ Captures target relationship | â€¢ Can overfit<br>â€¢ More complex | City with 50+ values |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework for Train-Test Split | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "**Key Question**: What split ratio should I use?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "How much data do you have?\n",
    "â”œâ”€ < 1000 samples â†’ Use 70/30 or 80/20\n",
    "â”‚   â””â”€ Why? Need enough test data for reliable evaluation\n",
    "â”‚\n",
    "â”œâ”€ 1000-10,000 samples â†’ Use 80/20\n",
    "â”‚   â””â”€ Why? Standard split, good balance\n",
    "â”‚\n",
    "â””â”€ > 10,000 samples â†’ Use 90/10 or 95/5\n",
    "    â””â”€ Why? Large datasets, can use less for testing\n",
    "```\n",
    "\n",
    "#### Comparison Table:\n",
    "\n",
    "| Split Ratio | When to Use | Pros | Cons | Example |\n",
    "|-------------|-------------|------|------|---------|\n",
    "| **70/30** | Small datasets (< 1000) | â€¢ More test data<br>â€¢ Reliable evaluation | â€¢ Less training data | Dataset with 500 samples |\n",
    "| **80/20** | Medium datasets (1000-10k) | â€¢ Standard split<br>â€¢ Good balance | â€¢ May need adjustment | Dataset with 5000 samples |\n",
    "| **90/10** | Large datasets (> 10k) | â€¢ More training data<br>â€¢ Still enough test data | â€¢ Less test data | Dataset with 50,000 samples |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: House Price Prediction\n",
    "- **Features**: Size (1000-5000 sq ft), Age (0-50 years), Location (A, B, C)\n",
    "- **Scaling Decision**: STANDARDIZATION (different scales, normal-ish distribution)\n",
    "- **Encoding Decision**: ONE-HOT for Location (nominal, 3 categories)\n",
    "- **Split Decision**: 80/20 (medium dataset)\n",
    "- **Reasoning**: Size and age need scaling, location needs encoding, standard split\n",
    "\n",
    "#### Example 2: Customer Segmentation\n",
    "- **Features**: Income ($30k-$200k), Education (Bachelor, Master, PhD), Age (18-65)\n",
    "- **Scaling Decision**: STANDARDIZATION (different scales)\n",
    "- **Encoding Decision**: LABEL ENCODING for Education (ordinal: Bachelor < Master < PhD)\n",
    "- **Split Decision**: 80/20\n",
    "- **Reasoning**: Income needs scaling, education has order, standard split\n",
    "\n",
    "#### Example 3: Image Classification\n",
    "- **Features**: Pixel values (0-255), Category (10 classes)\n",
    "- **Scaling Decision**: NORMALIZATION (need [0,1] for neural networks)\n",
    "- **Encoding Decision**: ONE-HOT for Category (nominal, 10 classes)\n",
    "- **Split Decision**: 80/20\n",
    "- **Reasoning**: Pixels need [0,1] range, categories need one-hot, standard split\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Standardization for normal data** - Use when data is normally distributed\n",
    "2. **Normalization for bounded values** - Use when you need [0,1] range\n",
    "3. **Label encoding for ordinal** - Use when categories have inherent order\n",
    "4. **One-hot for nominal** - Use when categories have no order\n",
    "5. **80/20 is standard** - Use for most datasets (adjust for very small/large)\n",
    "6. **Always scale before encoding** - Scale numeric features first, then encode categorical\n",
    "7. **Test your choices** - Try different methods and compare results\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario**: You have a dataset with:\n",
    "- Income: $20,000 - $150,000 (normally distributed)\n",
    "- Education: High School, Bachelor, Master, PhD (ordinal)\n",
    "- City: 15 different cities (nominal)\n",
    "- 5,000 samples\n",
    "\n",
    "**Your task**: Decide preprocessing methods for each feature!\n",
    "\n",
    "**Answers**:\n",
    "1. **Income**: STANDARDIZATION (normal distribution, different scale)\n",
    "2. **Education**: LABEL ENCODING (ordinal: High School < Bachelor < Master < PhD)\n",
    "3. **City**: ONE-HOT ENCODING (nominal, 15 categories is acceptable)\n",
    "4. **Split**: 80/20 (medium dataset, standard split)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Example 4: Linear Regression** - Uses preprocessed data to build models\n",
    "- ğŸ““ **Example 5: Polynomial Regression** - Extends linear regression with preprocessed data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
