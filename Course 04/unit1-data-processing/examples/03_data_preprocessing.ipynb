{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Data Preprocessing | Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Example 1: Data Loading and Exploration** - Know your data structure\n",
    "- âœ… **Example 2: Data Cleaning** - Have clean data to preprocess\n",
    "- âœ… **Basic ML concepts**: Features, targets, training vs testing\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding why preprocessing is needed\n",
    "- Knowing which preprocessing method to use\n",
    "- Understanding the difference between scaling methods\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the THIRD example** - it prepares clean data for machine learning!\n",
    "\n",
    "**Why this example THIRD?**\n",
    "- **Before** you can build ML models, you need preprocessed data\n",
    "- **Before** you can train models, you need scaled features\n",
    "- **Before** you can make predictions, you need encoded categories\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Example 1: Data Loading (we know our data)\n",
    "- ğŸ““ Example 2: Data Cleaning (we have clean data)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 4: Linear Regression (needs preprocessed data)\n",
    "- ğŸ““ Example 5: Polynomial Regression (needs scaled features)\n",
    "- ğŸ““ All ML models (all need preprocessing!)\n",
    "\n",
    "**Why this order?**\n",
    "1. Preprocessing transforms **clean data into ML-ready format**\n",
    "2. Preprocessing teaches you **scaling vs encoding** (different problems, different solutions)\n",
    "3. Preprocessing shows you **train-test split** (essential for evaluation)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Preparing Ingredients for Cooking | Ø§Ù„Ù‚ØµØ©: ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ù„Ù„Ø·Ø¨Ø®\n",
    "\n",
    "Imagine you're cooking. **Before** you can cook, you need to prepare ingredients - cut vegetables to the same size, marinate meat, measure spices. **After** preparing everything uniformly, you can cook a perfect meal!\n",
    "\n",
    "Same with machine learning: **Before** building models, we preprocess data - scale features to same range, encode categories to numbers, split into train/test. **After** preprocessing, we can build accurate models!\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Preprocessing Matters | Ù„Ù…Ø§Ø°Ø§ ØªÙ‡Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ\n",
    "\n",
    "Preprocessing is essential for ML success:\n",
    "- **Feature Scaling**: Algorithms work better when features are on similar scales\n",
    "- **Encoding**: ML algorithms need numbers, not text categories\n",
    "- **Train-Test Split**: We need separate data to evaluate model performance\n",
    "- **Without Preprocessing**: Models fail or perform poorly\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Scale features using Standardization and Normalization\n",
    "2. Encode categorical variables (Label vs One-Hot)\n",
    "3. Split data into training and testing sets\n",
    "4. Understand when to use each preprocessing method\n",
    "5. Build a complete preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "\n",
      "ğŸ“š What each preprocessing tool does:\n",
      "   - StandardScaler: Scale features to mean=0, std=1 (z-score)\n",
      "   - MinMaxScaler: Scale features to range [0, 1]\n",
      "   - LabelEncoder: Convert categories to numbers (ordinal)\n",
      "   - OneHotEncoder: Convert categories to binary columns (nominal)\n",
      "   - train_test_split: Split data into train/test sets\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "# These libraries help us preprocess data for machine learning\n",
    "\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np   # For numerical operations\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,    # For standardization (mean=0, std=1)\n",
    "    MinMaxScaler,      # For normalization (range 0-1)\n",
    "    LabelEncoder,      # For ordinal encoding\n",
    "    OneHotEncoder      # For nominal encoding\n",
    ")\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"\\nğŸ“š What each preprocessing tool does:\")\n",
    "print(\"   - StandardScaler: Scale features to mean=0, std=1 (z-score)\")\n",
    "print(\"   - MinMaxScaler: Scale features to range [0, 1]\")\n",
    "print(\"   - LabelEncoder: Convert categories to numbers (ordinal)\")\n",
    "print(\"   - OneHotEncoder: Convert categories to binary columns (nominal)\")\n",
    "print(\"   - train_test_split: Split data into train/test sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting the Scene | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ù‡Ø¯\n",
    "\n",
    "**BEFORE**: We have clean data, but it's not ready for machine learning - features have different scales, categories are text, and we haven't split the data.\n",
    "\n",
    "**AFTER**: We'll preprocess the data - scale features, encode categories, split into train/test - making it ready for ML models!\n",
    "\n",
    "**Why this matters**: Most ML algorithms require preprocessed data. Without it, models fail or give poor results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Creating sample data...\n",
      "Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ©...\n",
      "\n",
      "ğŸ“Š Original Data:\n",
      "   age  salary  experience    city education  target\n",
      "0   58   65222    9.899761  Jeddah  Bachelor       0\n",
      "1   48   93335   12.258333  Dammam    Master       0\n",
      "2   34   40965    8.328012  Riyadh    Master       0\n",
      "3   27   54538    7.944759  Jeddah    Master       0\n",
      "4   40   38110    3.627784  Dammam  Bachelor       1\n",
      "\n",
      "ğŸ“ Data Shape: (100, 6)\n",
      "\n",
      "ğŸ” Notice:\n",
      "   - Numeric features have VERY different scales (age: 20-60, salary: 30k-100k)\n",
      "   - Categorical features are text (city, education)\n",
      "   - ML algorithms need same-scale numbers!\n"
     ]
    }
   ],
   "source": [
    "# Create sample data with different scales and categories\n",
    "# This simulates real-world data that needs preprocessing\n",
    "\n",
    "print(\"\\n1. Creating sample data...\")\n",
    "print(\"Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ©...\")\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data = {\n",
    "    'age': np.random.randint(20, 60, 100),              # Range: 20-60\n",
    "    'salary': np.random.randint(30000, 100000, 100),    # Range: 30k-100k (different scale!)\n",
    "    'experience': np.random.uniform(0, 15, 100),        # Range: 0-15 (different scale!)\n",
    "    'city': np.random.choice(['Riyadh', 'Jeddah', 'Dammam', 'Khobar'], 100),  # Categorical (text)\n",
    "    'education': np.random.choice(['Bachelor', 'Master', 'PhD'], 100),        # Categorical (text)\n",
    "    'target': np.random.choice([0, 1], 100)  # Binary target for classification\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nğŸ“Š Original Data:\")\n",
    "print(df.head())\n",
    "print(f\"\\nğŸ“ Data Shape: {df.shape}\")\n",
    "print(\"\\nğŸ” Notice:\")\n",
    "print(\"   - Numeric features have VERY different scales (age: 20-60, salary: 30k-100k)\")\n",
    "print(\"   - Categorical features are text (city, education)\")\n",
    "print(\"   - ML algorithms need same-scale numbers!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample Data | Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ù…ÙˆØ°Ø¬ÙŠØ©\n",
    "\n",
    "**BEFORE**: We need to learn preprocessing, but we need sample data with different scales and categories.\n",
    "\n",
    "**AFTER**: We'll create data with numeric features (different scales) and categorical features (text) to practice preprocessing!\n",
    "\n",
    "**Why create this data?** Real datasets have these characteristics - we need to learn how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. Feature Scaling - Standardization\n",
      "Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Feature Scaling - Standardization\")\n",
    "print(\"Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Scaling - Standardization | Ø§Ù„Ø®Ø·ÙˆØ© 2: Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ\n",
    "\n",
    "**BEFORE**: Features have very different scales (age: 20-60, salary: 30k-100k). Algorithms will be biased toward larger numbers!\n",
    "\n",
    "**AFTER**: We'll standardize features so they all have mean=0 and std=1, putting them on the same scale!\n",
    "\n",
    "**Why Standardization?**\n",
    "- Formula: `(x - mean) / std`\n",
    "- Result: Mean = 0, Standard Deviation = 1\n",
    "- Use when: Data is normally distributed, outliers are important\n",
    "- Good for: Linear models, neural networks, distance-based algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… StandardScaler created\n",
      "   Formula: (x - mean) / std\n"
     ]
    }
   ],
   "source": [
    "# Create StandardScaler object\n",
    "# Standardization formula: (x - mean) / std\n",
    "# This transforms data so mean=0 and std=1\n",
    "scaler_standard = StandardScaler()\n",
    "print(\"   âœ… StandardScaler created\")\n",
    "print(\"   Formula: (x - mean) / std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Scaling columns: ['age', 'salary', 'experience']\n"
     ]
    }
   ],
   "source": [
    "# Select numeric columns for scaling\n",
    "# Why only numeric? Categorical columns need encoding, not scaling!\n",
    "numeric_cols = ['age', 'salary', 'experience']\n",
    "df_scaled_standard = df.copy()\n",
    "print(f\"   Scaling columns: {numeric_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… After Standardization (mean=0, std=1):\n",
      "Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ (Ù…ØªÙˆØ³Ø·=0ØŒ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÙŠØ§Ø±ÙŠ=1):\n",
      "   Age - Mean: 0.0000, Std: 1.0050\n",
      "   Salary - Mean: 0.0000, Std: 1.0050\n",
      "   Experience - Mean: 0.0000, Std: 1.0050\n",
      "\n",
      "   âœ… All features now have meanâ‰ˆ0 and stdâ‰ˆ1!\n",
      "\n",
      "ğŸ“„ Scaled data (first 5 rows):\n",
      "        age    salary  experience\n",
      "0  1.670713 -0.057025    0.366648\n",
      "1  0.801003  1.290892    0.918449\n",
      "2 -0.416591 -1.220060   -0.001071\n",
      "3 -1.025388 -0.569284   -0.090735\n",
      "4  0.105235 -1.356947   -1.100715\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the data\n",
    "# .fit_transform() learns the mean/std from data, then transforms it\n",
    "# Why fit_transform? We learn parameters from training data, then apply to all data\n",
    "\n",
    "df_scaled_standard[numeric_cols] = scaler_standard.fit_transform(df[numeric_cols])\n",
    "\n",
    "print(\"\\nâœ… After Standardization (mean=0, std=1):\")\n",
    "print(\"Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ (Ù…ØªÙˆØ³Ø·=0ØŒ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÙŠØ§Ø±ÙŠ=1):\")\n",
    "print(f\"   Age - Mean: {df_scaled_standard['age'].mean():.4f}, Std: {df_scaled_standard['age'].std():.4f}\")\n",
    "print(f\"   Salary - Mean: {df_scaled_standard['salary'].mean():.4f}, Std: {df_scaled_standard['salary'].std():.4f}\")\n",
    "print(f\"   Experience - Mean: {df_scaled_standard['experience'].mean():.4f}, Std: {df_scaled_standard['experience'].std():.4f}\")\n",
    "print(\"\\n   âœ… All features now have meanâ‰ˆ0 and stdâ‰ˆ1!\")\n",
    "print(\"\\nğŸ“„ Scaled data (first 5 rows):\")\n",
    "print(df_scaled_standard[numeric_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Scaling - Normalization (Min-Max) | Ø§Ù„Ø®Ø·ÙˆØ© 3: Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª - Ø§Ù„ØªØ·Ø¨ÙŠØ¹\n",
    "\n",
    "**BEFORE**: Features have different ranges. We want them all in the same range [0, 1].\n",
    "\n",
    "**AFTER**: We'll normalize features so they all range from 0 to 1!\n",
    "\n",
    "**Why Normalization (Min-Max)?**\n",
    "- Formula: `(x - min) / (max - min)`\n",
    "- Result: Range [0, 1]\n",
    "- Use when: Data is not normally distributed, you want bounded values\n",
    "- Good for: Neural networks, algorithms that need [0,1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Min-Max Normalization (range 0-1):\n",
      "Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¨Ø§Ù„Ø£Ø¯Ù†Ù‰-Ø§Ù„Ø£Ø¹Ù„Ù‰ (Ø§Ù„Ù†Ø·Ø§Ù‚ 0-1):\n",
      "Age - Min: 0.0000, Max: 1.0000\n",
      "Salary - Min: 0.0000, Max: 1.0000\n",
      "        age    salary  experience\n",
      "0  0.974359  0.507795    0.664901\n",
      "1  0.717949  0.915484    0.824535\n",
      "2  0.358974  0.156025    0.558521\n",
      "3  0.179487  0.352858    0.532582\n",
      "4  0.512821  0.114622    0.240399\n"
     ]
    }
   ],
   "source": [
    "# Normalization: (x - min) / (max - min) -> range [0, 1]\n",
    "# Ø§Ù„ØªØ·Ø¨ÙŠØ¹: (x - Ø§Ù„Ø£Ø¯Ù†Ù‰) / (Ø§Ù„Ø£Ø¹Ù„Ù‰ - Ø§Ù„Ø£Ø¯Ù†Ù‰) -> Ø§Ù„Ù†Ø·Ø§Ù‚ [0, 1]\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_scaled_minmax = df.copy()\n",
    "df_scaled_minmax[numeric_cols] = scaler_minmax.fit_transform(df[numeric_cols])\n",
    "print(\"\\nAfter Min-Max Normalization (range 0-1):\")\n",
    "print(\"Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø¨Ø§Ù„Ø£Ø¯Ù†Ù‰-Ø§Ù„Ø£Ø¹Ù„Ù‰ (Ø§Ù„Ù†Ø·Ø§Ù‚ 0-1):\")\n",
    "print(f\"Age - Min: {df_scaled_minmax['age'].min():.4f}, Max: {df_scaled_minmax['age'].max():.4f}\")\n",
    "print(f\"Salary - Min: {df_scaled_minmax['salary'].min():.4f}, Max: {df_scaled_minmax['salary'].max():.4f}\")\n",
    "print(df_scaled_minmax[numeric_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. Label Encoding\n",
      "Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 4. Label Encoding (for ordinal categories)\n",
    "# Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª (Ù„Ù„ÙØ¦Ø§Øª Ø§Ù„ØªØ±ØªÙŠØ¨ÙŠØ©)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Label Encoding\")\n",
    "print(\"Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. Label Encoding\n",
      "Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª\n",
      "============================================================\n",
      "   âœ… LabelEncoder created\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Label Encoding\")\n",
    "print(\"Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create LabelEncoder\n",
    "# This converts text categories to numbers\n",
    "label_encoder = LabelEncoder()\n",
    "print(\"   âœ… LabelEncoder created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Encoding for Education:\n",
      "Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ù„Ù„ØªØ¹Ù„ÙŠÙ…:\n",
      "Mapping / Ø§Ù„ØªØ¹ÙŠÙŠÙ†:\n",
      "  Bachelor: 0\n",
      "  Master: 1\n",
      "  PhD: 2\n",
      "\n",
      "Encoded values:\n",
      "  education  education_encoded\n",
      "0  Bachelor                  0\n",
      "1    Master                  1\n",
      "2    Master                  1\n",
      "3    Master                  1\n",
      "4  Bachelor                  0\n",
      "5    Master                  1\n",
      "6    Master                  1\n",
      "7  Bachelor                  0\n",
      "8       PhD                  2\n",
      "9  Bachelor                  0\n"
     ]
    }
   ],
   "source": [
    "# Encode education (if we assume ordinal: Bachelor < Master < PhD)\n",
    "df_encoded = df.copy()\n",
    "df_encoded['education_encoded'] = label_encoder.fit_transform(df['education'])\n",
    "print(\"\\nLabel Encoding for Education:\")\n",
    "print(\"Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ù„Ù„ØªØ¹Ù„ÙŠÙ…:\")\n",
    "print(\"Mapping / Ø§Ù„ØªØ¹ÙŠÙŠÙ†:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label}: {i}\")\n",
    "print(\"\\nEncoded values:\")\n",
    "print(df_encoded[['education', 'education_encoded']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. One-Hot Encoding\n",
      "Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 5. One-Hot Encoding (for nominal categories)\n",
    "# Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù† (Ù„Ù„ÙØ¦Ø§Øª Ø§Ù„Ø§Ø³Ù…ÙŠØ©)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. One-Hot Encoding\")\n",
    "print(\"Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. One-Hot Encoding\n",
      "Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. One-Hot Encoding\")\n",
    "print(\"Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-Hot Encoded columns:\n",
      "Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø´ÙØ±Ø© Ø¨Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†:\n",
      "['age', 'salary', 'experience', 'target', 'city_Dammam', 'city_Jeddah', 'city_Khobar', 'city_Riyadh', 'edu_Bachelor', 'edu_Master', 'edu_PhD']\n",
      "\n",
      "Sample of One-Hot Encoded data:\n",
      "   age  salary  city_Dammam  city_Jeddah  city_Khobar  city_Riyadh  \\\n",
      "0   58   65222        False         True        False        False   \n",
      "1   48   93335         True        False        False        False   \n",
      "2   34   40965        False        False        False         True   \n",
      "3   27   54538        False         True        False        False   \n",
      "4   40   38110         True        False        False        False   \n",
      "\n",
      "   edu_Bachelor  edu_Master  edu_PhD  \n",
      "0          True       False    False  \n",
      "1         False        True    False  \n",
      "2         False        True    False  \n",
      "3         False        True    False  \n",
      "4          True       False    False  \n"
     ]
    }
   ],
   "source": [
    "# Using pandas get_dummies (easier)\n",
    "# Ø§Ø³ØªØ®Ø¯Ø§Ù… pandas get_dummies (Ø£Ø³Ù‡Ù„)\n",
    "df_onehot = pd.get_dummies(df, columns=['city', 'education'], prefix=['city', 'edu'])\n",
    "print(\"\\nOne-Hot Encoded columns:\")\n",
    "print(\"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø´ÙØ±Ø© Ø¨Ø§Ù„ÙˆØ§Ø­Ø¯-Ø§Ù„Ø³Ø§Ø®Ù†:\")\n",
    "print(df_onehot.columns.tolist())\n",
    "print(\"\\nSample of One-Hot Encoded data:\")\n",
    "print(df_onehot[['age', 'salary'] + [col for col in df_onehot.columns if 'city_' in col or 'edu_' in col]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using sklearn OneHotEncoder ---\n",
      "City encoding classes:\n",
      "[array(['Dammam', 'Jeddah', 'Khobar', 'Riyadh'], dtype=object)]\n",
      "\n",
      "Encoded shape: (100, 3)\n",
      "First 5 encoded values:\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn OneHotEncoder (for more control)\n",
    "# Ø§Ø³ØªØ®Ø¯Ø§Ù… sklearn OneHotEncoder (Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ­ÙƒÙ…)\n",
    "print(\"\\n--- Using sklearn OneHotEncoder ---\")\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "city_encoded = onehot_encoder.fit_transform(df[['city']])\n",
    "print(\"City encoding classes:\")\n",
    "print(onehot_encoder.categories_)\n",
    "print(f\"\\nEncoded shape: {city_encoded.shape}\")\n",
    "print(\"First 5 encoded values:\")\n",
    "print(city_encoded[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using sklearn OneHotEncoder ---\n",
      "   City encoding classes:\n",
      "   [array(['Dammam', 'Jeddah', 'Khobar', 'Riyadh'], dtype=object)]\n",
      "\n",
      "   Encoded shape: (100, 3)\n",
      "   (4 cities â†’ 3 columns, because drop='first' removes one)\n",
      "\n",
      "   First 5 encoded values:\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "   âœ… Each row has 1 in one column (the city), 0 in others\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Using sklearn OneHotEncoder (for more control)\n",
    "# Why sklearn? More control - can drop first column to avoid multicollinearity\n",
    "# drop='first' removes one column (redundant - can be inferred from others)\n",
    "\n",
    "print(\"\\n--- Using sklearn OneHotEncoder ---\")\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "city_encoded = onehot_encoder.fit_transform(df[['city']])\n",
    "\n",
    "print(\"   City encoding classes:\")\n",
    "print(f\"   {onehot_encoder.categories_}\")\n",
    "print(f\"\\n   Encoded shape: {city_encoded.shape}\")\n",
    "print(f\"   (4 cities â†’ 3 columns, because drop='first' removes one)\")\n",
    "print(\"\\n   First 5 encoded values:\")\n",
    "print(city_encoded[:5])\n",
    "print(\"\\n   âœ… Each row has 1 in one column (the city), 0 in others\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train-Test Split | Ø§Ù„Ø®Ø·ÙˆØ© 6: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±\n",
    "\n",
    "**BEFORE**: We have all our data together. We can't evaluate model performance on data it has seen!\n",
    "\n",
    "**AFTER**: We'll split data into training (80%) and testing (20%) sets!\n",
    "\n",
    "**Why Train-Test Split?**\n",
    "- **Training set**: Used to train/learn the model\n",
    "- **Test set**: Used to evaluate model performance (unseen data)\n",
    "- **Why separate?** Testing on training data gives false high scores (overfitting)\n",
    "- **Standard split**: 80% train, 20% test (can vary based on dataset size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[['age', 'salary', 'experience']]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data shape: (100, 3)\n",
      "Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: (100, 3)\n",
      "\n",
      "Training set:\n",
      "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n",
      "  X_train: (80, 3)\n",
      "  y_train: (80,)\n",
      "\n",
      "Test set:\n",
      "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\n",
      "  X_test: (20, 3)\n",
      "  y_test: (20,)\n",
      "\n",
      "Train percentage: 80.0%\n",
      "Test percentage: 20.0%\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets (80% train, 20% test)\n",
    "# Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± (80% ØªØ¯Ø±ÙŠØ¨ØŒ 20% Ø§Ø®ØªØ¨Ø§Ø±)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "print(f\"\\nOriginal data shape: {X.shape}\")\n",
    "print(f\"Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: {X.shape}\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "print(f\"\\nTrain percentage: {len(X_train) / len(X) * 100:.1f}%\")\n",
    "print(f\"Test percentage: {len(X_test) / len(X) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Preprocessing Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. Complete Preprocessing Pipeline\n",
      "Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Ù…Ø«Ø§Ù„ Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"7. Complete Preprocessing Pipeline\")\n",
    "print(\"Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def preprocess_data(df, numeric_cols, categorical_cols, target_col, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline\n",
    "    Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df[numeric_cols + categorical_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_encoded[numeric_cols] = scaler.fit_transform(X_encoded[numeric_cols])\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_encoded, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
