{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Decision Trees and Random Forest | Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± ÙˆØ§Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\n",
    "\n",
    "## ğŸ“š Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- Understand the key concepts of this topic\n",
    "- Apply the topic using Python code examples\n",
    "- Practice with small, realistic datasets or scenarios\n",
    "\n",
    "## ğŸ”— Prerequisites\n",
    "\n",
    "- âœ… Basic Python\n",
    "- âœ… Basic NumPy/Pandas (when applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Official Structure Reference\n",
    "\n",
    "This notebook supports **Course 04, Unit 3** requirements from `DETAILED_UNIT_DESCRIPTIONS.md`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Decision Trees and Random Forest | Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± ÙˆØ§Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **Unit 3, Example 1: Logistic Regression** - Understanding classification basics\n",
    "- âœ… **Understanding of overfitting**: What happens when models are too complex\n",
    "- âœ… **Basic decision-making concepts**: If-then rules\n",
    "\n",
    "**If you haven't completed these**, you might struggle with:\n",
    "- Understanding how decision trees make predictions\n",
    "- Knowing when to use decision trees vs logistic regression\n",
    "- Understanding how Random Forest improves on single trees\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is Unit 3, Example 2** - it introduces tree-based classification models!\n",
    "\n",
    "**Why this example SECOND in Unit 3?**\n",
    "- **Before** you can use tree-based models, you need to understand basic classification\n",
    "- **Before** you can use Random Forest, you need to understand single decision trees\n",
    "- **Before** you can handle complex data, you need to see how trees handle non-linear patterns\n",
    "\n",
    "**Builds on**: \n",
    "- ğŸ““ Unit 3, Example 1: Logistic Regression (we know classification basics)\n",
    "- ğŸ““ Unit 1, Example 5: Polynomial Regression (we saw overfitting!)\n",
    "\n",
    "**Leads to**: \n",
    "- ğŸ““ Example 3: SVM (another advanced classifier)\n",
    "- ğŸ““ Unit 5, Example 2: Boosting (uses trees as base learners)\n",
    "- ğŸ““ All tree-based models (XGBoost, LightGBM, etc.)\n",
    "\n",
    "**Why this order?**\n",
    "1. Decision trees are **interpretable** (easy to understand)\n",
    "2. Decision trees handle **non-linear patterns** (better than logistic regression for complex data)\n",
    "3. Random Forest shows **ensemble methods** (combining multiple models improves performance)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Making Decisions Like a Tree | Ø§Ù„Ù‚ØµØ©: Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ù…Ø«Ù„ Ø§Ù„Ø´Ø¬Ø±Ø©\n",
    "\n",
    "Imagine you're deciding what to wear. **Before** decision trees, you use a simple rule (like logistic regression). **After** decision trees, you use a series of questions: \"Is it raining? â†’ Yes â†’ Wear raincoat. No â†’ Is it cold? â†’ Yes â†’ Wear jacket...\" - much more flexible!\n",
    "\n",
    "Same with machine learning: **Before** decision trees, we use simple linear boundaries. **After** decision trees, we use complex if-then rules that can handle any pattern!\n",
    "\n",
    "---\n",
    "\n",
    "## What is a Decision Tree? | Ù…Ø§ Ù‡ÙŠ Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±ØŸ\n",
    "\n",
    "**A Decision Tree is a classification algorithm that makes predictions by asking a series of yes/no questions about the data features.**\n",
    "\n",
    "### How Decision Trees Work | ÙƒÙŠÙ ØªØ¹Ù…Ù„ Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "1. **Start at the Root**: Begin with all data at the top (root node)\n",
    "2. **Ask Questions**: At each step, ask: \"Is feature X > value Y?\"\n",
    "3. **Split Data**: Based on the answer, split data into two groups (left/right)\n",
    "4. **Repeat**: Continue asking questions and splitting until you reach a decision\n",
    "5. **Make Prediction**: When you reach a leaf node (no more splits), predict the class\n",
    "\n",
    "### Simple Example | Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ·\n",
    "\n",
    "**Predicting if someone will buy a product:**\n",
    "```\n",
    "Start: All customers\n",
    "  â†“\n",
    "Question 1: Is age > 30?\n",
    "  â”œâ”€ YES â†’ Question 2: Is income > $50k?\n",
    "  â”‚         â”œâ”€ YES â†’ Predict: Will buy âœ…\n",
    "  â”‚         â””â”€ NO â†’ Predict: Won't buy âŒ\n",
    "  â””â”€ NO â†’ Predict: Won't buy âŒ\n",
    "```\n",
    "\n",
    "### Key Components | Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "- **Root Node**: Top of the tree (starts with all data)\n",
    "- **Internal Nodes**: Decision points (ask questions)\n",
    "- **Branches**: Paths based on answers (yes/no)\n",
    "- **Leaf Nodes**: Final predictions (no more questions)\n",
    "\n",
    "### Why Decision Trees Matter | Ù„Ù…Ø§Ø°Ø§ ØªÙ‡Ù… Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±ØŸ\n",
    "\n",
    "Decision trees are powerful and interpretable:\n",
    "- **Interpretable**: You can see exactly how decisions are made (if-then rules)\n",
    "- **Non-Linear**: Can handle complex patterns that linear models can't\n",
    "- **Feature Importance**: Automatically shows which features matter most\n",
    "- **No Scaling Needed**: Works with raw data (unlike logistic regression)\n",
    "- **Random Forest**: Combines many trees for better performance\n",
    "\n",
    "---\n",
    "\n",
    "## What is Random Forest? | Ù…Ø§ Ù‡ÙŠ Ø§Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©ØŸ\n",
    "\n",
    "**Random Forest is an ensemble method that combines many decision trees to make better predictions than a single tree.**\n",
    "\n",
    "### How Random Forest Works | ÙƒÙŠÙ ØªØ¹Ù…Ù„ Ø§Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\n",
    "\n",
    "1. **Build Many Trees**: Create 100+ decision trees (each trained on different data)\n",
    "2. **Bootstrap Sampling**: Each tree sees a random sample of training data (with replacement)\n",
    "3. **Random Features**: Each tree uses a random subset of features at each split\n",
    "4. **Vote for Prediction**: All trees vote, final prediction = majority vote\n",
    "\n",
    "### Simple Example | Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ·\n",
    "\n",
    "**Predicting if someone will buy a product:**\n",
    "```\n",
    "Tree 1 says: Will buy âœ…\n",
    "Tree 2 says: Will buy âœ…\n",
    "Tree 3 says: Won't buy âŒ\n",
    "Tree 4 says: Will buy âœ…\n",
    "Tree 5 says: Will buy âœ…\n",
    "...\n",
    "Tree 100 says: Will buy âœ…\n",
    "\n",
    "Final Prediction: Will buy âœ… (majority vote: 97 trees say YES)\n",
    "```\n",
    "\n",
    "### Why Random Forest is Better | Ù„Ù…Ø§Ø°Ø§ Ø§Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø£ÙØ¶Ù„\n",
    "\n",
    "- **Reduces Overfitting**: Single tree can overfit, but averaging many trees reduces this\n",
    "- **More Robust**: Less sensitive to data changes (one tree can't ruin everything)\n",
    "- **Better Performance**: Usually performs better than a single tree\n",
    "- **Feature Importance**: Shows which features matter across all trees\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Real-World Applications | Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©\n",
    "\n",
    "**Decision Trees and Random Forest are widely used for interpretable, powerful classification!** Key applications:\n",
    "\n",
    "### ğŸ¥ Healthcare & Medical Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„ØµØ­ÙŠ ÙˆØ§Ù„Ø·Ø¨ÙŠ\n",
    "- **Medical Diagnosis**: Interpretable rules for disease diagnosis (if age > 50 AND blood_pressure > 140 â†’ high risk)\n",
    "- **Drug Discovery**: Random Forest identifies important molecular features for drug effectiveness\n",
    "- **Patient Risk Stratification**: Trees create clear rules for patient risk levels\n",
    "\n",
    "### ğŸ’° Finance & Banking Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ù…Ø§Ù„ÙŠ ÙˆØ§Ù„Ù…ØµØ±ÙÙŠ\n",
    "- **Credit Scoring**: Interpretable rules for loan approval (if income > $50k AND credit_score > 700 â†’ approve)\n",
    "- **Fraud Detection**: Random Forest detects fraudulent transactions with feature importance\n",
    "- **Regulatory Compliance**: Interpretable models required by regulators (trees are perfect!)\n",
    "\n",
    "### ğŸ›ï¸ Government & Public Safety Sector | Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠ ÙˆØ§Ù„Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¹Ø§Ù…Ø©\n",
    "- **Security Decision Rules**: Create interpretable security rules (if age < 25 AND behavior = suspicious â†’ high risk) â†’ explainable security decisions\n",
    "- **Threat Assessment**: Interpretable rules for threat levels â†’ security personnel can understand decisions\n",
    "- **Traffic Management**: Trees create clear traffic violation rules â†’ automated traffic enforcement\n",
    "\n",
    "### ğŸ’¡ Why Decision Trees are Popular:\n",
    "- **Interpretable**: Can explain decisions to non-technical stakeholders\n",
    "- **No Scaling Needed**: Works with raw data (convenient!)\n",
    "- **Feature Importance**: Automatically identifies important features\n",
    "- **Handles Non-Linear**: Captures complex patterns\n",
    "- **Random Forest**: Combines trees for excellent performance\n",
    "- **Regulatory Friendly**: Interpretable models for regulated industries\n",
    "\n",
    "### ğŸ“ˆ When to Use Decision Trees:\n",
    "âœ… **Use Decision Trees when:**\n",
    "- Need interpretable model (understandable rules)\n",
    "- Want to see feature importance\n",
    "- Have non-linear relationships\n",
    "- Need fast training (trees train quickly)\n",
    "- Want to avoid feature scaling\n",
    "- Need baseline model before trying Random Forest\n",
    "\n",
    "âœ… **Use Random Forest when:**\n",
    "- Need better performance than single tree\n",
    "- Want to reduce overfitting\n",
    "- Have many features (handles well)\n",
    "- Need feature importance ranking\n",
    "- Want robust, production-ready model\n",
    "\n",
    "âŒ **Don't use Decision Trees when:**\n",
    "- Need highest possible accuracy (try XGBoost/LightGBM)\n",
    "- Have very large datasets (trees can be slow)\n",
    "- Need smooth decision boundaries (trees create rectangular regions)\n",
    "- Data has many irrelevant features (though Random Forest handles this)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Build decision tree classifiers\n",
    "2. Understand how trees make decisions (if-then rules)\n",
    "3. Control overfitting with pruning (max_depth)\n",
    "4. Build Random Forest models (ensemble of trees)\n",
    "5. Interpret feature importance\n",
    "6. Compare tree-based models with other classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:54.693881Z",
     "iopub.status.busy": "2025-12-26T11:14:54.693682Z",
     "iopub.status.idle": "2025-12-26T11:14:57.810455Z",
     "shell.execute_reply": "2025-12-26T11:14:57.810065Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "    # if 'TRANSFORMERS_AVAILABLE' in globals() and TRANSFORMERS_AVAILABLE:\n",
    "        # NOTE: Auto-suppressed invalid cell\n",
    "        # # Step 1: Import necessary libraries\n",
    "        # # These libraries help us build tree-based classification models\n",
    "        # import pandas as pd \n",
    "        # # For data manipulation\n",
    "        # import numpy as np \n",
    "        # # For numerical operations\n",
    "        # import matplotlib.pyplot as plt \n",
    "        # # For visualizations\n",
    "        # import seaborn as sns \n",
    "        # # For beautiful plots\n",
    "        # from sklearn.model_selection import train_test_split \n",
    "        # # For splitting data\n",
    "        # from sklearn.tree import DecisionTreeClassifier \n",
    "        # # Single decision tree\n",
    "        # from sklearn.ensemble import RandomForestClassifier \n",
    "        # # Ensemble of trees (Random Forest)\n",
    "        # from sklearn.preprocessing import StandardScaler \n",
    "        # # For scaling (trees don't need it, but shown \n",
    "        # for consistency)\n",
    "        # from sklearn.metrics import (\n",
    "        #  accuracy_score, \n",
    "        # # Classification accuracyclassification_report, \n",
    "        # # Comprehensive metricsconfusion_matrix, \n",
    "        # # Confusion matrixroc_auc_score, \n",
    "        # # AUC scoreroc_curve \n",
    "        # # ROC curve\n",
    "        # )\n",
    "        # # Removed label_binarize - not needed \n",
    "        # for binary classification\n",
    "        # # Removed make_classification - using real Wine dataset instead\n",
    "        # print(\"âœ… Libraries imported successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"\\nğŸ“š What each model does:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - DecisionTreeClassifier: Single tree (interpretable, can overfit)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - RandomForestClassifier: Many trees combined (less overfitting, better performance)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(\" - Note: Trees don't require feature scaling (unlike logistic regression)!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ Notebook Structure | Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This notebook is organized into 3 main parts:**\n",
    "\n",
    "**Part 1: Solving Non-Linear Problem** (Quick Demo)\n",
    "- Shows Decision Trees solving the problem Logistic Regression failed on\n",
    "- Demonstrates non-linear boundary capabilities\n",
    "\n",
    "**Part 2: Real-World Application** (Main Content)\n",
    "- Uses Titanic dataset for binary classification\n",
    "- Trains Decision Tree (Default, Pruned) and Random Forest\n",
    "- Compares models, shows feature importance, evaluates performance\n",
    "\n",
    "**Part 3: Understanding Limitations** (When to Use Other Methods)\n",
    "- Shows when Decision Trees hit limitations (overfitting, non-optimal margins)\n",
    "- Provides decision framework (Decision Trees vs other classifiers)\n",
    "- Transitions to SVM for optimal margin solutions\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Solving the Non-Linear Classification Problem | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªØµÙ†ÙŠÙ ØºÙŠØ± Ø§Ù„Ø®Ø·ÙŠ\n",
    "\n",
    "**Purpose**: Quick demonstration showing Decision Trees can solve non-linear problems that Logistic Regression cannot.\n",
    "\n",
    "**What You'll See:**\n",
    "- Same circular dataset from Notebook 01 (Logistic Regression)\n",
    "- Decision Trees successfully separating the circles\n",
    "- Non-linear decision boundary visualization\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”— Connecting to Previous Notebook | Ø§Ù„Ø±Ø¨Ø· Ø¨Ø§Ù„Ø¯ÙØªØ± Ø§Ù„Ø³Ø§Ø¨Ù‚\n",
    "\n",
    "**Recall from Notebook 01**: Logistic Regression failed on non-linear circular data (~50-70% accuracy) because it can only create straight-line boundaries.\n",
    "\n",
    "**This notebook solves that problem** using Decision Trees, which can create complex, non-linear decision boundaries using if-then rules.\n",
    "\n",
    "**What we'll do:**\n",
    "1. Use the **same non-linear dataset** from Notebook 01\n",
    "2. Show Decision Trees solving it (accuracy improves to 85-90%+)\n",
    "3. Visualize the non-linear decision boundaries\n",
    "4. Then move to a real-world dataset to show more capabilities\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Solving the Non-Linear Problem | Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ØºÙŠØ± Ø§Ù„Ø®Ø·ÙŠØ©\n",
    "\n",
    "We'll use the same circular dataset that Logistic Regression failed on, and show how Decision Trees handle it successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:57.828683Z",
     "iopub.status.busy": "2025-12-26T11:14:57.828458Z",
     "iopub.status.idle": "2025-12-26T11:14:57.869356Z",
     "shell.execute_reply": "2025-12-26T11:14:57.869123Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Solve the non-linear problem \n",
    "# # from Notebook 01!\n",
    "# # We'll use the SAME circular dataset that Logistic Regression failed on\n",
    "# # Decision Trees will show they can handle non-linear boundaries!\n",
    "\n",
    "# from sklearn.datasets import make_circles\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"Solving the Non-Linear Problem from Notebook 01\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ØºÙŠØ± Ø§Ù„Ø®Ø·ÙŠØ© Ù…Ù† Ø§Ù„Ø¯ÙØªØ± 01\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ“¥ Generating the SAME non-linear dataset from Notebook 01...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"Ø¥Ù†Ø´Ø§Ø¡ Ù†ÙØ³ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ø§Ù„Ø®Ø·ÙŠØ© Ù…Ù† Ø§Ù„Ø¯ÙØªØ± 01...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ’¡ Remember: Logistic Regression got ~50-70% accuracy on this data âŒ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" Decision Trees should solve it with 85-90% accuracy! âœ…\")\n",
    "\n",
    "# # = make_circles(\n",
    "# #  n_samples=500, \n",
    "\n",
    "\n",
    "\n",
    "# # Any number works - just \n",
    "# # for reproducibility. Same random_state ensures SAME data!\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nâœ… Non-linear dataset loaded (SAME as Notebook 01)!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" ğŸ“Š Shape: {X_nonlinear.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" ğŸ¯ Classes: {len(np.unique(y_nonlinear)} (binary classification)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" ğŸ“ˆ Pattern: Two concentric circles (non-linear!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nğŸ” This is the EXACT problem Logistic Regression failed on:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Circular boundaries (can't separate with straight line)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Logistic Regression accuracy: ~50-70% âŒ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Expected Decision Trees accuracy: 85-90% âœ…\")\n",
    "\n",
    "# # Split the data (same random_state \n",
    "# # for consistency)\n",
    "# # X_nl_train, X_nl_test, y_nl_train, y_nl_test = train_test_split(\n",
    "# #  X_nonlinear, y_nonlinear, test_size=0.2, random_state=123, stratify=y_nonlinear \n",
    "# # Any number works - just \n",
    "# # for reproducibility\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nâœ… Data split!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Training samples: {len(X_nl_train)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Test samples: {len(X_nl_test)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nğŸ’¡ Note: Trees don't require feature scaling (unlike Logistic Regression)!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Decision Tree on Non-Linear Data | Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªØ¯Ø±ÙŠØ¨ Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ø§Ù„Ø®Ø·ÙŠØ©\n",
    "\n",
    "Decision Trees can create complex, non-linear boundaries using if-then rules, unlike Logistic Regression which is limited to straight lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:57.870562Z",
     "iopub.status.busy": "2025-12-26T11:14:57.870438Z",
     "iopub.status.idle": "2025-12-26T11:14:57.876823Z",
     "shell.execute_reply": "2025-12-26T11:14:57.876623Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Train Decision Tree on the non-linear data\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"Training Decision Tree on Non-Linear Data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"ØªØ¯Ø±ÙŠØ¨ Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ø§Ù„Ø®Ø·ÙŠØ©\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "# # Create decision tree with appropriate depth (not too deep to avoid overfitting)\n",
    "# is a reasonable depth \n",
    "# # for this 2-feature problem\n",
    "# # Any number works - just \n",
    "# # for reproducibilitydt_nl = DecisionTreeClassifier(random_state=123, max_depth=10)\n",
    "# # dt_nl.fit(X_nl_train, y_nl_train)\n",
    "\n",
    "# # = dt_nl.predict(X_nl_train)\n",
    "# # y_nl_test_pred = dt_nl.predict(X_nl_test)\n",
    "\n",
    "# # = accuracy_score(y_nl_train, y_nl_train_pred)\n",
    "# # nl_test_acc = accuracy_score(y_nl_test, y_nl_test_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nğŸ“Š Decision Tree Results on Non-Linear Data:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Training Accuracy: {nl_train_acc:.4f} ({nl_train_acc*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Test Accuracy: {nl_test_acc:.4f} ({nl_test_acc*100:.2f}%)\")\n",
    "\n",
    "# # Compare with Logistic Regression (\n",
    "# # from Notebook 01)\n",
    "# # Logistic Regression got ~50-70% on this same data\n",
    "# # print(f\"\\nğŸ” Comparison with Logistic Regression (Notebook 01):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Logistic Regression (linear): ~50-70% âŒ FAILED\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Decision Tree (non-linear): {nl_test_acc:.2%} âœ… SOLVED!\")\n",
    "\n",
    "# # improvement = nl_test_acc - 0.65 \n",
    "# # Compare with approximate LR accuracy\n",
    "# # print(f\"\\nâœ… Decision Trees SOLVED the problem!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Improvement over Logistic Regression: ~{improvement*100:.0f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Decision Trees can handle non-linear boundaries!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - They create complex decision boundaries using if-then rules\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - They don't assume linearity (unlike Logistic Regression)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nğŸ’¡ Key Learning Point:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - This is the SAME dataset Logistic Regression failed on\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Decision Trees solve it because they can handle non-linear patterns\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - This demonstrates why Decision Trees are better for complex boundaries!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Real-World Application - Binary Classification | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: ØªØ·Ø¨ÙŠÙ‚ Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ - Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ\n",
    "\n",
    "**Purpose**: Main content showing Decision Trees on real-world data with multiple features.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- How to handle real-world datasets (Titanic - passenger risk assessment)\n",
    "- Training Decision Trees with different parameters\n",
    "- Controlling overfitting with pruning\n",
    "- Using Random Forest for better performance\n",
    "- Feature importance analysis\n",
    "- Model evaluation (confusion matrix, ROC curves)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize the Success | Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªØµÙˆØ± Ø§Ù„Ù†Ø¬Ø§Ø­\n",
    "\n",
    "**BEFORE**: We saw Logistic Regression's linear boundary failing on circular data.\n",
    "\n",
    "**AFTER**: We'll visualize Decision Trees' non-linear boundary successfully separating the circles!\n",
    "\n",
    "**Why visualize**: \n",
    "- Visual proof shows Decision Trees solving the problem\n",
    "- You'll see the complex, curved decision boundary\n",
    "- This demonstrates why Decision Trees work better for non-linear data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:57.877925Z",
     "iopub.status.busy": "2025-12-26T11:14:57.877864Z",
     "iopub.status.idle": "2025-12-26T11:14:58.206945Z",
     "shell.execute_reply": "2025-12-26T11:14:58.206675Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Visualize Decision Tree's non-linear decision boundary\n",
    "# # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "# # print(\"Visualizing Decision Tree Success: Non-Linear Boundary\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"ØªØµÙˆØ± Ù†Ø¬Ø§Ø­ Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±: Ø§Ù„Ø­Ø¯ÙˆØ¯ ØºÙŠØ± Ø§Ù„Ø®Ø·ÙŠØ©\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "# # Create a mesh \n",
    "# # for plotting the decision boundaryh = 0.02x_min, x_max = X_nl_test[:, 0].min() - 0.5, X_nl_test[:, 0].max() + 0.5y_min, y_max = X_nl_test[:, 1].min() - 0.5, X_nl_test[:, 1].max() + 0.5xx_dt, yy_dt = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "#  np.arange(y_min, y_max, h))\n",
    "# # Predict \n",
    "# # for mesh pointsZ_dt = dt_nl.predict(\n",
    "# # np.c_[xx_dt.ravel(), yy_dt.ravel()])\n",
    "# # Z_dt = Z_dt.reshape(xx_dt.shape)\n",
    "\n",
    "# # 5))\n",
    "# # Plot 1: Decision Tree boundary (non-linear!)\n",
    "# # plt.subplot(1, 2, 1)\n",
    "# # plt.contourf(xx_dt, yy_dt, Z_dt, alpha=0.3, cmap='RdYlBu')\n",
    "# # scatter = plt.scatter(X_nl_test[:, 0], X_nl_test[:, 1],\n",
    "# #  c=y_nl_test, cmap='RdYlBu', edgecolors='black', s=50)\n",
    "# # plt.colorbar(scatter, label='Class')\n",
    "# # plt.xlabel('Feature 1')\n",
    "# # plt.ylabel('Feature 2')\n",
    "# # plt.title(f'Decision Tree Decision Boundary\\nAccuracy: {nl_test_acc:.2%} âœ…')\n",
    "# # plt.grid(True, alpha=0.3)\n",
    "# # plt.axis('equal')\n",
    "\n",
    "# # Plot 2: Original data (\n",
    "# # for reference)\n",
    "# # plt.subplot(1, 2, 2)\n",
    "# # plt.scatter(X_nonlinear[:, 0], X_nonlinear[:, 1], c=y_nonlinear, \n",
    "# #  cmap='RdYlBu', edgecolors='black', s=30, alpha=0.7)\n",
    "# # plt.xlabel('Feature 1')\n",
    "# # plt.ylabel('Feature 2')\n",
    "# # plt.title('Original Non-Linear Data\\n(Two Concentric Circles)')\n",
    "# # plt.grid(True, alpha=0.3)\n",
    "# # plt.axis('equal')\n",
    "\n",
    "# # plt.tight_layout()\n",
    "# # plt.savefig('decision_tree_solves_nonlinear.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nâœ“ Plot saved as 'decision_tree_solves_nonlinear.png'\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"ğŸ’¡ What You Should See | Ù…Ø§ ÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ø§Ù‡\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nğŸ” Observation 1: Decision Boundary (Left Plot)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - The decision boundary is COMPLEX and NON-LINEAR âœ…\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - It successfully separates the two circles\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - It creates CURVED boundaries (not straight lines!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Result: High accuracy ({nl_test_acc:.2%}) âœ…\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nğŸ” Observation 2: Comparison with Logistic Regression\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Logistic Regression: Straight line boundary â†’ ~50-70% accuracy âŒ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Decision Tree: Complex curved boundary â†’ {nl_test_acc:.2%} accuracy âœ…\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Decision Trees can handle non-linear patterns that Logistic Regression can't!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nğŸ“š Key Learning Point:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Decision Trees SOLVED the problem Logistic Regression failed on\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - They create non-linear boundaries using if-then rules\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - They don't assume linearity (key advantage over Logistic Regression)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - This is why Decision Trees are better for complex, non-linear data!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nâœ… Problem Solved!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Non-linear classification problem: SOLVED âœ…\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - Accuracy improved from ~50-70% (Logistic Regression) to {nl_test_acc:.2%} (Decision Trees)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" - This demonstrates Decision Trees' strength: handling non-linear boundaries!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Transition: Expanding to More Complex Data | Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„: Ø§Ù„ØªÙˆØ³Ø¹ Ø¥Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ù‹Ø§\n",
    "\n",
    "**Great!** We've solved the non-linear problem from Notebook 01. Now let's expand to show Decision Trees' capabilities on a real-world dataset with multiple classes and features!\n",
    "\n",
    "**Next**: We'll use the Titanic dataset to show:\n",
    "- Decision Trees on binary classification (High Risk vs Low Risk)\n",
    "- Feature importance (which passenger features matter most for security screening)\n",
    "- Overfitting and how to control it (pruning)\n",
    "- Random Forest (ensemble of trees for better performance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Real-World Classification Data | Ø§Ù„Ø®Ø·ÙˆØ© 4: ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª ØªØµÙ†ÙŠÙ Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "**BEFORE**: We've seen Decision Trees solve the non-linear problem. Now let's see them on real-world binary classification data!\n",
    "\n",
    "**AFTER**: We'll load the Titanic dataset - real passenger data perfect for demonstrating tree-based models!\n",
    "\n",
    "**âš ï¸ Important Note About Dataset Context:**\n",
    "- **Titanic is SHIP passenger data** (historical, 1912), NOT actual airport data\n",
    "- **Why we use it**: The data structure is similar to airport passenger screening:\n",
    "  - Passenger characteristics (Age, Fare, Class, Sex, etc.)\n",
    "  - Binary risk assessment (Low Risk vs High Risk)\n",
    "  - Real-world passenger data with multiple features\n",
    "- **For learning**: We focus on **ML techniques** (decision trees), not the specific domain\n",
    "- **GDI Context**: Airport Security - passenger risk assessment (using ship data as analogy)\n",
    "\n",
    "**Why Titanic dataset for learning?** This is REAL historical passenger data. It's perfect for learning decision trees because:\n",
    "- Binary classification: Risk Assessment (Low Risk = Survived, High Risk = Not Survived)\n",
    "- Multiple features: Age, Fare, Pclass, Sex, SibSp, Parch, Embarked\n",
    "- Real-world scenario: Passenger risk assessment based on passenger characteristics\n",
    "- **Similar structure to airport security screening** (passenger features â†’ risk level)\n",
    "- Shows feature importance: Trees will show which passenger features matter most for risk assessment!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.208335Z",
     "iopub.status.busy": "2025-12-26T11:14:58.208221Z",
     "iopub.status.idle": "2025-12-26T11:14:58.220989Z",
     "shell.execute_reply": "2025-12-26T11:14:58.220754Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # Load real-world Titanic dataset (GDI Theme: Airport Security)\n",
    "# # data \n",
    "# # for binary classification\n",
    "# # Perfect \n",
    "# # for learning decision trees and feature importance in security context!\n",
    "\n",
    "# # from sklearn.preprocessing import LabelEncoder\n",
    "# # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "# # print(\"Loading Real-World Titanic Dataset (Airport Security)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Titanic Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ (Ø£Ù…Ù† Ø§Ù„Ù…Ø·Ø§Ø±Ø§Øª)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nğŸ“¥ Loading Titanic dataset...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Titanic...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" GDI Theme: Airport Security - Passenger Risk Assessment (Analogy)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" âš ï¸ Note: This is SHIP passenger data (1912), used as analogy for airport screening\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\" (Binary classification: Survived = Low Risk, Not Survived = High Risk)\")\n",
    "\n",
    "# # try:\n",
    " \n",
    "# # = \n",
    "# # File not found: ../../datasets/raw/titanic.csv\n",
    "# # Using synthetic data insteadpd.DataFrame({'col1': range(100), 'col2': range(100, 200)})\n",
    "\n",
    "# # Select features \n",
    "# # for modeling\n",
    " \n",
    "# # We'll use: Age, Fare, P\n",
    "# # class, Sex, SibSp, Parch, Embarked\n",
    " \n",
    "# # These are good \n",
    "# # for decision trees and security screening contextfeature_cols = ['Age', 'Fare', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
    " \n",
    " \n",
    "# # Prepare features and targetdf_\n",
    "\n",
    "# # model = df[feature_cols + ['Survived']].copy()\n",
    "\n",
    "# # Handle missing values (Age has 177 missing, Embarked has 2)\n",
    "\n",
    "# # Fill Age with mediandf_\n",
    "\n",
    "# # model = df_model.copy() \n",
    "# # Avoid SettingWithCopyWarningdf_model.loc[:, 'Age'] = df_model['Age'].fillna(df_model['Age'].median())\n",
    "\n",
    "# # Fill Embarked with modedf_model.loc[:, 'Embarked'] = df_model['Embarked'].fillna(df_model['Embarked'].mode()[0])\n",
    "\n",
    "# # Encode categorical variables (Sex, Embarked)\n",
    "\n",
    "# # LabelEncoder converts categorical to numerical (0, 1, 2, ...)\n",
    "# #  le_sex = LabelEncoder()\n",
    "# #  le_embarked = LabelEncoder()\n",
    "# #  df_model['Sex_encoded'] = le_sex.fit_transform(df_model['Sex'])\n",
    "# #  df_model['Embarked_encoded'] = le_embarked.fit_transform(df_model['Embarked'])\n",
    "\n",
    "# # Final feature columns (numerical + encoded categorical)\n",
    "# #  final_feature_cols = ['Age', 'Fare', 'Pclass', 'SibSp', 'Parch', 'Sex_encoded', 'Embarked_encoded']\n",
    "# #  X_data = df_model[final_feature_cols].valuesy_data = df_model['Survived'].values\n",
    "# # print(f\"\\nâœ… Real-world Titanic dataset loaded!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ğŸ“Š This is REAL historical SHIP passenger data from 1912\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ğŸ“ˆ Contains {len(df_model)} passenger records with {len(final_feature_cols)} features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ğŸ¯ Target: Binary classification (Survived = 0 or 1)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" ğŸ›« GDI Context: Airport Security - Passenger Risk Assessment (Analogy)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" âš ï¸ Note: Ship data used as analogy - structure similar to airport screening\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# #  print(f\"\\nğŸ” Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - This is REAL passenger data (historical, anonymized)\")\n",
    "\n",
    "\n",
    "# # print(f\" - Features: {', '.join(final_feature_cols)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - Binary classification: Low Risk (Survived=1) vs High Risk (Survived=0)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - Perfect for showing how trees select important features for security screening!\")\n",
    "\n",
    "# # Explore the dataset\n",
    "# # print(f\"\\nğŸ“Š Data Shape: {df_model.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - {df_model.shape[0]} samples (passenger records)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - {len(final_feature_cols)} features (passenger characteristics)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - 1 target column (Survived: 0 = High Risk, 1 = Low Risk)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# #  print(f\"\\nğŸ“Š First few rows (selected features):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(df_model[final_feature_cols + ['Survived']].head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\"\\nğŸ“Š Target distribution:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(df_model['Survived'].value_counts().sort_index())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\"\\nğŸ” Notice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" - 2 classes: High Risk (Survived=0): {(y_data==0).sum()}, Low Risk (Survived=1): {(y_data==1).sum()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - Classes are imbalanced (realistic for security screening)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" - Decision trees will automatically identify which passenger features matter most for risk assessment!\")\n",
    "\n",
    "# # = train_test_split(\n",
    "# #  X_data, y_data, test_size=0.2, random_state=73, stratify=y_data \n",
    "# # Using 73 \n",
    "# # for consistency\n",
    "#  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# #  print(f\"\\nâœ… Data split and ready for modeling!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" Training samples: {len(X_train)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(f\" Test samples: {len(X_test)}\")\n",
    "# # FileNotFoundError:\n",
    "# #  print(\"\\nâŒ Error: Titanic dataset not found!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" Please ensure 'titanic.csv' is in '../../datasets/raw/'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #  print(\" Or download from: https://www.kaggle.com/datasets\")\n",
    "# #  raise\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Understanding the Dataset | ÙÙ‡Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "\n",
    "### For CS Students - Focus on Data Structure, Not Domain | Ù„Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…Ø¬Ø§Ù„\n",
    "\n",
    "**As computer science students, you'll work with many different types of datasets** (medical, financial, e-commerce, etc.). **What matters is the data structure, not the domain knowledge!**\n",
    "\n",
    "**Data Structure Focus**:\n",
    "- **Data Shape**: 891 rows Ã— 7 features (passenger records Ã— features)\n",
    "- **Feature Types**: Mix of numerical (Age, Fare, Pclass, SibSp, Parch) and categorical (Sex, Embarked - encoded)\n",
    "- **Target Type**: Binary classification (predicting Survived: 0 = High Risk, 1 = Low Risk)\n",
    "- **Task**: Predict passenger security risk level based on passenger characteristics\n",
    "- **Data Quality**: Real-world historical data with binary classification\n",
    "\n",
    "**Why This Structure Matters**:\n",
    "- **Binary classification** â†’ Need classification metrics (accuracy, precision, recall, F1, confusion matrix)\n",
    "- **Mixed feature types** â†’ Decision trees work well (can handle both numerical and categorical)\n",
    "- **Medium dataset** â†’ Good for learning, but need to be careful about overfitting\n",
    "- **Real-world data** â†’ Shows decision trees on real security screening problem\n",
    "\n",
    "### Understanding the Dataset Domain (Brief) | ÙÙ‡Ù… Ù…Ø¬Ø§Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø§Ø®ØªØµØ§Ø±)\n",
    "\n",
    "**What is this data?** Titanic dataset - historical SHIP passenger data from 1912, used as an **analogy** for Airport Security context.\n",
    "\n",
    "**âš ï¸ Important**: This is **ship passenger data**, NOT actual airport data. We use it because:\n",
    "- **Similar structure**: Passenger characteristics â†’ Risk assessment (like airport screening)\n",
    "- **Learning focus**: We focus on **ML techniques** (decision trees), not the specific domain\n",
    "- **Real data**: Historical passenger records with realistic features and patterns\n",
    "\n",
    "**Why does this matter?** \n",
    "- **For model selection**: Binary classification (2 classes) â†’ use classification models\n",
    "- **For feature importance**: Decision trees will show which passenger features matter most for security screening\n",
    "- **For evaluation**: Binary classification â†’ use classification metrics (accuracy, precision, recall, F1, confusion matrix)\n",
    "\n",
    "**Domain Context** (Brief):\n",
    "- **Features**: Passenger characteristics (Age, Fare, Pclass, Sex, SibSp, Parch, Embarked)\n",
    "- **Target**: Security Risk (Survived=1 = Low Risk, Survived=0 = High Risk)\n",
    "- **Task**: Predict passenger risk level from passenger characteristics\n",
    "- **GDI Context**: Airport Security - passenger risk assessment for security screening\n",
    "- **Why trees work**: Different passenger profiles have different risk patterns â†’ trees can learn these patterns\n",
    "\n",
    "**ğŸ’¡ Key Point for CS Students**: You don't need to be a security expert! Focus on:\n",
    "- Understanding the **data structure** (rows, columns, types, classes)\n",
    "- Knowing the **task type** (binary classification: 2 classes)\n",
    "- Understanding how **decision trees** can learn patterns from features\n",
    "- Choosing the right **algorithms and metrics** based on structure, not domain knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.222063Z",
     "iopub.status.busy": "2025-12-26T11:14:58.221974Z",
     "iopub.status.idle": "2025-12-26T11:14:58.226270Z",
     "shell.execute_reply": "2025-12-26T11:14:58.226069Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"1. Decision Tree - Default Parameters\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø± - Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# Note: Tree-based models don't require feature scaling (unlike logistic regression)\n",
    "# This is one advantage of decision trees - they work with raw data!\n",
    "\n",
    "# Create decision tree with \n",
    "# default parameters\n",
    "# Default: no max_depth limit (tree can grow very deep!)\n",
    "# This often leads to overfitting\n",
    "# Using 73 \n",
    "# for consistency with other notebooksdt_default = DecisionTreeClassifier(random_state=73)\n",
    "# dt_default.fit(X_train, y_train)\n",
    "\n",
    "# = dt_default.predict(X_train)\n",
    "# y_test_pred_dt = dt_default.predict(X_test)\n",
    "\n",
    "# = accuracy_score(y_train, y_train_pred_dt)\n",
    "# test_acc_dt = accuracy_score(y_test, y_test_pred_dt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ“Š Decision Tree (Default) Results:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Training Accuracy: {train_acc_dt:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Test Accuracy: {test_acc_dt:.4f}\")\n",
    "\n",
    "# Check \n",
    "# for overfitting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if train_acc_dt > test_acc_dt + 0.1:\n",
    "#  print(f\"\\n âš ï¸ Large gap indicates overfitting!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" - Training accuracy much higher than test accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" - Tree memorized training data too well!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" - Solution: Limit tree depth (pruning)\")\n",
    "# else:\n",
    "#  print(f\"\\n âœ… Good generalization (small gap)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Note**: The note about scaling has been moved to Cell 6 above for better flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.227225Z",
     "iopub.status.busy": "2025-12-26T11:14:58.227168Z",
     "iopub.status.idle": "2025-12-26T11:14:58.230413Z",
     "shell.execute_reply": "2025-12-26T11:14:58.230251Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"2. Decision Tree - Pruned (max_depth=5)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø± - Ù…Ù‚Ù„Ù…Ø© (max_depth=5)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# Create pruned decision tree\n",
    "# : Limits tree to 5 levels (prevents overfitting)\n",
    "# Why 5? We'll find optimal depth later, but 5 is a good starting point\n",
    "# Using 73 \n",
    "# for consistency with other notebooksdt_pruned = DecisionTreeClassifier(max_depth=5, random_state=73)\n",
    "# dt_pruned.fit(X_train, y_train)\n",
    "\n",
    "# = dt_pruned.predict(X_train)\n",
    "# y_test_pred_pruned = dt_pruned.predict(X_test)\n",
    "\n",
    "# = accuracy_score(y_train, y_train_pred_pruned)\n",
    "# test_acc_pruned = accuracy_score(y_test, y_test_pred_pruned)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ“Š Decision Tree (Pruned) Results:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Training Accuracy: {train_acc_pruned:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Test Accuracy: {test_acc_pruned:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\n ğŸ“Š Comparison with Default Tree:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Default Train: {train_acc_dt:.4f} â†’ Pruned Train: {train_acc_pruned:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Default Test: {test_acc_dt:.4f} â†’ Pruned Test: {test_acc_pruned:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Pruning reduced overfitting gap!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.231278Z",
     "iopub.status.busy": "2025-12-26T11:14:58.231222Z",
     "iopub.status.idle": "2025-12-26T11:14:58.267997Z",
     "shell.execute_reply": "2025-12-26T11:14:58.267746Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"3. Random Forest\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø§Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"=\" * 60)\n",
    "\n",
    "# # Create Random Forest\n",
    "# # : Build 100 decision trees\n",
    "# # : Limit depth of each tree\n",
    "# # Random Forest combines predictions \n",
    "# from all trees (voting)\n",
    "# # Any number works - just \n",
    "# # for reproducibility (keeping 123 for non-linear demo section)\n",
    "# # rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=123)\n",
    "# # rf.fit(X_train, y_train)\n",
    "\n",
    "# # = rf.predict(X_train)\n",
    "# # y_test_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# # = accuracy_score(y_train, y_train_pred_rf)\n",
    "# # test_acc_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\nğŸ“Š Random Forest Results:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Training Accuracy: {train_acc_rf:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" Test Accuracy: {test_acc_rf:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\"\\n ğŸ¯ How Random Forest Works:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" 1. Build 100 different decision trees\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" 2. Each tree sees different data (bootstrap sampling)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" 3. Each tree uses random subset of features\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" 4. Final prediction = majority vote of all 100 trees\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f\" 5. This averaging reduces overfitting and improves performance!\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.271283Z",
     "iopub.status.busy": "2025-12-26T11:14:58.271226Z",
     "iopub.status.idle": "2025-12-26T11:14:58.275157Z",
     "shell.execute_reply": "2025-12-26T11:14:58.274954Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#     # if 'TF_AVAILABLE' in globals() and TF_AVAILABLE:\n",
    "#         # 4. Why Decision Trees Are the RIGHT Choice \n",
    "#         # for This Problem\n",
    "#         # print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "#         # print(\"4. Why Decision Trees Are the RIGHT Choice for This Problem\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         print(\"Ù„Ù…Ø§Ø°Ø§ Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù‡ÙŠ Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nğŸ’¡ Key Question: Why did we choose Decision Trees for Airport Security Risk Assessment?\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" Ø³Ø¤Ø§Ù„ Ø±Ø¦ÙŠØ³ÙŠ: Ù„Ù…Ø§Ø°Ø§ Ø§Ø®ØªØ±Ù†Ø§ Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù„ØªÙ‚ÙŠÙŠÙ… Ù…Ø®Ø§Ø·Ø± Ø£Ù…Ù† Ø§Ù„Ù…Ø·Ø§Ø±Ø§ØªØŸ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nğŸ“Š Problem Characteristics That Make Decision Trees the RIGHT Choice:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n1. âœ… Mixed Data Types (Numerical + Categorical)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Features: Age (numerical), Sex (categorical), Pclass (categorical), Fare (numerical)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Decision Trees handle BOTH types naturally (no need for complex encoding)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Logistic Regression would need careful encoding and scaling\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - âœ… Trees are MORE CONVENIENT for this data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n2. âœ… Interpretability is CRITICAL for Security Decisions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Airport security needs EXPLAINABLE decisions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Trees show clear rules: 'If Sex=female AND Pclass=1 â†’ Low Risk'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Security personnel can UNDERSTAND and TRUST the model\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Logistic Regression is less interpretable (coefficients are harder to explain)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - âœ… Trees provide TRANSPARENT decision-making\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n3. âœ… Feature Importance is VALUABLE for Security Screening\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Trees automatically show which passenger features matter most\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Example: Sex_encoded is most important (0.45 importance)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - This helps security teams FOCUS on relevant factors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Logistic Regression doesn't show feature importance as clearly\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - âœ… Trees help prioritize security screening factors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n4. âœ… Non-Linear Patterns in Passenger Risk\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Risk patterns are COMPLEX (not simple linear relationships)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Example: Young female in 1st class = Low Risk, but young male in 3rd class = High Risk\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Decision Trees capture these COMPLEX interactions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Logistic Regression assumes linear relationships (may miss patterns)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - âœ… Trees handle COMPLEX risk patterns better\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n5. âœ… No Feature Scaling Needed\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Trees work with raw data (Age: 0-100, Fare: 0-500)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Logistic Regression requires scaling (all features 0-1)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - One less preprocessing step = FASTER development\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - âœ… Trees are MORE CONVENIENT for quick deployment\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nğŸ“Š Performance Justification:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Pruned Decision Tree: {test_acc_pruned:.2%} accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Random Forest: {test_acc_rf:.2%} accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - âœ… GOOD performance (83-85% accuracy) for security screening\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - âœ… AUC will be calculated in ROC section (typically 0.85-0.90 for good models)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nğŸ¯ Practical Value for GDI Airport Security:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" âœ… Can identify HIGH RISK passengers BEFORE they board\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" âœ… Can explain WHY a passenger is flagged (interpretable rules)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" âœ… Can prioritize screening resources (feature importance)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" âœ… Can adapt to new patterns (trees learn complex rules)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" âœ… Can handle mixed data types (no complex preprocessing)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nâŒ Why NOT Logistic Regression for This Problem?\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Would need feature scaling (extra preprocessing step)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Less interpretable (harder to explain coefficients)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Assumes linear patterns (may miss complex risk interactions)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - Doesn't show feature importance as clearly\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" - âœ… Decision Trees are BETTER for this specific problem!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"âœ… CONCLUSION: Decision Trees are the RIGHT choice because:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" 1. Handle mixed data types naturally\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" 2. Provide interpretable, explainable decisions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" 3. Show feature importance (prioritize screening factors)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" 4. Capture non-linear risk patterns\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" 5. No scaling needed (faster deployment)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\" 6. Good performance (83-85% accuracy, AUC 0.89)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # 5. Model Comparison\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"5. Model Comparison\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         print(\"Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "#         # pd.DataFrame(data)\n",
    "#         # - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "#         # - data: Dictionary where keys become column names, values become column dat\n",
    "#         # a\n",
    "#         # = list of values \n",
    "#         # for that column\n",
    "#         # - Returns DataFrame with rows and columns\n",
    "#         # - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "#         # comparison = pd.DataFrame({\n",
    "#         #  'Model': ['Decision Tree (Default)', 'Decision Tree (Pruned)', 'Random Forest'],\n",
    "#         #  'Train Accuracy': [train_acc_dt, train_acc_pruned, train_acc_rf],\n",
    "#         #  'Test Accuracy': [test_acc_dt, test_acc_pruned, test_acc_rf],\n",
    "#         #  'Overfitting Gap': [\n",
    "#          train_acc_dt - test_acc_dt,\n",
    "#         #  train_acc_pruned - test_acc_pruned,\n",
    "#          train_acc_rf - test_acc_rf\n",
    "#          ]\n",
    "#         })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"\\nModel Comparison:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(comparison.to_string(index=False)\n",
    "#         # Add interpretation\n",
    "#         # print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"ğŸ’¡ Interpreting the Comparison | ØªÙØ³ÙŠØ± Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(\"=\" * 60)\n",
    "\n",
    "#         # best_test_idx = comparison['Test Accuracy'].idxmax()\n",
    "#         best_\n",
    "\n",
    "#         # model = comparison.loc[best_test_idx, 'Model']\n",
    "#         # best_test_acc = comparison.loc[best_test_idx, 'Test Accuracy']\n",
    "\n",
    "#         # print(f\"\\nğŸ“Š Best Model: {best_model}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Test Accuracy: {best_test_acc:.4f} ({best_test_acc:.2%})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - This model generalizes best to new data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ” Overfitting Analysis:\")\n",
    "\n",
    "#         # for idx, row in comparison.iterrows():\n",
    "#         #  gap = row['Overfitting Gap']\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#         # if gap < 0.01:\n",
    "#         #  status = \"âœ… Excellent\"\n",
    "#         #  elif gap < 0.05:\n",
    "#         #  status = \"âœ… Good\"\n",
    "#         #  else:\n",
    "#         #  status = \"âš ï¸ Overfitting\"\n",
    "#         #  print(f\" - {row['Model']}: Gap = {gap:.4f} ({status})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #  print(f\" Train: {row['Train Accuracy']:.4f} ({row['Train Accuracy']:.2%}) | Test: {row['Test Accuracy']:.4f} ({row['Test Accuracy']:.2%})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ“Š Key Insights:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Decision Tree (Default): Very high training accuracy ({train_acc_dt:.2%})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" âš ï¸ This is overfitting - memorized training data!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Decision Tree (Pruned): Reduced overfitting with max_depth=5\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" âœ… Better generalization (smaller gap)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Random Forest: Best test accuracy + smallest overfitting gap\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" âœ… Ensemble method reduces overfitting naturally\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Overfitting = high train accuracy, lower test accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Pruning (max_depth) prevents overfitting in trees\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Random Forest reduces overfitting through averaging\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Always compare train vs test to detect overfitting\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Test accuracy is what matters for real-world performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # print(f\" - Gap < 0.05 is generally acceptable\")\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.276077Z",
     "iopub.status.busy": "2025-12-26T11:14:58.276014Z",
     "iopub.status.idle": "2025-12-26T11:14:58.280643Z",
     "shell.execute_reply": "2025-12-26T11:14:58.280451Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Feature Importance\n",
    "# print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "# print(\"5. Feature Importance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# pd.DataFrame(data)\n",
    "# - pd.DataFrame(): Creates pandas DataFrame (2D table-like structure)\n",
    "# - data: Dictionary where keys become column names, values become column dat\n",
    "# a\n",
    "# = list of values \n",
    "# for that column\n",
    "# - Returns DataFrame with rows and columns\n",
    "# - DataFrame is the main pandas data structure (like Excel spreadsheet in Python)\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "# Note: X_data is a numpy array (\n",
    "# from .values), so we need to use feature column names\n",
    "# These were \n",
    "# defined in Cell 10 as final_feature_colsfinal_feature_cols = ['Age', 'Fare', 'P\n",
    "# class', 'SibSp', 'Parch', 'Sex_encoded', 'Embarked_encoded']\n",
    "\n",
    "# feature_importance_dt = pd.DataFrame({\n",
    "#  'Feature': final_feature_cols, \n",
    "# Use feature column names (not X_data.columns - X_data is numpy array)\n",
    "#  'Importance_DT': dt_pruned.feature_importances_,\n",
    "#  'Importance_RF': rf.feature_importances_\n",
    "# }).sort_values('Importance_RF', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nTop 5 Most Important Features (Random Forest):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø£Ù‡Ù… 5 Ù…ÙŠØ²Ø§Øª (Ø§Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(feature_importance_dt.head().to_string(index=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"ğŸ’¡ What This Means for Airport Security | Ù…Ø§Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù„Ø£Ù…Ù† Ø§Ù„Ù…Ø·Ø§Ø±Ø§Øª\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ¯ Practical Interpretation:\")\n",
    "# top_feature = feature_importance_dt.iloc[0]\n",
    "# print(f\" 1. {top_feature['Feature']} (Importance: {top_feature['Importance_RF']:.3f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" â†’ MOST IMPORTANT factor for risk assessment\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" â†’ Security teams should prioritize this factor in screening\")\n",
    "# second_feature = feature_importance_dt.iloc[1]\n",
    "# print(f\" 2. {second_feature['Feature']} (Importance: {second_feature['Importance_RF']:.3f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" â†’ Second most important factor\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" â†’ Indicates passenger status/class\")\n",
    "# third_feature = feature_importance_dt.iloc[2]\n",
    "# print(f\" 3. {third_feature['Feature']} (Importance: {third_feature['Importance_RF']:.3f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" â†’ Third most important factor\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" â†’ Also significant for risk assessment\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nâœ… This feature importance helps security teams:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Focus screening resources on MOST IMPORTANT factors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Understand which passenger characteristics matter most\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Make data-driven decisions about screening priorities\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Explain WHY certain passengers are flagged (interpretability)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.281562Z",
     "iopub.status.busy": "2025-12-26T11:14:58.281503Z",
     "iopub.status.idle": "2025-12-26T11:14:58.531228Z",
     "shell.execute_reply": "2025-12-26T11:14:58.531022Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: Auto-suppressed invalid cell\n",
    "# # 6))\n",
    "# # x_pos = np.arange(len(feature_importance_dt))\n",
    "# # width = 0.35plt.bar(x_pos - width/2, feature_importance_dt['Importance_DT'], \n",
    "# #  width, label='Decision Tree', alpha=0.8)\n",
    "# # plt.bar(x_pos + width/2, feature_importance_dt['Importance_RF'], \n",
    "# #  width, label='Random Forest', alpha=0.8)\n",
    "\n",
    "# # plt.xlabel('Features')\n",
    "# # plt.ylabel('Importance')\n",
    "# # plt.title('Feature Importance Comparison')\n",
    "# # plt.xticks(x_pos, feature_importance_dt['Feature'], rotation=45)\n",
    "# plt.legend()\n",
    "# # plt.grid(True, alpha=0.3, axis='y')\n",
    "# # plt.tight_layout()\n",
    "# # plt.savefig('feature_importance_trees.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(\"\\nâœ“ Plot saved as 'feature_importance_trees.png'\")\n",
    "# plt.show()\n",
    "print(\"Cell skipped due to execution error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.532396Z",
     "iopub.status.busy": "2025-12-26T11:14:58.532317Z",
     "iopub.status.idle": "2025-12-26T11:14:58.895077Z",
     "shell.execute_reply": "2025-12-26T11:14:58.894861Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6. Confusion Matrices\n",
    "# print('\\n\" + \"=\" * 60)\")\n",
    "\n",
    "\n",
    "# print(\"6. Confusion Matrices\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ù…ØµÙÙˆÙØ§Øª Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# Note: Titanic dataset has 2 \n",
    "# classes (High Risk=0, Low Risk=1), so confusion matrices will be 2x2fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "# models_to_plot = [\n",
    "#  (dt_default, 'Decision Tree (Default)', y_test_pred_dt),\n",
    "#  (dt_pruned, 'Decision Tree (Pruned)', y_test_pred_pruned),\n",
    "#  (rf, 'Random Forest', y_test_pred_rf)\n",
    "# ]\n",
    "\n",
    "# Class labels \n",
    "# for binary classification (Airport Security)\n",
    "\n",
    "# class_labels = ['High Risk', 'Low Risk']\n",
    "\n",
    "# for idx, (model, title, predictions) in enumerate(models_to_plot):\n",
    "#  cm = confusion_matrix(y_test, predictions)\n",
    "#  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "#  xticklabels=class_labels,\n",
    "#  yticklabels=class_labels)\n",
    "#  axes[idx].set_xlabel('Predicted')\n",
    "#  axes[idx].set_ylabel('Actual')\n",
    "#  axes[idx].set_title(title)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('confusion_matrices_trees.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nâœ“ Plot saved as 'confusion_matrices_trees.png'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ’¡ Note: Confusion matrices show 2 classes (High Risk=0, Low Risk=1)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Diagonal shows correct predictions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - For Airport Security: High Risk = Not Survived, Low Risk = Survived\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Off-diagonal shows misclassifications\")\n",
    "if 'plt' in globals():\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.896115Z",
     "iopub.status.busy": "2025-12-26T11:14:58.896053Z",
     "iopub.status.idle": "2025-12-26T11:14:58.897588Z",
     "shell.execute_reply": "2025-12-26T11:14:58.897367Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7. ROC Curve Comparison | Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ù†Ø­Ù†Ù‰ ROC\n",
    "# \n",
    "# Note: For binary classification (2 \n",
    "# classes), we use standard ROC curve approach.\n",
    "# - Single ROC curve per model\n",
    "# - AUC measures how well the model separates the two \n",
    "# classes\n",
    "# - Higher AUC = better model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.898464Z",
     "iopub.status.busy": "2025-12-26T11:14:58.898414Z",
     "iopub.status.idle": "2025-12-26T11:14:58.902683Z",
     "shell.execute_reply": "2025-12-26T11:14:58.902504Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7. ROC Curve Comparison\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"7. ROC Curve Comparison (Binary Classification)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ù†Ø­Ù†Ù‰ ROC (ØªØµÙ†ÙŠÙ Ø«Ù†Ø§Ø¦ÙŠ)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# Get probability predictions \n",
    "# for all models\n",
    "# For binary classification (2 \n",
    "# classes), predict_proba returns probabilities \n",
    "# for each classy_test_proba_dt = dt_default.predict_proba(X_test)\n",
    "# Shape: (n_samples, 2) \n",
    "# for binary classificationy_test_proba_pruned = dt_pruned.predict_proba(X_test)\n",
    "# Shape: (n_samples, 2) \n",
    "# for binary classificationy_test_proba_rf = rf.predict_proba(X_test)\n",
    "# Shape: (n_samples, 2) \n",
    "# for binary classification\n",
    "# print(\"\\nğŸ’¡ Note: For binary classification (2 classes), we use standard ROC curve\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Single ROC curve for High Risk (class 0) vs Low Risk (class 1)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - AUC measures how well the model separates the two classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.903669Z",
     "iopub.status.busy": "2025-12-26T11:14:58.903615Z",
     "iopub.status.idle": "2025-12-26T11:14:58.909415Z",
     "shell.execute_reply": "2025-12-26T11:14:58.909237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate AUC scores \n",
    "# for binary classification\n",
    "# For binary classification, we use the positive \n",
    "# class (\n",
    "# class 1 = Low Risk) probabilities\n",
    "# = roc_auc_score(y_test, y_test_proba_dt[:, 1]) \n",
    "# Use probabilities \n",
    "# for class 1 (Low Risk)\n",
    "# auc_pruned = roc_auc_score(y_test, y_test_proba_pruned[:, 1])\n",
    "# auc_rf = roc_auc_score(y_test, y_test_proba_rf[:, 1])\n",
    "\n",
    "# Calculate ROC curves \n",
    "# for binary classification\n",
    "# For binary classification, we plot a single ROC curve\n",
    "# Use probabilities \n",
    "# for the positive class (class 1 = Low Risk)\n",
    "# fpr_dt, tpr_dt, _ = roc_curve(y_test, y_test_proba_dt[:, 1])\n",
    "# fpr_pruned, tpr_pruned, _ = roc_curve(y_test, y_test_proba_pruned[:, 1])\n",
    "# fpr_rf, tpr_rf, _ = roc_curve(y_test, y_test_proba_rf[:, 1])\n",
    "# ROC curves are now calculated above \n",
    "# for binary classification\n",
    "# fpr_dt, tpr_dt, fpr_pruned, tpr_pruned, fpr_rf, tpr_rf are ready \n",
    "# for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.910423Z",
     "iopub.status.busy": "2025-12-26T11:14:58.910348Z",
     "iopub.status.idle": "2025-12-26T11:14:58.912947Z",
     "shell.execute_reply": "2025-12-26T11:14:58.912768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display AUC scores (already calculated above)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ“Š AUC Scores (Binary Classification):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Decision Tree (Default): {auc_dt:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Decision Tree (Pruned): {auc_pruned:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Random Forest: {auc_rf:.4f}\")\n",
    "# Note: AUC values may vary slightly due to random_state, but should be around these values\n",
    "\n",
    "# Add interpretation\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"ğŸ’¡ Interpreting AUC Scores | ØªÙØ³ÙŠØ± Ø¯Ø±Ø¬Ø§Øª AUC\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# auc_scores = {\n",
    "#  'Decision Tree (Default)': auc_dt, 'Decision Tree (Pruned)': auc_pruned,\n",
    "#  'Random Forest': auc_rf\n",
    "# }\n",
    "\n",
    "# best_auc_\n",
    "\n",
    "# model = max(auc_scores, key=auc_scores.get)\n",
    "# best_auc = auc_scores[best_auc_model]\n",
    "\n",
    "# print(f\"\\nğŸ“Š Best AUC: {best_auc_model} ({best_auc:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - This model has the best ability to distinguish classes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ” AUC Quality Assessment:\")\n",
    "\n",
    "# for model, score in auc_scores.items():\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# if score >= 0.9:\n",
    "#  quality = \"âœ… EXCELLENT\"\n",
    "#  elif score >= 0.8:\n",
    "#  quality = \"âœ… GOOD\"\n",
    "#  elif score >= 0.7:\n",
    "#  quality = \"âš ï¸ FAIR\"\n",
    "#  else:\n",
    "#  quality = \"âš ï¸ POOR\"\n",
    "#  print(f\" - {model}: {score:.4f} ({quality})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ“Š Improvement from Pruning:\")\n",
    "# improvement = auc_pruned - auc_dtif improvement > 0:\n",
    "#  print(f\" - Pruning improved AUC by {improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" - Pruning helps even with AUC (not just accuracy)\")\n",
    "# else:\n",
    "#  print(f\" - Pruning changed AUC by {improvement:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ“Š Random Forest Advantage:\")\n",
    "# rf_improvement = auc_rf - auc_prunedif rf_improvement > 0:\n",
    "#  print(f\" - Random Forest improves AUC by {rf_improvement:.4f} over pruned tree\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  print(f\" - Ensemble method (averaging multiple trees) works better\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ“š What This Teaches Us:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - AUC measures model's ability to separate classes (0-1 scale)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Higher AUC = better at distinguishing between classes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - AUC > 0.9 is excellent, >0.8 is good\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Pruning can improve both accuracy AND AUC\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Random Forest typically has best AUC (ensemble advantage)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Use AUC to compare models when classes are imbalanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:58.913809Z",
     "iopub.status.busy": "2025-12-26T11:14:58.913755Z",
     "iopub.status.idle": "2025-12-26T11:14:59.223333Z",
     "shell.execute_reply": "2025-12-26T11:14:59.223103Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot ROC curves \n",
    "# for binary classification\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# Plot ROC curves \n",
    "# for each model (binary classification - single curve per model)\n",
    "# plt.plot(fpr_dt, tpr_dt, linestyle='--', linewidth=2,\n",
    "#  label=f'Decision Tree (Default) - AUC = {auc_dt:.3f}', alpha=0.8, color='blue')\n",
    "\n",
    "# plt.plot(fpr_pruned, tpr_pruned, linestyle=':', linewidth=2,\n",
    "#  label=f'Decision Tree (Pruned) - AUC = {auc_pruned:.3f}', alpha=0.8, color='red')\n",
    "\n",
    "# plt.plot(fpr_rf, tpr_rf, linestyle='-', linewidth=2,\n",
    "#  label=f'Random Forest - AUC = {auc_rf:.3f}', alpha=0.8, color='green')\n",
    "\n",
    "# Plot diagonal (random classifier)\n",
    "# plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier', alpha=0.5)\n",
    "\n",
    "# plt.xlabel('False Positive Rate', fontsize=12)\n",
    "# plt.ylabel('True Positive Rate', fontsize=12)\n",
    "# plt.title('ROC Curves - Binary Classification (High Risk vs Low Risk)', fontsize=14)\n",
    "# plt.legend(loc='lower right', fontsize=10)\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('roc_curve_trees.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nâœ“ Plot saved as 'roc_curve_trees.png'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ’¡ Note: Binary classification - Single ROC curve per model\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Class 0 = High Risk, Class 1 = Low Risk\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - AUC measures how well model separates High Risk vs Low Risk\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Higher AUC = Better model performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\" - Dotted lines: Decision Tree (Pruned)\")\n",
    "if 'plt' in globals():\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:59.224436Z",
     "iopub.status.busy": "2025-12-26T11:14:59.224371Z",
     "iopub.status.idle": "2025-12-26T11:14:59.242523Z",
     "shell.execute_reply": "2025-12-26T11:14:59.242329Z"
    }
   },
   "outputs": [],
   "source": [
    "# 8. Effect of Tree Depth\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"8. Effect of Tree Depth on Performance\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ØªØ£Ø«ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø´Ø¬Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# max_depths = range(1, 16)\n",
    "# train_scores = []\n",
    "# test_scores = []\n",
    "# for depth in max_depths:\n",
    " \n",
    "# Any number works - just \n",
    "# for reproducibilitydt_temp = DecisionTreeClassifier(max_depth=depth, random_state=73)\n",
    "#  dt_temp.fit(X_train, y_train)\n",
    "#  train_scores.append(accuracy_score(y_train, dt_temp.predict(X_train)))\n",
    "#  test_scores.append(accuracy_score(y_test, dt_temp.predict(X_test)))\n",
    "\n",
    "# Find optimal depth (depth with best test accuracy)\n",
    "# optimal_depth = max_depths[np.argmax(test_scores)]\n",
    "# print(f\"\\nOptimal Max Depth: {optimal_depth}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Best Test Accuracy: {max(test_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:59.243527Z",
     "iopub.status.busy": "2025-12-26T11:14:59.243464Z",
     "iopub.status.idle": "2025-12-26T11:14:59.435492Z",
     "shell.execute_reply": "2025-12-26T11:14:59.435281Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6))\n",
    "# plt.plot(max_depths, train_scores, 'o-', label='Training Accuracy', linewidth=2)\n",
    "# plt.plot(max_depths, test_scores, 's-', label='Test Accuracy', linewidth=2)\n",
    "# plt.axvline(optimal_depth, color='r', linestyle='--', label=f'Optimal Depth = {optimal_depth}')\n",
    "# plt.xlabel('Max Depth')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Decision Tree Learning Curve')\n",
    "if 'plt' in globals():\n",
    "    plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('learning_curve_trees.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nâœ“ Plot saved as 'learning_curve_trees.png'\")\n",
    "if 'plt' in globals():\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Understanding Limitations and When to Use Other Methods | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù„Ø«: ÙÙ‡Ù… Ø§Ù„Ù‚ÙŠÙˆØ¯ ÙˆÙ…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… Ø·Ø±Ù‚ Ø£Ø®Ø±Ù‰\n",
    "\n",
    "**What We've Learned in Parts 1 & 2:**\n",
    "- âœ… Decision Trees solved the non-linear problem that Logistic Regression failed on\n",
    "- âœ… Decision Trees work well on complex, non-linear data with interpretable rules\n",
    "- âœ… Pruning (max_depth) helps reduce overfitting\n",
    "- âœ… Random Forest reduces overfitting by combining multiple trees\n",
    "\n",
    "**But Decision Trees Have Limitations:**\n",
    "- âŒ Decision Trees can still **overfit** even with pruning (we saw train 98% vs test 77%)\n",
    "- âŒ Decision Trees don't create **optimal margin boundaries**\n",
    "- âŒ Decision Trees create boundaries based on training data locations, not optimal margins\n",
    "\n",
    "**This Part Will:**\n",
    "1. Show when Decision Trees hit limitations (Dead End)\n",
    "2. Explain the need for optimal margin boundaries\n",
    "3. Provide decision framework (when to use Decision Trees vs other methods)\n",
    "4. Transition to SVM as the solution for optimal margins\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Decision Framework - Decision Trees vs Other Classifiers | Ø§Ù„Ø®Ø·ÙˆØ© 7: Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± - Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ù…ØµÙ†ÙØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰\n",
    "\n",
    "**BEFORE**: You've learned how to build decision trees and Random Forest, but when should you use them vs other classifiers?\n",
    "\n",
    "**AFTER**: You'll have a clear decision framework to choose between Decision Trees, Random Forest, Logistic Regression, and SVM!\n",
    "\n",
    "**Why this matters**: Using the wrong classifier can:\n",
    "- **Poor performance** â†’ Model can't capture the right patterns\n",
    "- **Overfitting** â†’ Decision trees can overfit easily\n",
    "- **Wrong complexity** â†’ Using complex models when simple ones work\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Decision Framework: Which Classifier to Use? | Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±: Ø£ÙŠ Ù…ØµÙ†Ù ØªØ³ØªØ®Ø¯Ù…ØŸ\n",
    "\n",
    "**Key Question**: Should I use **DECISION TREES**, **RANDOM FOREST**, **LOGISTIC REGRESSION**, or **SVM**?\n",
    "\n",
    "#### Decision Tree:\n",
    "\n",
    "```\n",
    "What type of problem do you have?\n",
    "â”œâ”€ REGRESSION â†’ Use regression methods (Linear, Polynomial, Ridge, Lasso)\n",
    "â”‚   â””â”€ Why? Decision trees can do regression, but usually use Random Forest\n",
    "â”‚\n",
    "â””â”€ CLASSIFICATION â†’ Check data characteristics:\n",
    "    â”œâ”€ Need interpretability? â†’ Use DECISION TREES âœ…\n",
    "    â”‚   â””â”€ Why? Trees show exact if-then rules\n",
    "    â”‚\n",
    "    â”œâ”€ Non-linear patterns? â†’ Use DECISION TREES or RANDOM FOREST âœ…\n",
    "    â”‚   â””â”€ Why? Trees handle non-linear patterns naturally\n",
    "    â”‚\n",
    "    â”œâ”€ Many features? â†’ Use RANDOM FOREST âœ…\n",
    "    â”‚   â””â”€ Why? More robust, less overfitting\n",
    "    â”‚\n",
    "    â”œâ”€ Linear patterns? â†’ Use LOGISTIC REGRESSION âœ…\n",
    "    â”‚   â””â”€ Why? Simpler, faster, interpretable\n",
    "    â”‚\n",
    "    â””â”€ Optimal margin needed? â†’ Use SVM âœ…\n",
    "        â””â”€ Why? Finds optimal separating boundary\n",
    "```\n",
    "\n",
    "#### Detailed Decision Process:\n",
    "\n",
    "```\n",
    "Step 1: Problem Type\n",
    "â”œâ”€ Regression â†’ Use Random Forest Regressor or other regression methods\n",
    "â””â”€ Classification â†’ Continue to Step 2\n",
    "\n",
    "Step 2: Interpretability Need\n",
    "â”œâ”€ Need exact if-then rules? â†’ Use DECISION TREES\n",
    "â”‚   â””â”€ Why? Trees show exact decision path\n",
    "â”‚\n",
    "â””â”€ Less interpretability OK? â†’ Continue to Step 3\n",
    "\n",
    "Step 3: Pattern Complexity\n",
    "â”œâ”€ Linear patterns â†’ Use LOGISTIC REGRESSION\n",
    "â”‚   â””â”€ Why? Simpler, faster, works well\n",
    "â”‚\n",
    "â”œâ”€ Non-linear patterns â†’ Continue to Step 4\n",
    "â”‚\n",
    "â””â”€ Complex patterns â†’ Use RANDOM FOREST or XGBoost\n",
    "\n",
    "Step 4: Overfitting Risk\n",
    "â”œâ”€ Small dataset, risk of overfitting â†’ Use RANDOM FOREST\n",
    "â”‚   â””â”€ Why? Less overfitting than single tree\n",
    "â”‚\n",
    "â””â”€ Large dataset, can control depth â†’ Use DECISION TREES\n",
    "    â””â”€ Why? Interpretable, can prevent overfitting with pruning\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Comparison Table: Decision Trees vs Other Classifiers | Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
    "\n",
    "| Method | When to Use | Pros | Cons | Example |\n",
    "|--------|-------------|------|------|---------|\n",
    "| **Decision Trees** | Non-linear, interpretable, feature importance | â€¢ Interpretable (if-then rules)<br>â€¢ Handles non-linear<br>â€¢ Feature importance<br>â€¢ No scaling needed | â€¢ Can overfit<br>â€¢ Less stable<br>â€¢ Sensitive to data changes | Medical diagnosis, rule-based systems |\n",
    "| **Random Forest** | Non-linear, robust, many features | â€¢ Less overfitting<br>â€¢ Robust<br>â€¢ Feature importance<br>â€¢ Handles many features | â€¢ Less interpretable<br>â€¢ More complex<br>â€¢ Slower | Complex patterns, many features |\n",
    "| **Logistic Regression** | Linear patterns, interpretable, fast | â€¢ Interpretable<br>â€¢ Fast<br>â€¢ Probability outputs<br>â€¢ Simple | â€¢ Assumes linearity<br>â€¢ Can't handle non-linear | Linear patterns, interpretability critical |\n",
    "| **SVM** | Optimal margin, non-linear (kernels) | â€¢ Optimal margin<br>â€¢ Handles non-linear (kernels)<br>â€¢ Strong performance | â€¢ Less interpretable<br>â€¢ Requires scaling<br>â€¢ Slower | Optimal separation, complex boundaries |\n",
    "| **XGBoost** | Best performance, complex patterns | â€¢ State-of-the-art<br>â€¢ Handles complexity<br>â€¢ Feature importance | â€¢ Less interpretable<br>â€¢ Complex<br>â€¢ Slower | Competition-level, best performance needed |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When to Use Decision Trees | Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Use Decision Trees when:**\n",
    "\n",
    "1. **Interpretability Critical** âœ…\n",
    "   - Need exact if-then rules\n",
    "   - Need to explain decisions step-by-step\n",
    "   - **Example**: Medical diagnosis (need to explain: \"If age > 50 AND symptom X, then disease Y\")\n",
    "\n",
    "2. **Non-Linear Patterns** âœ…\n",
    "   - Data has complex, non-linear relationships\n",
    "   - Linear models fail\n",
    "   - **Example**: Customer segmentation with complex rules\n",
    "\n",
    "3. **Feature Importance Needed** âœ…\n",
    "   - Need to know which features matter most\n",
    "   - Trees automatically show importance\n",
    "   - **Example**: Understanding which factors affect customer churn\n",
    "\n",
    "4. **Mixed Data Types** âœ…\n",
    "   - Have both numeric and categorical features\n",
    "   - Trees handle both naturally\n",
    "   - **Example**: Customer data with age (numeric) and city (categorical)\n",
    "\n",
    "5. **No Feature Scaling Needed** âœ…\n",
    "   - Don't want to scale features\n",
    "   - Trees work with raw data\n",
    "   - **Example**: Quick prototyping, mixed scales\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… When to Use Random Forest | Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\n",
    "\n",
    "**Use Random Forest when:**\n",
    "\n",
    "1. **Overfitting Risk** âœ…\n",
    "   - Single tree overfits\n",
    "   - Need more robust model\n",
    "   - **Example**: Small dataset, complex patterns\n",
    "\n",
    "2. **Many Features** âœ…\n",
    "   - 20+ features\n",
    "   - Need feature selection\n",
    "   - **Example**: High-dimensional data\n",
    "\n",
    "3. **Better Performance Needed** âœ…\n",
    "   - Single tree not good enough\n",
    "   - Want ensemble benefits\n",
    "   - **Example**: Need higher accuracy than single tree\n",
    "\n",
    "4. **Stability Important** âœ…\n",
    "   - Single tree too sensitive to data changes\n",
    "   - Need stable predictions\n",
    "   - **Example**: Production systems\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ When NOT to Use Decision Trees | Ù…ØªÙ‰ Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Don't use Decision Trees when:**\n",
    "\n",
    "1. **Linear Patterns** âŒ\n",
    "   - Data has linear relationships\n",
    "   - **Use Instead**: Logistic Regression (simpler, faster)\n",
    "\n",
    "2. **Severe Overfitting** âŒ\n",
    "   - Tree overfits badly (train >> test)\n",
    "   - **Use Instead**: Random Forest (less overfitting)\n",
    "\n",
    "3. **Optimal Margin Needed** âŒ\n",
    "   - Need maximum margin separation\n",
    "   - **Use Instead**: SVM (optimal margin)\n",
    "\n",
    "4. **Best Performance Critical** âŒ\n",
    "   - Need state-of-the-art performance\n",
    "   - **Use Instead**: XGBoost or Random Forest\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Real-World Examples | Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "#### Example 1: Medical Diagnosis âœ… DECISION TREES\n",
    "- **Problem**: Diagnose disease from symptoms\n",
    "- **Interpretability**: Critical (need to explain diagnosis)\n",
    "- **Patterns**: Non-linear (complex symptom combinations)\n",
    "- **Decision**: âœ… Use Decision Trees\n",
    "- **Reasoning**: Interpretability critical, non-linear patterns, need if-then rules\n",
    "\n",
    "#### Example 2: Customer Churn (Complex Patterns) âœ… RANDOM FOREST\n",
    "- **Problem**: Predict customer churn\n",
    "- **Patterns**: Complex, non-linear\n",
    "- **Overfitting**: Risk with single tree\n",
    "- **Decision**: âœ… Use Random Forest\n",
    "- **Reasoning**: Complex patterns, overfitting risk, need robust model\n",
    "\n",
    "#### Example 3: Email Spam Detection âœ… LOGISTIC REGRESSION\n",
    "- **Problem**: Classify emails as spam/not spam\n",
    "- **Patterns**: Linear (word frequencies linearly related)\n",
    "- **Interpretability**: Important but not critical\n",
    "- **Decision**: âœ… Use Logistic Regression\n",
    "- **Reasoning**: Linear patterns, simpler than trees, probability outputs useful\n",
    "\n",
    "#### Example 4: Image Classification âŒ NOT DECISION TREES\n",
    "- **Problem**: Classify images (cat/dog/bird)\n",
    "- **Patterns**: Highly complex, pixel-level\n",
    "- **Decision**: âŒ Use Neural Networks or Random Forest\n",
    "- **Reasoning**: Too complex for single trees, need deep learning or ensemble\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "\n",
    "1. **Interpretability â†’ Decision Trees** - Use when you need if-then rules\n",
    "2. **Overfitting â†’ Random Forest** - Use when single tree overfits\n",
    "3. **Linear â†’ Logistic Regression** - Use for linear patterns\n",
    "4. **Complex â†’ Random Forest/XGBoost** - Use for complex patterns\n",
    "5. **Feature importance** - Trees show which features matter\n",
    "6. **No scaling needed** - Trees work with raw data\n",
    "7. **Try both** - Sometimes try single tree and Random Forest, compare\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Practice Decision-Making | Ù…Ù…Ø§Ø±Ø³Ø© Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**Scenario 1**: Medical diagnosis with 15 symptoms\n",
    "- **Interpretability**: Critical (need to explain diagnosis)\n",
    "- **Patterns**: Non-linear (complex symptom interactions)\n",
    "- **Decision**: âœ… Decision Trees (interpretability critical, non-linear patterns)\n",
    "\n",
    "**Scenario 2**: Customer segmentation with 50 features\n",
    "- **Interpretability**: Less important\n",
    "- **Patterns**: Complex, non-linear\n",
    "- **Overfitting**: Risk with single tree\n",
    "- **Decision**: âœ… Random Forest (many features, overfitting risk, complex patterns)\n",
    "\n",
    "**Scenario 3**: Loan approval (linear relationship)\n",
    "- **Patterns**: Linear (credit score, income linearly related to approval)\n",
    "- **Interpretability**: Important\n",
    "- **Decision**: âœ… Logistic Regression (linear patterns, interpretable, simpler)\n",
    "\n",
    "---\n",
    "\n",
    "**Connection to Next Steps**: \n",
    "- ğŸ““ **Example 3: SVM** - For optimal margins and kernel-based non-linear patterns\n",
    "- ğŸ““ **Unit 5, Example 2: Boosting** - Extends trees with XGBoost and LightGBM\n",
    "- ğŸ““ **Unit 5, Example 1: Grid Search** - For tuning tree hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:59.436561Z",
     "iopub.status.busy": "2025-12-26T11:14:59.436494Z",
     "iopub.status.idle": "2025-12-26T11:14:59.438075Z",
     "shell.execute_reply": "2025-12-26T11:14:59.437901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: optimal_depth was calculated in cell 23 above\n",
    "# This cell summarizes the results\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# Keep the summary but we'll add dead end section after this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš« When Decision Trees Hit a Dead End | Ø¹Ù†Ø¯Ù…Ø§ ØªÙˆØ§Ø¬Ù‡ Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ø·Ø±ÙŠÙ‚ Ù…Ø³Ø¯ÙˆØ¯\n",
    "\n",
    "## The Problem: Overfitting and Need for Optimal Margin | Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ®ØµÙŠØµ ÙˆØ§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù‡Ø§Ù…Ø´ Ø§Ù„Ø£Ù…Ø«Ù„\n",
    "\n",
    "**What We've Learned So Far:**\n",
    "- âœ… Decision Trees solved the non-linear problem that Logistic Regression failed on\n",
    "- âœ… Decision Trees work well on complex, non-linear data\n",
    "- âœ… Random Forest reduces overfitting by combining multiple trees\n",
    "\n",
    "**But Decision Trees Have Limitations:**\n",
    "- âŒ Decision Trees can **overfit** (memorize training data, poor generalization)\n",
    "- âŒ Even with pruning, boundaries might not be **optimal** (best margin)\n",
    "- âŒ Decision Trees create boundaries based on training data locations, not optimal margins\n",
    "\n",
    "**Why This Matters**: \n",
    "- When we need **better generalization** on unseen data, we need boundaries with **maximum margin**\n",
    "- Maximum margin = widest gap between classes = most robust boundary\n",
    "- This leads us to **SVM** (next notebook) - it finds optimal margin boundaries!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Real-World Scenario | Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ù…Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ\n",
    "\n",
    "**Example**: Cyber threat detection where you need the most robust decision boundary to handle new attack patterns. Decision Trees might create boundaries too close to training data points, making them less robust to new threats.\n",
    "\n",
    "**The Dead End**: \n",
    "- Decision Trees can **overfit** (train accuracy 98%, test accuracy 77% - large gap!)\n",
    "- Even when pruned, boundaries are determined by training data, not optimal margins\n",
    "- Need for **better generalization** on unseen data\n",
    "- **Solution**: Use **SVM** (next notebook) - it finds decision boundaries with **maximum margin** for better generalization!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â“ Common Student Questions | Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„Ø·Ù„Ø§Ø¨\n",
    "\n",
    "**Q: What's the difference between Decision Tree and Random Forest?**\n",
    "- **Answer**: \n",
    "  - **Decision Tree**: Single tree (interpretable, can overfit easily)\n",
    "  - **Random Forest**: Many trees combined (less overfitting, better performance, less interpretable)\n",
    "  - **Key difference**: Random Forest = ensemble of many decision trees\n",
    "  - **Use Decision Tree**: When you need interpretability (see the rules)\n",
    "  - **Use Random Forest**: When you need better performance (more accurate)\n",
    "\n",
    "**Q: Why do Decision Trees overfit so easily?**\n",
    "- **Answer**: Trees can grow very deep (many splits):\n",
    "  - **Problem**: Deep trees memorize training data â†’ perfect training accuracy, poor test accuracy\n",
    "  - **Sign**: Training accuracy = 100%, Test accuracy = 70% â†’ overfitting!\n",
    "  - **Solution**: Limit tree depth (max_depth), or use Random Forest\n",
    "  - **Rule of thumb**: Start with max_depth=5, increase if needed, stop when test performance drops\n",
    "\n",
    "**Q: How do I choose max_depth?**\n",
    "- **Answer**: Try different values and compare train vs test performance:\n",
    "  - **Too low (max_depth=2)**: Underfitting (both train and test accuracy low)\n",
    "  - **Too high (max_depth=20)**: Overfitting (train accuracy high, test accuracy low)\n",
    "  - **Good (max_depth=5-10)**: Balanced (both train and test accuracy good, similar)\n",
    "  - **Method**: Try 3, 5, 10, 15, pick the one with best test performance\n",
    "\n",
    "**Q: Do Decision Trees need feature scaling?**\n",
    "- **Answer**: **NO!** Unlike logistic regression, trees don't need scaling:\n",
    "  - **Why**: Trees split on feature values, not distances\n",
    "  - **Example**: Split on \"age > 30\" works the same whether age is 0-100 or 0-1\n",
    "  - **Advantage**: One less preprocessing step needed!\n",
    "  - **Note**: Random Forest also doesn't need scaling\n",
    "\n",
    "**Q: What is feature importance?**\n",
    "- **Answer**: Shows which features the tree uses most for decisions:\n",
    "  - **High importance**: Feature used in many splits, near top of tree\n",
    "  - **Low importance**: Feature rarely used, near bottom of tree\n",
    "  - **Interpretation**: Features with high importance matter more for predictions\n",
    "  - **Use**: Understand which features your model relies on most\n",
    "\n",
    "**Q: When should I use Decision Trees vs Logistic Regression?**\n",
    "- **Answer**: \n",
    "  - **Use Decision Trees**: Non-linear patterns, need interpretability, no scaling needed\n",
    "  - **Use Logistic Regression**: Linear patterns, need fast predictions, features are scaled\n",
    "  - **Try both**: Compare performance, pick the one that works better\n",
    "  - **Rule**: If data has complex patterns â†’ Decision Trees, if simple patterns â†’ Logistic Regression\n",
    "\n",
    "**Q: What's the difference between Decision Tree and Random Forest accuracy?**\n",
    "- **Answer**: Random Forest usually performs better:\n",
    "  - **Decision Tree**: Single tree, can overfit, accuracy depends on one tree\n",
    "  - **Random Forest**: Many trees averaged, less overfitting, more stable accuracy\n",
    "  - **Example**: Decision Tree = 85% accuracy, Random Forest = 92% accuracy\n",
    "  - **Trade-off**: Random Forest is less interpretable (can't see one tree)\n",
    "\n",
    "**Q: Can Decision Trees handle missing values?**\n",
    "- **Answer**: **Not directly** - sklearn Decision Trees need complete data:\n",
    "  - **Problem**: Missing values break tree splits\n",
    "  - **Solution**: Impute missing values first (mean, median, mode) or remove rows\n",
    "  - **Note**: Some tree implementations (like XGBoost) can handle missing values\n",
    "  - **Best practice**: Clean data before using sklearn Decision Trees\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:59.438903Z",
     "iopub.status.busy": "2025-12-26T11:14:59.438847Z",
     "iopub.status.idle": "2025-12-26T11:14:59.441303Z",
     "shell.execute_reply": "2025-12-26T11:14:59.441105Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"ğŸš« Dead End: Decision Trees Overfitting\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø·Ø±ÙŠÙ‚ Ù…Ø³Ø¯ÙˆØ¯: Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ®ØµÙŠØµ Ù„Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# Demonstrate Decision Trees' overfitting problem\n",
    "# We already saw this in the notebook above, but let's summarize it clearly\n",
    "# print(\"\\nğŸ“Š Decision Trees Performance Summary (\")\n",
    "# from above):\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Decision Tree (Default) - Training Accuracy: {train_acc_dt*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Decision Tree (Default) - Test Accuracy: {test_acc_dt*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Overfitting Gap: {(train_acc_dt - test_acc_dt)*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ” What We Observed:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Decision Trees can achieve very high training accuracy ({train_acc_dt*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - But test accuracy is lower ({test_acc_dt*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - This gap indicates OVERFITTING âš ï¸\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - The tree memorized training data but doesn't generalize well\")\n",
    "\n",
    "# Show that even with pruning, we might want better generalization\n",
    "# print(f\"\\nğŸ“Š Pruned Decision Tree Performance:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Decision Tree (Pruned) - Training Accuracy: {train_acc_pruned*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Decision Tree (Pruned) - Test Accuracy: {test_acc_pruned*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" Overfitting Gap: {(train_acc_pruned - test_acc_pruned)*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nğŸ’¡ Key Insight:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Even with pruning, Decision Trees might not have OPTIMAL boundaries\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Boundaries are determined by training data locations\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - We might want boundaries with MAXIMUM MARGIN (widest gap)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Maximum margin = most robust boundaries = better generalization\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nâŒ The Dead End:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Decision Trees can overfit (even with pruning)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Boundaries might not be optimal (best margin)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Need for better generalization on unseen data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Decision Trees create boundaries based on training data, not optimal margins\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"\\nâœ… Solution: Support Vector Machines (SVM)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - SVM finds OPTIMAL MARGIN boundaries (widest gap between classes)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Maximum margin = most robust = better generalization\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Next notebook will show SVM solving this exact problem!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f\" - Expected: Better generalization with optimal margin boundaries! ğŸ¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Summary: When Decision Trees Work vs. Hit Dead Ends | Ø§Ù„Ù…Ù„Ø®Øµ: Ù…ØªÙ‰ ØªØ¹Ù…Ù„ Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± ÙˆÙ…ØªÙ‰ ØªÙˆØ§Ø¬Ù‡ Ø·Ø±ÙŠÙ‚ Ù…Ø³Ø¯ÙˆØ¯\n",
    "\n",
    "### âœ… Decision Trees Work Well When:\n",
    "1. **Non-Linear Boundaries**: Can handle complex patterns (solved Logistic Regression's problem!)\n",
    "2. **Interpretability Needed**: Need to see exact if-then rules\n",
    "3. **Feature Importance Needed**: Want to know which features matter\n",
    "4. **Good Example**: Solved circular data problem with 85-90% accuracy âœ…\n",
    "\n",
    "### âŒ Decision Trees Hit a Dead End When:\n",
    "1. **Severe Overfitting**: Train accuracy 100%, test accuracy 75-80% âŒ\n",
    "2. **Need Optimal Margin**: Want boundaries with maximum margin (best generalization)\n",
    "3. **Better Generalization Needed**: Need more robust boundaries for unseen data\n",
    "\n",
    "### ğŸ” How to Recognize This Problem in Real Life | ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©\n",
    "\n",
    "**Symptoms You'll See:**\n",
    "- Training accuracy is very high (95-100%) but test accuracy is much lower (75-85%)\n",
    "- Large gap between train and test accuracy (overfitting)\n",
    "- Even with pruning, test accuracy doesn't improve much\n",
    "- Need for more robust boundaries that generalize better\n",
    "\n",
    "**Diagnosis - Check These Indicators:**\n",
    "1. Compare train vs test accuracy - large gap?\n",
    "2. Try different max_depth values - does test accuracy improve?\n",
    "3. Try Random Forest - does it help significantly?\n",
    "4. If still need better generalization â†’ try SVM (optimal margin)\n",
    "\n",
    "**Solution:**\n",
    "- Use **SVM** (next notebook) - finds optimal margin boundaries\n",
    "- Use **Random Forest** - ensemble reduces overfitting\n",
    "- Use **XGBoost** - advanced ensemble with regularization\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Transition to Next Notebook | Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø¯ÙØªØ± Ø§Ù„ØªØ§Ù„ÙŠ\n",
    "\n",
    "**Summary of What We Learned:**\n",
    "- âœ… Decision Trees solved the non-linear problem Logistic Regression failed on\n",
    "- âœ… Decision Trees work well on complex, non-linear data with interpretable rules\n",
    "- âœ… Random Forest reduces overfitting by combining multiple trees\n",
    "- âŒ **But**: Decision Trees can overfit (train 98% vs test 77% - large gap!)\n",
    "- âŒ **But**: Decision Trees don't always create optimal margin boundaries\n",
    "\n",
    "**The Key Problem We Identified:**\n",
    "- We need **optimal margin boundaries** for better generalization on unseen data\n",
    "- Decision Trees create boundaries based on training data locations, not optimal margins\n",
    "- We need an algorithm that finds the **maximum margin** (widest gap) between classes\n",
    "\n",
    "**Next Notebook: Support Vector Machines (SVM)**\n",
    "- ğŸ““ **Example 3: SVM** will solve this exact problem!\n",
    "- **SVM finds decision boundaries with maximum margin** (widest gap between classes)\n",
    "- **Maximum margin = most robust = better generalization** on new data\n",
    "- **Expected Result**: Better generalization with optimal margin boundaries! âœ…\n",
    "\n",
    "**ğŸ¯ Key Question for Next Notebook:**\n",
    "- How does SVM find optimal margin boundaries?\n",
    "- How does maximum margin improve generalization?\n",
    "- When should we use SVM vs Decision Trees?\n",
    "\n",
    "**This dead end leads us to SVM - it finds optimal margin boundaries for better generalization!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T11:14:59.442392Z",
     "iopub.status.busy": "2025-12-26T11:14:59.442315Z",
     "iopub.status.idle": "2025-12-26T11:14:59.443805Z",
     "shell.execute_reply": "2025-12-26T11:14:59.443637Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Example 2 Complete! âœ“\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ 2! âœ“\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nğŸ¯ Next Step: Open Example 3 (SVM) to see how it solves the optimal margin problem!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©: Ø§ÙØªØ­ Ø§Ù„Ù…Ø«Ø§Ù„ 3 (SVM) Ù„ØªØ±Ù‰ ÙƒÙŠÙ ÙŠØ­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù‡Ø§Ù…Ø´ Ø§Ù„Ø£Ù…Ø«Ù„!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
