{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04. K-Nearest Neighbors (KNN) | Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ø¬ÙŠØ±Ø§Ù† Ø§Ù„Ø£Ù‚Ø±Ø¨ (KNN)\n",
        "\n",
        "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
        "\n",
        "**BEFORE starting this notebook**, you should have completed:\n",
        "- âœ… **Unit 3, Examples 1-3**: Logistic Regression, Decision Trees, and SVM\n",
        "- âœ… **Understanding of distance metrics**: Euclidean distance, Manhattan distance\n",
        "- âœ… **Understanding of feature scaling**: Why scaling matters for distance-based algorithms\n",
        "\n",
        "**If you haven't completed these**, you might struggle with:\n",
        "- Understanding how KNN makes predictions based on neighbors\n",
        "- Knowing why feature scaling is critical for KNN\n",
        "- Understanding how to choose the right K value\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
        "\n",
        "**This is Unit 3, Example 4** - it introduces instance-based classification!\n",
        "\n",
        "**Why this example FOURTH in Unit 3?**\n",
        "- **Before** you can use KNN, you need to understand basic classification\n",
        "- **Before** you can choose K, you need to understand overfitting vs underfitting\n",
        "- **Before** you can use distance metrics, you need to understand feature scaling\n",
        "\n",
        "**Builds on**: \n",
        "- ğŸ““ Unit 3, Example 1: Logistic Regression (classification basics)\n",
        "- ğŸ““ Unit 3, Example 2: Decision Trees (overfitting concepts)\n",
        "- ğŸ““ Unit 3, Example 3: SVM (feature scaling importance)\n",
        "\n",
        "**Leads to**: \n",
        "- ğŸ““ Unit 4: Clustering (K-Means uses similar distance concepts)\n",
        "- ğŸ““ Unit 5: Model Selection (hyperparameter tuning for K)\n",
        "- ğŸ““ All instance-based learning algorithms\n",
        "\n",
        "**Why this order?**\n",
        "1. KNN is **simple to understand** (just find nearest neighbors)\n",
        "2. KNN demonstrates **lazy learning** (no training, just prediction)\n",
        "3. KNN shows **distance-based classification** (different from other methods)\n",
        "\n",
        "---\n",
        "\n",
        "## The Story: Learning from Neighbors | Ø§Ù„Ù‚ØµØ©: Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø¬ÙŠØ±Ø§Ù†\n",
        "\n",
        "Imagine you're moving to a new neighborhood. **Before** KNN, you make decisions alone. **After** KNN, you look at your neighbors: \"What do most of my neighbors do? I'll do that too!\" - simple and effective!\n",
        "\n",
        "Same with machine learning: **Before** KNN, we train complex models. **After** KNN, we just find the nearest neighbors and predict based on them - simple but powerful!\n",
        "\n",
        "---\n",
        "\n",
        "## Why KNN Matters | Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… KNNØŸ\n",
        "\n",
        "KNN is a simple but powerful classifier:\n",
        "- **Simple**: Easy to understand and implement\n",
        "- **No Training**: Lazy learning - no model training needed\n",
        "- **Non-Parametric**: Makes no assumptions about data distribution\n",
        "- **Versatile**: Works for classification and regression\n",
        "- **Effective**: Often performs well on many datasets\n",
        "\n",
        "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
        "1. Build KNN classifiers\n",
        "2. Understand how KNN makes predictions (nearest neighbors)\n",
        "3. Choose the right K value (avoid overfitting/underfitting)\n",
        "4. Understand distance metrics (Euclidean, Manhattan)\n",
        "5. Know why feature scaling is critical for KNN\n",
        "6. Compare KNN with other classification algorithms\n",
        "7. Understand when to use KNN vs other methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "# These libraries help us build KNN classification models\n",
        "\n",
        "import pandas as pd  # For data manipulation\n",
        "import numpy as np   # For numerical operations\n",
        "import matplotlib.pyplot as plt  # For visualizations\n",
        "import seaborn as sns  # For beautiful plots\n",
        "from sklearn.model_selection import train_test_split  # For splitting data\n",
        "from sklearn.neighbors import KNeighborsClassifier  # KNN classifier\n",
        "from sklearn.preprocessing import StandardScaler  # CRITICAL for KNN! Must scale features\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,        # Classification accuracy\n",
        "    classification_report,  # Comprehensive metrics\n",
        "    confusion_matrix,      # Confusion matrix\n",
        "    roc_auc_score,         # AUC score\n",
        "    roc_curve              # ROC curve\n",
        ")\n",
        "from sklearn.datasets import load_breast_cancer  # Real-world dataset\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n",
        "print(\"\\nğŸ“š Key KNN Concepts:\")\n",
        "print(\"   - KNeighborsClassifier: KNN classifier (finds K nearest neighbors)\")\n",
        "print(\"   - K parameter: Number of neighbors to consider (critical hyperparameter)\")\n",
        "print(\"   - Distance metrics: Euclidean (default), Manhattan, etc.\")\n",
        "print(\"   - Lazy learning: No training phase, just prediction\")\n",
        "print(\"\\n   âš ï¸  IMPORTANT: KNN requires feature scaling! Always use StandardScaler!\")\n",
        "print(\"   ğŸ’¡ Why? Distance calculations are affected by feature scales!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding KNN | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: ÙÙ‡Ù… KNN\n",
        "\n",
        "### ğŸ”— What is KNN? | Ù…Ø§ Ù‡Ùˆ KNNØŸ\n",
        "\n",
        "**K-Nearest Neighbors (KNN)** is a simple, instance-based learning algorithm:\n",
        "\n",
        "1. **No Training Phase**: KNN doesn't \"train\" a model - it stores all training data\n",
        "2. **Prediction Phase**: For a new data point, KNN finds the K nearest neighbors\n",
        "3. **Voting**: The class of the new point is determined by majority vote of its K neighbors\n",
        "\n",
        "**Key Concepts:**\n",
        "- **K**: Number of neighbors to consider (hyperparameter)\n",
        "- **Distance Metric**: How to measure \"nearest\" (Euclidean, Manhattan, etc.)\n",
        "- **Lazy Learning**: No model is built during training - all computation happens during prediction\n",
        "\n",
        "**Why Feature Scaling Matters:**\n",
        "- KNN uses **distance** to find neighbors\n",
        "- If features have different scales (e.g., age: 0-100, income: 0-100000), distance will be dominated by the larger scale\n",
        "- **Solution**: Always scale features before using KNN!\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Load and Prepare Data | Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù…ÙŠÙ„ ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "\n",
        "**BEFORE**: We need to understand the dataset structure.\n",
        "\n",
        "**AFTER**: We'll load the Wisconsin Breast Cancer dataset and prepare it for KNN!\n",
        "\n",
        "**Why this dataset**: \n",
        "- Real-world medical data\n",
        "- Binary classification (perfect for KNN demonstration)\n",
        "- Multiple features (shows importance of scaling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Wisconsin Breast Cancer dataset\n",
        "# This is REAL medical data for binary classification\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Loading Wisconsin Breast Cancer Dataset\")\n",
        "print(\"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø³Ø±Ø·Ø§Ù† Ø§Ù„Ø«Ø¯ÙŠ Ù…Ù† ÙˆÙŠØ³ÙƒÙˆÙ†Ø³Ù†\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cancer_data = load_breast_cancer()\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
        "df['target'] = cancer_data.target  # 0 = Malignant, 1 = Benign\n",
        "\n",
        "print(f\"\\nâœ… Dataset loaded!\")\n",
        "print(f\"   ğŸ“Š Shape: {df.shape}\")\n",
        "print(f\"   ğŸ¯ Classes: {len(np.unique(df['target']))} (binary classification)\")\n",
        "print(f\"   ğŸ“ˆ Features: {len(df.columns)-1} features (measurements from cell nuclei)\")\n",
        "print(f\"\\nğŸ” Class Distribution:\")\n",
        "print(df['target'].value_counts().to_string())\n",
        "print(f\"\\nğŸ’¡ This is REAL medical diagnosis data from University of Wisconsin\")\n",
        "print(f\"   - Features are measurements from cell nuclei\")\n",
        "print(f\"   - Target: 0 = Malignant (cancerous), 1 = Benign (non-cancerous)\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "print(f\"\\nâœ… Features and target separated!\")\n",
        "print(f\"   X shape: {X.shape}\")\n",
        "print(f\"   y shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Understanding Why Feature Scaling is Critical | Ø§Ù„Ø®Ø·ÙˆØ© 2: ÙÙ‡Ù… Ù„Ù…Ø§Ø°Ø§ ØªØ­Ø¬ÙŠÙ… Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹\n",
        "\n",
        "**BEFORE**: KNN uses distance to find neighbors. If features have different scales, distance will be wrong!\n",
        "\n",
        "**AFTER**: We'll demonstrate why scaling matters by comparing scaled vs unscaled KNN!\n",
        "\n",
        "**Why this matters**: \n",
        "- **Unscaled**: Features with larger values dominate distance calculations\n",
        "- **Scaled**: All features contribute equally to distance calculations\n",
        "- **Result**: Scaling dramatically improves KNN performance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate why feature scaling is critical for KNN\n",
        "# We'll compare KNN with and without scaling\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Demonstrating Feature Scaling Importance\")\n",
        "print(\"Ø¥Ø¸Ù‡Ø§Ø± Ø£Ù‡Ù…ÙŠØ© ØªØ­Ø¬ÙŠÙ… Ø§Ù„Ù…ÙŠØ²Ø§Øª\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=123, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Data split!\")\n",
        "print(f\"   Training samples: {len(X_train)}\")\n",
        "print(f\"   Test samples: {len(X_test)}\")\n",
        "\n",
        "# Show feature scales BEFORE scaling\n",
        "print(f\"\\nğŸ“Š Feature Scales BEFORE Scaling:\")\n",
        "print(f\"   Mean of first feature: {X_train.iloc[:, 0].mean():.2f}\")\n",
        "print(f\"   Std of first feature: {X_train.iloc[:, 0].std():.2f}\")\n",
        "print(f\"   Mean of last feature: {X_train.iloc[:, -1].mean():.2f}\")\n",
        "print(f\"   Std of last feature: {X_train.iloc[:, -1].std():.2f}\")\n",
        "print(f\"\\nğŸ’¡ Notice: Features have VERY different scales!\")\n",
        "print(f\"   - Some features range from 0-1\")\n",
        "print(f\"   - Others range from 0-1000+\")\n",
        "print(f\"   - This will make distance calculations dominated by large-scale features!\")\n",
        "\n",
        "# Train KNN WITHOUT scaling (BAD!)\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "print(f\"\\nâŒ KNN WITHOUT Scaling:\")\n",
        "print(f\"   Accuracy: {acc_unscaled:.4f} ({acc_unscaled*100:.2f}%)\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nâœ… Features scaled using StandardScaler!\")\n",
        "print(f\"   - Mean of all features: ~0\")\n",
        "print(f\"   - Std of all features: ~1\")\n",
        "print(f\"   - All features now contribute equally to distance!\")\n",
        "\n",
        "# Train KNN WITH scaling (GOOD!)\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"\\nâœ… KNN WITH Scaling:\")\n",
        "print(f\"   Accuracy: {acc_scaled:.4f} ({acc_scaled*100:.2f}%)\")\n",
        "\n",
        "improvement = acc_scaled - acc_unscaled\n",
        "print(f\"\\nğŸ¯ Improvement from Scaling: {improvement:.4f} ({improvement*100:.2f}%)\")\n",
        "print(f\"\\nğŸ’¡ Key Learning Point:\")\n",
        "print(f\"   - Feature scaling is CRITICAL for KNN!\")\n",
        "print(f\"   - Without scaling: {acc_unscaled*100:.1f}% accuracy\")\n",
        "print(f\"   - With scaling: {acc_scaled*100:.1f}% accuracy\")\n",
        "print(f\"   - Always use StandardScaler before KNN!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Choosing the Right K Value | Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ø®ØªÙŠØ§Ø± Ù‚ÙŠÙ…Ø© K Ø§Ù„ØµØ­ÙŠØ­Ø©\n",
        "\n",
        "**BEFORE**: We need to understand how K affects model performance.\n",
        "\n",
        "**AFTER**: We'll test different K values and find the optimal one!\n",
        "\n",
        "**Why K matters**: \n",
        "- **K too small (K=1)**: Overfitting - model is too sensitive to noise\n",
        "- **K too large**: Underfitting - model loses local patterns\n",
        "- **Optimal K**: Balances bias and variance for best generalization\n",
        "\n",
        "**Rule of thumb**: Start with K = âˆšn (square root of number of samples), then tune!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the optimal K value\n",
        "# We'll test different K values and see which gives best performance\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Finding Optimal K Value\")\n",
        "print(\"Ø¥ÙŠØ¬Ø§Ø¯ Ù‚ÙŠÙ…Ø© K Ø§Ù„Ø£Ù…Ø«Ù„\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test different K values\n",
        "k_values = range(1, 31, 2)  # Test odd numbers from 1 to 29\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "print(f\"\\nğŸ” Testing K values from 1 to 29 (odd numbers only)...\")\n",
        "print(f\"   Why odd? Avoids ties in binary classification!\")\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    train_pred = knn.predict(X_train_scaled)\n",
        "    test_pred = knn.predict(X_test_scaled)\n",
        "    \n",
        "    train_acc = accuracy_score(y_train, train_pred)\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "    \n",
        "    train_scores.append(train_acc)\n",
        "    test_scores.append(test_acc)\n",
        "\n",
        "# Find best K\n",
        "best_k_idx = np.argmax(test_scores)\n",
        "best_k = k_values[best_k_idx]\n",
        "best_test_acc = test_scores[best_k_idx]\n",
        "\n",
        "print(f\"\\nâœ… Best K value: {best_k}\")\n",
        "print(f\"   Test Accuracy: {best_test_acc:.4f} ({best_test_acc*100:.2f}%)\")\n",
        "\n",
        "# Visualize K vs Accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(k_values, train_scores, 'o-', label='Training Accuracy', linewidth=2, markersize=8)\n",
        "plt.plot(k_values, test_scores, 's-', label='Test Accuracy', linewidth=2, markersize=8)\n",
        "plt.axvline(x=best_k, color='r', linestyle='--', linewidth=2, label=f'Best K = {best_k}')\n",
        "plt.xlabel('K Value (Number of Neighbors)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "plt.title('KNN: Finding Optimal K Value | KNN: Ø¥ÙŠØ¬Ø§Ø¯ Ù‚ÙŠÙ…Ø© K Ø§Ù„Ø£Ù…Ø«Ù„', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nğŸ’¡ Key Observations:\")\n",
        "print(f\"   - K=1: High training accuracy, lower test accuracy (overfitting)\")\n",
        "print(f\"   - K too large: Lower accuracy on both (underfitting)\")\n",
        "print(f\"   - Optimal K={best_k}: Best balance (test accuracy: {best_test_acc*100:.2f}%)\")\n",
        "print(f\"\\nğŸ“š Learning Points:\")\n",
        "print(f\"   - Small K: Model is too sensitive to noise (overfitting)\")\n",
        "print(f\"   - Large K: Model loses local patterns (underfitting)\")\n",
        "print(f\"   - Optimal K: Balances bias and variance for best generalization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train Final KNN Model and Evaluate | Ø§Ù„Ø®Ø·ÙˆØ© 4: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ KNN Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "\n",
        "**BEFORE**: We found the optimal K value.\n",
        "\n",
        "**AFTER**: We'll train the final model with optimal K and evaluate it comprehensively!\n",
        "\n",
        "**Why comprehensive evaluation matters**: \n",
        "- Accuracy alone doesn't tell the full story\n",
        "- Precision, Recall, F1, and Confusion Matrix give complete picture\n",
        "- ROC Curve shows model's ability to separate classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final KNN model with optimal K and evaluate comprehensively\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training Final KNN Model with Optimal K\")\n",
        "print(\"ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ KNN Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ù‚ÙŠÙ…Ø© K Ø§Ù„Ø£Ù…Ø«Ù„\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train with best K\n",
        "knn_final = KNeighborsClassifier(n_neighbors=best_k)\n",
        "knn_final.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = knn_final.predict(X_train_scaled)\n",
        "y_test_pred = knn_final.predict(X_test_scaled)\n",
        "y_test_proba = knn_final.predict_proba(X_test_scaled)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "test_precision = precision_score(y_test, y_test_pred)\n",
        "test_recall = recall_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred)\n",
        "test_auc = roc_auc_score(y_test, y_test_proba)\n",
        "\n",
        "print(f\"\\nğŸ“Š Final KNN Model Performance (K={best_k}):\")\n",
        "print(f\"   Training Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
        "print(f\"   Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "print(f\"   Test Precision: {test_precision:.4f} ({test_precision*100:.2f}%)\")\n",
        "print(f\"   Test Recall: {test_recall:.4f} ({test_recall*100:.2f}%)\")\n",
        "print(f\"   Test F1-Score: {test_f1:.4f} ({test_f1*100:.2f}%)\")\n",
        "print(f\"   Test AUC: {test_auc:.4f} ({test_auc*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nâœ… KNN Model trained and evaluated successfully!\")\n",
        "print(f\"\\nğŸ’¡ Interpretation:\")\n",
        "print(f\"   - Accuracy: Overall correctness ({test_acc*100:.1f}% correct)\")\n",
        "print(f\"   - Precision: Of predicted positives, {test_precision*100:.1f}% are actually positive\")\n",
        "print(f\"   - Recall: Of actual positives, we caught {test_recall*100:.1f}%\")\n",
        "print(f\"   - F1: Balance between precision and recall ({test_f1*100:.1f}%)\")\n",
        "print(f\"   - AUC: Model's ability to separate classes ({test_auc*100:.1f}%)\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "print(f\"\\nğŸ“Š Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"\\n   True Negatives (TN): {cm[0,0]} - Correctly predicted Malignant\")\n",
        "print(f\"   False Positives (FP): {cm[0,1]} - Predicted Benign but actually Malignant\")\n",
        "print(f\"   False Negatives (FN): {cm[1,0]} - Predicted Malignant but actually Benign\")\n",
        "print(f\"   True Positives (TP): {cm[1,1]} - Correctly predicted Benign\")\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Count'})\n",
        "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Actual Label', fontsize=12, fontweight='bold')\n",
        "plt.title('KNN Confusion Matrix | Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ Ù„Ù€ KNN', fontsize=14, fontweight='bold')\n",
        "plt.xticks([0.5, 1.5], ['Malignant (0)', 'Benign (1)'])\n",
        "plt.yticks([0.5, 1.5], ['Malignant (0)', 'Benign (1)'])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f'KNN (AUC = {test_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.title('KNN ROC Curve | Ù…Ù†Ø­Ù†Ù‰ ROC Ù„Ù€ KNN', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nâœ… Comprehensive evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: When to Use KNN vs Other Algorithms | Ø§Ù„Ø®Ø·ÙˆØ© 5: Ù…ØªÙ‰ Ù†Ø³ØªØ®Ø¯Ù… KNN Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰\n",
        "\n",
        "**BEFORE**: We understand how KNN works.\n",
        "\n",
        "**AFTER**: We'll understand when KNN is the right choice and when to use other algorithms!\n",
        "\n",
        "**Decision Framework:**\n",
        "\n",
        "| Scenario | Best Algorithm | Why |\n",
        "|----------|---------------|-----|\n",
        "| **Small dataset, interpretability important** | Decision Trees | Easy to understand |\n",
        "| **Large dataset, need fast predictions** | Logistic Regression | Fast, efficient |\n",
        "| **Non-linear patterns, need optimal margin** | SVM | Maximum margin boundaries |\n",
        "| **No clear pattern, need simple solution** | KNN | Simple, no assumptions |\n",
        "| **Need feature importance** | Random Forest | Built-in feature importance |\n",
        "| **Lazy learning acceptable** | KNN | No training phase needed |\n",
        "\n",
        "**KNN Advantages:**\n",
        "- âœ… Simple to understand and implement\n",
        "- âœ… No assumptions about data distribution\n",
        "- âœ… Works well for non-linear patterns\n",
        "- âœ… Can be used for both classification and regression\n",
        "\n",
        "**KNN Disadvantages:**\n",
        "- âŒ Slow prediction (must compute distances to all training points)\n",
        "- âŒ Sensitive to irrelevant features\n",
        "- âŒ Requires feature scaling\n",
        "- âŒ Memory intensive (stores all training data)\n",
        "- âŒ Sensitive to K value choice\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary | Ø§Ù„Ù…Ù„Ø®Øµ\n",
        "\n",
        "### What We Learned | Ù…Ø§ ØªØ¹Ù„Ù…Ù†Ø§Ù‡\n",
        "\n",
        "1. **KNN Basics**: KNN finds K nearest neighbors and predicts based on majority vote\n",
        "2. **Feature Scaling**: CRITICAL for KNN - always use StandardScaler!\n",
        "3. **Choosing K**: Balance between overfitting (small K) and underfitting (large K)\n",
        "4. **Lazy Learning**: KNN doesn't train a model - all computation happens during prediction\n",
        "5. **Distance Metrics**: Euclidean distance (default) measures \"nearest\"\n",
        "6. **When to Use**: Simple solution, no assumptions, non-linear patterns\n",
        "\n",
        "### Key Takeaways | Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
        "\n",
        "- âš ï¸ **Always scale features before KNN** - distance calculations depend on feature scales\n",
        "- ğŸ¯ **Choose K carefully** - too small = overfitting, too large = underfitting\n",
        "- ğŸ’¡ **KNN is simple but powerful** - often performs well with proper preprocessing\n",
        "- ğŸ“Š **Evaluate comprehensively** - use accuracy, precision, recall, F1, AUC, confusion matrix\n",
        "- ğŸ” **Understand trade-offs** - KNN is slow for large datasets but simple to understand\n",
        "\n",
        "### Next Steps | Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©\n",
        "\n",
        "- ğŸ““ **Unit 4: Clustering** - K-Means uses similar distance concepts\n",
        "- ğŸ““ **Unit 5: Model Selection** - Hyperparameter tuning for K\n",
        "- ğŸ““ **Advanced Topics** - Weighted KNN, distance metrics, approximate nearest neighbors\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** ğŸ‰ You've completed all classification algorithms in Unit 3!\n",
        "**ØªÙ‡Ø§Ù†ÙŠÙ†Ø§!** ğŸ‰ Ù„Ù‚Ø¯ Ø£ÙƒÙ…Ù„Øª Ø¬Ù…ÙŠØ¹ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØµÙ†ÙŠÙ ÙÙŠ Ø§Ù„ÙˆØ­Ø¯Ø© 3!\n",
        "a"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
